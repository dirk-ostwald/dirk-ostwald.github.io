# Matrizen {#sec-matrizen}
\normalsize

Matrizen sind die Worte der Sprache der modernen Datenanalyse. Ein Verständnis
moderner datenanalytischer Verfahren und ihrer Implementation ist ohne ein
Grundverständnis des Matrixbegriffs und ein Wissen um die grundlegenden 
Matrixoperationen nicht möglich. Matrizen können dabei sehr unterschiedliche 
Rollen spielen. So können Matrizen zum Beispiel Daten, experimentelle Designs
und Modellparameter repräsentieren. Im Kontext der Linearen Algebra dienen 
Matrizen zur Repräsentation linearer Abbildungen und von Vektorräumen,
hier werden Vektoren dann als spezielle Matrizen aufgefasst. 

In diesem Kapitel geben wir eine Einführung zum Umgang mit Matrizen, wobei 
wir auf abstrakte Begrifflichkeiten der Linearen Algebra im Wesentlichen verzichten. 
Wir führen zunächst den Matrixbegriff ein und diskutieren dann mit der Matrixaddition, 
Matrixsubtraktion, Skalarmultiplikation und der Matrixtransposition erste 
grundlegende Matrixoperationen (@sec-matrix-definition und @sec-grundlegende-matrixoperationen). 
Wir führen dann die zentralen Begriffe der Matrixmultiplikation und der Matrixinversion ein
(@sec-matrixmultiplikation und @sec-matrixinversion). Mit der Matrixdeterminante 
diskutieren wir dann in @sec-determinanten eine erste Maßzahl zur Beschreibung 
von Matrizen. Wir schließen in @sec-spezielle-matrizen  mit einer Übersicht zu 
besonders häufig auftretenden Matrizen.

## Definition {#sec-matrix-definition}
Wir beginnen mit der Definition einer Matrix.

:::{#def-matrix}
Eine Matrix ist eine rechteckige Anordnung von Zahlen, die wie folgt bezeichnet
wird
\begin{equation}
A := \begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1m} \\
a_{21} & a_{22} & \cdots & a_{2m} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n1} & a_{n2} & \cdots & a_{nm}
\end{pmatrix}
:= {(a_{ij})}_{1\le i\le n,\, 1\le j\le m}.
\end{equation}
:::

 Matrizen bestehen aus *Zeilen (rows)* und *Spalten (columns)*. Die Matrixeinträge 
$a_{ij}$ werden mit einem *Zeilenindex* $i$ und einem *Spaltenindex* $j$ indiziert.
Zum Beispiel gilt für
\begin{equation}
A:=\begin{pmatrix}
2 & 7 & 5 & 2 \\
8 & 2 & 5 & 6 \\
6 & 4 & 0 & 9 \\
9 & 2 & 1 & 2
\end{pmatrix},
\end{equation}
dass $a_{32} = 4$. Die *Größe* oder *Dimension* einer Matrix ergibt sich aus
der Anzahl ihrer Zeilen $n \in \mathbb{N}$ und Spalten $m \in \mathbb{N}$. Matrizen 
mit $n = m$ heißen *quadratische Matrizen*.

In der Folge benötigen wir nur Matrizen mit reellen Einträgen, also $a_{ij} \in \mathbb{R}$
für alle  $i = 1,...,n$ und  $j = 1,...,m$. Wir nennen die Matrizen mit reellen 
Einträge *reelle Matrizen* und bezeichnen die Menge der reellen Matrizen mit 
$n$ Zeilen und $m$ Spalten mit $\mathbb{R}^{n \times m}$. An dem Ausdruck 
\begin{equation}
A \in \mathbb{R}^{n\times m}
\end{equation}
können wir also ablesen, dass $A$ eine reelle Matrix mit $n$ Zeilen und $m$ Spalten ist. 
Wir identifizieren dabei die Menge $\mathbb{R}^{1 \times 1}$ mit der Menge $\mathbb{R}$,
die Menge $\mathbb{R}^{n \times 1}$ mit der Menge $\mathbb{R}^n$. Reelle Matrizen
mit einer Spalte und $n$ Zeilen entsprechen also $n$-dimensionalen reellen Vektoren
und reelle Matrizen mit einer Spalte und einer Zeile entsprechen reellen Zahlen.

**Definition von Matrizen in R**

In **R** werden Matrizen definiert, indem **R** Vektoren mithilfe der `matrix()` Funktion
in die Repräsentation einer mathematischen Matrix transformiert werden. Die 
Einträge eines **R** Vektors werden dabei anhand der spezifizierten Zeilenanzahl
`nrow` anhand ihrer Gesamtanzahl auf die Matrix verteilt. Wollen wir beispielsweise
die Matrix 
\begin{equation}
A := 
\begin{pmatrix}
2 & 3 & 0 \\ 
1 & 6 & 5
\end{pmatrix}
\end{equation}
in **R** definieren, so ergibt sich

\footnotesize
```{r}
# Spaltenweise Definition von A (R default)
A = matrix(c(2,1,3,6,0,5), nrow = 2)
print(A)
```
\normalsize

**R** folgt hier per default einer sogenannten *column-major-order*, das heißt, die Elemente des 
**R** Vektors `c(2,1,3,6,0,5)` werden der Reihe nach von oben nach unten in die Spalten
der Matrix von links nach rechts überführt. Einen etwas klareren Zusammenhang
zwischen dem visuellen Layout des **R** Codes und der resultierenden Matrix erhält
man, indem man den **R** Vektor mithilfe von Zeilenumbrüchen anhand des intendierten
Matrixlayouts formatiert und dann die column-major-order mithilfe des Arguments
`byrow = TRUE` zu einer *row-major-order* umstellt. Es wird dann zunächst 
die erste Zeile der Matrix von  links nach rechts mit den Elementen des **R** Vektors 
gefüllt wird und dann die zweite Zeile usw. bis alle Elemente des Vektors auf
die Matrix verteilt sind. 

\footnotesize
```{r}
# Reihenweise Definition von A (R default)
A = matrix(c(2,3,0,
             1,6,5),
             nrow = 2,
             byrow = TRUE)
print(A)
```

```{r}
# Zeilenweise Definition von B
B = matrix(c(4,1,0,
            -4,2,0), 
            nrow = 2, 
            byrow = TRUE)
print(B)
```

\normalsize

## Grundlegende Matrixoperationen {#sec-grundlegende-matrixoperationen}

Man kann mit Matrizen rechnen. Dabei sind folgende Matrixoperationen grundlegend:

* Die Addition von Matrizen gleicher Größe, genannt *Matrixaddition* 
* Die Subtraktion von Matrizen gleicher Größe, genannt *Matrixsubtraktion*
* Die Multiplikation einer Matrix mit einem Skalar, genannt *Skalarmultiplikation*
* Das Vertauschen der Zeilen- und Spalten einer Matrix, genannt *Matrixtransposition*.

Wir führen diese Operationen in der Folge in Operatorform, also als Funktionen
ein. Dies dient insbesondere dazu, bei jeder Operation mit Hilfe ihrer Definitionsmenge
zu betonen, von welcher Art die Objekte der jeweiligen Operation sind und
mithilfe ihrer Bildmenge zu betonen, von welcher Art das Resultat
der jeweiligen Operation ist. 

### Matrixaddition
:::{#def-Matrixaddition}
Es seien $A,B\in \mathbb{R}^{n\times m}$. Dann ist  die *Addition* von $A$
und $B$ definiert als die Abbildung
\begin{equation}
+ : \mathbb{R}^{n\times m} \times \mathbb{R}^{n\times m} \to \mathbb{R}^{n \times m}, \,
(A,B) \mapsto +(A,B) := A + B
\end{equation}
mit
\begin{align}
\begin{split}
A + B
& =
\begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1m} \\
a_{21} & a_{22} & \cdots & a_{2m} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n1} & a_{n2} & \cdots & a_{nm}
\end{pmatrix}
+
\begin{pmatrix}
b_{11} & b_{12} & \cdots & b_{1m} \\
b_{21} & b_{22} & \cdots & b_{2m} \\
\vdots & \vdots & \ddots & \vdots \\
b_{n1} & b_{n2} & \cdots & b_{nm}
\end{pmatrix}
\\
&
:=
\begin{pmatrix}
a_{11} + b_{11} & a_{12} + b_{12} & \cdots & a_{1m} + b_{1m} \\
a_{21} + b_{21} & a_{22} + b_{22} & \cdots & a_{2m} + b_{2m} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n1} + b_{n1} & a_{n2} + b_{n2} & \cdots & a_{nm} + b_{nm}
\end{pmatrix}.
\end{split}
\end{align}
:::

Die Definition der Matrixaddition legt insbesondere fest, dass nur Matrizen 
gleicher Größe addiert werden können und dass die Operation der Matrixaddition 
elementweise definiert ist.

**Beispiel**

Es seien $A,B\in \mathbb{R}^{2\times 3}$ definiert als
\begin{equation}
A:=\begin{pmatrix}
2 & -3 & 0\\
1 &  6 & 5\\
\end{pmatrix}
\mbox{ und }
B := \begin{pmatrix}
 4 & 1 & 0\\
-4 & 2 & 0\\
\end{pmatrix}.
\end{equation}
Da $A$ und $B$ gleich groß sind, können wir sie addieren
\begin{align}
\begin{split}
C
= A+B
& =
\begin{pmatrix}
2 & -3 & 0\\
1 &  6 & 5\\
\end{pmatrix}
+
\begin{pmatrix}
 4 & 1 & 0\\
-4 & 2 & 0\\
\end{pmatrix}\\
& =
\begin{pmatrix}
2 + 4 & -3 + 1 & 0 + 0\\
1 - 4 &  6 + 2 & 5 + 0\\
\end{pmatrix}\\
& =
\begin{pmatrix}
6 & -2 & 0\\
-3 &  8 & 5 \\
\end{pmatrix}.
\end{split}
\end{align}

In **R** führt man obige Rechnung  wie folgt aus.

\footnotesize
```{r}
# Definition
A = matrix(c(2, -3, 0,
             1,  6, 5),
             nrow  = 2,
             byrow = TRUE)
B = matrix(c( 4, 1, 0,
             -4, 2, 0),
              nrow = 2,
             byrow = TRUE)

# Addition
C = A + B
print(C)
```
\normalsize

### Matrixsubtraktion
Die Subtraktion von Matrizen gleicher Größe ist analog zur Addition definiert.

:::{#def-matrixsubtraktion}
## Matrixsubtraktion
Es seien $A,B\in \mathbb{R}^{n\times m}$. Dann ist  die *Subtraktion* von $A$
und $B$ definiert als die Abbildung
\begin{equation}
- : \mathbb{R}^{n\times m} \times \mathbb{R}^{n\times m} \to \mathbb{R}^{n\times m}, \,
(A,B) \mapsto -(A,B) := A - B
\end{equation}
mit
\begin{align}
\begin{split}
A - B
& =
\begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1m} \\
a_{21} & a_{22} & \cdots & a_{2m} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n1} & a_{n2} & \cdots & a_{nm}
\end{pmatrix}
-
\begin{pmatrix}
b_{11} & b_{12} & \cdots & b_{1m} \\
b_{21} & b_{22} & \cdots & b_{2m} \\
\vdots & \vdots & \ddots & \vdots \\
b_{n1} & b_{n2} & \cdots & b_{nm}
\end{pmatrix}
\\
&
:=
\begin{pmatrix}
a_{11} - b_{11} & a_{12} - b_{12} & \cdots & a_{1m} - b_{1m} \\
a_{21} - b_{21} & a_{22} - b_{22} & \cdots & a_{2m} - b_{2m} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n1} - b_{n1} & a_{n2} - b_{n2} & \cdots & a_{nm} - b_{nm}
\end{pmatrix}.
\end{split}
\end{align}
:::

Wie bei der Matrixaddition legt die Definition der Matrixsubtraktion fest, 
dass nur Matrizen gleicher Größe voneinander subtrahiert werden können und dass
die Subktration zweier gleich großer Matrizen elementweise definiert ist.

**Beispiel**

Wir können die im Beispiel zur Matrixaddition definierten Matrizen $A$ und $B$ 
auch voneinander subtrahieren,
\begin{align}
\begin{split}
D
= A-B
& =
\begin{pmatrix}
2 & -3 & 0\\
1 &  6 & 5\\
\end{pmatrix}
-
\begin{pmatrix}
 4 & 1 & 0\\
-4 & 2 & 0\\
\end{pmatrix}\\
& =
\begin{pmatrix}
2 - 4 & -3 - 1 & 0 - 0\\
1 + 4 &  6 - 2 & 5 - 0\\
\end{pmatrix}\\
& =
\begin{pmatrix}
-2 & -4 & 0\\
5 &  4 & 5 \\
\end{pmatrix}.
\end{split}
\end{align}

In **R** führt man diese Rechnung wie folgt aus.

\footnotesize
```{r}
# Subtraktion
D = A - B
print(D)
```
\normalsize

### Skalarmultiplikation

Die *Skalarmultiplikation* einer Matrix bezeichnet die Multiplikation eines
Skalars mit einer Matrix.

:::{#def-skalarmultiplikation}
## Skalarmultiplikation
Es sei $c \in \mathbb{R}$ ein Skalar und $A \in \mathbb{R}^{n\times m}$. Dann
ist die *Skalarmultiplikation* von $c$ und $A$ definiert als die Abbildung
\begin{equation}
\cdot : \mathbb{R} \times \mathbb{R}^{n\times m} \to \mathbb{R}^{n\times m}, \,
(c,A) \mapsto \cdot (c,A) := cA
\end{equation}
mit
\begin{align}
\begin{split}
cA
=
c
\begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1m} \\
a_{21} & a_{22} & \cdots & a_{2m} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n1} & a_{n2} & \cdots & a_{nm}
\end{pmatrix}
:=
\begin{pmatrix}
ca_{11} & ca_{12} & \cdots & ca_{1m}  \\
ca_{21} & ca_{22} & \cdots & ca_{2m}  \\
\vdots  & \vdots  & \ddots & \vdots    \\
ca_{n1} & ca_{n2} & \cdots & ca_{nm}
\end{pmatrix}.
\end{split}
\end{align}
:::

Die Skalarmultiplikation ist mit dieser Definition also elementweise definiert.

**Beispiel**

Es seien $c:=-3$ und $A\in \mathbb{R}^{4\times 3}$ definiert als

\begin{equation}
A := \begin{pmatrix}
3 & 1 & 1\\
5 & 2 & 5\\
2 & 7 & 1\\
3 & 4 & 2
\end{pmatrix}.
\end{equation}
Dann ergibt  sich
\begin{align}
\begin{split}
B :=
cA
= -3\begin{pmatrix}
3 & 1 & 1\\
5 & 2 & 5\\
2 & 7 & 1\\
3 & 4 & 2
\end{pmatrix}
= \begin{pmatrix}
-3\cdot3 & -3\cdot1 & -3\cdot1\\
-3\cdot5 & -3\cdot2 & -3\cdot5\\
-3\cdot2 & -3\cdot7 & -3\cdot1\\
-3\cdot3 & -3\cdot4 & -3\cdot2
\end{pmatrix}
= \begin{pmatrix}
-9  &  -3 & -3  \\
-15 &  -6 & -15 \\
-6  & -21 & -3  \\
-9  & -12 & -6
\end{pmatrix}.
\end{split}
\end{align}

In **R** führt man diese Skalarmultiplikation aus wie folgt.
\footnotesize
```{r}
# Definitionen
A = matrix(c(3,1,1,
             5,2,5,
             2,7,1,
             3,4,2),
           nrow = 4,
           byrow = TRUE)
c = -3

# Skalarmultiplikation
B = c*A
print(B)
```
\normalsize

Mithilfe der Definition von Matrixaddition und Skalarmultiplikation ist es 
möglich, einen Vektorraum zu definieren, dessen Elemente die reellen Matrizen 
sind. Insbesondere legt diese Definition auch die Rechenregeln beim Umgang
mit Matrixaddition und Skalarmultiplikation fest.

:::{#thm-vektorraum-der-reellwertigen-Matrizen}
## Vektorraum der reellwertigen Matrizen
Das Tripel $(\mathbb{R}^{n \times m}, +, \cdot)$ mit der oben definierten
Matrixaddition und Skalarmultiplikation ist ein Vektorraum. Insbesondere gelten
damit für $A,B,C\in \mathbb{R}^{n \times m}$ und $r,s,t\in \mathbb{R}$ folgende
Rechenregeln:

(1) Kommutativität der Addition: $A + B = B + A$.
(2) Assoziativität der Addition: $(A + B) + C = A + (B + C)$.
(3) Existenz eines neutralen Elements der Addition: $\exists\, 0 \in \mathbb{R}^{n \times m}$ mit $A + 0 = 0 + A = A$.
(4) Existenz inverser Elemente der Addition: $\forall A\,\exists -A \in \mathbb{R}^{n \times m}$ mit  $A + (-A) = 0$.
(5) Existenz eines neutralen Elements der Skalarmultiplikation: $\exists\, 1 \in \mathbb{R}$ mit $1 \cdot A = A$.
(6) Assoziativität der Skalarmultiplikation: $r \cdot (s \cdot t) = (r \cdot s)\cdot t$.
(7) Distributivität hinsichtlich der Matrixaddition:  $r\cdot (A + B) = r\cdot A + r\cdot B$.
(8) Distributivität hinsichtlich der Skalaraddition: $(r + s)\cdot A = r\cdot A + s\cdot A$.
:::

Wir verzichten auf einen Beweis, der sich mit einigem Notationsaufwand direkt
aus dem elementweisen Charakter von Matrixaddition und Skalarmultiplikation 
sowie den aus dem Umgang mit den reellen Zahlen bekannten Rechenregeln ergibt. 
Das im Theorem erwähnte neutrale Element der Addition wird *Nullmatrix* genannt, 
wir werden dazu später eine allgemeine Notation einführen. Die inversen Elemente 
der Addition sind durch 
\begin{equation}
-A := (-a_{ij})_{1\le i \le n, 1 \le j \le m}
\end{equation}
gegeben und erlauben es, die Matrixsubtraktion als Spezialfall der Matrixaddition zu betrachten. 

### Matrixtransposition

Eine weitere häufig auftretende grundlegende Matrixoperation ist das 
Vertauschen der Zeilen- und Spaltenanordnung einer Matrix, genannt *Matrixtransposition*.

:::{#def-matrixtransposition}
## Matrixtransposition
Es sei $A \in \mathbb{R}^{n\times m}$. Dann ist  die *Transposition*
von $A$ definiert als die Abbildung
\begin{equation}
\cdot^{T} : \mathbb{R}^{n\times m} \to \mathbb{R}^{m \times n}, \,
A \mapsto \cdot^{T}(A) := A^T
\end{equation}
mit
\begin{align}
\begin{split}
A^T
=
\begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1m} \\
a_{21} & a_{22} & \cdots & a_{2m} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n1} & a_{n2} & \cdots & a_{nm}
\end{pmatrix}^T
:=
\begin{pmatrix}
a_{11} & a_{21} & \cdots & a_{n1} \\
a_{12} & a_{22} & \cdots & a_{n2} \\
\vdots & \vdots & \ddots & \vdots \\
a_{1m} & a_{2m} & \cdots & a_{nm}
\end{pmatrix}.
\end{split}
\end{align}
:::

Für $A \in \mathbb{R}^{n \times m}$ gilt damit also immer $A^T \in \mathbb{R}^{m \times n}$.
Weiterhin gelten folgende Rechenregeln der Matrixtransposition, wie man sich 
an Beispielen klar macht: 

(1) Für $A \in \mathbb{R}^{1 \times 1}$ gilt 
\begin{equation}
A^T = A.
\end{equation}
(2) Es gilt 
\begin{equation}
\left(A^T\right)^T = A.
\end{equation}
(3) Es gilt 
\begin{equation}
\left(a_{ii}\right)_{1 \le i \le \mbox{min}(n,m)} = \left(a_{ii}\right)^T_{1 \le i \le \mbox{min}(n,m)}.
\end{equation}

Letztere Eigenschaft der Transposition besagt, dass die Elemente auf der Hauptdiagonalen 
einer Matrix bei Transposition unberührt bleiben.

**Beispiel**

Es sei $A \in \mathbb{R}^{2 \times 3}$ definiert durch
\begin{equation}
A:=\begin{pmatrix}
2 & 3 & 0 \\
1 & 6 & 5 \\
\end{pmatrix},
\end{equation}
Dann gilt $A^T \in \mathbb{R}^{3 \times 2}$ und speziell
\begin{equation}
A^{T} :=
\begin{pmatrix}
2  & 1 \\
3  & 6 \\
0  & 5 \\
\end{pmatrix}.
\end{equation}
Weiterhin gilt offenbar $\min(m,n) = 2$ und folglich
\begin{equation}
(a_{11}) = \left(a_{11}\right)^T
\mbox{ und }
(a_{22}) = \left(a_{22}\right)^T.
\end{equation}
In **R** führt man die Transposition einer Matrix wie folgt durch.

\footnotesize
```{r}
# Definition
A = matrix(c(2,3,0,
             1,6,5),
           nrow = 2,
           byrow = TRUE)
print(A)

# Transposition
AT = t(A)
print(AT)
```
\normalsize

Schließlich gelten in der Verbindung mit der Matrixaddition, Matrixsubtraktion 
und der Skalarmultiplikation folgende Rechenregeln, wie man sich an Beispielen 
klar macht:

(1) Für $A,B\in \mathbb{R}^{n \times m}$ gilt
\begin{equation}
(A+B)^T = A^T + B^T.
\end{equation}
(2) Für $A,B\in \mathbb{R}^{n \times m}$ gilt 
\begin{equation}
(A-B)^T = A^T - B^T.
\end{equation}
(3) Für $c\in \mathbb{R}$ und $A \in \mathbb{R}^{n \times m}$ gilt 
\begin{equation}
(cA)^T = cA^T.
\end{equation}

## Matrixmultiplikation {#sec-matrixmultiplikation}

Die Matrixmultiplikation ist die zentrale Operation beim Rechnen mit Matrizen.
Sie ist definiert wie folgt.

:::{#def-matrixmultiplikation}
## Matrixmultiplikation
Es seien $A\in \mathbb{R}^{n \times m}$ und $B \in \mathbb{R}^{m \times k}$. Dann
ist  die *Matrixmultiplikation* von $A$ und $B$ definiert als die Abbildung
\begin{equation}
\cdot : \mathbb{R}^{n\times m} \times \mathbb{R}^{m\times k} \to \mathbb{R}^{n \times k}, \,
(A,B) \mapsto \cdot(A,B) := AB
\end{equation}
mit
\begin{align}
\begin{split}
AB
& =
\begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1m} \\
a_{21} & a_{22} & \cdots & a_{2m} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n1} & a_{n2} & \cdots & a_{nm}
\end{pmatrix}
\begin{pmatrix}
b_{11} & b_{12} & \cdots & b_{1k} \\
b_{21} & b_{22} & \cdots & b_{2k} \\
\vdots & \vdots & \ddots & \vdots \\
b_{m1} & b_{m2} & \cdots & b_{mk}
\end{pmatrix}
\\
&
:=
\begin{pmatrix}
\sum_{i=1}^m a_{1i}b_{i1} & \sum_{i=1}^m a_{1i}b_{i2} & \cdots & \sum_{i=1}^m a_{1i}b_{ik}  \\
\sum_{i=1}^m a_{2i}b_{i1} & \sum_{i=1}^m a_{2i}b_{i2} & \cdots & \sum_{i=1}^m a_{2i}b_{ik}  \\
\vdots                    & \vdots                    & \ddots & \vdots                     \\
\sum_{i=1}^m a_{ni}b_{i1} & \sum_{i=1}^m a_{ni}b_{i2} & \cdots & \sum_{i=1}^m a_{ni}b_{ik}
\end{pmatrix}
\\
&
= \left(\sum_{i=1}^m a_{ji}b_{il} \right)_{1 \le j \le n, 1 \le l \le k}
\end{split}
\end{align}
:::
Das Matrixprodukt $AB$ ist  also nur dann definiert, wenn $A$ genau so viele 
Spalten hat wie $B$ Zeilen hat. Informell gilt für die beteiligten Matrixgrößen
dabei die Merkregel
\begin{equation}
(n \times m)(m \times k) = (n \times k).
\end{equation}
Der Eintrag $(AB)_{ij}$ in $AB$ entspricht der Summe der multiplizierten $i$ten 
Zeile von $A$ und $j$ten Spalte von $B$. Zum Berechnen von $(AB)_{ij}$ geht man 
für $i = 1,...,n$ und $j = 1,...,k$ also in Gedanken wie folgt vor:

(1) Man legt die Tranposition der $i$ten Zeile von $A$ über die $j$te Spalte von $B$.
(2) Weil $A$ genau $m$ Spalten hat und $B$ genau $m$ Zeilen hat, gibt es dann zu jedem 
Element der Zeile aus $A$ ein korrespondierendes Element in der Spalte von $B$.
(3) Man multipliziert die korrespondierenden Elemente miteinander.
(4) Die Summe dieser Produkte ist dann der Eintrag mit Index $ij$ in $AB$.

**Beispiel**

$A\in \mathbb{R}^{2\times 3}$ und $B\in \mathbb{R}^{3\times 2}$ seien definiert als
\begin{equation}
A := \begin{pmatrix}
2 & -3 &  0   \\
1 &  6 &  5
\end{pmatrix}
\mbox{ und }
B := \begin{pmatrix}
 4 & 2  \\
-1 & 0  \\
 1 & 3
\end{pmatrix}.
\end{equation}
Wir wollen $C := AB$ und $D := BA$ berechnen. Mit $n = 2, m = 3$ und $k = 2$ 
wissen wir schon, dass $C \in \mathbb{R}^{2 \times 2}$ und $D \in \mathbb{R}^{3 \times 3}$, 
weil
\begin{equation}
(2 \times 3)(3 \times 2) = (2 \times 2)
\end{equation}
und
\begin{equation}
(3 \times 2)(2 \times 3) = (3 \times 3).
\end{equation}
Es gilt hier also sicher $AB \neq BA$. Für $C$ ergibt sich dann
\begin{align}
\begin{split}
C
& = AB
\\
& = \begin{pmatrix}
2 & -3 & 0 \\
1 &  6 & 5 \\
\end{pmatrix}
\begin{pmatrix}
4  & 2 \\
-1 & 0 \\
1  & 3
\end{pmatrix}
\\
& =
\begin{pmatrix}
2\cdot 4 + (-3)\cdot (-1) + 0\cdot 1 & 2\cdot 2 + (-3)\cdot 0 + 0\cdot 3 \\
1\cdot 4 +    6\cdot (-1) + 5\cdot 1 & 1\cdot 2 +  6\cdot 0 + 5\cdot 3 \\
\end{pmatrix}
\\
& =
\begin{pmatrix}
8 + 3 + 0 & 4 + 0 + 0 \\
4 - 6 + 5 & 2 + 0 + 15 \\
\end{pmatrix}
\\
& =
\begin{pmatrix}
11 & 4 \\
3 & 17 \\
\end{pmatrix}.
\end{split}
\end{align}

In **R** nutzt man für die Matrixmultiplikation den `%*%` Operator.

\footnotesize
```{r}
# Definitionen
A = matrix(c(2,-3,0,
             1, 6,5),
           nrow  = 2,
           byrow = TRUE)
B = matrix(c( 4,2,
             -1,0,
              1,3),
           nrow  = 3,
           byrow = TRUE)

# Matrixmultiplikation
C = A %*% B
print(C)
```
\normalsize

Für $D$ ergibt sich weiterhin
\begin{align}
\begin{split}
D
& = BA
\\
& =
\begin{pmatrix}
4  & 2 \\
-1 & 0 \\
1  & 3
\end{pmatrix}
\begin{pmatrix}
2 & -3 & 0 \\
1 &  6 & 5 \\
\end{pmatrix}
\\
& =
\begin{pmatrix}
  4    \cdot   2  + 2 \cdot 1
& 4    \cdot (-3) + 2 \cdot 6
& 4    \cdot   0  + 2 \cdot 5
\\
  (-1) \cdot  2  + 0 \cdot 1
& (-1) \cdot(-3) + 0 \cdot 6
& (-1) \cdot  0  + 0 \cdot 5
\\
  1    \cdot  2  + 3 \cdot 1
& 1    \cdot(-3) + 3 \cdot 6
& 1    \cdot  0  + 3 \cdot 5
\end{pmatrix}
\\
& =
\begin{pmatrix}
    8 + 2
& -12 + 12
&   0 + 5
\\
   -2 + 0
&   3 + 0
&   0 + 0
\\
    2 + 3
&  -3 + 18
&   0 + 15
\end{pmatrix}
\\
& =
\begin{pmatrix}
  10
&  0
& 10
\\
  -2
&  3
&  0
\\
   5
& 15
& 15
\\
\end{pmatrix}
\end{split}
\end{align}

In **R** überprüft man diese Rechnung wie folgt.

\footnotesize
```{r}
# Definitionen
A = matrix(c(2,-3,0,
             1, 6,5),
           nrow  = 2,
           byrow = TRUE)
B = matrix(c( 4,2,
             -1,0,
              1,3),
           nrow  = 3,
           byrow = TRUE)

# Matrixmultiplikation
D = B %*% A
print(D)
```
\normalsize

Ist allerdings eine Matrixmultiplikation aufgrund nicht-adäquater Matrizengrößen
nicht definiert, so lässt sich diese auch nicht numerisch auswerten.

\footnotesize
```{r, error = TRUE}
# Beispiel für eine undefinierte Matrixmultipliation
E = t(A) %*% B      # (3 x 2)(3 x 2)
```
\normalsize

Folgendes Theorem, das wir nicht beweisen wollen, stellt den Bezug zwischen
dem Skalarprodukt zweier Vektoren und der Multiplikation zweier Matrizen her.
Dieser ergibt sich im Wesentlichen durch die Identifikation von  $\mathbb{R}^{n}$
und $\mathbb{R}^{n \times 1}$ und der Tatsache, dass nach Definition der 
Eintrag $(AB)_{ij}$ im Produkt von  $A \in \mathbb{R}^{n \times m}$ und 
$B \in \mathbb{R}^{m \times k}$ dem Vektorskalarprodukt der $i$ten Spalte von 
$A^T$  und der $j$ten Spalte von $B$ entspricht.

:::{#thm-matrixmultiplikation-skalarprodukt}
## Matrixmultiplikation und Vektorskalarprodukt
Es seien $x,y \in \mathbb{R}^n$. Dann gilt
\begin{equation}
\langle x,y \rangle = x^Ty.
\end{equation}
Weiterhin seien für $A \in \mathbb{R}^{n\times m}$ für $i = 1,...,n$
\begin{equation}
\bar{a}_i := (a_{ji})_{1 \le j \le m} \in \mathbb{R}^m
\end{equation}
die Spalten von $A^T$ und für $B \in \mathbb{R}^{m \times k}$ für $i = 1,...,k$
\begin{equation}
\bar{b}_j := (b_{ij})_{1 \le j \le m} \in \mathbb{R}^m
\end{equation}
die Spalten von $B$, also
\begin{equation}
A^T =
\begin{pmatrix}
\bar{a}_1 & \bar{a}_2 & \cdots & \bar{a}_n
\end{pmatrix}
\in \mathbb{R}^{m \times n}
\mbox{ und }
B =
\begin{pmatrix}
\bar{b}_1 & \bar{b}_2 & \cdots & \bar{b}_k
\end{pmatrix}
\in \mathbb{R}^{m \times k}.
\end{equation}
Dann gilt
\begin{equation}
AB = \left(\langle \bar{a}_i,\bar{b}_j \rangle \right)_{1 \le i \le n, 1 \le j \le k}.
\end{equation}
:::

### Rechenregeln der Matrixmultiplikation

Im Folgenden stellen wir einige grundlegende Rechenregeln der Matrixmultiplikation,
insbesondere auch in Kombination mit anderen Matrixoperationen zusammen. 

Für Beweise der folgenden zwei Theoreme zur Assoziativität und Distributivität, 
die sich im Wesentlichen mit den entsprechenden Rechenregeln für Summen und 
Produkte der reellen Zahlen ergeben, verweisen wir auf die weiterführende Literatur.

:::{#thm-assoziativität}
## Assoziativität
Es seien $A \in \mathbb{R}^{n \times m}$, $B \in \mathbb{R}^{m \times k}$, 
$C \in \mathbb{R}^{k \times p}$ und $c \in \mathbb{R}$. Dann gelten

(1) Die Multiplikation von Matrizen ist assoziativ, es gilt
\begin{equation}
A(BC) = (AB)C.
\end{equation}
(2) Die Kombination von Matrizenmultiplikation und Skalarmultiplikation ist assoziativ, 
\begin{equation}
c(AB) = (cA)B = A(cB).
\end{equation}
:::

Die Assoziativität von Matrizenmultiplikation und Skalarmultiplikation erkennt
man leicht bei Betrachtung des $j,l$ten Elements von $c(AB), (cA)B$ und $A(cB)$
anhand von 
\begin{equation}
c\left(\sum_{i = 1}^m a_{ji}b_{il}\right) 
= \sum_{i = 1}^m \left(c a_{ji}\right) b_{il}  
= \sum_{i = 1}^m a_{ji}\left(c b_{il}\right). 
\end{equation}

:::{#thm-distributivität}
## Distributivität
Es seien $A \in \mathbb{R}^{n \times m}$, $B \in \mathbb{R}^{n \times m}$, 
$C \in \mathbb{R}^{m \times p}$. Dann gelten 
\begin{equation}
(A + B)C = AC + BC
\end{equation}
und
\begin{equation}
C^T(A + B) = C^TA + C^TB
\end{equation}
:::

Im Gegensatz zur Kommutativität der Multiplikation reeller Zahlen ist die
Matrixmultiplikation im Allgemeinen nicht kommutativ. 

:::{#thm-nichtkommutativität}
## Nichtkommutativität
Es seien $A \in \mathbb{R}^{n \times m}$ und $B \in \mathbb{R}^{m \times p}$.
Dann gilt im Allgemeinen 
\begin{equation}
AB \neq BA.
\end{equation}
:::
:::{.proof}
Im Fall $p \neq n$ ist $BA$ nicht definiert, wir betrachten also nur den Fall $p = n$.
Wir zeigen durch Angabe eines Gegenbeispiels mit $A,B\in \mathbb{R}^{2 \times n}$, 
dass im Allgemeinen $AB = BA$ *nicht* gilt. 
Es seien
\begin{equation}
A := \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix}
\mbox{ und }
B := \begin{pmatrix} 0 & 0 \\ 1 & 0 \end{pmatrix}.
\end{equation}
Dann gilt
\begin{equation}
AB 
=
\begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix}
\begin{pmatrix} 0 & 0 \\ 1 & 0 \end{pmatrix}
=
\begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix}
\neq 
\begin{pmatrix} 0 & 0 \\ 0 & 1 \end{pmatrix}
=
\begin{pmatrix} 0 & 0 \\ 1 & 0 \end{pmatrix}
\begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix}
= 
BA.
\end{equation}
:::

:::{#thm-matrixmultiplikation-transposition}
## Kombination von Matrixmultiplikation und Transposition
Es seien $A \in \mathbb{R}^{m \times n}$ und $B \in \mathbb{R}^{n \times k}$. Dann gilt
\begin{equation}
(AB)^T = B^TA^T.
\end{equation}
:::

:::{.proof}
Ein Beweis ergibt sich wie folgt
\begin{align}
\begin{split}
(AB)^T
& = \left(\left(\sum_{i=1}^m a_{ji}b_{il} \right)_{1 \le j \le n, 1 \le l \le k}\right)^T \\
& = \left(\sum_{i=1}^m a_{ij}b_{li} \right)_{1 \le i \le k, 1 \le j \le n}  \\
& = \left(\sum_{i=1}^m b_{li}a_{ij} \right)_{1 \le j \le k, 1 \le l \le n}  \\
& = B^TA^T.
\end{split}
\end{align}
:::


## Matrixinversion {#sec-matrixinversion}

Um den Begriff der inversen Matrix zu motivieren, betrachten
wir zunächst das Problem des *Lösens eines linearen Gleichungssystems*. Dazu
seien $A\in \mathbb{R}^{n \times n},\, x \in \mathbb{R}^n$ und $b \in \mathbb{R}^n$
und es gelte
\begin{equation}
Ax = b.
\end{equation}
$A$ und $b$ seien als bekannt vorausgesetzt, $x$ sei unbekannt. Konkret seien
beispielsweise
\begin{equation}
A := \begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix} \mbox{ und }b := \begin{pmatrix}  5 \\ 11 \end{pmatrix}.
\end{equation}

Dann liegt folgendes lineares Gleichungssystem mit zwei Gleichungen und zwei Unbekannten vor:
\begin{equation}
Ax = b
\Leftrightarrow
\begin{pmatrix}
1 & 2 \\
3 & 4
\end{pmatrix}
\begin{pmatrix}
x_1 \\
x_2
\end{pmatrix}
= \begin{pmatrix}
5 \\
11
\end{pmatrix}
\Leftrightarrow
\begin{matrix}
1x_1 + 2x_2 & = 5 \\
3x_1 + 4x_2 & = 11
\end{matrix}.
\end{equation}

Ziel des Lösens von linearen Gleichungssystemen ist bekanntlich, herauszufinden,
für welche $x$ das Gleichungssystem erfüllt ist. Um in diesem Kontext den
Begriff der inversen Matrix von $A$ einzuführen, vereinfachen wir die Situation weiter.
Wir nehmen an, dass $A = a$ eine $1 \times 1$ Matrix, also ein Skalar, sei und 
ebenso $x$ und $b$, dass wir also für $a,x,b \in \mathbb{R}$ die Gleichung
\begin{equation}
ax = b
\end{equation}
haben. Um diese Gleichung nach $x$ aufzulösen würde man natürlich beide Seiten
der Gleichung mit dem *multiplikativem Inversen* von $a$ multiplizieren, wobei 
das *multiplikative Inverse* von $a$ den Wert bezeichnet, der mit $a$ multipliziert 
$1$ ergibt. Dieser ist bekanntlich durch
\begin{equation}
a^{-1} = \frac{1}{a}
\end{equation}
gegeben. Dann würde gelten
\begin{equation}
ax = b \Leftrightarrow a^{-1}ax = a^{-1}b \Leftrightarrow 1 \cdot x = a^{-1}b \Leftrightarrow x = \frac{b}{a}.
\end{equation}
Ganz konkret etwa
\begin{equation}
2x = 6 \Leftrightarrow 2^{-1} 2x = 2^{-1}6 \Leftrightarrow \frac{1}{2}2x = \frac{1}{2}6 \Leftrightarrow x = 3.
\end{equation}
Analog zu dem Fall, dass die Matrizen in $Ax = b$ allesamt Skalare sind, möchte man
im Fall eines linearen Gleichungssystems beide Seiten der Gleichung mit dem 
*multiplikativen Inversen* $A^{-1}$ von $A$ multiplizieren können, sodass eine
Gleichung der Form
\begin{equation}
A^{-1}A = "1".
\end{equation}
resultiert. Dann hätte man nämlich
\begin{equation}
Ax = b \Leftrightarrow A^{-1}Ax = A^{-1}b \Leftrightarrow x = A^{-1}b.
\end{equation}
Diese intuitive Idee des multiplikativen Inversen einer Matrix $A$ wird im 
Folgenden unter dem Begriff der *inversen Matrix* formalisiert. Dazu benötigen wir 
zunächst den Begriff der *Einheitsmatrix*.

:::{#def-einheitsmatrix}
## Einheitsmatrix
Die Matrix
\begin{equation}
I_n
:= (a_{ij})_{1\le i \le n, 1 \le j \le n}  \in \mathbb{R}^{n \times n}
:=
\begin{pmatrix}
1      & 0      & \cdots & 0       \\
0      & 1      & \cdots & 0       \\
\vdots & \vdots & \ddots & \vdots  \\
0      & 0      & \cdots & 1       \\
\end{pmatrix}
\end{equation}
mit $a_{ij} = 1$ für $i = j$  und  $a_{ij} = 0$ für  $i \neq j$ heißt *$n$-dimensionale Einheitsmatrix*.
:::
In **R** wird $I_n$ mit dem Befehl `diag(n)` erzeugt. Die Einheitsmatrix ist für
die Matrixmultiplikation das Analog zur 1 bei der Multiplikation reeller Zahlen.
Das ist die Aussage folgenden Theorems.

:::{#thm-neutrales-element-der-matrixmultiplikation}
## Neutrales Element der Matrixmultiplikation
$I_n$ ist das neutrale Element der Matrixmultiplikation, das heißt es gilt für $A \in \mathbb{R}^{n \times m}$,
dass
\begin{equation}
I_nA = A \mbox{ und } AI_m = A.
\end{equation}
:::

:::{.proof}
Es sei $B = (b_{ij}) = I_nA \in \mathbb{R}^{n\times m}$. Dann gilt für alle $1 \le i \le n$
und alle $1 \le j \le n$
\begin{equation}
d_{ij}
= 0 \cdot a_{1j}
+ 0 \cdot a_{2j}
+ \cdots
+ 0 \cdot a_{i-1,j}
+ 1 \cdot a_{ij} 
+ \cdots
+ 0 \cdot a_{i+1,j}
+ 0 \cdot a_{nj}
= a_{ij}.
\end{equation}
Analog zeigt man dies für $AI_m$.
:::

Mit dem Begriff der Einheitsmatrix können wir jetzt die Begriffe der inversen Matrix
und der invertierbaren Matrix definieren:

:::{#def-invertierbare-matrix-und-inverse-matrix}
## Invertierbare Matrix und inverse Matrix
Eine quadratische Matrix $A \in \mathbb{R}^{n \times n}$ heißt *invertierbar*, wenn es eine
quadratische Matrix $A^{-1} \in \mathbb{R}^{n \times n}$ gibt, so dass
\begin{equation}
A^{-1}A = AA^{-1} = I_n
\end{equation}
ist. Die Matrix $A^{-1}$ heißt die *inverse Matrix von $A$*.
:::

Man beachte, dass sich die Begriffe der inversen Matrix und der Invertierbarkeit 
*nur* auf quadratische Matrizen beziehen. Insbesondere können quadratische Matrizen 
invertierbar sein, müssen es aber nicht sein (lineare Gleichungssysteme können also 
Lösungen haben, müssen es aber nicht). Nicht invertierbare Matrizen nennt man auch 
*singuläre* Matrizen, invertierbare Matrizen manchmal auch *nicht-singuläre* Matrizen. 
Schließlich beachte man, dass @def-invertierbare-matrix-und-inverse-matrix 
lediglich aussagt, was eine inverse Matrix ist, aber nicht wie man sie berechnet. 

**Beispiel für eine invertierbare Matrix**

Die Matrix
\begin{equation}
A := \begin{pmatrix} 2.0 & 1.0 \\ 3.0 & 4.0 \end{pmatrix}
\end{equation}
ist invertierbar und ihre inverse Matrix ist gegeben durch
\begin{equation}
A^{-1} = \begin{pmatrix} 0.8 & -0.2 \\  -0.6 & 0.4 \end{pmatrix},
\end{equation}
denn
\begin{equation}
\begin{pmatrix} 2.0 &  1.0 \\   3.0 & 4.0 \end{pmatrix}
\begin{pmatrix} 0.8 & -0.2 \\  -0.6 & 0.4 \end{pmatrix}
=
\begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}
=
\begin{pmatrix} 0.8 & -0.2  \\  -0.6 & 0.4 \end{pmatrix}
\begin{pmatrix} 2.0 &  1.0  \\   3.0 & 4.0 \end{pmatrix},
\end{equation}
wovon man sich durch Nachrechnen überzeugt.

**Beispiel für eine nicht-invertierbare Matrix**

Die Matrix 
\begin{equation}
B := \begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix}
\end{equation}
ist nicht invertierbar,
denn wäre $B$ invertierbar, dann gäbe es 
\begin{equation}
\begin{pmatrix} a & b \\ c & d \end{pmatrix}
\end{equation}
mit
\begin{equation}
\begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix}
\begin{pmatrix} a & b \\ c & d \end{pmatrix}
=
\begin{pmatrix} a & b \\ 0 & 0 \end{pmatrix}
=
\begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}.
\end{equation}
Das würde aber bedeuten, dass $0 = 1$ in $\mathbb{R}$ und das ist ein Widerspruch.
Also kann $B$ nicht invertierbar sein.

**Zum Berechnen inverser Matrizen**

$2 \times 2$ bis etwa $5 \times 5$ Matrizen kann man prinzipiell per Hand 
invertieren, dazu stellt die Lineare Algebra verschiedene Verfahren bereit.
Wir wollen hier auf eine Einführung in die Matrizeninvertierung per Hand
verzichten, da in der Anwendung Matrizen standardmäßig numerisch invertiert
werden. Die numerische Matrixinversion ist dann auch ein großes Feld der 
Forschung zur Numerischen Mathematik, die eine Vielzahl von Algorithmen zu 
diesem Zweck bereitstellt. In **R** werden Matrizen per default mit der Funktion
`solve()`, in Anlehnung an das Lösen linearer Gleichungssysteme, invertiert.
Für das obige Beispiel einer invertierbaren Matrix ergibt sich dabei folgender
**R** Code.

\footnotesize
```{r}
# Definition
A = matrix(c(2,1,
             3,4),
           nrow  = 2,
           byrow = TRUE)

# Berechnen von A^{-1}
print(solve(A))

# Überprüfen der Eigenschaften einer inversen Matrix
print(solve(A) %*% A)

# Bei der umgekehrten Berechnung ergebn sich kleine Rundungsfehler
print(A %*% solve(A))
```
\normalsize

Nicht-invertierbare Matrizen sind dabei natürlich auch numerisch nicht-invertierbar,
wie folgende Fehlermeldung in **R** bezüglich obigen Beispiels einer nicht-invertierbaren
Matrix demonstriert. 

\footnotesize
```{r, error = TRUE}
B = matrix(c(1,0,
             0,0),
           nrow  = 2,
           byrow = 2)
solve(B)
```
\normalsize

```{r, echo = F, eval = F}
library(latex2exp)
pdf(
file        = "./_figures/109-determinanten.pdf",
width       = 9,
height      = 3)
par(
family      = "sans",
mfcol       = c(1,3),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2,1,0),
xaxs        = "i",
yaxs        = "i",
cex.main    = 1.2)

# Vektordefinitionen
x           = c(1,2)
y           = c(3,1)
z           = c(4,3)

# Visualisierung
plot(
NULL,
xlab        = TeX("$x_1$"),
ylab        = TeX("$x_2$"),
main        = TeX("$|A_1|s = 5$"),
xlim        = c(0,4),
ylim        = c(0,4))
grid()
points(
c(x[1],y[1]),
c(x[2],y[2]),
pch = 19)
arrows(
x0          = c(0,0,x[1],y[1]),
y0          = c(0,0,x[2],y[2]),
x1          = c(x[1],y[1],z[1],z[1]),
y1          = c(x[2],y[2],z[2],z[2]),
angle       = 20,
length      = .1,
col         = c("black", "black","gray60", "gray60"))
mtext(LETTERS[1], adj=1, line=1, cex = 1.2, at = -1)

# Vektordefinitionen
x           = c(2,0)
y           = c(0,2)
z           = x + y

# Visualisierung
plot(
NULL,
xlab        = TeX("$x_1$"),
ylab        = TeX("$x_2$"),
main        = TeX("$|A_2|= 4$"),
xlim        = c(0,4),
ylim        = c(0,4))
grid()
points(
c(x[1],y[1]),
c(x[2],y[2]),
pch = 19)
arrows(
x0          = c(0,0,x[1],y[1]),
y0          = c(0,0,x[2],y[2]),
x1          = c(x[1],y[1],z[1],z[1]),
y1          = c(x[2],y[2],z[2],z[2]),
angle       = 20,
length      = .1,
col         = c("black", "black","gray60", "gray60"),
xpd         = TRUE)
mtext(LETTERS[2], adj=1, line=1, cex = 1.2, at = -1)


# Vektordefinitionen
x           = c(2,2)
y           = c(2,2)
z           = x + y

# Visualisierung
plot(
NULL,
xlab        = TeX("$x_1$"),
ylab        = TeX("$x_2$"),
main        = TeX("$|A_3|= 0$"),
xlim        = c(0,4),
ylim        = c(0,4))
grid()
points(
c(x[1],y[1]),
c(x[2],y[2]),
pch = 19)
arrows(
x0          = c(0,0,x[1],y[1]),
y0          = c(0,0,x[2],y[2]),
x1          = c(x[1],y[1],z[1],z[1]),
y1          = c(x[2],y[2],z[2],z[2]),
angle       = 20,
length      = .1,
col         = c("black", "black","gray60", "gray60"),
xpd         = TRUE)
mtext(LETTERS[3], adj=1, line=1, cex = 1.2, at = -1)
dev.off()
```

## Determinanten {#sec-determinanten}

Die Determinante ist eine vielseitig einsetzbare Maßzahl einer quadratischen
Matrix. Für das Verständnis der *Eigenanalyse* und der *Matrixzerlegung* ist 
der Begriff der Determinante im Kontext des *charakteristischen Polynoms* grundlegend. 

Allgemein ist eine Determinante eine nichtlineare Abbildung der Form
\begin{equation}
\lvert \cdot \rvert: \mathbb{R}^{n \times n} \to \mathbb{R}, A \mapsto \lvert A \rvert,
\end{equation}
das heißt, eine Determinante ordnet einer quadratischen Matrix $A$ die reelle 
Zahl $\lvert A \rvert$ zu. Die Zahl $\lvert A \rvert$  wird dabei rekursiv 
anhand folgender Definition bestimmt.

:::{#def-Determinante}
## Determinante
Für $A = (a_{ij})_{1 \le i,j \le n} \in \mathbb{R}^{n \times n}$ mit $n>1$ sei
$A_{ij} \in \mathbb{R}^{n-1 \times n-1}$ die Matrix, die aus $A$ durch
Entfernen der $i$ten Zeile und der $j$ten Spalte entsteht.
Dann heißt die Zahl
\begin{align}
\lvert A \rvert & := a_{11} \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad   \mbox{ für } n = 1\\
\lvert A \rvert & := \sum_{j = 1}^n a_{1j}(-1)^{1+j} \det\left(A_{1j}\right)               \mbox{ für } n > 1
\end{align}
die *Determinante von $A$*.
:::

Die Definition führt die Bestimmung der Determinante einer quadratischen Matrix
also sukzessive durch Streichen von Zeilen und Spalten auf die Determinante
einer $1 \times 1$ Matrix zurück, die durch ihr einziges Element gegeben ist.
Für
\begin{equation}
A :=
\begin{pmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9 \\
\end{pmatrix}
\in \mathbb{R}^{3 \times 3}
\end{equation}
ergeben sich dabei zum Beispiel folgende Matrizen der Form $A_{ij} \in \mathbb{R}^{3-1 \times 3-1}$: 
\begin{equation}
A_{11}
=
\begin{pmatrix}
5 & 6 \\
8 & 9 \\
\end{pmatrix},
A_{12}
=
\begin{pmatrix}
4 & 6 \\
7 & 9 \\
\end{pmatrix},
A_{21}
=
\begin{pmatrix}
2 & 3 \\
8 & 9 \\
\end{pmatrix},
A_{22}
=
\begin{pmatrix}
1 & 3 \\
7 & 9 \\
\end{pmatrix}.
\end{equation}

Für die Berechnung der Determinanten von zwei- und dreidimensionalen quadratischen 
Matrizen gibt es direkte, nicht-rekursive Rechenregeln, die in folgendem Theorem
festgehalten sind.

:::{#thm-determinanten-von-zwei-und-dreidimensionalen-matrizen}
## Determinanten von zwei- und dreidimensionalen Matrizen
$\quad$

Es sei $A = (a_{ij})_{1 \le i,j \le 2} \in \mathbb{R}^{2 \times 2}$. Dann gilt
\begin{equation}
\lvert A \rvert = a_{11}a_{22} - a_{12}a_{21}.
\end{equation}
Es sei $A = (a_{ij})_{1 \le i,j \le 3} \in \mathbb{R}^{3 \times 3}$. Dann gilt
\begin{equation}
\lvert A \rvert= a_{11}a_{22}a_{33} + a_{12}a_{23}a_{31} + a_{13}a_{21}a_{32} - a_{12}a_{21}a_{33} - a_{11}a_{23}a_{32} - a_{13}a_{22}a_{31}.
\end{equation}
:::

:::{.proof}
Für $A \in \mathbb{R}^{2 \times 2}$ gilt nach Definition
\begin{align}
\begin{split}
\lvert A \rvert
& = \sum_{j = 1}^n a_{1j}(-1)^{1+j} |A_{1j}| \\
& = a_{11}(-1)^{1 + 1}|A_{11}| + a_{12}(-1)^{1 + 2}|A_{12}| \\
& = a_{11}|(a_{22})| - a_{12}|(a_{21})| \\
& = a_{11}a_{22} - a_{12}a_{21}. \\
\end{split}
\end{align}
Für $A \in \mathbb{R}^{3 \times 3}$ gilt nach Definition und mit der Formel für Determinanten von $2 \times 2$ Matrizen
\begin{align}
\begin{split}
\lvert A \rvert
& = \sum_{j = 1}^n a_{1j}(-1)^{1+j} |(A_{1j}| \\
& =   a_{11}(-1)^{1+1} |A_{1j}| + a_{12}(-1)^{1+2} |A_{12}| +  a_{13}(-1)^{1+3}|A_{13}| \\
& =   a_{11}|A_{11}| - a_{12}|A_{12}| + a_{13}|A_{13}| \\
& =   a_{11}\left\vert\begin{pmatrix} a_{22} & a_{23} \\ a_{32} & a_{33}\end{pmatrix}\right\vert
    - a_{12}\left\vert\begin{pmatrix} a_{21} & a_{23} \\ a_{31} & a_{33}\end{pmatrix}\right\vert
    + a_{13}\left\vert\begin{pmatrix} a_{21} & a_{22} \\ a_{31} & a_{32}\end{pmatrix}\right\vert \\
& =   a_{11}(a_{22}a_{33} - a_{23}a_{32})
    - a_{12}(a_{21}a_{33} - a_{23}a_{31})
    + a_{13}(a_{21}a_{32} - a_{22}a_{31}) \\
& =   a_{11}a_{22}a_{33} - a_{11}a_{23}a_{32}
    - a_{12}a_{21}a_{33} + a_{12}a_{23}a_{31}
    + a_{13}a_{21}a_{32} - a_{13}a_{22}a_{31} \\
& =   a_{11}a_{22}a_{33} + a_{12}a_{23}a_{31} + a_{13}a_{21}a_{32}
    - a_{12}a_{21}a_{33} - a_{11}a_{23}a_{32} - a_{13}a_{22}a_{31}.
\end{split}
\end{align}
:::

Für die Bestimmung der Determinanten von $2 \times 2$ und $3 \times 3$ Matrizen 
gilt somit die sogennante *Sarrusche Merkregel*:
\small
\begin{equation*}
\mbox{``Summe der Produkte auf den Diagonalen minus Summe der Produkte auf den Gegendiagonalen.''}
\end{equation*}
\normalsize
Dabei bezieht sich die Merkregeln bei $3 \times 3$ Matrizen auf das Schema
\begin{equation}
\begin{pmatrix}
a_{11} & a_{12} & a_{13} & \vert & a_{11} & a_{12} \\
a_{21} & a_{22} & a_{23} & \vert & a_{21} & a_{22} \\
a_{31} & a_{32} & a_{33} & \vert & a_{31} & a_{32}
\end{pmatrix}.
\end{equation}

**Beispiele für Determinanten von $2 \times 2$ und $3 \times 3$ Matrizen** 

Es seien
\begin{equation}
A :=
\begin{pmatrix}
2 & 1 \\
3 & 4
\end{pmatrix},
B :=
\begin{pmatrix}
1 & 0 \\
0 & 0
\end{pmatrix}
\mbox{ und }
C :=
\begin{pmatrix}
2 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 3
\end{pmatrix}
\end{equation}
Dann ergeben sich
\begin{equation}
\lvert A \rvert
= 2 \cdot 4 - 1 \cdot 3 = 8 - 3 = 5 
\end{equation}
und
\begin{equation}
\lvert B \rvert
= 1 \cdot 0 - 0 \cdot 0 = 0 - 0 = 0 
\end{equation}
und
\begin{equation}
\lvert C \rvert
= 2 \cdot 1 \cdot 3  + 0 \cdot 0 \cdot 0 + 0 \cdot 0 \cdot 0 - 0 \cdot 0 \cdot 3 - 0 \cdot 0 \cdot 0  - 0 \cdot 1 \cdot 0
= 2 \cdot 1 \cdot 3
= 6.
\end{equation}

In **R** rechnet man dies mithilfe der `det()` Funktion wie folgt nach.

\footnotesize
```{r}
# Matrixdefinition und Determinantenberechnung
A = matrix(c(2,1,
             3,4),
           nrow = 2,
           byrow = TRUE)
det(A)

# Matrixdefinition und Determinantenberechnung
B = matrix(c(1,0,
             0,0),
           nrow = 2,
           byrow = TRUE)
det(B)

# Matrixdefinition und Determinantenberechnung
C = matrix(c(2,0,0,
             0,1,0,
             0,0,3),
           nrow = 3,
           byrow = TRUE)
det(C)
```
\normalsize

Für Determinanten bestehen zahlreiche Rechenregeln im Zusammenspiel mit 
Matrixmultiplikation und Matrixinversion. Ohne Beweis stellen wir diese in 
folgendem Theorem zusammen.

:::{#thm-rechenregeln-für-determinanten}
## Rechenregeln für Determinanten
$\quad$

(Determinantenmultiplikationssatz). Für $A,B \in \mathbb{R}^{n \times n}$ gilt
\begin{equation}
|AB| = \lvert A \rvert\lvert B \rvert.
\end{equation}
(Transposition). Für $A \in \mathbb{R}^{n \times n}$ gilt
\begin{equation}
\lvert A \rvert = \left\vert A^T \right\vert.
\end{equation}
(Inversion). Für eine invertierbare Matrix $A \in \mathbb{R}^{n \times n}$ gilt
\begin{equation}
\left\vert A^{-1}\right\vert = \frac{1}{\lvert A \rvert}.
\end{equation}
(Dreiecksmatrizen). Für Matrizen $A = (a_{ij})_{1 \le i,j\le n} \in \mathbb{R}^{n \times n}$
mit $a_{ij} = 0$ für $i > j$ oder $a_{ij} = 0$ \mbox{ für } $j > i$ gilt
\begin{equation}
\lvert A \rvert = \prod_{i=1}^n a_{ii}.
\end{equation}
:::

Folgendes sehr tiefgehendes Theorem, welches wir nicht vollständig beweisen wollen,
gibt eine Möglichkeit an, anhand der Determinante einer quadratischen Matrix
zu bestimmen, ob sie invertierbar ist.

:::{#thm-invertierbarkeit-und-determinante}
$A \in \mathbb{R}^{n \times n}$ ist dann und nur dann invertierbar, wenn gilt,
dass $\lvert A \rvert \neq 0$. Es gilt also
\begin{equation}
A \mbox{ ist invertierbar} \Leftrightarrow \lvert A \rvert \neq 0
\mbox{ und }
A \mbox{ ist nicht invertierbar} \Leftrightarrow \lvert A \rvert = 0.
\end{equation}
:::

:::{.proof}
Wir deuten einen Beweis lediglich an und zeigen, dass
aus der Invertierbarkeit von $A$ folgt, dass $\lvert A \rvert$ nicht gleich Null sein kann. Nehmen wir
also an, dass $A$ invertierbar ist. Dann gibt es eine Matrix $B$ mit $AB = I_n$
und mit dem Determinantenmultiplikationssatz folgt
\begin{equation}
\lvert AB \rvert = \lvert A \rvert\lvert B \rvert= |I_n| = 1.
\end{equation}
Also kann $\lvert A \rvert = 0$ nicht gelten, denn sonst wäre $0 = 1$.
:::

**Visuelle Intuition**

Der abstrakte Begriff der Determinante einer quadratischen Matrix kann mithilfe
des Vektorraumbegriffs etwas veranschaulicht werden. Dazu seien $a_1,...,a_n \in \mathbb{R}^n$ 
die Spalten von $A \in \mathbb{R}^{n \times n}$. Dann gilt (wie wir nicht beweisen
wollen), dass $\lvert A \rvert$ dem signierten Volumen des von $a_1,...,a_n\in \mathbb{R}^n$ 
aufgespannten Parallelotops entspricht. Um dies visuell zu veranschaulichen
betrachten wir die Matrizen
\begin{equation}
A_1 =
\begin{pmatrix}
3 & 1 \\
1 & 2
\end{pmatrix},
A_2 =
\begin{pmatrix}
2 & 0 \\
0 & 2
\end{pmatrix},
A_3 =
\begin{pmatrix}
2 & 2 \\
2 & 2
\end{pmatrix}
\end{equation}
mit den jeweiligen Determinanten
\begin{equation}
\lvert A_1 \rvert = 3\cdot 2 - 1 \cdot 1 = 5, \quad
\lvert A_2 \rvert= 2\cdot 2 - 0 \cdot 0 = 4, \quad
\lvert A_3 \rvert = 2\cdot 2 - 2 \cdot 2 = 0.
\end{equation}

@fig-determinanten visualisiert die entsprechende Intuition.

![Determinanten als Parallelotopvolumina.](./_figures/109-determinanten){#fig-determinanten fig-align="center" width=100%}

## Spezielle Matrizen  {#sec-spezielle-matrizen}
In dieser Sektion stellen wir einige häufig auftretende Typen von Matrizen 
und ihre Eigenschaften zusammen. Zum Beweis der allermeisten Eigenschaften verweisen
wir dabei auf die weiterführende Literatur.

### Einheitsmatrizen

Die Einheitsmatrix und die Einheitsvektoren haben wir bereits kennengelernt.
Wir fassen sie hier noch einmal in einer gemeinsamen Definition zusammen.

:::{#def-einheitsmatrizen-und-einheitsvektoren}
## Einheitsmatrix und Einheitsvektoren
Wir bezeichnen die *Einheitsmatrix* mit
\begin{equation}
I_{n} := (i_{jk})_{1 \le j \le n, 1 \le k \le n} \in \mathbb{R}^{n \times n} \mbox{ mit } i_{jk} = 1 \mbox{ für } j = k \mbox{ und } i_{jk} = 0 \mbox{ für } j \neq k.
\end{equation}
Wir bezeichnen die *Einheitsvektoren* $e_i, i = 1,...,n$ mit
\begin{equation}
e_{i} := (e_{{i}_j})_{1 \le j \le n} \in \mathbb{R}^{n} \mbox{ mit } e_{{i}_j} = 1 \mbox{ für } i = j \mbox{ und } e_{{i}_j} = 0 \mbox{ für } i \neq j.
\end{equation}
:::

Die Einheitsmatrix $I_n$ besteht nur aus Nullen und Diagonalelementen gleich Eins,
die Einheitsvektoren bestehen nur aus Nullen und einer Eins in der jeweils indizierten
Komponente. Es gilt
\begin{equation}
I_n = \begin{pmatrix} e_1 & \cdots & e_n \end{pmatrix} 
\in \mathbb{R}^{n \times n}
\end{equation}
Für $n = 3$ gilt also zum Beispiel
\begin{equation}
I_3 = 
\begin{pmatrix} 
1 & 0 & 0 \\ 
0 & 1 & 0 \\ 
0 & 0 & 1
\end{pmatrix}
\mbox{ und }
e_1 = \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix},
e_2 = \begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix},
e_3 = \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}.
\end{equation}
Weiterhin gelten für die Einheitsvektoren bekanntlich für $1 \le i,j \le n$
\begin{equation}
e^T_ie_j = 0 \mbox{ für } i \neq j,  e^T_ie_i = 1 \mbox{ und } e^T_iv = v^Te_i = v_i \mbox{ für } v \in \mathbb{R}^n.
\end{equation}

### Einsmatrizen und Nullmatrizen
:::{#def-nullmatrizen-nullvektoren-einsmatrizen-einsvektoren}
## Nullmatrizen, Nullvektoren, Einsmatrizen, Einsvektoren
Wir bezeichnen *Nullmatrizen* und *Nullvektoren* mit
\begin{equation}
0_{nm} := (0)_{1 \le i \le m, 1 \le j \le n} \in \mathbb{R}^{n \times m}
\mbox{ und }
0_{n} := (0)_{1 \le i \le n} \in \mathbb{R}^{n}.
\end{equation}
 Wir bezeichnen *Einsmatrizen* und *Einsvektoren* mit
\begin{equation}
1_{nm} := (1)_{1 \le i \le n, 1 \le j \le m} \in \mathbb{R}^{n \times m}
\mbox{ und }
1_n := (1)_{1 \le i \le n} \in \mathbb{R}^n.
\end{equation}
:::

$0_{nm}$ und $0_{n}$ bestehen also nur aus Nullen und $1_{nm}$ und $1_{n}$ 
bestehen nur aus Einsen. Es gilt also beispielsweise
\begin{equation}
0_{32} = \begin{pmatrix} 0 & 0 \\ 0 & 0 \\ 0 & 0 \end{pmatrix},
0_{3}  = \begin{pmatrix} 0  \\ 0  \\ 0  \end{pmatrix},
1_{32} = \begin{pmatrix} 1 & 1 \\ 1 & 1 \\ 1 & 1 \end{pmatrix} \mbox{ und }
1_{3}  = \begin{pmatrix} 1  \\ 1  \\ 1  \end{pmatrix}.
\end{equation}
Weiterhin gelten zum Beispiel
\begin{equation}
0_n0_n^T = 0_{nn} \mbox{ und } 1_n1_n^T = 1_{nn},
\end{equation}
wovon man sich durch Nachrechnen überzeugt.

### Diagonalmatrizen
:::{#def-diagonalmatrix}
## Diagonalmatrix
Eine Matrix $D \in \mathbb{R}^{n \times m}$ heißt *Diagonalmatrix*, wenn $d_{ij} = 0$ für $1 \le i \le n, 1 \le j \le m$ mit $i \neq j$.
:::
Eine quadratische Diagonalmatrix $D\in \mathbb{R}^{n \times n}$ mit den 
Diagonalelementen $d_1,...,d_n \in \mathbb{R}$ schreibt man auch als
\begin{equation}
D = \mbox{diag}(d_1,...,d_n).
\end{equation}
Zum Beispiel gelten
\begin{equation}
D 
:= \mbox{diag}(1,2,3) 
= \begin{pmatrix} 
1 & 0 & 0 \\ 
0 & 2 & 0 \\ 
0 & 0 & 3
\end{pmatrix}
\end{equation}
und für $\sigma^2 \in \mathbb{R}$
\begin{equation}
\Sigma 
= \mbox{diag}(\sigma^2,\sigma^2,\sigma^2) 
= \begin{pmatrix} 
\sigma^2 & 0 & 0 \\ 
0 & \sigma^2 & 0 \\ 
0 & 0 & \sigma^2
\end{pmatrix}
= \sigma^2I_3.
\end{equation}

In folgendem Theorem stellen wir einige wichtige Eigenschaften von 
quadratischen Diagonalmatrizen zusammen.


:::{#thm-diagonalmatrix}
## Eigenschaften quadratischer Diagonalmatrizen
$\quad$

(Determinante.) $D := \mbox{diag}(d_1,...,d_n) \in \mathbb{R}^{n \times n}$ sei 
eine quadratische Diagonalmatrix. Dann gilt 
\begin{equation}
|D| = \prod_{i=1}^n d_i.
\end{equation}
:::

### Symmetrische Matrizen

Symmetrische Matrizen sind quadratische Matrizen, die bei Transposition unverändert
bleiben:

:::{#def-symmetrische-matrix}
Eine Matrix $S \in \mathbb{R}^{n \times n}$ heißt *symmetrisch*, wenn $S^T = S$.
:::
Ein Beispiel für eine symmetrische Matrix ist
\begin{equation}
S := 
\begin{pmatrix} 
1 & 2 & 3 \\ 
2 & 1 & 2 \\ 
3 & 2 & 1
\end{pmatrix}.
\end{equation}

In folgendem Theorem stellen wir einige wichtige Eigenschaften symmetrischer Matrizen zusammen.

:::{#thm-symmetrische-matrix}
## Eigenschaften symmetrischer Matrizen
$\quad$

(Summation.) $S_1 \in \mathbb{R}^{n \times n}$ und $S_2 \in \mathbb{R}^{n \times n}$
seien symmetrische Matrizen. Dann gilt
\begin{equation}
S_1 + S_2 = (S_1 + S_2)^T.
\end{equation}
(Inverse.) $S$ sei eine invertierbare symmetrische Matrix und $S^{-1}$ ihre Inverse. 
Dann ist auch $S^{-1}$ eine symmetrische Matrix, das heißt es gilt
\begin{equation}
\left(S^{-1}\right)^T = S^{-1}.
\end{equation}
:::

### Orthogonale Matrizen

:::{#def-orthogonale-matrix}
Eine Matrix $Q \in \mathbb{R}^{n \times n}$ heißt *orthogonal*, wenn $Q^TQ = I_n$.
:::

Die Spalten einer orthogonalen Matrix sind also paarweise orthogonal, es gilt für
\begin{equation}
Q = \begin{pmatrix} q_1 & \cdots & q_n \end{pmatrix} \mbox{ mit } q_i \in \mathbb{R}^n \mbox{ für } 1 \le i \le n, 
\end{equation}
dass
\begin{equation}
q_i^Tq_j = 0 \mbox{ für } i \neq j \mbox{ und }  q_i^Tq_j = 1 \mbox{ für } i = j \mbox{ mit } 1 \le i,j \le n.
\end{equation}

:::{#thm-orthogonale-matrizen}
## Eigenschaften orthogonaler Matrizen
$Q \in \mathbb{R}^{n \times n}$ sei eine orthogonale Matrix. Dann gelten folgende
Eigenschaften von $Q$.
$\quad$

(Inverse.) Die Inverse von $Q$ ist $Q^T$, es gilt  
\begin{equation}
Q^{-1} = Q^T.
\end{equation}
(Transposition) Die Zeilen von $Q$ sind orthonormal, es gilt
\begin{equation}
QQ^T = I_n
\end{equation}
:::

:::{.proof}
(Inverse) Unter der Annahme, dass $Q^{-1}$ existiert, gilt 
\begin{equation}
Q^TQ = I_n \Leftrightarrow Q^TQQ^{-1} = I_nQ^{-1} \Leftrightarrow  Q^{-1} = Q^T.
\end{equation}
(Transposition) Es gilt
\begin{equation}
Q^TQ = I_n \Leftrightarrow QQ^TQ = QI_n \Leftrightarrow  QQ^TQQ^T = QQ^T \Leftrightarrow  QQ^T = I_n. 
\end{equation}
:::



### Positiv-definite Matrizen

Positiv-definite Matrizen sind für die probabilistiche Modellbildung unter Verwendung 
multivariater Normalverteilungen zentral.

:::{#def-positiv-definite-matrix}
Eine quadratische Matrix $C \in \mathbb{R}^{n \times n}$ heißt positiv-definit ($\mbox{p.d.}$), wenn

* $C$ eine symmetrische Matrix ist und
* für alle $x \in \mathbb{R}^n, x \neq 0_n$ gilt, dass $x^TCx > 0$ ist.
:::

In folgendem Theorem stellen wir einige wichtige Eigenschaften positiv-definiter Matrizen zusammen.

:::{#thm-positiv-definite-matrix}
## Eigenschaften positiv-definiter Matrizen
$\quad$

(Inverse.) $C \in \mathbb{R}^{n \times n}$ sei eine positiv-definite Matrix. Dann gilt,
dass $C^{-1}$ existiert und ebenfalls positiv-definit ist.
:::

## Literaturhinweise {#literaturhinweise}

 @searle1982 gibt eine umfassende Einführung in die Matrixtheorie vor dem Hintergrund
 der probabilistischen Datenanalyse, @strang2009 gibt ein umfassende Einführung
 in die Matrixtheorie im Kontext der linearen Algebra. In ihrer modernen Inkarnation 
 tauchen Matrizen als algebraische Objekte wohl zunächst in den Arbeiten von 
 Arthur Caley (1821-1895) auf, siehe zum Beispiel @caley1858.

## Selbstkontrollfragen {#selbstkontrollfragen}
\small

1. Geben Sie die Definition einer Matrix wieder.
1. Nennen Sie sechs Matrixoperationen.
1. Geben Sie die Definitionen der Matrixaddition und der Matrixsubtraktion wieder.
1. Geben Sie die Definition der Skalarmultiplikation für Matrizen wieder.
1. Geben Sie die Definition der Matrixtransposition wieder.
1. Es seien
\begin{equation}
A :=
\begin{pmatrix}
1 & 2 \\
2 & 1
\end{pmatrix},
B :=
\begin{pmatrix}
3 & 0 \\
1 & 2
\end{pmatrix} 
\mbox{ und }
c := 2.
\end{equation}
Berechnen Sie
\begin{equation}
D := c\left(A - B^T\right)
\mbox{ und }
E := \left(cA\right)^T + B.
\end{equation}
1. Geben Sie die Definition der Matrixmultiplikation wieder.
1. Es seien $A \in \mathbb{R}^{3 \times 2}, B \in \mathbb{R}^{2\times 4}$
und $C \in \mathbb{R}^{3 \times 4}$. Prüfen Sie, ob folgende Matrixprodukte
definiert sind, und wenn ja, geben Sie die Größe der resultierenden Matrix an:
\begin{equation}
ABC, ABC^T, A^TCB^T, BAC.
\end{equation}
1. Es seien
\begin{equation}
A :=
\begin{pmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
3 & 2 & 0
\end{pmatrix}
B :=
\begin{pmatrix}
1 & 2 & 2 \\
1 & 3 & 1 \\
2 & 0 & 0
\end{pmatrix}
\mbox{ und }
C :=
\begin{pmatrix}
1 \\ 3 \\ 2
\end{pmatrix}.
\end{equation}
Berechnen Sie die Matrixprodukte
\begin{equation}
AB,
B^TA^T,
\left(B^TA^T\right)^T,
AC.
\end{equation}
1. Definieren Sie die Begriff der inversen Matrix und der Invertierbarkeit einer Matrix.
1. Geben Sie die Formel für die Determinante von $A := (A_{ij})_{1 \le i,j \le 2} \in \mathbb{R}^2$ wieder.
1. Geben Sie die Formel für die Determinante von $A := (A_{ij})_{1 \le i,j \le 3} \in \mathbb{R}^3$ wieder.
1. Berechnen Sie die Determinanten von
\begin{equation}
A := \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}
B := \begin{pmatrix} 3 & 2 & 1 \\ 2 & 3 & 2 \\ 1 & 2 & 3 \end{pmatrix} \mbox{ und }
C := \mbox{diag}(1,2,3).
\end{equation}
1. Geben Sie die Definitionen von Einheitsmatrix und Einheitsvektoren wieder.
1. Geben Sie die Definitionen von Nullmatrizen und Einsmatrizen wieder.
1. Geben Sie die Definition einer symmetrischen Matrix wieder.
1. Geben Sie die Definition einer Diagonalmatrix wieder.
1. Geben Sie die Definition einer positiv-definiten Matrix wieder.

\normalsize
