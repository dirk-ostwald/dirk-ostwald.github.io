# Erwartungswert  {#sec-erwartungswerte}
\normalsize

In diesem Kapitel führen wir mit den Begriffen des *Erwartungswerts* und der *Varianz*
einer Zufallsvariable skalare Zusammenfassungen von Verteilungen ein, die häufig
als charakteristische Kennzahlen von Wahrscheinlichkeitsverteilungen dienen. Dabei
ist der Erwartungswert als ein Maß der "durchschnittlichen Realisierung" und die
Varianz als Maß der "durchschnittlichen Variabilität" einer Wahrscheinlichkeitsverteilung 
zu verstehen. Weiterhin führen wir mit der *Kovarianz* zweier Zufallsvariablen ein
Maß für linear-affine Abhängigkeiten zwischen Zufallsvariablen ein. Wir ergänzen diese
Begriffe um ihre Analoga in Bezug auf Zufallsvektoren (*Erwartungswert* und *Kovarianzmatrix*)
und ihre deskriptiv-statistischen Äquivalente, das *Stichprobenmittel*, die *Stichprobenvarianz*,
und die *Stichprobenkovarianz*. 

## Erwartungswert {#sec-erwartungswert}
:::{#def-erwartungswert}
## Erwartungswert
$(\Omega, \mathcal{A},\mathbb{P})$ sei ein Wahrscheinlichkeitsraum und $\xi$
sei eine  Zufallsvariable. Dann ist der *Erwartungswert von $\xi$* definiert als

* $\mathbb{E}(\xi) := \sum_{x \in \mathcal{X}} x\,p_\xi(x)$, wenn $\xi : \Omega \to \mathcal{X}$ diskret mit WMF $p_\xi$ ist,
* $\mathbb{E}(\xi) := \int_{-\infty}^\infty x \,p_\xi(x)\,dx$, wenn $\xi : \Omega \to \mathbb{R}$ kontinuierlich mit WDF $p_\xi$ ist.

Man sagt, dass der Erwartungswert einer Zufallsvariable *existiert*, wenn er endlich ist.
:::

Der Erwartungswert ist also eine skalare Zusammenfassung der Verteilung einer 
Zufallsvariable. Eine integrierte Definition des Erwartungswertes, die ohne
eine Fallunterscheidung in kontinuierliche und diskrete Zufallsvariablen auskommt,
ist möglich, erfordert aber mit der Einführung des Lebesgue-Integrals einigen
technischen Aufwand. Wir verweisen dahingehend auf die weiterführende Literatur
(vgl. @schmidt2009, @meintrup2005). Intuitiv entspricht der Erwartungswert einer
Zufallsvariable dem im langfristigen Mittel zu erwartenden Wert der Zufallsvariable,
also etwa
\begin{equation}
\mathbb{E}(\xi) \approx \frac{1}{n}\sum_{i=1}^n \xi_i
\end{equation}
für eine große Zahl $n$ von Kopien $\xi_i$ von $\xi$. Wir werden diese Intuition
im Kontext der Gesetze der großen Zahl in @sec-grenzwerte weiter ausarbeiten. 

### Beispiele {-}

Mit dem Erwartungswert einer Bernoulli-Zufallsvariable und dem Erwartungswert
einer normalverteilten Zufallsvariable wollen wir nun zwei erste Beispiele
für den Erwartungswert einer diskreten und einer kontinuierlichen Zufallsvariable
betrachten.

:::{#thm-erwartungwert-einer-bernoulli-zufallsvariable}
## Erwartungswert einer Bernoulli Zufallsvariable
Es sei $\xi \sim \mbox{Bern}(\mu)$. Dann gilt $\mathbb{E}(\xi) = \mu$.
:::

:::{.proof}
$\xi$ ist diskret mit $\mathcal{X} = \{0,1\}$. Also gilt
\begin{align}
\begin{split}
\mathbb{E}(\xi)
& = \sum_{x \in \{0,1\}} x\,\mbox{Bern}(x;\mu) \\
& = 0\cdot \mu^0 (1 - \mu)^{1-0} + 1\cdot \mu^1 (1 - \mu)^{1-1} \\
& = 1\cdot \mu^1 (1 - \mu)^{0} \\
& = \mu.
\end{split}
\end{align}
:::

Es ergibt sich hier also, dass der Parameter $\mu \in [0,1]$ der Verteilung
einer Bernoulli-Zufallsvariable gleichzeitig auch ihr Erwartungswert ist.

:::{#thm-erwartungwert-einer-normalverteilten-zufallsvariable}
## Erwartungswert einer normalverteilten Zufallsvariable
Es sei $\xi \sim N(\mu,\sigma^2)$. Dann gilt $\mathbb{E}(\xi) = \mu$.
:::

:::{.proof}
Die Herleitung des Erwartungswerts einer normalverteilten Zufallsvariable ist 
überraschend aufwändig. Wir müssen in diesem Fall einige grundlegende Eigenschaften
der Exponentialfunktion als gegeben annehmen. Dazu halten wir zunächst ohne 
Beweis fest, dass 
$$
\int_{-\infty}^\infty \exp\left(-x^2\right)\,dx = \sqrt{\pi} 
$$ {#eq-gauss-integral}
und dass
$$
\lim_{x \to -\infty} \exp\left(-x^2\right) = 0 \mbox{ und } \lim_{x \to \infty}\exp\left(-x^2\right) = 0.
$$ {#eq-exp-limits}
@eq-gauss-integral  ist unter der Bezeichnung *Gauss-* oder *Euler-Poisson-Integral*
bekannt. Mit der Definition des Erwartungswerts für kontinuierliche Zufallsvariablen 
gilt dann zunächst
\begin{equation}
\mathbb{E}(\xi)
= \int_{-\infty}^\infty x \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{1}{2\sigma^2}(x - \mu)^2\right) \,dx.
\end{equation}
Mit der allgemeinen Substitutionsregel (vgl. @thm-rechenregeln-für-stammfunktionen)
\begin{equation}
\int_{g(a)}^{g(b)} f(x)\,dx = \int_a^b f(g(x))g'(x)\,dx
\end{equation}
und der Definition von
\begin{equation}
g : \mathbb{R} \to \mathbb{R}, x \mapsto g(x) := \sqrt{2\sigma^2}x + \mu
\mbox{ with } g'(x) = \sqrt{2\sigma^2},
\end{equation}
gilt dann
\begin{align}
\begin{split}
\mathbb{E}(\xi)
& = \frac{1}{\sqrt{2\pi\sigma^2}}
\int_{-\infty}^\infty x
\exp\left(-\frac{1}{2\sigma^2}(x - \mu)^2\right) \,dx \\
& = \frac{1}{\sqrt{2\pi\sigma^2}}
\int_{-\infty}^\infty (\sqrt{2\sigma^2}x + \mu)
\exp\left(-\frac{1}{2\sigma^2}\left(\left(\sqrt{2\sigma^2}x + \mu \right) - \mu\right)^2\right)
\sqrt{2\sigma^2}\,dx \\
& = \frac{\sqrt{2\sigma^2}}{\sqrt{2\pi\sigma^2}}
\int_{-\infty}^\infty (\sqrt{2\sigma^2}x + \mu)
\exp\left(-x^2\right) \,dx \\
& = \frac{1}{\sqrt{\pi}}
\left(\sqrt{2\sigma^2} \int_{-\infty}^\infty x \exp\left(-x^2\right) \,dx
      + \mu \int_{-\infty}^\infty \exp\left(-x^2\right) \,dx \right) \\
& = \frac{1}{\sqrt{\pi}}
\left(\sqrt{2\sigma^2} \int_{-\infty}^\infty x \exp\left(-x^2\right) \,dx
      + \mu \sqrt{\pi} \right).
\end{split}
\end{align}
Eine Stammfunktion von $x \exp\left(-x^2\right)$ ist $-\frac{1}{2}\exp\left(-x^2\right)$,
weil
\begin{equation}
\frac{d}{dx}\left(-\frac{1}{2}\exp\left(-x^2\right)\right)
= -\frac{1}{2} \frac{d}{dx}\exp\left(-x^2\right)
= -\frac{1}{2}\exp\left(-x^2\right)(-2x)
= x\exp\left(-x^2\right) 
\end{equation}
Mit @eq-exp-limits und der Definition des uneigentlichen Integrals 
(vgl. @def-uneigentliche-integrale) verschwindet der Integralterm $\int_{-\infty}^\infty x \exp\left(-x^2\right) \,dx$
damit und wir erhalten
\begin{align}
\mathbb{E}(\xi)
= \frac{1}{\sqrt{\pi}}\left(\mu \sqrt{\pi}\right)
= \mu.
\end{align}
:::

Der Erwartungswert einer univariaten Normalverteilung ist also durch ihren Parameter
$\mu\in \mathbb{R}$ gegeben. 

In Verallgemeinerung von @def-erwartungswert geben wir folgende
Definition für den Erwartungswert einer Funktion einer Zufallsvariable

:::{#def-erwartungswert-einer-funktion-einer-zufallsvariable}
## Erwartungswert einer Funktion einer Zufallsvariable
$(\Omega, \mathcal{A},\mathbb{P})$ sei ein Wahrscheinlichkeitsraum, $\xi$
sei eine  Zufallsvariable mit Ergebnisraum $\mathcal{X}$ und $f: \mathcal{X} \to \mathcal{Z}$
sei eine Funktion mit Zielmenge $\mathcal{Z}$. Dann ist der *Erwartungswert der
Funktion $f$ der Zufallsvariable $\xi$* definiert als

* $\mathbb{E}(f(\xi)) := \sum_{x \in \mathcal{X}} f(x)\,p_\xi(x)$,
wenn $\xi : \Omega \to \mathcal{X}$ diskret mit WMF $p_\xi$ ist,
* $\mathbb{E}(f(\xi)) := \int_{-\infty}^\infty f(x) \,p_\xi(x)\,dx$,
wenn $\xi : \Omega \to \mathbb{R}$ kontinuierlich mit WDF $p_\xi$ ist.
:::

Der Erwartungswert einer Zufallsvariable ergibt sich anhand von 
@def-erwartungswert-einer-funktion-einer-zufallsvariable als der Spezialfall,
in dem gilt, dass 
\begin{equation}
f : \mathcal{X} \to \mathcal{Z}, x \mapsto f(x) := x.
\end{equation}
In der englischsprachigen Literatur ist @def-erwartungswert-einer-funktion-einer-zufallsvariable 
auch als "Law of the unconscious statistician" bekannt und wird oft auch direkt zur Definition des
Erwartungswertes herangezogen. 

Weiterhin ist man wie im univariaten Fall manchmal darum bemüht, die Verteilung 
eines Zufallsvektors mit einigen wenigen Maßzahlen zu charakterisieren. Das multivariate Analogon des
des Erwartungswerts einer Zufallsvariablen ist der *Erwartungswert eines Zufallsvektors*,
der wie folgt definiert ist.

:::{#def-erwartungswert-eines-zufallsvektors}
## Erwartungswert eines Zufallsvektors
$\xi$ sei ein $n$-dimensionaler Zufallvektor. Dann ist der *Erwartungwert*
von $\xi$ definiert als der $n$-dimensionale reelle Vektor
\begin{equation}
\mathbb{E}(\xi) :=
\begin{pmatrix}
\mathbb{E}(\xi_1) \\
\vdots            \\
\mathbb{E}(\xi_n)
\end{pmatrix}
\end{equation}
:::

Der Erwartungswert eines Zufallsvektors $\xi$ ist also der Vektor der Erwartungswerte
der Komponenten $\xi_1, ...,\xi_n$ von $\xi$, ist also direkt im Sinne von Erwartungswerten
von Zufallsvariablen definiert. In Analogie zu @def-erwartungswert-einer-funktion-einer-zufallsvariable
definiert man für die Funktion eines Zufallsvektors den Erwartungswert dieser Transformation
wie folgt.

:::{#def-erwartungswert-einer-funktion-einer-zufallsvektors}
## Erwartungswert einer Funktion eines Zufallsvektors
$(\Omega, \mathcal{A},\mathbb{P})$ sei ein Wahrscheinlichkeitsraum, $\xi$
sei ein Zufallsvektor mit Ergebnisraum $\mathcal{X}$ und $f: \mathcal{X} \to \mathcal{Z}$
sei eine Funktion mit Zielmenge $\mathcal{Z}$. Dann ist der *Erwartungswert der
Funktion $f$ des Zufallsvektors $\xi$* definiert als

* $\mathbb{E}(f(\xi)) := \sum_{x \in \mathcal{X}} f(x)\,p_\xi(x)$,
wenn $\xi : \Omega \to \mathcal{X}$ diskret mit WMF $p_\xi$ ist,
* $\mathbb{E}(f(\xi)) := \int_{-\infty}^\infty f(x) \,p_\xi(x)\,dx$,
wenn $\xi : \Omega \to \mathbb{R}$ kontinuierlich mit WDF $p_\xi$ ist.
:::


Folgendes Theorem gibt nun einige Rechenregeln im Umgang mit Erwartungswerten an,
die uns an vielen Stellen begegnen werden. Diese Rechenregeln folgen direkt aus
der Summen- bzw. Integraldefinition des Erwartungswertes, im Beweis des Theorems
betrachten wir dementsprechend lediglich den Fall kontinuierlicher Zufallsvariablen.

:::{#thm-eigenschaften-des-erwartungswerts}
## Eigenschaften des Erwartungswerts
(1) (Linear-affine Transformation) Für eine Zufallsvariable $\xi$ und $a,b\in \mathbb{R}$ gilt
\begin{equation}
\mathbb{E}(a\xi + b) = a\mathbb{E}(\xi) + b.
\end{equation}
(2) (Linearkombination) Für Zufallsvariablen $\xi_1,...,\xi_n$ und $a_1,...,a_n \in \mathbb{R}$ gilt
\begin{equation}
\mathbb{E}\left(\sum_{i=1}^n a_i\xi_i \right) = \sum_{i = 1}^n a_i \mathbb{E}(\xi_i).
\end{equation}
(3) (Faktorisierung bei Unabhängigkeit) Für unabhängige Zufallsvariablen $\xi_1,...,\xi_n$ gilt
\begin{equation}
\mathbb{E}\left(\prod_{i=1}^n \xi_i \right) = \prod_{i = 1}^n \mathbb{E}(\xi_i).
\end{equation}
:::

:::{.proof}
Eigenschaft (1) folgt aus den Linearitätseigenschaften von Summen und Integralen. 
Wir betrachten nur den Fall einer kontinuierlichen Zufallsvariable $\xi$ mit WDF $p_\xi$ 
genauer und definieren zunächst $\upsilon := a\xi + b$. Dann gilt

\begin{align}
\begin{split}
\mathbb{E}(\upsilon)
& = \mathbb{E}(a\xi + b) 						                \\
& = \int_{-\infty}^\infty (ax + b)p_\xi(x) \,dx				                \\
& = \int_{-\infty}^\infty  axp_\xi(x)  + b p_\xi(x) \,dx			            \\
& = a\int_{-\infty}^\infty xp_\xi(x) \,dx + b \int_{-\infty}^\infty p_\xi(x) \,dx	        \\
& = a\mathbb{E}(\xi) + b.
\end{split}
\end{align}
Eigenschaft (2) folgt gleichfalls aus den Linearitätseigenschaften von Summen 
und Integralen. Wir wollen nur den Fall von zwei kontinuierlichen Zufallsvariablen
$\xi_1$ und $\xi_2$ mit bivariater WDF $p_{\xi_1,\xi_2}$ genauer betrachten. 
In diesem Fall gilt
\begin{align}
\begin{split}
\mathbb{E}\left(\sum_{i=1}^2 a_i\xi_i\right)
& = \mathbb{E}(a_1\xi_1 + a_2\xi_2) \\
& = \iint_{\mathbb{R}^2} (a_1x_1 + a_2x_2)p_{\xi_1,\xi_2}(x_1,x_2)\,dx_1\,dx_2 	\\
& = \iint_{\mathbb{R}^2} a_1x_1 p_{\xi_1,\xi_2}(x_1,x_2)
                                   + a_2x_2 p_{\xi_1,\xi_2}(x_1,x_2)\,dx_1\,dx_2			\\
& =  a_1\iint_{\mathbb{R}^2} x_1 p_{\xi_1,\xi_2}(x_1,x_2) \,dx_1\,dx_2
  +  a_2\iint_{\mathbb{R}^2} x_2 p_{\xi_1,\xi_2}(x_1,x_2)\,dx_1\,dx_2			\\
& =  a_1\int_{-\infty}^\infty x_1 \left(\int_{-\infty}^\infty p_{\xi_1,\xi_2}(x_1,x_2) \,dx_2 \right)\,dx_1
  +  a_2\int_{-\infty}^\infty x_2 \left(\int_{-\infty}^\infty p_{\xi_1,\xi_2}(x_1,x_2) \,dx_1 \right) \,dx_2 \\
& =  a_1\int_{-\infty}^\infty x_1 p_{\xi_1}(x_1) \,dx_1
  +  a_2\int_{-\infty}^\infty x_2 p_{\xi_2}(x_2) \,dx_2 \\
& =  a_1 \mathbb{E}(\xi_1) +  a_2\mathbb{E}(\xi_2) \\
& = \sum_{i=1}^2 a_i \mathbb{E}(\xi_i).
\end{split}
\end{align}
Ein Induktionsbeweis erlaubt dann die Generalisierung vom bivariaten auf den
$n$-variaten Fall.

Zu Eigenschaft (3) betrachten wir den Fall von $n$ kontinuierlichen Zufallsvariablen
mit gemeinsamer WDF $p_{\xi_1,...,\xi_n}$. Weil als $\xi_1,...,\xi_n$ unabhängig vorausgesetzt
sind, gilt
\begin{equation}
p_{\xi_1,...,\xi_n}(x_1,...,x_n) = \prod_{i=1}^n p_{\xi_i}(x_i).
\end{equation}
Weiterhin gilt also
\begin{align}
\begin{split}
\mathbb{E}\left(\prod_{i=1}^n \xi_i \right)
& = \int_{-\infty}^\infty\cdots\int_{-\infty}^\infty \left(\prod_{i=1}^n x_i\right)
		p_{\xi_1,...,\xi_n}(x_1,...,x_n) \,dx_1...\,dx_n	\\
& = \int_{-\infty}^\infty\cdots\int_{-\infty}^\infty  \prod_{i=1}^n x_i
		 \prod_{i=1}^n p_{\xi_i}(x_i)\,dx_1...\,dx_n	\\
& = \int_{-\infty}^\infty\cdots \int_{-\infty}^\infty  \prod_{i=1}^n x_i p_{\xi_i}(x_i) \,dx_1...\,dx_n	\\
& = \prod_{i=1}^n \int_{-\infty}^\infty x_i p_{\xi_i}(x_i) \,dx_i	\\
& = \prod_{i=1}^n \mathbb{E}(\xi_i).
\end{split}
\end{align}
:::

## Kennzahlen univariater Stichproben {#sec-stichprobenkennzahlen}

Wie wir @sec-grundbegriffe-frequentistischer-inferenz noch ausführlich diskutieren, 
werden ist eine Charakteristikum der probabilistischen Modellierung, beobachtete 
Daten als Realisierungen von Zufallsvariablen zu verstehen. Hat meine Menge 
$\xi_1,...,\xi_n$ von Zufallsvariablen, so nennt man diese auch eine *Stichprobe*. 
Basierend auf einer Stichprobe kann man nun Kennzahlen berechnen, die auf den 
ersten Blick den Begriffen von Erwartungswert, Varianz und Standardabweichung
ähneln, mit diesen aber keinesfalls zu verwechseln sind. Defacto dienen die
in folgender Definition aufgeführten Stichprobenkennzahlen oft als *Schätzer* für
die Kennzahlen von Zufallsvariablen, wie wir in @sec-punktschaetzung ausführlich
darlegen wollen. Gewissermaßen Vorgriff zur Abgrenzung der Begrifflichkeiten 
und auch als Grundlage für @sec-grenzwerte definieren wir hier einige deskriptive
Stichprobenkennzahlen.

:::{#def-stichprobenkennzahlen}
## Stichprobenmittel, Stichprobenvarianz, Stichprobenstandardabweichung
$\xi_1,...,\xi_n$ sei eine Menge von Zufallsvariablen, genannt *Stichprobe*.

* Das *Stichprobenmittel* von $\xi_1,...,\xi_n$ ist definiert als 
\begin{equation}
\bar{\xi} := \frac{1}{n}\sum_{i=1}^n \xi_i.
\end{equation}
* Die *Stichprobenvarianz* von $\xi_1,...,\xi_n$ ist definiert als
\begin{equation}
S^2 := \frac{1}{n-1}\sum_{i=1}^n (\xi_i - \bar{\xi})^2.
\end{equation}
* Die *Stichprobenstandardabweichung* ist definiert als
\begin{equation}
S := \sqrt{S^2}.
\end{equation}
:::

Zur Abgrenzung erinnern wir noch einmal daran, dass Erwartungswert $\mathbb{E}(\xi)$,
Varianz $\mathbb{V}(\xi)$ und Standardabweichung $\mathbb{S}(\xi)$ Kennzahlen 
einer Zufallsvariable $\xi$ sind, wohingegen $\bar{\xi}, S^2$, und $S$ 
Kennzahlen einer Stichprobe $\xi_1,...,\xi_n$ sind. 

**Beispiel**

Wir wollen die Bestimmung der in @def-stichprobenkennzahlen eingeführten 
Stichprobenkennzahlen an einem Beispiel erläutern. Dazu halten wir nochmals fest, dass
$\bar{\xi}, S^2$, $S$ Zufallsvariablen sind und wollen ihre Realisationen im 
Folgenden mit $\bar{x}, s^2$ und $s$ bezeichnen. Nehmen wir also an, wir haben für
$n := 10$ die in folgender Tabelle gezeigten Realisationen von u.i.v. 
nach $N(1,2)$ verteilten Zufallsvariable $\xi_1,...,\xi_{10}$, wobei für 
$i = 1,...,10$ die Realisation von $\xi_i$ mit $x_i$ bezeichnen ist:

\begin{center}
\begin{tabular}{ccccccccccc}
   $x_1$
&  $x_2$
&  $x_3$
&  $x_4$
&  $x_5$
&  $x_6$
&  $x_7$
&  $x_8$
&  $x_9$
&  $x_{10}$ \\\hline
   0.54
&  1.01
& -3.28
&  0.35
&  2.75
& -0.51
&  2.32
&  1.49
&  0.96
&  1.25
\end{tabular}
\end{center}

Nach @def-stichprobenkennzahlen ist die Stichprobenmittelrealisation dann gegeben durch
\begin{equation}
\bar{x}
= \frac{1}{10}\sum_{i = 1}^{10}x_i
= \frac{6.88}{10}
= 0.68,
\end{equation}
die Stichprobenvarianzrealisation gegeben durch
\begin{equation}
s^2
= \frac{1}{9}\sum_{i=1}^{10} (x_i - \bar{x})^2
= \frac{1}{9}\sum_{i=1}^{10} (x_i - 0.68)^2
= \frac{25.37}{9}
= 2.82.
\end{equation}
und die Stichprobenstandardabweichungrealisation gegeben durch
\begin{equation}
s = \sqrt{s^2} = \sqrt{2.82} = 1.68.
\end{equation}