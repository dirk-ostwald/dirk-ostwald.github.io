[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Probabilistische Datenwissenschaft für die Psychologie",
    "section": "",
    "text": "Willkommen\nHerzlich willkommen zur Arbeitsversion von Probabilistische Datenwissenschaft für die Psychologie (PDWP), einem Lehrbuch zur datenwissenschaftlichen Methodenlehre am Institut für Psychologie der Otto-von-Guericke-Universität Magdeburg.",
    "crumbs": [
      "Willkommen"
    ]
  },
  {
    "objectID": "index.html#zitation",
    "href": "index.html#zitation",
    "title": "Probabilistische Datenwissenschaft für die Psychologie",
    "section": "Zitation",
    "text": "Zitation\nOstwald, D. (2024) Probabilistische Datenwissenschaft für die Psychologie. 10.5281/zenodo.10730199",
    "crumbs": [
      "Willkommen"
    ]
  },
  {
    "objectID": "index.html#lizenz",
    "href": "index.html#lizenz",
    "title": "Probabilistische Datenwissenschaft für die Psychologie",
    "section": "Lizenz",
    "text": "Lizenz\nDieses Werk ist lizenziert unter einer Creative Commons Namensnennung 4.0 International Lizenz.",
    "crumbs": [
      "Willkommen"
    ]
  },
  {
    "objectID": "101-Sprache-und-Logik.html",
    "href": "101-Sprache-und-Logik.html",
    "title": "1  Sprache und Logik",
    "section": "",
    "text": "1.1 Mathematik ist eine Sprache\nMathematik ist die Sprache der naturwissenschaftlichen Modellbildung. So entspricht zum Beispiel der Ausdruck \\[\\begin{equation}\nF = ma\n\\end{equation}\\] im Sinne des zweiten Newtonschen Axioms einer Theorie zur Bewegung von Objekten unter der Einwirkung von Kräften (Newton (1687)). Gleichermaßen entspricht der Ausdruck \\[\\begin{equation}\n\\max_{q(z)} \\int q(z) \\ln \\left(\\frac{p(y,z)}{q(z)}\\right)\\,dz\n\\end{equation}\\] im Sinne der Variational Inference der zeitgenössischen Theorie zur Funktionsweise des Gehirns (Friston (2005)). Mathematische Symbolik dient dabei insbesondere der genauen Kommunikation wissenschaftlicher Erkenntnisse und zielt darauf ab, komplexe Sachverhalte exakt und effizient zu beschreiben. Wie beim reflektierten Umgang mit jeder Form von Sprache steht also die Frage “Was soll das heißen?” als Leitfrage im Umgang mit mathematischen Inhalten und Symbolismen immer im Vordergrund.\nAls Sprachgebäude weist die Mathematik einige Besonderheiten auf. Zum einen sind ihre Inhalte oft abstrakt. Dies rührt daher, dass sich die Mathematik um eine möglichst breite Allgemeinverständlichkeit und Anwendbarkeit bemüht. Mathematische Zugänge zu den Phänomenen der Welt sind dabei an einer möglichst einfache Transferierbarkeit von Erkenntnissen in andere Kontexte interessiert. Um dies zu ermöglichen, versucht die Mathematik möglichst genau und verständlich, also im Sinne präziser Begriffe zu arbeiten. Sie geht dabei insbesondere streng hierarchisch vor, so dass an späterer Stelle eingeführte Begriffe oft ein gutes Verständnis der ihnen zugrundeliegenden und an früherer Stelle eingeführten Begriffe voraussetzen.\nDie Genauigkeit der mathematischen Sprache impliziert eine hohe Informationsdichte. Sie ist daher eher nüchtern und lässt überflüssiges weg, so dass in mathematischen Texten im besten Fall alles für die Kommunikation einer Idee relevant ist. Als Rezipient:in mathematischer Texte nimmt man die Informationsdichte mathematischer Texte anhand des hohen Verbrauchs kognitiver Energie beim Lesen eines Textes wahr. Dieser hohe Energieverbrauch gebietet insbesondere Ruhe und Langsamkeit bei einem auf ein gutes Verständnis abzielenden Lesen. Als Leitsatz im Umgang mit mathematischen Texten mag dabei folgendes Zitat dienen: “Einen mathematischen Text kann man nicht lesen wie einen Roman, man muss ihn sich erarbeiten” (Unger (2000)). Nach dem Lesen eines kurzen mathematischen Textes sollte man sich immer kritisch fragen, ob man das Gelesene wirklich verstanden hat oder ob man zur Klärung des Sachverhaltes weitere Quellen heranziehen sollte. Auch ist es hilfreich, sich im Sinne des berühmten Zitats “What I cannot create, I do not understand” von Richard Feynman eigene Aufzeichnungen anzufertigen und mathematische Sprachgebäude selbst nachzukonstruieren.\nMöchte man sich also die Welt der naturwissenschaftliche Modellbildung erschließen, so ist es hilfreich, beim Umgang mit ihrer mathematischen Ausdrucksweise und Symbolik die gleichen Strategien wie beim Erlernen einer Fremdsprache anzuwenden. Hierzu gehört neben dem Eintauchen in den entsprechenden Sprachraum, also der ständigen Exposition mit mathematischen Ausdrucksweisen, sicherlich auch zunächst einmal das Auswendiglernen von Begriffen und das Übersetzen von Texten in die Alltagssprache. Ein tiefes und sicheres Verständnis mathematischer Modellbildung ergibt sich dann insbesondere durch die Anwendung mathematischer Herangehensweisen in schriftlicher und mündlicher Form.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sprache und Logik</span>"
    ]
  },
  {
    "objectID": "101-Sprache-und-Logik.html#sec-grundbausteine",
    "href": "101-Sprache-und-Logik.html#sec-grundbausteine",
    "title": "1  Sprache und Logik",
    "section": "1.2 Grundbausteine",
    "text": "1.2 Grundbausteine\nIm Folgenden stellen wir mit den Begriffen der Definition, des Theorems und des Beweises drei Grundbausteine mathematischer Kommunikation vor, die uns durchgängig begleiten.\nDefinition\nEine Definition ist eine Grundannahme eines mathematischen Systems, die innerhalb dieses Systems weder begründet noch deduktiv abgeleitet wird. Definitionen können nur nach ihrer Nützlichkeit innerhalb eines mathematischen Systems bewertet werden. Eine Definition lernt man am besten erst einmal auswendig und hinterfragt sie erst dann, wenn man ihren Nutzen in der Anwendung verstanden hat oder von diesem nicht überzeugt ist. Etwas Entspannung und Ruhe beim Umgang mit auf den ersten Blick komplexen Definitionen ist generell hilfreich. Um zu kennzeichnen, dass wir ein Symbol als etwas definieren, nutzen wir die Schreibweise “\\(:=\\)”. Zum Beispiel definiert der Ausdruck “\\(a := 2\\)” das Symbol \\(a\\) als die Zahl Zwei. Definitionen enden in diesem Text immer mit dem Symbol \\(\\bullet\\).\nTheorem\nEin Theorem ist eine mathematische Aussage, die mittels eines Beweises als wahr (richtig) erkannt werden kann. Das heißt, ein Theorem wird immer aus Definitionen und/oder anderen Theoremen hergeleitet. Theoreme sind in diesem Sinne die empirischen Ergebnisse der Mathematik. Im Deutschen werden Theoreme oft auch als Sätze bezeichnet. In der angewandten, datenanalytischen Mathematik sind Theoreme häufig für Berechnungen hilfreich. Es lohnt sich also, sie auswendig zu lernen, da sie meist die Grundlage für Datenauswertung und Dateninterpretation bilden. Oft tauchen in Theoremen Gleichungen auf. Diese ergeben sich dabei aus den Voraussetzungen des Theorems. Um Gleichungen zu kennzeichnen nutzen wir das Gleichheitszeichen “\\(=\\)”. So besagt also beispielsweise der Ausdruck “\\(a = 2\\)” in einem gegebenen Kontext, dass aufgrund bestimmter Voraussetzungen das Symbol oder die Variable \\(a\\) den Wert zwei hat. Theoreme enden in diesem Text immer mit dem Symbol \\(\\circ\\).\nBeweis\nEin Beweis ist eine logische Argumentationskette, die auf bekannte Definitionen und Theoreme zurückgreift, um die Wahrheit (Richtigkeit) eines Theorems zu belegen. Kurze Beweise tragen dabei oft zum Verständnis eines Theorems bei, lange Beweise eher nicht. Beweise sind also insbesondere die Antwort auf die Frage, warum eine mathematische Aussage gilt (“Warum ist das so?”). Beweise lernt man nicht auswendig. Wenn Beweise kurz sind, ist es sinnvoll, sie durchzuarbeiten, da sie meist als bekannt vorausgesetzte Inhalte wiederholen. Wenn sie lang sind, ist es sinnvoller sie zunächst zu übergehen, um sich nicht in Details zu verlieren und vom eigentlichen Weg durch das mathematische Gebäude abzukommen. Beweise enden in diesem Text immer mit dem Symbol \\(\\Box\\).\nNeben Definitionen, Theoremen und Beweisen gibt es mit Axiomen, Lemmata, Korollaren und Vermutungen noch weitere typische Bausteine mathematischer Texte. Wir werden diese Begriff nicht verwenden und geben deshalb für sie nur einen kurzen Überblick.\n\nAxiome sind unbeweisbare Theoreme, in dem Sinne, als dass sie als Grundannahmen zum Aufbau mathematischer Systeme dienen. Der Übergang zwischen Definitionen und Axiomen ist dabei oft fließend. Da wir mathematisch nicht besonders tief arbeiten, bevorzugen wir in den allermeisten Fällen den Begriff der Definition.\nEin Lemma ist ein “Hilfstheorem”, also eine mathematische Aussage, die zwar bewiesen wird, aber nicht so bedeutend ist wie ein Theorem. Da wir einerseits auf bedeutende Inhalte fokussieren und andererseits mathematische Aussagen nicht diskriminieren wollen, verzichten wir auf diesen Begriff und nutzen stattdessen durchgängig den Begriff des Theorems.\nEin Korollar ist eine mathematische Aussage, die sich durch einen einfachen Beweis aus einem Theorem ergibt. Da die “Einfachheit” mathematischer Beweise eine relative Eigenschaft ist, verzichten wir auf diesen Begriff und nutzen stattdessen auch hier durchgängig den Begriff des Theorems.\nVermutungen sind mathematische Aussagen, von denen unbekannt ist, ob sie beweisbar oder widerlegbar sind. Da wir im Bereich der angewandten Mathematik arbeiten, treffen wir nicht auf Vermutungen.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sprache und Logik</span>"
    ]
  },
  {
    "objectID": "101-Sprache-und-Logik.html#sec-aussagenlogik",
    "href": "101-Sprache-und-Logik.html#sec-aussagenlogik",
    "title": "1  Sprache und Logik",
    "section": "1.3 Aussagenlogik",
    "text": "1.3 Aussagenlogik\nNachdem wir nun einige Grundbausteine mathematischer Modellbildung kennengelernt haben, wollen wir uns mit der Aussagenlogik einem einfachem System nähern, das es erlaubt, Beziehungen zwischen mathematischen Aussagen herzustellen und zu formalisieren. Die Aussagenlogik spielt zum Beispiel in der Definition von Mengenoperationen, bei Optimierungsbedingungen von Funktionen und in vielen Beweisen einen tragende Rolle. In der datenanalytischen Anwendung ist die Aussagenlogik die Grundlage der Booleschen Logik der Programmierung. In der mathematischen Psychologie schließlich ist die Aussagenlogik beispielsweise die Grundlage der Repräsentationstheorie des Messens.\nWir beginnen mit der Definition des Begriffs der mathematischen Aussage.\n\nDefinition 1.1 (Aussage) Eine Aussage ist ein Satz, dem eindeutig die Eigenschaft wahr oder falsch zugeordnet werden kann.\n\nDas Adjektiv wahr kann auch als richtig verstanden werden. Wir kürzen wahr mit “w” und falsch mit “f” ab. Im Körper der reellen Zahlen ist zum Beispiel die Aussage \\(1 + 1 = 2\\) wahr und die Aussage \\(1 + 1 = 3\\) falsch. Man beachte, dass die Binärität des Wahrheitsgehalts von Aussagen eine Grundannahme der Aussagenlogik und damit formal wissenschaftlich und nicht empirisch zu verstehen ist. Wahrheitsgehalte beziehen sich nicht auf Definitionen, Definitionen sind immer wahr.\nEine erste Möglichkeit, mit Aussagen zu arbeiten, ist, sie zu negieren. Dies führt auf folgende Definition.\n\nDefinition 1.2 (Negation) \\(A\\) sei eine Aussage. Dann ist die Negation von \\(A\\) die Aussage, die falsch ist, wenn \\(A\\) wahr ist und die wahr ist, wenn \\(A\\) falsch ist. Die Negation von \\(A\\) wird mit \\(\\neg A\\), gesprochen als “nicht \\(A\\)”, bezeichnet.\n\nBeispielsweise ist die Negation der Aussage “Die Sonne scheint” die Aussage “Die Sonne scheint nicht”. Die Negation der Aussage \\(1 + 1 = 2\\) ist die Aussage \\(1 + 1 \\neq 2\\) und die Negation der Aussage \\(x&gt;1\\) ist die Aussage \\(x \\le 1\\). Tabellarisch stellt man die Definition der Negation einer Aussage \\(A\\) wie folgt dar:\n\n\n\nTabelle 1.1: Wahrheitstafel der Negation\n\n\n\n\n\n\\(A\\)\n\\(\\neg A\\)\n\n\n\n\nw\nf\n\n\nf\nw\n\n\n\n\n\n\nTabellen dieser Form nennt man Wahrheitstafeln. Sie sind ein beliebtes Hilfsmittel in der Aussagenlogik, das wir im Folgenden oft einsetzen werden.\nMöchte man zwei Aussagen logisch verbinden, so bieten sich zunächst die Begriffe der Konjunktion und Disjunktion an.\n\nDefinition 1.3 (Konjunktion) \\(A\\) und \\(B\\) seien Aussagen. Dann ist die Konjunktion von \\(A\\) und \\(B\\) die Aussage, die dann und nur dann wahr ist, wenn \\(A\\) und \\(B\\) beide wahr sind. Die Konjunktion von \\(A\\) und \\(B\\) wird mit \\(A \\land B\\), gesprochen als “\\(A\\) und \\(B\\)”, bezeichnet.\n\nDie Definition der Konjunktion impliziert folgende Wahrheitstafel:\n\n\n\nTabelle 1.2: Wahrheitstafel der Konjunktion\n\n\n\n\n\n\\(A\\)\n\\(B\\)\n\\(A \\land B\\)\n\n\n\n\nw\nw\nw\n\n\nw\nf\nf\n\n\nf\nw\nf\n\n\nf\nf\nf\n\n\n\n\n\n\nAls Beispiel sei \\(A\\) die Aussage \\(2\\ge1\\) und \\(B\\) die Aussage \\(2&gt;1\\). Da sowohl \\(A\\) und \\(B\\) wahr sind, ist auch die Aussage \\(2 \\ge 1 \\land 2 &gt; 1\\) wahr. Als weiteres Beispiel sei \\(A\\) die Aussage \\(1\\ge 1\\) und \\(B\\) die Aussage \\(1&gt;1\\). Hier ist nur \\(A\\) wahr und \\(B\\) falsch. Also ist die Aussage \\(1 \\ge 1 \\land 1 &gt; 1\\) falsch.\n\nDefinition 1.4 (Disjunktion) \\(A\\) und \\(B\\) seien Aussagen. Dann ist die Disjunktion von \\(A\\) und \\(B\\) die Aussage, die dann und nur dann wahr ist, wenn mindestens eine der beiden Aussagen \\(A\\) und \\(B\\) wahr ist. Die Disjunktion von \\(A\\) und \\(B\\) wird mit \\(A \\lor B\\), gesprochen als “\\(A\\) oder \\(B\\)”, bezeichnet.\n\nDie Definition der Disjunktion impliziert folgende Wahrheitstafel:\n\n\n\nTabelle 1.3: Wahrheitstafel der Disjunktion\n\n\n\n\n\n\\(A\\)\n\\(B\\)\n\\(A \\lor B\\)\n\n\n\n\nw\nw\nw\n\n\nw\nf\nw\n\n\nf\nw\nw\n\n\nf\nf\nf\n\n\n\n\n\n\n\\(A \\lor B\\) ist also insbesondere auch dann wahr, wenn \\(A\\) und \\(B\\) beide wahr sind. Damit ist das hier betrachtete “oder” genauer ein “und/oder”. Man nennt die Disjunktion daher auch ein “nicht-exklusives oder”. Als Beispiel sei \\(A\\) die Aussage \\(2\\ge1\\) und \\(B\\) die Aussage \\(2&gt;1\\). \\(A\\) ist wahr und \\(B\\) ist wahr. Also ist die Aussage \\(2 \\ge 1 \\lor 2 &gt; 1\\) wahr. Sei nun wiederrum \\(A\\) die Aussage \\(1\\ge 1\\) wahr und \\(B\\) die Aussage \\(1&gt;1\\). Dann ist \\(A\\) wahr und \\(B\\) falsch. Also ist die Aussage \\(1 \\ge 1 \\lor 1 &gt; 1\\) wahr.\nEine Möglichkeit, Aussagen in einen mechanischen logischen Zusammenhang zu stellen, ist die Implikation. Diese ist wie folgt definiert.\n\nDefinition 1.5 (Implikation) \\(A\\) und \\(B\\) seien Aussagen. Dann ist die Implikation, bezeichnet mit \\(A \\Rightarrow B\\), die Aussage, die dann und nur dann falsch ist, wenn \\(A\\) wahr und \\(B\\) falsch ist. \\(A\\) heißt dabei die Voraussetzung (Prämisse) und \\(B\\) der Schluss (Konklusion) der Implikation. \\(A \\Rightarrow B\\) spricht man als “aus \\(A\\) folgt \\(B\\)”, “\\(A\\) impliziert \\(B\\)”, oder “wenn \\(A\\), dann \\(B\\)”.\n\nDie Definition der Implikation kann durch folgende Wahrheitstafel dargestellt werden:\n\n\n\nTabelle 1.4: Wahrheitstafel der Implikation\n\n\n\n\n\n\\(A\\)\n\\(B\\)\n\\(A \\Rightarrow B\\)\n\n\n\n\nw\nw\nw\n\n\nw\nf\nf\n\n\nf\nw\nw\n\n\nf\nf\nw\n\n\n\n\n\n\nEin intuitives Verständnis der Definition der Implikation im Sinne obiger Wahrheitstafel ergibt sich am ehesten, indem man sie als Versuch liest, die intuitive Vorstellung einer Folgerung im Kontext der Aussagenlogik abzubilden und zu formalisieren. Um dies nachzuvollziehen, liest man die Wahrheitszustände der obige Tabelle am besten in der Reihenfolge Wahrheitszustand von \\(A\\), Wahrheitszustand von \\(A \\Rightarrow B\\) und betrachtet schließlich den Wahrheitszustand von \\(B\\). Liest man die Wahrheitstafel auf diese Weise, so sieht man, dass wenn \\(A\\) wahr ist und \\(A \\Rightarrow B\\) wahr ist, \\(B\\) wahr ist. Konstruiert man basierend auf einer wahren Aussage also (zum Beispiel durch das Umformen von Gleichungen) eine wahre Implikation, so folgt, dass auch \\(B\\) wahr ist. Ist dies nicht möglich, wenn also gilt, dass, wenn \\(A\\) wahr ist, \\(A \\Rightarrow B\\) immer falsch ist, dann ist auch \\(B\\) falsch. So mag man Aussagen widerlegen. Schließlich sieht man, dass wenn \\(A\\) falsch ist und \\(A \\Rightarrow B\\) wahr ist, \\(B\\) wahr oder falsch sein kann. Nur aus einer wahren Voraussetzung folgt also bei wahrer Implikation immer eine wahre Konklusion. Insbesondere genügt die Definition der Implikation damit der Forderung “Aus Falschem folgt beliebiges (ex falso sequitur quodlibet)”. Man kann aus falschen Aussagen also mithilfe der Implikation nichts Sinnvolles folgern.\nIm Kontext der Implikation ergeben sich die Begriffe der hinreichenden und der notwendigen Bedingungsaussagen: Wenn \\(A \\Rightarrow B\\) wahr ist, sagt man, dass “\\(A\\) hinreichend für \\(B\\) ist” und dass “\\(B\\) notwendig für \\(A\\) ist”. Diese Sprechweise erklärt sich im Kontext der Implikation folgendermaßen: Wenn \\(A \\Rightarrow B\\) wahr ist, gilt dass, wenn \\(A\\) wahr ist, auch \\(B\\) wahr ist. Die Wahrheit von \\(A\\) reicht also für die Wahrheit von \\(B\\) aus. \\(A\\) ist also hinreichend (ausreichend) für \\(B\\). Weiterhin gilt, dass wenn \\(A \\Rightarrow B\\) wahr ist, dass wenn \\(B\\) falsch ist, dann auch \\(A\\) falsch ist. Die Wahrheit von \\(B\\) ist also für die Wahrheit von \\(A\\) notwendig.\nEine sehr häufig autretender Zusammenhang zwischen zwei Aussagen ist ihre Äquivalenz.\n\nDefinition 1.6 (Äquivalenz) \\(A\\) und \\(B\\) seien Aussagen. Die Äquivalenz von \\(A\\) und \\(B\\) ist die Aussage, die dann und nur dann wahr ist, wenn \\(A\\) und \\(B\\) beide wahr sind oder wenn \\(A\\) und \\(B\\) beide falsch sind. Die Äquivalenz von \\(A\\) und \\(B\\) wird mit \\(A \\Leftrightarrow B\\) bezeichnet und gesprochen als “\\(A\\) genau dann wenn \\(B\\)” oder “\\(A\\) ist äquivalent zu \\(B\\)”.\n\nDie Definition der Äquivalenz impliziert folgende Wahrheitstafel:\n\n\n\nTabelle 1.5: Wahrheitstafel der Äquivalenz\n\n\n\n\n\n\\(A\\)\n\\(B\\)\n\\(A \\Leftrightarrow B\\)\n\n\n\n\nw\nw\nw\n\n\nw\nf\nf\n\n\nf\nw\nf\n\n\nf\nf\nw\n\n\n\n\n\n\nDie Definition des Begriffes der logischen Äquivalenz erlaubt es unter anderem, die Äquivalenz zweier Aussagen mithilfe von Implikationen nachzuweisen.\n\nDefinition 1.7 (Logische Äquivalenz) Zwei Aussagen heißen logisch äquivalent, wenn ihre Wahrheitstafeln gleich sind.\n\nAls Beispiele für logische Äquivalenzen, die häufig in Beweisargumentationen genutzt werden, zeigen wir die Aussagen folgenden Theorems.\n\nTheorem 1.1 (Logische Äquivalenzen) \\(A\\) und \\(B\\) seien zwei Aussagen. Dann sind folgende Aussagen logisch äquivalent:\n\n\\(A \\Leftrightarrow B\\) und \\((A \\Rightarrow B) \\land (B \\Rightarrow A)\\)\n\\(A \\Rightarrow B\\) und \\((\\neg B) \\Rightarrow (\\neg A)\\)\n\n\n\nBeweis. Nach Definition des Begriffs der logischen Äquivalenz müssen wir zeigen, dass die Wahrheitstafeln der betrachteten Aussagen gleich sind. Wir zeigen erst (1), dann (2).\n(1) Wir erinnern an die Wahrheitstafel von \\(A \\Leftrightarrow B\\):\n\n\n\n\\(A\\)\n\\(B\\)\n\\(A \\Leftrightarrow B\\)\n\n\n\n\nw\nw\nw\n\n\nw\nf\nf\n\n\nf\nw\nf\n\n\nf\nf\nw\n\n\n\nWir betrachten weiterhin die Wahrheitstafel von \\((A \\Rightarrow B) \\land (B \\Rightarrow A)\\):\n\n\n\n\n\n\n\n\n\n\n\\(A\\)\n\\(B\\)\n\\(A \\Rightarrow B\\)\n\\(B \\Rightarrow A\\)\n\\((A \\Rightarrow B) \\land (B \\Rightarrow A)\\)\n\n\n\n\nw\nw\nw\nw\nw\n\n\nw\nf\nf\nw\nf\n\n\nf\nw\nw\nf\nf\n\n\nf\nf\nw\nw\nw\n\n\n\nDer Vergleich der Wahrheitstafel von \\(A \\Leftrightarrow\\) mit den ersten beiden und der letzten Spalte der Wahrheitstafel von \\((A \\Rightarrow B) \\land (B \\Rightarrow A)\\) zeigt ihre Gleichheit.\n(2) Wir erinnern an die Wahrheitstafel von \\(A \\Rightarrow B\\):\n\n\n\n\\(A\\)\n\\(B\\)\n\\(A \\Rightarrow B\\)\n\n\n\n\nw\nw\nw\n\n\nw\nf\nf\n\n\nf\nw\nw\n\n\nf\nf\nw\n\n\n\nWir betrachten weiterhin die Wahrheitstafel von \\((\\neg B) \\Rightarrow (\\neg A)\\):\n\n\n\n\n\n\n\n\n\n\n\\(A\\)\n\\(B\\)\n\\(\\neg B\\)\n\\(\\neg A\\)\n\\((\\neg B) \\Rightarrow (\\neg A)\\)\n\n\n\n\nw\nw\nf\nf\nw\n\n\nw\nf\nw\nf\nf\n\n\nf\nw\nf\nw\nw\n\n\nf\nf\nw\nw\nw\n\n\n\nDer Vergleich der Wahrheitstafel von \\(A \\Rightarrow B\\) mit den ersten beiden und der letzten Spalte der Wahrheitstafel von \\((\\neg B) \\Rightarrow (\\neg A)\\) zeigt ihre Gleichheit.\n\nDie erste Aussage von Theorem 1.1 besagt, dass die Aussage “\\(A\\) und \\(B\\) sind äquivalent” logisch äquivalent zur Aussage “Aus \\(A\\) folgt \\(B\\)” und zur Aussage “Aus \\(B\\) folgt \\(A\\)” ist. Dies ist die Grundlage für viele sogenannte direkte Beweise mithilfe von Äquivalenzumformungen. Die zweite Aussage von Theorem 1.1 besagt, dass die Aussage “Aus \\(A\\) folgt \\(B\\)” logisch äquivalent zur Aussage “Aus nicht \\(B\\) folgt nicht \\(A\\)” ist. Dies ist die Grundlage für die Technik des indirekten Beweises. Wir betrachten diese Beweistechniken im folgenden Abschnitt genauer. Zunächst fassen wir die Bedeutungen der in diesem Abschnitt eingeführte Symbole noch einmal in untenstehender Tabelle zusammen.\n\n\n\nTabelle 1.6: Symbolübersicht. \\(A\\) und \\(B\\) sind Aussagen, d.h. \\(A\\) und \\(B\\) sind entweder wahr oder falsch.\n\n\n\n\n\n\n\n\n\n\nSymbol\nBedeutung\nBemerkung\n\n\n\n\n\\(\\neg A\\)\nNicht \\(A\\)\nWahr, wenn \\(A\\) falsch ist und umgekehrt\n\n\n\\(A \\land B\\)\n\\(A\\) und \\(B\\)\nNur wahr, wenn \\(A\\) und \\(B\\) beide wahr sind\n\n\n\\(A \\lor B\\)\n\\(A\\) und/oder \\(B\\)\nWahr, wenn mindestens eine der Aussagen wahr ist\n\n\n\\(A \\Rightarrow B\\)\nAus \\(A\\) folgt \\(B\\)\n\\(B\\) ist notwendig für \\(A\\), \\(A\\) ist hinreichend für \\(A\\)\n\n\n\\(A \\Leftrightarrow B\\)\n\\(A\\) ist äquivalent zu \\(B\\)\nEs gelten \\(A \\Rightarrow B\\) und \\(B \\Rightarrow A\\)",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sprache und Logik</span>"
    ]
  },
  {
    "objectID": "101-Sprache-und-Logik.html#äquivalenzumformungen",
    "href": "101-Sprache-und-Logik.html#äquivalenzumformungen",
    "title": "1  Sprache und Logik",
    "section": "1.4 Äquivalenzumformungen",
    "text": "1.4 Äquivalenzumformungen\nMathematische Probleme führen oft auf den Fall, dass Information über eine unbekannte Variable implizit mithilfe einer Gleichung oder einer Ungleichung dargestellt wird. Um die Information über die entsprechende Variable explizit darzustellen, also im Falle von Gleichungen ihren Wert zu bestimmen oder im Falle von Ungleichungen den Zahlenbereich zu ermitteln, in dem der Wert der Variable liegt, nutzt man Äquivalenzumformungen. Wir betrachten hier lediglich die aus der Schulmathematik bekannten Äquivalenzumformungen bezüglich Gleichungen und Ungleichungen reeller Variablen. Äquivalenzumformungen von Gleichungen und Ungleichungen haben dabei die Eigenschaft, dass sich der Wahrheitsgehalt der durch eine Gleichung oder Ungleichung formulierten Aussage bei Anwendung der entsprechenden Umformung nicht ändert. Dabei werden stets beide Seiten der Gleichung oder Ungleichung umgeformt. Damit es sich bei der Anwendung einer mathematischen Operation auf eine Gleichungs- oder Ungleichungsaussage \\(A\\) in eine Gleichungs- bzw. Ungleichungsaussage \\(B\\) um eine Äquivalenzumformung der Form \\(A \\Leftrightarrow B\\) handelt, müssen bekanntlich sowohl \\(A \\Rightarrow B\\) als auch \\(B \\Rightarrow A\\) gelten. Dies impliziert, dass es sich bei Äquivalenzumformungen um umkehrbare (invertierbare) Operationen handelt.\nBei Gleichungen sind zulässige Äquivalenzumformungen insbesondere\n\ndie Addition einer Zahl auf beiden Seiten der Gleichung\ndie Subtraktion einer Zahl auf beiden Seiten der Gleichung\ndie Multiplikation mit einer Zahl auf beiden Seiten der Gleichung\ndie Division durch eine von Null verschiedenen Zahl auf beiden Seiten der Gleichung\ndie Anwendung einer invertierbaren Funktion auf beiden Seiten der Gleichung\n\nBeispiel\nBetrachten wir beispielsweise unter Vorgriff auf Kapitel 4.3 die Aussage \\[\\begin{equation}\n2 \\exp(x) - 2 = 0.\n\\end{equation}\\] Dann gelten \\[\\begin{align}\n\\begin{split}\n                2 \\exp(x) - 2                   & = 0                           \\\\\n\\Leftrightarrow 2 \\exp(x) - 2 + 2               & = + 2                         \\\\\n\\Leftrightarrow 2 \\exp(x)                       & = 2                           \\\\\n\\Leftrightarrow  \\frac{1}{2} \\cdot 2 \\exp(x)    & =  \\frac{1}{2} \\cdot 2        \\\\\n\\Leftrightarrow  \\exp(x)                        & =  1                          \\\\       \n\\Leftrightarrow  \\ln(\\exp(x))                   & =  \\ln(1)                     \\\\\n\\Leftrightarrow  x                              & =  0.                         \\\\\n\\end{split}\n\\end{align}\\] Zusammengefasst gilt also \\[\\begin{equation}\n2 \\exp(x) - 2 \\Leftrightarrow x = 0.\n\\end{equation}\\]\nBei Ungleichungen sind zulässige Äquivalenzumformungen insbesondere\n\ndie Addition einer Zahl auf beiden Seiten der Ungleichung\ndie Subtraktion einer Zahl auf beiden Seiten der Ungleichung\ndie Multiplikation mit einer Zahl auf beiden Seiten der Gleichung, wobei die Multiplikation mit einer negativen Zahl das Ungleichungszeichen umkehrt,\ndie Division durch eine von Null verschiedenen positiven Zahl auf beiden Seiten der Gleichung, wobei die Division mit einer negativen Zahl das Ungleichungszeichen umkehrt,\ndie Anwendung einer invertierbaren monotonen Funktion auf beiden Seiten der Ungleichung\n\nBeispiel\nBetrachten wir beispielsweise die Aussage \\[\\begin{equation}\n-5 x - 2 \\ge 8.\n\\end{equation}\\] Dann gelten \\[\\begin{align}\n\\begin{split}\n                -5x - 2                 & \\ge 8                                 \\\\\n\\Leftrightarrow -5x - 2 + 2             & \\ge 8 + 2                             \\\\\n\\Leftrightarrow -5x                     & \\ge 10                                \\\\\n\\Leftrightarrow -\\frac{1}{5} \\cdot 5x   & \\ge \\frac{1}{5} \\cdot 10              \\\\\n\\Leftrightarrow -x                      & \\ge 2                                 \\\\\n\\Leftrightarrow  x                      & \\le 2.                                \\\\\n\\end{split}\n\\end{align}\\] Zusammengefasst gilt also \\[\\begin{equation}\n-5x - 2 \\ge 8   \\Leftrightarrow x \\le 2.\n\\end{equation}\\]",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sprache und Logik</span>"
    ]
  },
  {
    "objectID": "101-Sprache-und-Logik.html#sec-beweistechniken",
    "href": "101-Sprache-und-Logik.html#sec-beweistechniken",
    "title": "1  Sprache und Logik",
    "section": "1.5 Beweistechniken",
    "text": "1.5 Beweistechniken\nIn diesem Abschnitt wollen wir mit den Begriffen der direkten und indirekten Beweise sowie des Beweises durch Widerspruch drei fundamentale Beweistechniken skizzieren. Dabei wird vor allem die erste im Folgenden immer wieder zur Begründung von Theoremen herangezogen werden.\nEs gilt dabei\n\nDirekte Beweise nutzen Äquivalenzumformungen, um \\(A \\Rightarrow B\\) zu zeigen.\nIndirekte Beweise nutzen die logische Äquivalenz von \\(A \\Rightarrow B\\) und \\((\\neg B) \\Rightarrow (\\neg A)\\).\nBeweise durch Widerspruch zeigen, dass \\((\\neg B) \\land A\\) falsch ist.\n\nDamit ausgestattet wollen wir nun folgendes Theorem mithilfe eines direkten Beweises, eines indirekten Beweises und eines Beweises durch Widerspruch beweisen (vgl. Arens et al. (2018)).\n\nTheorem 1.2 (Quadrate positiver Zahlen) Es seien \\(a\\) und \\(b\\) zwei positive Zahlen. Dann gilt \\(a^2 &lt; b^2 \\Rightarrow a &lt; b\\).\n\n\nBeweis. Wir geben zunächst einen direkten Beweis. Dazu sei \\(a^2 &lt; b^2\\) die Aussage \\(A\\) und \\(a &lt; b\\) die Aussage \\(B\\). Dann gilt \\[\\begin{equation}\na^2 &lt; b^2\n\\Leftrightarrow 0 &lt; b^2 - a^2\n\\Leftrightarrow 0 &lt; (b+a)(b-a)\n\\Leftrightarrow 0 &lt; (b-a)\n\\Leftrightarrow a &lt; b.\n\\end{equation}\\] Wir geben nun einen indirekten Beweis. Es sei \\(a^2 \\ge b^2\\) die Aussage \\(\\neg A\\). Weiterhin sei \\(a \\ge b\\) die Aussage \\(\\neg B\\). Dann gilt \\[\\begin{equation}\na \\ge b\n\\Leftrightarrow a^2 \\ge ab \\land ab \\ge b^2\n\\Leftrightarrow a^2 \\ge b^2.\n\\end{equation}\\] Schließlich geben wir einen Beweis durch Widerspruch. Wir zeigen, dazu, dass die Annahme \\((\\neg B) \\land A\\) auf eine falsche Aussage führt. Es gilt \\[\\begin{equation}\na \\ge b \\land a^2 &lt; b^2 \\Leftrightarrow  a^2  \\ge ab \\land a^2 &lt; b^2   \\Leftrightarrow ab \\le a^2 &lt; b^2.\n\\end{equation}\\] Weiterhin gilt \\[\\begin{equation}\na \\ge b \\land a^2 &lt; b^2 \\Leftrightarrow  ab  \\ge b^2 \\land a^2 &lt; b^2   \\Leftrightarrow a^2 &lt; b^2 \\le ab.\n\\end{equation}\\] Insgesamt gilt dann also die falsche Aussage \\[\\begin{equation}\nab \\le a^2 &lt; b^2 \\le ab \\Leftrightarrow ab &lt; ab.\n\\end{equation}\\]",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sprache und Logik</span>"
    ]
  },
  {
    "objectID": "101-Sprache-und-Logik.html#sec-selbstkontrollfragen-sprache-und-logik",
    "href": "101-Sprache-und-Logik.html#sec-selbstkontrollfragen-sprache-und-logik",
    "title": "1  Sprache und Logik",
    "section": "1.6 Selbstkontrollfragen",
    "text": "1.6 Selbstkontrollfragen\n\nErläutern Sie die Besonderheiten der mathematischen Sprache.\nWas sind wesentliche Tätigkeiten zum Erlernen einer Sprache?\nErläutern Sie den Begriff der Definition.\nErläutern Sie den Begriff des Theorems.\nErläutern Sie den Begriff des Beweises.\nGeben Sie die Definition einer mathematischen Aussage wieder.\nGeben Sie die Definition der Negation einer mathematischen Aussage wieder.\nGeben Sie die Definition der Konjunktion zweier mathematischer Aussagen wieder.\nGeben Sie die Definition der Disjunktion zweier mathematischer Aussagen wieder.\nGeben Sie die Definition der Implikation wieder.\nGeben Sie die Definition der Äquivalenz wieder.\nGeben Sie die Definition der logischen Äquivalenz wieder.\nErläutern Sie die Begriffe des direkten Beweises, des indirekten Beweises und des Beweises durch Widerspruch.\n\n\n\n\n\nArens, T., Hettlich, F., Karpfinger, C., Kockelkorn, U., Lichtenegger, K., & Stachel, H. (2018). Mathematik. Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-662-56741-8\n\n\nFriston, K. (2005). A Theory of Cortical Responses. Philosophical Transactions of the Royal Society B: Biological Sciences, 360(1456), 815–836. https://doi.org/10.1098/rstb.2005.1622\n\n\nNewton, I. (1687). Philosophiae Naturalis Principia Mathematica. Royal Society.\n\n\nUnger, L. (2000). Grundkurs Mathematik.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sprache und Logik</span>"
    ]
  },
  {
    "objectID": "102-Mengen.html",
    "href": "102-Mengen.html",
    "title": "2  Mengen",
    "section": "",
    "text": "2.1 Grundlegende Definitionen\nMengen fassen mathematische Objekte wie beispielsweise Zahlen zusammen und bilden die Grundlage der modernen Mathematik. Wir beginnen mit folgender Definition.\nZur Definition von Mengen gibt es mindestens folgende Möglichkeiten:\nDie Schreibweise \\(\\{x \\in \\mathbb{N}|x &lt; 4\\}\\) wird gelesen als “\\(x \\in \\mathbb{N}\\), für die gilt, dass \\(x &lt; 4\\) ist”, wobei die Bedeutung von \\(\\mathbb{N}\\) im Folgenden noch zu erläutern sein wird. Es ist wichtig zu erkennen, dass Mengen ungeordnete mathematische Objekte sind, dass heißt die Reihenfolge der Auflistung der Elemente einer Menge spielt keine Rolle. Zum Beispiel bezeichnen \\(\\{1,2,3\\}\\), \\(\\{1,3,2\\}\\) und \\(\\{2,3,1\\}\\) dieselbe Menge, nämlich die Menge der ersten drei natürlichen Zahlen.\nGrundlegende Beziehungen zwischen mehreren Mengen werden in der nächsten Definition festgelegt.\nBeispiel\nBetrachten wir zum Beispiel die Mengen \\(M := \\{1\\}\\), \\(N := \\{1,2\\}\\), und \\(O := \\{1,2\\}\\). Dann gilt mit obigen Definitionen, dass \\(M \\subset N\\), weil \\(1 \\in M\\) und \\(1 \\in N\\), aber \\(2 \\in N\\) und \\(2 \\notin M\\). Weiterhin gilt, dass \\(N \\subseteq O\\), weil \\(1 \\in N\\) und \\(1 \\in O\\) sowie \\(2 \\in N\\) und \\(2 \\in O\\) und es kein Element von \\(O\\) gibt, welches nicht in \\(N\\) ist. Ebenso gilt \\(O \\subseteq N\\), weil \\(1 \\in O\\) und \\(1 \\in N\\) sowie \\(2 \\in O\\) und \\(2 \\in N\\) und es kein Element von \\(N\\) gibt, welches nicht in \\(O\\) ist. Schließlich gilt sogar \\(N = O\\), weil für jedes Element \\(n \\in N\\) gilt, dass auch \\(n \\in O\\), und gleichzeitig für jedes Element \\(o \\in O\\) gilt, dass auch \\(o\\in N\\). Wir stellen diese Zusammenhänge schematisch mit Hilfe von Venn-Euler-Diagrammen in Abbildung 2.1 dar.\nEine wichtige Eigenschaft einer Menge ist die Anzahl der in ihr enthaltenen Elemente. Diese wird als Kardinalität der Menge bezeichnet.\nEine besondere Menge ist die Menge ohne Elemente.\nAls Beispiele seien \\(M := \\{1,2,3\\}\\), \\(N = \\{a,b,c,d\\}\\) und \\(O := \\{\\,\\}\\). Dann gelten \\(|M| = 3\\), \\(N = 4\\) und \\(|O| = 0\\).\nZu jeder Menge kann man die Menge aller Teilmengen dieser Menge betrachten. Dies führt auf den wichtigen Begriff der Potenzmenge.\nMan beachte, dass die leere Untermenge von \\(M\\) und \\(M\\) selbst auch immer Elemente von \\(\\mathcal{P}(M)\\) sind. Wir betrachten zunächst vier Beispiele zum Begriff der Potenzmenge.\nIn den obigen Beispielen haben wir die Fälle\nwovon man sich durch Nachzählen der Elemente der entsprechenden Potenzmengen oben überzeugt.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Mengen</span>"
    ]
  },
  {
    "objectID": "102-Mengen.html#grundlegende-definitionen",
    "href": "102-Mengen.html#grundlegende-definitionen",
    "title": "2  Mengen",
    "section": "",
    "text": "Definition 2.1 (Mengen) Nach Cantor (1895) ist eine Menge definiert als “eine Zusammenfassung \\(M\\) von bestimmten wohlunterschiedenen Objekten \\(m\\) unsere Anschauung oder unseres Denken (welche die Elemente der Menge genannt werden) zu einem Ganzen”. Wir schreiben \\[\\begin{equation}\nm \\in M \\mbox{ bzw. } m \\notin M\n\\end{equation}\\] um auszudrücken, dass \\(m\\) ein Element bzw. kein Element von \\(M\\) ist.\n\n\n\nAuflisten der Elemente in geschweiften Klammern, z.B. \\(M := \\{1,2,3\\}\\).\nAngabe der Eigenschaften der Elemente, z.B. \\(M := \\{x \\in \\mathbb{N}|x &lt; 4\\}\\).\nGleichsetzen mit einer anderen eindeutig definieren Menge, z.B. \\(M := \\mathbb{N}_3\\).\n\n\n\n\nDefinition 2.2 (Teilmengen und Mengengleichheit) \\(M\\) und \\(N\\) seien zwei Mengen.\n\nEine Menge \\(M\\) heißt Teilmenge einer Menge \\(N\\), wenn für jedes Element \\(m \\in M\\) gilt, dass auch \\(m\\in N\\). Ist \\(M\\) eine Teilmenge von \\(N\\), so schreibt man \\[\\begin{equation}\nM \\subseteq N\n\\end{equation}\\] und nennt \\(M\\) Untermenge von \\(N\\) und \\(N\\) Obermenge von \\(M\\).\nEine Menge \\(M\\) heißt echte Teilmenge einer Menge \\(N\\), wenn für jedes Element \\(m \\in M\\) gilt, dass auch \\(m\\in N\\), es aber zumindest ein Element \\(n \\in N\\) gibt, für das gilt \\(n \\notin M\\). Ist \\(M\\) eine echte Teilmenge von \\(N\\), so schreibt man \\[\\begin{equation}\nM \\subset N.\n\\end{equation}\\]\nZwei Mengen \\(M\\) und \\(N\\) heißen gleich, wenn für jedes Element \\(m \\in M\\) gilt, dass auch \\(m \\in N\\), und wenn für jedes Element \\(n \\in N\\) gilt, dass auch \\(n\n\\in M\\). Sind die Mengen \\(M\\) und \\(N\\) gleich, so schreibt man \\[\\begin{equation}\nM = N.\n\\end{equation}\\]\n\n\n\n\n\n\n\n\n\n\nAbbildung 2.1: Venn-Euler Diagramme der Teilmengeneigenschaften von Mengen \\(M := \\{1\\}\\), \\(N := \\{1,2\\}\\) und \\(O := \\{1,2\\}\\).\n\n\n\n\n\nDefinition 2.3 (Kardinalität) Die Anzahl der Elemente einer Menge \\(M\\) heißt Kardinalität und wird mit \\(|M|\\) bezeichnet.\n\n\n\nDefinition 2.4 Eine Menge mit Kardinalität Null heißt leere Menge und wird mit \\(\\emptyset\\) bezeichnet.\n\n\n\n\nDefinition 2.5 (Potenzmenge) Die Menge aller Teilmengen einer Menge \\(M\\) heißt Potenzmenge von \\(M\\) und wird mit \\(\\mathcal{P}(M)\\) bezeichnet.\n\n\n\n\\(M_0 := \\emptyset\\) sei die leere Menge. Dann gilt \\[\\begin{equation}\n\\mathcal{P}(M_0) = \\emptyset.\n\\end{equation}\\]\n\\(M_1\\) sei die einelementige Menge \\(M_1 := \\{a\\}\\). Dann gilt \\[\\begin{equation}\n\\mathcal{P}(M_1) = \\{\\emptyset,\\{a\\}\\}.\n\\end{equation}\\]\nEs sei \\(M_2 := \\{a,b\\}\\). Dann hat \\(M_2\\) sowohl ein- als auch zweielementige Teilmengen und es gilt \\[\\begin{equation}\n\\mathcal{P}(M_2) = \\{\\emptyset,\\{a\\}\\, \\{b\\}, \\{a,b\\}\\}.\n\\end{equation}\\]\nSchließlich sei \\(M_3 := \\{a,b,c\\}\\). Dann hat \\(M_3\\) unter anderem ein-, zwei-, und dreielementige Teilmengen und es gilt \\[\\begin{equation}\n\\mathcal{P}(M_3) = \\{\\emptyset, \\{a\\},\\{b\\},\\{c\\},\\{a,b\\},\\{a,c\\},\\{b,c\\},\\{a,b,c\\}\\}.\n\\end{equation}\\]\n\n\nTheorem 2.1 (Kardinalität der Potenzmenge) Gegeben sei eine Menge \\(M\\) mit Kardinalität \\(|M| = n\\) und \\(\\mathcal{P}(M)\\) sei ihre Potenzmenge. Dann gilt \\(|\\mathcal{P}(M)| = 2^n\\)\n\n\nBeweis. Um die Aussage des Theorems zu beweisen assoziieren wir jedes Element \\(P\\) der Potenzmenge von \\(M\\) eindeutig mit einer binären Folge der Länge \\(n\\), wobei der Eintrag an der \\(i\\)ten Stelle repräsentiert, ob das \\(i\\)te Element von \\(M\\) ein Element von \\(P\\) ist oder nicht. Seien beispielsweise \\(M := \\{m_1,m_2,m_3\\}\\) und \\(P := \\{m_2,m_3\\}\\). Dann entspricht \\(P\\) die binäre Folge \\(011\\). Der leeren Menge \\(P := \\emptyset\\) entspricht die binäre Folge \\(000\\) und der Ausgangsmenge \\(P = M\\) entspricht die binäre Folge \\(111\\). Es ergibt sich also die Frage, wieviele eindeutige binäre Folgen der Länge \\(n\\) es gibt. Da es für jedes Element der Folge zwei mögliche Zustände gibt, ergeben sich \\(n\\) Faktoren \\(2 \\cdot 2 \\cdots 2\\), also \\(2^n\\).\n\n\n\n\\(|M_0| = 0 \\Rightarrow |\\mathcal{P}(M_0)| = 2^0 = 1\\),\n\\(|M_1| = 1 \\Rightarrow |\\mathcal{P}(M_1)| = 2^1 = 2\\),\n\\(|M_2| = 2 \\Rightarrow |\\mathcal{P}(M_2)| = 2^2 = 4\\),\n\\(|M_3| = 3 \\Rightarrow |\\mathcal{P}(M_3)| = 2^3 = 8\\),",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Mengen</span>"
    ]
  },
  {
    "objectID": "102-Mengen.html#sec-verknuepfungen",
    "href": "102-Mengen.html#sec-verknuepfungen",
    "title": "2  Mengen",
    "section": "2.2 Verknüpfungen",
    "text": "2.2 Verknüpfungen\nZwei Mengen können auf unterschiedliche Weise miteinander verknüpft werden. Das Ergebnis einer solchen Verknüpfung ist eine weitere Menge. Wir bezeichnen die Verknüpfung zweier Mengen als Mengenoperation und geben folgende Definitionen.\n\nDefinition 2.6 (Mengenoperationen) \\(M\\) und \\(N\\) seien zwei Mengen.\n\nDie Vereinigung von \\(M\\) und \\(N\\) ist definiert als die Menge \\[\\begin{equation}\nM \\cup N := \\{x | x \\in M \\lor x \\in N\\},\n\\end{equation}\\] wobei \\(\\lor\\) gemäß Definition 1.4 als nicht-exklusives oder, also als und/oder, zu verstehen ist.\nDer Durchschnitt von \\(M\\) und \\(N\\) ist definiert als die Menge \\[\\begin{equation}\nM \\cap N := \\{x | x \\in M \\land x \\in N\\}.\n\\end{equation}\\] Wenn für \\(M\\) und \\(N\\) gilt, dass \\(M \\cap N= \\emptyset\\), dann heißen \\(M\\) und \\(N\\) disjunkt.\nDie Differenz von \\(M\\) und \\(N\\) ist definiert als die Menge \\[\\begin{equation}\nM\\setminus N := \\{x | x \\in M \\land x \\notin N\\}.\n\\end{equation}\\] Die Differenz \\(M\\) und \\(N\\) heißt, insbesondere bei \\(M \\subseteq N\\), auch das Komplement von \\(N\\) bezüglich \\(M\\) und wird mit \\(N^c\\) bezeichnet.\nDie symmetrische Differenz von \\(M\\) und \\(N\\) ist definiert als die Menge \\[\\begin{equation}\nM \\Delta N := \\{x|(x \\in M \\lor x \\in N ) \\land x \\notin M \\cap N\\},\n\\end{equation}\\] Die symmetrische Differenz kann also als exklusives oder verstanden werden.\n\n\nBeispiel\nAls Beispiel betrachten wir die Mengen \\(M := \\{1,2,3\\}\\)und \\(N := \\{2,3,4,5\\}\\). Dann gelten\n\n\\(M \\cup N = \\{1,2,3,4,5\\}\\), weil \\(1 \\in M\\), \\(2 \\in M\\), \\(3 \\in M\\), \\(4 \\in N\\) und \\(5 \\in N\\).\n\\(M \\cap N = \\{2,3\\}\\), weil nur für \\(2\\) und \\(3\\) gilt, dass \\(2\\in M, 3 \\in M\\) und auch \\(2\\in N, 3 \\in N\\). Für \\(1\\) gilt lediglich, dass \\(1 \\in M\\) und für \\(4\\) und \\(5\\) gelten lediglich, dass \\(4 \\in N\\) und \\(5 \\in N\\).\n\\(M \\setminus N = \\{1\\}\\), weil \\(1 \\in M\\), aber \\(1 \\notin N\\) und \\(2 \\in M\\), aber auch \\(2 \\in N\\).\n\\(N \\setminus M = \\{4,5\\}\\), weil \\(2 \\in N\\) und \\(3 \\in N\\), aber auch \\(2\\in M\\) und \\(3 \\in M\\). Dies zeigt insbesondere, dass die Differenz von \\(M\\) Und \\(N\\) nicht symmetrisch ist, also dass nicht zwangsläufig gilt, dass \\(M\\setminus N\\) gleich \\(N \\setminus M\\) ist.\n\\(M \\Delta N = \\{1,4,5\\}\\), weil \\(1 \\in M\\), aber \\(1 \\notin \\{2,3\\}\\), \\(2 \\in M\\), aber \\(2 \\in \\{2,3\\}\\), \\(3 \\in M\\), aber \\(3 \\in \\{2,3\\}\\), \\(4 \\in N\\), aber \\(4 \\notin \\{2,3\\}\\) und \\(5 \\in N\\), aber \\(5 \\notin \\{2,3\\}\\).\n\nAbbildung 2.2 visualisiert die in diesem Beispiel betrachteten Mengenoperationen.\n\n\n\n\n\n\nAbbildung 2.2: Venn-Euler Diagramme der im Beispiel für \\(M := \\{1,2,3\\}\\) und \\(N := \\{2,3,4,5\\}\\) betrachteten Mengenoperationen. Die grau hinterlegten Flächen entsprechen jeweils den sich ergebenen Mengen.\n\n\n\nSchließlich wollen wir noch den Begriff der Partition einer Menge einführen.\n\nDefinition 2.7 (Partition) \\(M\\) sei eine Menge und \\(P := \\{N_i\\}\\) sei eine Menge von Mengen \\(N_i\\) mit \\(i = 1,...,n\\), so dass gilt \\[\\begin{equation}\nM = \\cup_{i=1}^n N_i \\land N_i \\cap N_j = \\emptyset \\mbox{ für } i = 1,...,n, j = 1,...,n, i \\neq j.\n\\end{equation}\\] Dann heißt \\(P\\) eine Partition von \\(M\\).\n\nIntuitiv entspricht die Partition einer Menge also dem Aufteilen der Menge in disjunkte Teilmengen. Partitionen sind generell nicht eindeutig, d.h. es gibt meist verschiedene Möglichkeiten eine gegebene Menge zu partitionieren.\nAls Beispiel betrachten wir die Menge \\(M := \\{1,2,3,4,5,6\\}\\). Dann sind \\(P_1 := \\{\\{1\\}, \\{2,3,4,5,6\\}\\}\\), \\(P_2 := \\{\\{1,2,3\\}, \\{4,5,6\\}\\}\\) und \\(P_3 := \\{\\{1,2\\},\\{3,4\\}, \\{5,6\\}\\}\\) drei mögliche Partitionen von \\(M\\). Abbildung 2.3 visualisiert die in diesem Beispiel betrachteten Partitionen.\n\n\n\n\n\n\nAbbildung 2.3: Diagramme der im Beispiel für \\(M := \\{1,2,3,4,5,6\\}\\) betrachteten Partitionen.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Mengen</span>"
    ]
  },
  {
    "objectID": "102-Mengen.html#spezielle-mengen",
    "href": "102-Mengen.html#spezielle-mengen",
    "title": "2  Mengen",
    "section": "2.3 Spezielle Mengen",
    "text": "2.3 Spezielle Mengen\n\nZahlenmengen\nIn der Naturwissenschaft versucht man, in der Vorstellung intuitiv als diskret oder kontinuierlich identifizierte Phänomene der Welt mit Zahlen zu beschreiben. Je nach Art des Phänomens bieten sich dazu verschiedene Zahlenmengen an. Die Mathematik stellt unter anderem die in folgender Definition gegebenen Zahlen bereit.\n\nDefinition 2.8 (Zahlenmengen) Es bezeichnen\n\n\\(\\mathbb{N}\\,\\,\\, := \\{1,2,3,...\\}\\) die natürlichen Zahlen,\n\\(\\mathbb{N}_n     := \\{1,2,3,...,n\\}\\) die natürlichen Zahlen der Ordnung \\(n\\),\n\\(\\mathbb{N}^0     := \\mathbb{N} \\cup \\{0\\}\\) die natürlichen Zahlen und Null,\n\\(\\mathbb{Z}\\,\\,\\, := \\{...,-3,-2,-1,0,1,2,3...\\}\\) die ganzen Zahlen,\n\\(\\mathbb{Q}\\,\\,\\, := \\{\\frac{p}{q}|p,q \\in \\mathbb{Z}, q \\neq 0\\}\\) die rationalen Zahlen,\n\\(\\mathbb{R}\\,\\,\\,\\) die reellen Zahlen, und\n\\(\\mathbb{C}\\,\\,\\, := \\{a + ib|a,b\\in \\mathbb{R}, i := \\sqrt{-1} \\}\\) die komplexen Zahlen.\n\n\nDie natürlichen und ganzen Zahlen eignen sich insbesondere zum Quantifizieren diskreter Phänomene. Die rationalen und insbesondere die reellen Zahlen eignen sich zum Quantifizieren kontinuierlicher Phänomene. \\(\\mathbb{R}\\) umfasst dabei die rationalen Zahlen und die sogenannten irrationalen Zahlen \\(\\mathbb{R}\\setminus\n\\mathbb{Q}\\). Rationale Zahlen sind Zahlen, die sich, durch Brüche ganzer Zahlen ausdrücken lassen. Dies sind alle ganzen Zahlen sowie die negativen und positiven Dezimalzahlen wie beipsielsweise \\(-\\frac{9}{10} = -0.9\\), \\(\\frac{1}{3} = 1.3\\bar{3}\\) und \\(\\frac{196}{100} = 1.96\\). Irrationale Zahlen sind Zahlen, die sich nicht als rationale Zahlen ausdrücken lassen. Beispiele für irrationale Zahlen sind die Eulersche Zahl \\(e \\approx 2.71\\), die Kreiszahl \\(\\pi \\approx 3.14\\) und die Quadratwurzel von \\(2\\), \\(\\sqrt{2} \\approx 1.41\\).\nDie reellen Zahlen enthalten als Teilmengen die natürlichen, ganzen und die rationalen Zahlen. Es gibt also sehr viele reelle Zahlen. Tatsächlich hat Cantor (1892) bewiesen, dass es mehr reelle Zahlen als natürliche Zahlen gibt, obwohl es sowohl unendlich viele reelle Zahlen als auch unendlich viele natürliche Zahlen gibt. Diese Eigenschaft der reellen Zahlen bezeichnet man als die Überabzählbarkeit der reellen Zahlen. Insbesondere gilt also \\[\\begin{equation}\n\\mathbb{N} \\subset \\mathbb{Z} \\subset \\mathbb{Q} \\subset \\mathbb{R}.\n\\end{equation}\\] Zwischen zwei reellen Zahlen gibt es unendlich viele weitere reelle Zahlen. Positiv-Unendlich \\(\\infty\\) und Negativ-Unendlich \\(-\\infty\\) sind keine Zahlen, mit denen in der Standardmathematik gerechnet werden könnte. Sie gehören auch nicht zu den in obiger Definition gegebenen Zahlenmengen, es gelten also \\(\\infty \\notin \\mathbb{R}\\) und \\(-\\infty \\notin \\mathbb{R}\\). Komplexe Zahlen eignen sich zur Beschreibung zweidimensionaler kontinuierlicher Phänomene. Dabei werden die Werte der ersten Dimension im reellen Teil \\(a\\) und die Werte der zweiten Dimension im komplexen Teil \\(b\\) einer komplexen Zahl repräsentiert. Komplexe Zahlen kommen insbesondere bei der Modellierung physikalischer Phänomene und im Bereich der Fourieranalyse zum Einsatz.\nWichtige Teilmengen der reellen Zahlen sind die sogenannten Intervalle. Wir geben folgende Definitionen.\n\n\nIntervalle\n\nDefinition 2.9 Zusammenhängende Teilmengen der reellen Zahlen heißen Intervalle. Für \\(a,b\\in \\mathbb{R}\\) unterscheidet man\n\ndas abgeschlossene Intervall \\[\\begin{equation}\n[a,b] := \\{x \\in \\mathbb{R}|a \\le x \\le b\\},\n\\end{equation}\\]\ndas offene Interval \\[\\begin{equation}\n]a,b[ := \\{x \\in \\mathbb{R}|a &lt; x &lt; b\\},\n\\end{equation}\\]\nund die halboffenen Intervalle \\[\\begin{equation}\n]a,b] := \\{x \\in \\mathbb{R}| a &lt; x \\le b\\} \\mbox{ und }\n[a,b[ := \\{x \\in \\mathbb{R}| a \\le x &lt; b\\}.\n\\end{equation}\\]\n\n\nExemplarisch stellen wir in Abbildung 2.4 die Intervalle \\([1,2]\\), \\(]1,2]\\), \\([1,2[\\) und \\(]1,2[\\) graphisch dar. Dabei stellt man sich die reellen Zahlen als kontinuierlicher Zahlenstrahl vor und muss jeweils beachten, ob die linken bzw. rechten Endpunkte Teil des Intervalls sind oder nicht. Wie oben erwähnt sind Positiv-Unendlich (\\(\\infty\\)) und Negativ-Unendlich \\(-\\infty\\) keine Elemente von \\(\\mathbb{R}\\). Es gilt also immer \\(]-\\infty,b]\\) oder \\(]-\\infty,b[\\) bzw. \\(]a,\\infty[\\) oder \\([a,\\infty[\\), sowie \\(\\mathbb{R} = ]-\\infty, \\infty[\\).\n\n\n\n\n\n\nAbbildung 2.4: Darstellung von Intervallen am Zahlenstrahl. Der schwarze Punkt signalisiert dabei, dass die entsprechende Zahl Teil des Intervalls ist.\n\n\n\n\n\nKartesische Produkte\nOft möchte man mehrere unabhängige Eigenschaften eines Phänomens gleichzeitig quantitativ beschreiben. Zu diesem Zweck können die oben definierten eindimensionalen Zahlenmenge durch Bildung Kartesischer Produkte auf mehrdimensionale Zahlenmengen erweitert werden. Die Elemente Kartesischer Produkte nennt man geordnete Tupel oder Vektoren.\n\nDefinition 2.10 (Kartesische Produkte) \\(M\\) und \\(N\\) seien zwei Mengen. Dann ist das Kartesische Produkt der Mengen \\(M\\) und \\(N\\) die Menge aller geordneten Tupel \\((m,n)\\) mit \\(m \\in M\\) und \\(n \\in N\\), formal \\[\\begin{equation}\nM \\times N := \\{(m,n)|m\\in M, n \\in N \\}.\n\\end{equation}\\]\nDas Kartesische Produkt einer Menge \\(M\\) mit sich selbst wird bezeichnet mit \\[\\begin{equation}\nM^2 := M \\times M.\n\\end{equation}\\] Seien weiterhin \\(M_1, M_2, ..., M_n\\) Mengen. Dann ist das Kartesische Produkt der Mengen \\(M_1,...,M_n\\) die Menge aller geordneten \\(n\\)-Tupel \\((m_1,...,m_n)\\) mit \\(m_i \\in M_i\\) für \\(i = 1,...,n\\), formal \\[\\begin{equation}\n\\prod_{i=1}^n M_i := M_1 \\times \\cdots \\times M_n := \\{(m_1,...,m_n)\n                |m_i \\in M_i \\mbox{ für } i = 1,...,n\\}.\n\\end{equation}\\] Das \\(n\\)-fache Kartesische Produkt einer Menge \\(M\\) mit sich selbst wird bezeichnet mit \\[\\begin{equation}\nM^n := \\prod_{i=1}^n M := \\{(m_1,,...,m_n)|m_i \\in M\\}.\n\\end{equation}\\]\n\nIm Gegensatz zu Mengen sind die in Definition 2.10 eingeführten Tupel geordnet. Das heißt, für Mengen gilt beispielsweise \\(\\{1,2\\} = \\{2,1\\}\\), aber für Tupel gilt \\((1,2) \\neq (2,1)\\).\nBeispiel\nEs seien \\(M := \\{1,2\\}\\) und \\(N := \\{1,2,3\\}\\). Dann ist das Kartesische Produkt \\(M\\times N\\) gegeben durch \\[\\begin{equation}\nM \\times N := \\{(1,1), (1,2), (1,3), (2,1), (2,2), (2,3)\\}\n\\end{equation}\\] und das Kartesische Produkt \\(N \\times M\\) ist gegeben durch \\[\\begin{equation}\nN \\times M := \\{(1,1), (1,2), (2,1), (2,2), (3,1), (3,2)\\}\n\\end{equation}\\] Das Kartesische Produkt ist also im Allgemeinen nicht kommutativ, es gilt also nicht notwendigweise, dass \\(M \\times N = N \\times M\\). Man mag sich die in diesem Beispiel konstruierten Mengen \\(M \\times N \\neq N \\times M\\) beispielsweise anhand untenstehender Tabellen Tabelle 2.1 und Tabelle 2.2 konstruieren.\n\n\n\nTabelle 2.1: Kartesisches Produkt \\(M \\times N\\)\n\n\n\n\n\n\\((m,n)\\)\n\\(n = 1\\)\n\\(n = 2\\)\n\\(n = 3\\)\n\n\n\n\n\\(m = 1\\)\n\\((1,1)\\)\n\\((1,2)\\)\n\\((1,3)\\)\n\n\n\\(m = 2\\)\n\\((2,1)\\)\n\\((2,2)\\)\n\\((2,3)\\)\n\n\n\n\n\n\n\n\n\nTabelle 2.2: Kartesisches Produkt \\(N \\times M\\)\n\n\n\n\n\n\\((n,m)\\)\n\\(m = 1\\)\n\\(m = 2\\)\n\n\n\n\n\\(n = 1\\)\n\\((1,1)\\)\n\\((1,2)\\)\n\n\n\\(n = 2\\)\n\\((2,1)\\)\n\\((2,2)\\)\n\n\n\\(n = 3\\)\n\\((3,1)\\)\n\\((3,2)\\)\n\n\n\n\n\n\n\\(\\mathbb{R}\\) hoch \\(n\\)\nWie oben beschrieben eignen sich insbesondere die reellen Zahlen zur Beschreibung kontinuierlicher Phänomene. Zur simultanen Beschreibung mehrere Aspekte eines kontinuierlichen Phänomens bietet sich entsprechend die Menge der reellen Tupel \\(n\\)-ter Ordnung, kurz \\(\\mathbb{R}\\) hoch \\(n\\) an.\n\nDefinition 2.11 (Menge der reellen Tupel \\(n\\)-ter Ordnung) Das \\(n\\)-fache Kartesische Produkt der reellen Zahlen mit sich selbst wird bezeichnet mit \\[\\begin{equation}\n\\mathbb{R}^n := \\prod_{i=1}^n \\mathbb{R} := \\{x := (x_1,...,x_n)|x_i \\in \\mathbb{R}\\}\n\\end{equation}\\] und wird “\\(\\mathbb{R}\\) hoch \\(n\\)” gesprochen. Wir schreiben die Elemente von \\(\\mathbb{R}^n\\) als Spalten \\[\\begin{equation}\nx :=\n\\begin{pmatrix}\nx_1\n\\\\\n\\vdots\n\\\\\nx_n\n\\end{pmatrix}\n\\end{equation}\\] und nennen sie \\(n\\)-dimensionale Vektoren. Zu Abgrenzung nennen wir die Elemente von \\(\\mathbb{R}^1 = \\mathbb{R}\\) auch Skalare.\n\nBeispiele\nVertraute Beispiele von \\(\\mathbb{R}^n\\) sind \\(\\mathbb{R}^1\\) als Menge der reellen Zahlen, \\(\\mathbb{R}^2\\) als Menge der reellen Tupel im Modell der zweidimensionalen Ebene und \\(\\mathbb{R}^3\\) als Menge der reellen Tripel im Modell des dreidimensionalen Raumes wie in ?fig-rhochn visualisiert.\n.\nEin Beispiel für ein \\(x \\in \\mathbb{R}^4\\) ist \\[\\begin{equation}\nx = \\begin{pmatrix} 0.16 \\\\ 1.76 \\\\ 0.23 \\\\ 7.11 \\end{pmatrix}.\n\\end{equation}\\]",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Mengen</span>"
    ]
  },
  {
    "objectID": "102-Mengen.html#sec-selbstkontrollfragen-mengen",
    "href": "102-Mengen.html#sec-selbstkontrollfragen-mengen",
    "title": "2  Mengen",
    "section": "2.4 Selbstkontrollfragen",
    "text": "2.4 Selbstkontrollfragen\n\nGeben Sie die Definition einer Menge nach Cantor (1895) wieder.\nNennen Sie drei Möglichkeiten zur Definition einer Menge.\nErläutern Sie die Ausdrücke \\(m \\in M, m \\notin N, M \\subseteq N, M \\subset N\\) für zwei Mengen \\(M\\) und \\(N\\).\nGeben Sie die Definition der Kardinalität einer Menge wieder.\nGeben Sie die Definition der Potenzmenge einer Menge wieder.\nEs sei \\(M := \\{1,2\\}\\). Bestimmen Sie \\(\\mathcal{P}(M)\\).\nEs seien \\(M := \\{1,2\\}, N := \\{1,4,5\\}\\). Bestimmen Sie \\(M \\cup N, M \\cap N, M\\setminus N, M \\Delta N\\).\nErläutern Sie die Symbole \\(\\mathbb{N}\\), \\(\\mathbb{N}_n\\), und \\(\\mathbb{N}^0\\).\nErläutern Sie die Unterschiede zwischen \\(\\mathbb{N}\\) und \\(\\mathbb{Z}\\) und zwischen \\(\\mathbb{R}\\) und \\(\\mathbb{Q}\\).\nGeben Sie die Definition abgeschlossener, offener, und halboffener Intervalle wieder.\nEs seien \\(M\\) und \\(N\\) Mengen. Erläutern Sie die Notation \\(M \\times N\\).\nGeben Sie die Definition von \\(\\mathbb{R}^n\\) wieder.\n\n\n\n\n\nCantor, G. (1892). Über Eine Eigenschaft Des Inbegriffes Aller Reellen Algebraischen Zahlen. Jahresbericht der Deutschen Mathematiker-Vereinigung, 1.\n\n\nCantor, G. (1895). Beiträge Zur Begründung Der Transfiniten Mengenlehre. Mathematische Annalen, 46(4), 481–512. https://doi.org/10.1007/BF02124929",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Mengen</span>"
    ]
  },
  {
    "objectID": "103-Summen-Produkte-Potenzen.html",
    "href": "103-Summen-Produkte-Potenzen.html",
    "title": "3  Summen, Produkte, Potenzen",
    "section": "",
    "text": "3.1 Summen\nDiese Einheit führt Schreibweisen für die Grundrechenarten ein.\nFür eine sinnvolle Benutzung des Summenzeichens ist es essentiell, mithilfe des Subskripts und des Superskripts den Anfang und das Ende der Summation festzulegen. Die genaue Bezeichnung des Laufindexes ist dagegen für den Wert der Summe irrelevant, es gilt \\[\\begin{equation}\n\\sum_{i=1}^n x_i = \\sum_{j=1}^n x_j.\n\\end{equation}\\] Manchmal wird der Laufindex auch als Element einer Indexmenge angegeben. Ist z.B. die Indexmenge \\(I := \\{1,5,7\\}\\) definiert, so ist \\[\\begin{equation}\n\\sum_{i \\in I}x_i := x_1 + x_5 + x_7.\n\\end{equation}\\] Im Folgenden wollen wir kurz einige Beispiele für die Benutzung des Summenzeichens betrachten.\nDer Umgang mit dem Summenzeichen kann oft durch die Anwendung folgender Rechenregeln vereinfacht werden.\nBeispiele\nAls erstes Beispiel für die Anwendung der in Theorem 3.1 festgehaltenen Rechenregeln betrachten wir die Auswertung eines Mittelwertes (manchmal auch Durchschnitt genannt). Dazu seien \\(x_1, x_2,...,x_n\\) reelle Zahlen. Der Mittelwert dieser Zahlen entspricht der Summe von \\(x_1, x_2,...,x_n\\) geteilt durch die Anzahl der Zahlen \\(n\\). Dabei ist es nach Aussage (3) von Theorem 3.1 irrelevant, ob zunächst die Zahlen aufaddiert werden und dann die resultierende Summe durch \\(n\\) geteilt wird, oder die Zahlen jeweils einzeln durch \\(n\\) geteilt werden und die entsprechenden Ergebenisse dann aufaddiert werden. Genauer gilt durch Anwendung Theorem 3.1 (3) mit \\(a =1/n\\), dass \\[\\begin{equation}\n\\frac{1}{n}\\sum_{i=1}^n x_i = \\sum_{i=1}^n \\frac{x_i}{n}.\n\\end{equation}\\] So ist zum Beispiel der Mittelwert von \\(x_1 := 1, x_2 := 4, x_3 := 2\\) \\(x_4 := 1\\) gegeben durch \\[\\begin{equation}\n\\frac{1}{4}\\sum_{i=1}^4 x_i\n= \\frac{1}{4}(1 + 4 + 2 + 1)\n= \\frac{8}{4}\n= 2\n= \\frac{8}{4}\n= \\frac{1}{4} + \\frac{4}{4} + \\frac{2}{4} + \\frac{1}{4}\n= \\sum_{i=1}^4 \\frac{x_i}{4}.\n\\end{equation}\\]\nAls zweites Beispiel betrachten wir die in Theorem 3.1 (5) festgehaltene Umindizierungsregel. Dazu seien \\(n := 3\\) und \\(m := 2\\), sowie \\(x_0 := 2\\), \\(x_1 := 3\\), \\(x_2 := 5\\) und \\(x_3 := 10\\). Dann gilt offenbar \\[\\begin{align}\n\\begin{split}\n\\sum_{i=0}^3 x_i\n& = x_0 + x_1 + x_2 + x_3   \\\\\n& = 2 + 3 + 5 + 10          \\\\\n& = 20.\n\\end{split}\n\\end{align}\\] Ebenso gilt aber auch \\[\\begin{align}\n\\begin{split}\n\\sum_{j = m}^{n + m} x_{j-m}\n& = \\sum_{j = 2}^{3 + 2} x_{j-2}            \\\\\n& = \\sum_{j = 2}^{5} x_{j-2}                \\\\\n& = x_{2-2} + x_{3-2} + x_{4-2} + x_{5-2}   \\\\\n& = x_{0} + x_{1} + x_{2} + x_{3}           \\\\\n& = 2 + 3 + 5 + 10                          \\\\\n& = 20.\n\\end{split}\n\\end{align}\\]\nDoppelsummen\nNicht selten trifft man in der Anwendung auf Ausdrücke, die mehrere Summationen hintereinander ausführen. Dabei gelten für jede der Summen die oben gelisteten Definitionen und Rechenregeln. Man macht sich die Bedeutung von Doppelsummen am besten dadurch klar, dass man sie von innen nach außen ausschreibt und dabei beachtet, dass für jede Iteration der inneren Summe der Laufindex der äußeren Summe konstant bleibt. Folgendes Beispiel mag dies verdeutlichen. Es gilt \\[\\begin{align}\n\\begin{split}\n\\sum_{i=1}^2 \\sum_{j = 1}^3 (x_i + y_j)\n& = \\sum_{i=1}^2 (x_i + y_1 + x_i + y_2 + x_i + y_3)                            \\\\\n& = (x_1 + y_1 + x_1 + y_2 + x_1 + y_3) + (x_2 + y_1 + x_2 + y_2 + x_2 + y_3).  \\\\\n\\end{split}\n\\end{align}\\]",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Summen, Produkte, Potenzen</span>"
    ]
  },
  {
    "objectID": "103-Summen-Produkte-Potenzen.html#sec-summen",
    "href": "103-Summen-Produkte-Potenzen.html#sec-summen",
    "title": "3  Summen, Produkte, Potenzen",
    "section": "",
    "text": "Definition 3.1 (Summenzeichen) Es bezeichnet \\[\\begin{equation}\n\\sum_{i=1}^{n} x_i = x_1 + x_{2} + \\cdots + x_{n}.\n\\end{equation}\\] Dabei stehen\n\n\\(\\Sigma\\) für das griechische Sigma, mnemonisch für Summe,\ndas Subskript \\(i = 1\\) für den Laufindex und den Startindex,\ndas Superskript \\(n\\) für den Endindex und\n\\(x_1, x_2, ..., x_n\\) für die Summanden.\n\n\n\n\nSummation vordefinierter Summanden. Es seien \\(x_1 := 2\\), \\(x_2 := 10\\), \\(x_3 := -4\\). Dann gilt \\[\\begin{equation}\n\\sum_{i=1}^3 x_i = x_1 + x_2 + x_3 = 2 + 10 - 4 = 8.\n\\end{equation}\\]\nSummation gewichteter vordefinierter Summanden. Es seien wiederum \\(x_1 := 2\\), \\(x_2 := 10\\), \\(x_3 := -4\\). Weiterhin seien die Wichtungskoeffizienten \\(a_1 := \\frac{1}{2}\\), \\(a_2 := \\frac{1}{5}\\), \\(a_3 := 2\\) definiert. Dann gilt \\[\\begin{equation}\n\\sum_{i=1}^3 a_i x_i = a_1x_1 + a_2x_2 + a_2x_3 =  \\frac{1}{2}\\cdot 2 + \\frac{1}{5}\\cdot 10 + 2\\cdot (-4) = 1 + 2 - 8 = -5.\n\\end{equation}\\] Ausdrücke der Form \\(\\sum_{i=1}^n a_i x_i\\) werden auch als Linearkombinationen der \\(x_1,...,x_n\\) mit den Koeffizienten oder Wichtungsparametern \\(a_1,...,a_n\\) bezeichnet.\nSummation natürlicher Zahlen. Es gilt \\[\\begin{equation}\n\\sum_{i=1}^5 i = 1 + 2 + 3 + 4 + 5 = 15.  \n\\end{equation}\\]\nSummation gerader natürlicher Zahlen. Es gilt \\[\\begin{equation}\n\\sum_{i=1}^5 2i = 2\\cdot 1  + 2\\cdot 2 + 2\\cdot 3 + 2\\cdot 4 + 2\\cdot 5 = 2 + 4 + 6 + 8 + 10 = 30.\n\\end{equation}\\]\nSummation ungerader natürlicher Zahlen. Es gilt \\[\\begin{equation}\n\\sum_{i=1}^5 (2i - 1) = 2\\cdot 1 - 1  + 2\\cdot 2 - 1 + 2\\cdot 3 - 1 + 2\\cdot 4 - 1 + 2\\cdot 5 - 1 = 1 + 3 + 5 + 7 + 9 = 25.\n\\end{equation}\\]\n\n\n\nTheorem 3.1 (Rechenregeln für Summen)  \n\nSummen gleicher Summanden \\[\\begin{equation}\n\\sum_{i=1}^n x = nx\n\\end{equation}\\]\nAssoziativität bei Summen gleicher Länge \\[\\begin{equation}\n\\sum_{i=1}^n x_i + \\sum_{i=1}^n y_i = \\sum_{i=1}^n (x_i + y_i)\n\\end{equation}\\]\nDistributivität bei Multiplikation mit einer Konstante \\[\\begin{equation}\n\\sum_{i=1}^n ax_i = a\\sum_{i=1}^n x_i\n\\end{equation}\\]\nAufspalten von Summen mit \\(1 &lt; m &lt; n\\) \\[\\begin{equation}\n\\sum_{i = 1}^n x_i = \\sum_{i=1}^m x_i + \\sum_{i=m+1}^n x_i\n\\end{equation}\\]\nUmindizierung \\[\\begin{equation}\n\\sum_{i=0}^n x_i = \\sum_{j = m}^{n+m} x_{j - m}\n\\end{equation}\\]\n\n\n\nBeweis. Man überzeugt sich von diesen Rechenregeln durch Ausschreiben der Summen und Anwenden der Rechenregeln von Addition und Multiplikation. Wir zeigen hier exemplarisch die Assoziativität bei Summen gleicher Länge und die Distributivität bei Multiplikation mit einer Konstante. Hinsichtlich ersterer haben wir \\[\\begin{align}\n\\begin{split}\n\\sum_{i=1}^n x_i + \\sum_{i=1}^n y_i\n& = x_1 + x_2 + \\cdots + x_n +  y_1 + y_2 + \\cdots + y_n    \\\\\n& = x_1 + y_1 + x_2 + y_2 + \\cdots + x_n + y_n               \\\\\n& = \\sum_{i=1}^n (x_i + y_i).\n\\end{split}\n\\end{align}\\] Hinsichtlich letzterer gilt \\[\\begin{align}\n\\begin{split}\n\\sum_{i=1}^n ax_i\n& = ax_1 + ax_2 + \\cdots + ax_n                             \\\\\n& = a(x_1 + x_2 + \\cdots + x_n)                             \\\\\n& = a\\sum_{i=1}^n x_i.\n\\end{split}\n\\end{align}\\]",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Summen, Produkte, Potenzen</span>"
    ]
  },
  {
    "objectID": "103-Summen-Produkte-Potenzen.html#sec-produkte",
    "href": "103-Summen-Produkte-Potenzen.html#sec-produkte",
    "title": "3  Summen, Produkte, Potenzen",
    "section": "3.2 Produkte",
    "text": "3.2 Produkte\nEine analoge Schreibweise zum Summenzeichen bietet das Produktzeichen für Produkte.\n\nDefinition 3.2 (Produktzeichen) Es bezeichnet \\[\\begin{equation}\n\\prod_{i=1}^{n} x_i = x_1 \\cdot x_{2} \\cdot \\cdots \\cdot x_{n}.\n\\end{equation}\\] Dabei stehen\n\n\\(\\prod\\) für das griechische Pi, mnemonisch für Produkt,\ndas Subskript \\(i = 1\\) für den Laufindex und den Startindex,\ndas Superskript \\(n\\) für den Endindex,\n\\(x_1, x_2, ..., x_n\\) für die Produktterme\n\n\nAnalog zum Summenzeichen gilt, dass das Produktzeichen nur mit Subskript und Superskripten zu Lauf- und Endindex Sinn ergibt. Die genaue Bezeichnung des Laufindizes ist wiederum irrelevant, es gilt \\[\\begin{equation}\n\\prod_{i=1}^n x_i = \\prod_{j=1}^n x_j.\n\\end{equation}\\] Auch hier wird in seltenen Fällen der Laufindex als Element einer Indexmenge angegeben. Ist z.B. die Indexmenge \\(J := \\mathbb{N}_2^0\\) definiert, so ist \\[\\begin{equation}\n\\prod_{j \\in J}x_j := x_0 \\cdot x_1 \\cdot x_2.\n\\end{equation}\\]\nEin Beispiel für die Benutzung des Produktzeichens ist etwa die Definition der Fakultät einer natürlichen Zahl \\(n\\) durch \\[\\begin{equation}\nn! := \\prod_{i=1}^n i.\n\\end{equation}\\] So ist etwa \\[\\begin{equation}\n3! := \\prod_{i=1}^3 i = 1 \\cdot 2 \\cdot 3 = 6.\n\\end{equation}\\]\nAuch für Produkte gibt es eine Reihe von Rechenregeln, die den Umgang mit ihnen oft vereinfachen. Wir listen einige in folgendem Theorem. Dabei machen wir vorgreifend Gebrauch der in Kapitel 3.3 definierten Schreibweise von Potenzen.\n\nTheorem 3.2 (Rechenregeln für Produkte)  \n\nProdukte gleicher Faktoren \\[\\begin{equation}\n\\prod_{i=1}^n x = x^n\n\\end{equation}\\]\nPotenzierung von Konstanten \\[\\begin{equation}\n\\prod_{i=1}^n ax_i = a^n\\prod_{i=1}^n x_i\n\\end{equation}\\]\nAufspalten von Produkten mit \\(1 &lt; m &lt; n\\) \\[\\begin{equation}\n\\prod_{i=1}^n x_i = \\prod_{i=1}^m x_i \\prod_{j=m+1}^n x_j\n\\end{equation}\\]\nProdukt von Produkten \\[\\begin{equation}\n\\prod_{i=1}^n x_iy_i = \\prod_{i=1}^n x_i \\prod_{i=1}^n y_i\n\\end{equation}\\]",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Summen, Produkte, Potenzen</span>"
    ]
  },
  {
    "objectID": "103-Summen-Produkte-Potenzen.html#sec-potenzen",
    "href": "103-Summen-Produkte-Potenzen.html#sec-potenzen",
    "title": "3  Summen, Produkte, Potenzen",
    "section": "3.3 Potenzen",
    "text": "3.3 Potenzen\nProdukte von Zahlen mit sich selbst können mithilfe der Potenzschreibweise abgekürzt werden.\n\nDefinition 3.3 (Potenz) Für \\(a \\in \\mathbb{R}\\) und \\(n \\in \\mathbb{N}^0\\) ist die \\(n\\)-te Potenz von \\(a\\) definiert durch \\[\\begin{equation}\na^0 := 1 \\mbox{ und } a^{n+1} := a^n \\cdot a.\n\\end{equation}\\] Weiterhin ist für \\(a\\in \\mathbb{R} \\setminus 0\\) und \\(n \\in \\mathbb{N}^0\\) die negative \\(n\\)-te Potenz von \\(a\\) definiert durch \\[\\begin{equation}\na^{-n} := (a^n)^{-1} := \\frac{1}{a^n}.\n\\end{equation}\\] \\(a\\) wird dabei Basis und \\(n\\) wird Exponent genannt.\n\nDie Art der Definition von \\(a^{n+1}\\) mit Rückbezug auf die Potenz \\(a^n\\) in obiger Definition nennt man rekursiv. Die Definition \\(a^0 := 1\\) nennt man dabei den Rekursionsanfang; er macht die rekursive Definition von \\(a^{n+1}\\) erst möglich. Die Definition \\(a^{n+1} := a^n \\cdot a\\) nennt man auch Rekursionsschritt. Folgende Rechenregeln vereinfachen das Rechnen mit Potenzen.\n\nTheorem 3.3 (Rechenregeln für Potenzen) Für \\(a,b\\in \\mathbb{R}\\) und \\(n,m \\in \\mathbb{Z}\\) mit \\(a\\neq 0\\) bei negativen Exponenten gelten folgende Rechenregeln: \\[\\begin{align}\na^n a^m & = a^{n+m} \\\\\n(a^n)^m & = a^{nm}  \\\\\n(ab)^n  & = a^nb^n\n\\end{align}\\]\n\nAnstelle eines Beweises betrachten wir folgende Beispiele\nBeispiel (1) \\[\\begin{equation}\n2^2 \\cdot 2^3 = (2\\cdot 2) \\cdot (2 \\cdot 2 \\cdot 2) = 2^5 = 2^{2 + 3},\n\\end{equation}\\]\nBeispiel (2) \\[\\begin{equation}\n(3^2)^3 = (3\\cdot 3)^3 = (3\\cdot 3)\\cdot(3\\cdot 3)\\cdot(3\\cdot 3)= 3^6 = 3^{2\\cdot3},\n\\end{equation}\\]\nBeispiel (3) \\[\\begin{equation}\n(2 \\cdot 4)^2 = (2\\cdot 4)\\cdot (2 \\cdot 4) = (2 \\cdot 2)\\cdot(4\\cdot 4) = 2^2 \\cdot 4^2.\n\\end{equation}\\]\nIn enger Beziehung zur Potenz steht die Definition der \\(n\\)ten Wurzel:\n\nDefinition 3.4 (\\(n\\)-te Wurzel) Für \\(a \\in \\mathbb{R}\\) und \\(n \\in \\mathbb{N}\\) ist die \\(n\\)-te Wurzel von \\(a\\) definiert als die reelle Zahl \\(r\\) mit \\[\\begin{equation}\nr^n = a.\n\\end{equation}\\]\n\nBeim Rechnen mit Wurzeln ist die Potenzschreibweise von Wurzeln oft hilfreich, da sie die direkte Anwendung der Rechenregeln für Potenzen ermöglicht.\n\nTheorem 3.4 (Potenzschreibweise der \\(n\\)-ten Wurzel) Es sei \\(a \\in \\mathbb{R}\\), \\(n \\in \\mathbb{N}\\) und \\(r\\) die \\(n\\)-te Wurzel von \\(a\\). Dann gilt \\[\\begin{equation}\nr = a^{\\frac{1}{n}}\n\\end{equation}\\]\n\n\nBeweis. Es gilt \\[\\begin{equation}\n\\left(a^{\\frac{1}{n}}\\right)^n\n= a^{\\frac{1}{n}}\\cdot a^{\\frac{1}{n}}\\cdot \\cdots \\cdot a^{\\frac{1}{n}}\n= a^{\\sum_{i=1}^n \\frac{1}{n}}\n= a^1\n= a.\n\\end{equation}\\] Also gilt mit Definition 3.4, dass \\(r = a^\\frac{1}{n}\\).\n\nDas Rechnen mit Quadratwurzeln wird durch die Potenzschreibweise \\[\\begin{equation}\n\\sqrt{x} = x^{\\frac{1}{2}}\n\\end{equation}\\] sehr erleichtert. So gilt zum Beispiel \\[\\begin{equation}\n\\frac{2\\pi}{\\sqrt{2\\pi}}\n= \\frac{2\\pi}{(2\\pi)^{\\frac{1}{2}}}\n= (2\\pi)^{1} \\cdot (2\\pi)^{-\\frac{1}{2}}\n= (2\\pi)^{1-\\frac{1}{2}}\n= (2\\pi)^{\\frac{1}{2}}\n= \\sqrt{2\\pi}.\n\\end{equation}\\]\nFolgender Zusammenhang zwischen Quadrat, Wurzel und Betrag wird des öfteren genutzt.\n\nTheorem 3.5 (Wurzel und Betrag) Es sei \\(x \\in \\mathbb{R}\\). Dann gilt \\[\\begin{equation}\n\\sqrt{x^2} = |x|.\n\\end{equation}\\]\n\n\nBeweis. Wir betrachten die Fälle \\(x \\ge 0\\) und \\(x &lt; 0\\). Sei zunächst \\(x \\ge 0\\). Dann gilt \\[\\begin{equation}\nx \\ge 0 \\Rightarrow x^2 \\ge 0 \\Rightarrow \\sqrt{x^2} = x  = |x|.\n\\end{equation}\\] Sei nun \\(x &lt; 0\\). Dann gilt \\[\\begin{equation}\nx &lt;  0 \\Rightarrow x^2 &gt; 0 \\Rightarrow \\sqrt{x^2} = -x = |x|.\n\\end{equation}\\]",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Summen, Produkte, Potenzen</span>"
    ]
  },
  {
    "objectID": "103-Summen-Produkte-Potenzen.html#sec-selbstkontrollfragen-summen-produkte-potenzen",
    "href": "103-Summen-Produkte-Potenzen.html#sec-selbstkontrollfragen-summen-produkte-potenzen",
    "title": "3  Summen, Produkte, Potenzen",
    "section": "3.4 Selbstkontrollfragen",
    "text": "3.4 Selbstkontrollfragen\n\nGeben Sie die Definition des Summenzeichens wieder.\nBerechnen Sie die Summen \\(\\sum_{i=1}^3 2\\), \\(\\sum_{i=1}^3 i^2\\), und \\(\\sum_{i=1}^3 \\frac{2}{3}i\\).\nSchreiben Sie die Summe \\(1 + 3 + 5 + 7 + 9 + 11\\) mithilfe des Summenzeichens.\nSchreiben Sie die Summe \\(0 + 2 + 4 + 6 + 8 + 10\\) mithilfe des Summenzeichens.\nGeben Sie die Definition des Produktzeichens wieder.\nGeben Sie die Definition der \\(n\\)-ten Potenz von \\(a \\in \\mathbb{R}\\) wieder.\nBerechnen Sie \\(2^2\\cdot 2^3\\) und \\(2^5\\) und geben Sie die zugehörige Potenzregel wieder.\nBerechnen Sie \\(6^2\\) und \\(2^2\\cdot 3^2\\) und geben Sie die zugehörige Potenzregel wieder.\nBegründen Sie, warum die \\(n\\)-te Wurzel von \\(a\\) als \\(a^{\\frac{1}{n}}\\) geschrieben werden kann.\nBerechnen Sie \\((\\sqrt{2})^{\\frac{2}{3}}, 9^{\\frac{1}{2}}\\), und \\(4^{-\\frac{1}{2}}\\).",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Summen, Produkte, Potenzen</span>"
    ]
  },
  {
    "objectID": "104-Funktionen.html",
    "href": "104-Funktionen.html",
    "title": "4  Funktionen",
    "section": "",
    "text": "4.1 Definition und Eigenschaften\nFunktionen und Mengen bilden die Grundpfeiler mathematischer Modellierung. In dieser Einheit definieren wir den Begriff der Funktion, führen erste Eigenschaften von Funktionen ein und geben eine Übersicht über einige elementare Funktionen.\nEs ist zentral, zwischen der Funktion \\(f\\) als Zuordnungsvorschrift und einem Wert der Funktion \\(f(x)\\) als Element von \\(Z\\) zu unterscheiden. \\(x\\) ist das Argument der Funktion (der Input der Funktion), \\(f(x)\\) der Wert, den die Funktion \\(f\\) für das Argument \\(x\\) annimmt (der Output der Funktion). Üblicherweise folgt in der Definition einer Funktion \\(f(x)\\) die Definition der funktionalen Form von \\(f\\), also einer Regel, wie aus \\(x\\) der Wert \\(f(x)\\) zu bilden ist. Zum Beispiel wird in folgender Definition einer Funktion \\[\\begin{equation}\nf : \\mathbb{R} \\to \\mathbb{R}_{\\ge 0}, x \\mapsto f(x) := x^2\n\\end{equation}\\] die Definition der Potenz genutzt. Man beachte, dass Funktionen immer eindeutig sind, als dass sie jedem \\(x \\in D\\) bei jeder Anwendung der Funktion immer ein und dasselbe \\(f(x) \\in Z\\) zuordnen.\nAbbildung 4.1 visualisiert einige Aspekte der Funktionsdefinition. Abbildung 4.1 A stellt dabei zunächst die zentralen Begriffe der Funktionsdefinition bildlich dar. Abbildung 4.1 B visualisiert ein erstes Beispiel für eine Funktion, bei der die den Argumenten zugeordneten Funktionswerte nicht durch eine Rechenregel, sondern durch direkte Defintion bestimmt sind. Abbildung 4.1 C und Abbildung 4.1 D bedienen sich der Bildsprache um zwei Aspekte der Definition von Funktionen mithilfe von Gegenbeispielen hervorzuheben: Bei der Zeichnung in Abbildung 4.1 C handelt es sich nicht um die Darstellung einer Funktion, da nach Definition 4.1 eine Funktion jedem Element einer Menge \\(D\\) genau ein Element einer Zielmenge \\(Z\\) zuordnet. Die hier angedeutete Zuordnungsvorschrift orndet dem Element \\(2 \\in D\\) aber kein Element in \\(Z\\) zu und ist deshalb keine Funktion. Gleichsam gilt nach Definition 4.1, dass eine Funktion jedem Element einer Menge \\(D\\) genau ein Element einer Zielmenge \\(Z\\) zuordnet. Die in Abbildung 4.1 D angedeutete Zuordnungsvorschrift ordnet \\(1 \\in D\\) aber sowohl das Element \\(a \\in Z\\) als auch das Element \\(b \\in Z\\) zu und ist deshalb mit Definition 4.1 nicht vereinbar.\nFunktionen setzen also Elemente von Mengen miteinander in Beziehung. Die Mengen dieser Elemente erhalten spezielle Bezeichnungen.\nMan beachte, dass der Wertebereich \\(f(D)\\) von \\(f\\) und die Zielmenge \\(Z\\) von \\(f\\) nicht notwendigerweise identisch sein müssen.\nBeispiel\nUm die in Definition 4.2 eingeführten Begriffe zu verdeutlichen betrachten wir die in Abbildung 4.2 A dargestellte Funktion \\[\\begin{equation}\nf : \\{1,2,3,4,5\\} \\to \\{a,b,c,d\\}, x \\mapsto f(x) := \\begin{cases} f(1) := b \\\\ f(2)  := d \\\\ f(3)  := c  \\\\ f(4) := c \\\\ f(5) := d \\end{cases}.\n\\end{equation}\\] Nach Definition 4.2 ist eine Bildmenge immer bezüglich einer Teilmenge \\(D'\\) der Definitionsmenge \\(D\\) definiert. Sei also wie in Abbildung 4.2 B dargestellt \\(D' := \\{2,3\\}\\subset D\\). Dann ist die Bildmenge von \\(D'\\) die Menge der \\(z \\in Z\\), für die ein \\(x \\in D'\\) existiert, so dass und \\(z = f(x)\\). Diejenigen \\(z \\in Z\\) für die ein \\(x \\in D'\\) mit \\(z = f(x)\\) existiert sind aber hier gerade \\(c,d\\in Z\\), da \\(f(2) = d\\) und \\(f(3) = c\\). Darüberhinaus gibt es keine \\(z \\in Z\\) mit \\(f(x) = z\\) und \\(x \\in \\{2,3\\}\\). Der Wertebereich \\(f(D)\\) ist nach Definition 4.2 die Teilemenge der \\(z \\in Z\\), für die gilt, dass ein \\(x \\in D\\) existiert, für das \\(z = f(x)\\) ist. Dies ist für alle Elemente von \\(Z\\) der Fall, außer für \\(a \\in Z\\), da für dieses kein \\(x \\in D\\) existiert mit \\(a = f(x)\\). Der Wertebereich von \\(f\\) ist für die betrachtete Funktion also durch \\(f(D) := \\{b,c,d\\}\\) gegeben.\nUmgekehrt ist nach Definition 4.2 ist eine Urbildmenge immer bezüglich einer Teilmenge \\(Z'\\) der Zielmenge \\(Z\\) definiert .Sei also wie in Abbildung 4.2 C dargestellt \\(Z' := \\{c,d\\}\\subset Z\\). Dann ist die Urbildmenge von \\(Z'\\) die Menge der \\(x \\in D\\), für die gilt, dass \\(f(x) \\in Z'\\). Diejenigen Elemente von \\(D\\), deren Funktionswerte unter \\(f\\) durch Elemente von \\(Z'\\) gegeben sind, sind gerade \\(\\{2,3,4,5\\}\\). \\(1 \\in D\\) dagegen ist kein Element von \\(Z'\\), da \\(f\\) \\(1 \\in D\\) auf \\(b \\in Z\\) abbildet und \\(b \\notin Z'\\). Nichtsdestotrotz ist natürlich \\(1 \\in D\\) ein Urbild von \\(b\\).\nGrundlegende Eigenschaften von Funktionen werden in folgender Definition benannt.\nBeispiele\nWir verdeutlichen Definition 4.3 zunächst anhand dreier (Gegen)beispiele in Abbildung 4.3. Abbildung 4.3 A visualisiert dabei die nicht-injektive Funktion \\[\\begin{equation}\nf : \\{1,2,3\\} \\to \\{a,b\\}, x \\mapsto f(x) := \\begin{cases} f(1) := a \\\\ f(2) := a \\\\ f(3)  := b \\end{cases}.\n\\end{equation}\\] Die Funktion ist nicht-injektiv, weil es zum Element \\(a\\) in der Bildmenge von \\(f\\) mehr als ein Urbild in der Definitionsmenge von \\(f\\) gibt, nämlich die Elemente \\(1\\) und \\(2\\). Abbildung 4.3 B visualisiert die nicht-surjektive Funktion \\[\\begin{equation}\ng : \\{1,2,3\\} \\to \\{a,b,c,d\\}, x \\mapsto g(x) := \\begin{cases} g(1) := a \\\\ g(2)  := b \\\\ g(3)  := d \\end{cases}.\n\\end{equation}\\] Die Funktion ist nicht surjektiv, weil das Element \\(c\\) in der Zielmenge von \\(f\\) kein Urbild in der Definitionsmenge von \\(f\\) hat. Abbildung 4.3 C schließlich visualisiert die bijektive Funktion \\[\\begin{equation}\nh : \\{1,2,3\\} \\to \\{a,b,c\\}, x \\mapsto g(x) := \\begin{cases} h(1)  := a \\\\ h(2)  := b \\\\ h(3)  := c \\end{cases}.\n\\end{equation}\\] Zu jedem Element in der Zielmenge von \\(h\\) gibt es genau ein Urbild, die Funktion ist also injektiv und surjektiv und damit bijektiv. Bijektive Abbildungen werden auch Eins-zu-Eins-Abbildungen (eng. one-to-one mappings) genannt.\nAls weiteres Beispiel betrachten wir die Funktion \\[\\begin{equation}\nf : \\mathbb{R} \\to \\mathbb{R}, x \\mapsto f(x) := x^2\n\\end{equation}\\] Diese Funktion ist nicht injektiv, weil z.B. für \\(x_1 = 2 \\neq -2 = x_2\\) gilt, dass \\(f(x_1) = 2^2 = 4 = (-2)^2 = f(x_2)\\). Weiterhin ist \\(f\\) auch nicht surjektiv, weil z.B. \\(-1 \\in \\mathbb{R}\\) kein Urbild unter \\(f\\) hat. Schränkt man die Definitionsmenge von \\(f\\) allerdings auf die nicht-negativen reellen Zahlen ein, definiert man also die Funktion \\[\\begin{equation}\n\\tilde{f} : [0,\\infty[ \\to [0,\\infty[, x \\mapsto \\tilde{f}(x) := x^2,\n\\end{equation}\\] so ist \\(\\tilde{f}\\) im Gegensatz zu \\(f\\) injektiv und surjektiv, also bijektiv.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Funktionen</span>"
    ]
  },
  {
    "objectID": "104-Funktionen.html#definition-und-eigenschaften",
    "href": "104-Funktionen.html#definition-und-eigenschaften",
    "title": "4  Funktionen",
    "section": "",
    "text": "Definition 4.1 (Funktion) Eine Funktion oder Abbildung \\(f\\) ist eine Zuordnungsvorschrift, die jedem Element einer Menge \\(D\\) genau ein Element einer Menge \\(Z\\) zuordnet. \\(D\\) wird dabei Definitionsmenge von \\(f\\) und \\(Z\\) wird Zielmenge von \\(f\\) genannt. Wir schreiben \\[\\begin{equation}\nf : D \\to Z, x \\mapsto f(x),\n\\end{equation}\\] wobei \\(f : D \\to Z\\) gelesen wird als “die Funktion \\(f\\) bildet alle Elemente der Menge \\(D\\) eindeutig auf Elemente in \\(Z\\) ab” und \\(x \\mapsto f(x)\\) gelesen wird als “\\(x\\), welches ein Element von \\(D\\) ist, wird durch die Funktion \\(f\\) auf \\(f(x)\\) abgebildet, wobei \\(f(x)\\) ein Element von \\(Z\\) ist”. Der Pfeil \\(\\to\\) steht für die Abbildung zwischen den Mengen \\(D\\) und \\(Z\\), der Pfeil \\(\\mapsto\\) steht für die Abbildung zwischen einem Element von \\(D\\) und einem Element von \\(Z\\).\n\n\n\n\n\n\n\n\n\nAbbildung 4.1: Aspekte der Funktionsdefinition\n\n\n\n\n\nDefinition 4.2 (Bildmenge, Wertebereich, Urbildmenge, Urbild) Es sei \\(f : D \\to Z, x \\mapsto f(x)\\) eine Funktion und es seien \\(D' \\subseteq D\\) und \\(Z' \\subseteq Z\\). Die Menge \\[\\begin{equation}\nf(D') := \\{z \\in Z| \\mbox{Es gibt ein } x \\in D' \\mbox{ mit } z = f(x)\\}\n\\end{equation}\\] heißt die Bildmenge von \\(D'\\)  und \\(f(D) \\subseteq Z\\) heißt der Wertebereich von \\(f\\). Weiterhin heißt die Menge \\[\\begin{equation}\nf^{-1}(Z') := \\{x \\in D | f(x) \\in Z'\\}\n\\end{equation}\\] die Urbildmenge von \\(Z'\\). Ein \\(x \\in D\\) mit \\(z = f(x) \\in Z\\) heißt ein Urbild von \\(z\\).\n\n\n\n\n\n\n\n\n\n\n\nAbbildung 4.2: Bildmenge, Wertebereich, Urbildmenge, Urbild\n\n\n\n\n\nDefinition 4.3 (Injektivität, Surjektivität, Bijektivität) \\(f : D \\to Z, x \\mapsto f(x)\\) sei eine Funktion. \\(f\\) heißt injektiv, wenn es zu jedem Bild \\(z \\in f(D)\\) genau ein Urbild \\(x \\in D\\) gibt. Äquivalent gilt, dass \\(f\\) injektiv ist, wenn aus \\(x_1,x_2 \\in D\\) mit \\(x_1 \\neq x_2\\) folgt, dass \\(f(x_1) \\neq f(x_2)\\) ist. \\(f\\) heißt surjektiv, wenn \\(f(D) = Z\\) gilt, wenn also jedes Element der Zielmenge \\(Z\\) ein Urbild in der Definitionsmenge \\(D\\) hat. Schließlich heißt \\(f\\) bijektiv, wenn \\(f\\) injektiv und surjektiv ist. Bijektive Funktionen werden auch eineindeutige Funktionen (engl. one-to-one mappings) genannt.\n\n\n\n\n\n\n\n\n\nAbbildung 4.3: Injektivität, Surjektivität, Bijektivität.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Funktionen</span>"
    ]
  },
  {
    "objectID": "104-Funktionen.html#funktionentypen",
    "href": "104-Funktionen.html#funktionentypen",
    "title": "4  Funktionen",
    "section": "4.2 Funktionentypen",
    "text": "4.2 Funktionentypen\nDurch Verkettung lassen sich aus Funktionen weitere Funktionen bilden.\n\nDefinition 4.4 (Verkettung von Funktionen) Es seien \\(f : D \\to Z\\) und \\(g : Z \\to S\\) zwei Funktionen, wobei die Wertemenge von \\(f\\) mit der Definitionsmenge von \\(g\\) übereinstimmen sollen. Dann ist durch \\[\\begin{equation}\ng \\circ f : D \\to S, x \\mapsto (g \\circ f)(x) := g(f(x))\n\\end{equation}\\] eine Funktion definiert, die die Verkettung von \\(f\\) und \\(g\\) genannt wird.\n\nDie Schreibweise für verkettete Funktionen ist etwas gewöhnungsbedürftig. Wichtig ist es zu erkennen, dass \\(g \\circ f\\) die verkette Funktion und \\((g \\circ f)(x)\\) ein Element in der Zielmenge der verketten Funktion bezeichnen. Intuitiv wird bei der Auswertung von \\((g \\circ f)(x)\\) zunächst die Funktion \\(f\\) auf \\(x\\) angewendet und dann die Funktion \\(g\\) das Element auf \\(f(x)\\) von \\(R\\) angewendet. Dies ist in der funktionalen Form \\(g(f(x))\\) festgehalten. Der Einfachheit halber benennt man die Verkettung zweier Funktionen auch oft mit einem einzelnen Buchstaben und schreibt beispielsweise, \\(h := g \\circ f\\) mit \\(h(x) = g(f(x))\\). Leicht zur Verwirrung kann es führen, wenn Elemente in der Zielmenge von \\(f\\) mit \\(y\\) bezeichnet werden, also die Schreibweise \\(y = f(x)\\) und \\(h(x) = g(y)\\) genutzt wird. Allerdings ist diese Schreibweise manchmal zur notationellen Vereinfachung nötig. Wir visualisieren Definition 4.4 in Abbildung 4.4.\n\n\n\n\n\n\nAbbildung 4.4: Verkettung von Funktionen.\n\n\n\nAls Beispiel für die Verkettung zweier Funktionen betrachten wir \\[\\begin{equation}\nf : \\mathbb{R} \\to \\mathbb{R}, x \\mapsto f(x) := -x^2\n\\end{equation}\\] und \\[\\begin{equation}\ng : \\mathbb{R} \\to \\mathbb{R}, x \\mapsto g(x) := \\exp(x).\n\\end{equation}\\] Die Verkettung von \\(f\\) und \\(g\\) ergibt sich in diesem Fall zu \\[\\begin{equation}\ng \\circ f : \\mathbb{R} \\to \\mathbb{R}, x \\mapsto (g \\circ f)(x) := g(f(x)) = \\exp\\left(-x^2\\right).\n\\end{equation}\\]\nEine erste Anwendung der Verkettung von Funktionen findet sich in folgender Definition.\n\nDefinition 4.5 (Inverse Funktion) Es sei \\(f : D \\to Z, x \\mapsto f(x)\\) eine bijektive Funktion. Dann heißt die Funktion \\(f^{-1}\\) mit \\[\\begin{equation}\nf^{-1} \\circ f : D \\to D, x \\mapsto (f^{-1} \\circ f)(x) := f^{-1}(f(x)) = x\n\\end{equation}\\] inverse Funktion, Umkehrfunktion oder einfach Inverse von \\(f\\).\n\nInverse Funktionen sind immer bijektiv. Dies folgt, weil \\(f\\) bijektiv ist und damit jedem \\(x \\in D\\) genau ein \\(f(x) = z \\in Z\\) zugeordnet wird. Damit wird aber auch jedem \\(z \\in Z\\) genau ein \\(x \\in D\\), nämlich \\(f^{-1}(f(x)) = x\\) zugeordnet. Intuitiv macht die inverse Funktion von \\(f\\) den Effekt von \\(f\\) auf ein Element \\(x\\) rückgängig. Wir visualisieren Definition 4.5 in Abbildung 4.5 A.\n\n\n\n\n\n\nAbbildung 4.5: Inverse Funktion.\n\n\n\nBetrachtet man konkret den Graphen einer Funktion in einem Kartesischen Koordinatensystem, so führt die Anwendung von einem Wert auf der \\(x\\)-Achse zu einem Wert auf der \\(y\\)-Achse. Die Anwendung der inversen Funktion führt dementsprechend von einem Wert auf der \\(y\\)-Achse zu einem Wert auf der \\(x\\)-Achse. Wir visualisieren dies in Abbildung 4.5 B. Betrachten wir beispielsweise die Funktion \\[\\begin{equation}\nf : \\mathbb{R} \\to \\mathbb{R}, x \\mapsto f(x) := 2x =:y.\n\\end{equation}\\] Dann ist die inverse Funktion von \\(f\\) gegeben durch \\[\\begin{equation}\nf^{-1} : \\mathbb{R} \\to \\mathbb{R}, y \\mapsto f^{-1}(y) := \\frac{1}{2}y,\n\\end{equation}\\] weil für jedes \\(x \\in \\mathbb{R}\\) gilt, dass \\[\\begin{equation}\n(f^{-1} \\circ f)(x) := f^{-1}(f(x)) = f^{-1}(2x) = \\frac{1}{2}\\cdot 2x = x.\n\\end{equation}\\]\nEine wichtige Klasse von Funktionen sind lineare Abbildungen.\n\nDefinition 4.6 (Lineare Abbildung) Eine Abbildung \\(f : D \\to Z, x \\mapsto f(x)\\) heißt lineare Abbildung, wenn für \\(x,y \\in D\\) und einen Skalar \\(c\\) gelten, dass \\[\\begin{equation}\nf(x + y) = f(x) + f(y)  f(cx) = cf(x) \\tag*{(Additivität)}\n\\end{equation}\\] und \\[\\begin{equation}\nf(cx) = cf(x) \\tag*{(Homogenität)}\n\\end{equation}\\] Eine Abbildung, für die obige Eigenschaften nicht gelten, heißt nicht-lineare Abbildung.\n\nLineare Abbildungen sind oft als “gerade Linien” bekannt. Die allgemeine Definition linearer Abbildungen ist mit dieser Intuition nicht komplett kongruent. Insbesondere sind lineare Abbildungen nur solche Funktionen, die den Nullpunkt auf den Nullpunkt abbilden. Wir zeigen dazu folgendes Theorem.\n\nTheorem 4.1 (Lineare Abbildung der Null) \\(f : D \\to Z\\) sei eine lineare Abbildung. Dann gilt \\[\\begin{equation}\nf(0) = 0.\n\\end{equation}\\]\n\n\nBeweis. Wir halten zunächst fest, dass mit der Additivität von \\(f\\) gilt, dass \\[\\begin{equation}\nf(0) = f(0 + 0) = f(0) + f(0).\n\\end{equation}\\] Addition von \\(-f(0)\\) auf beiden Seiten obiger Gleichung ergibt dann \\[\\begin{align}\n\\begin{split}\nf(0) - f(0) & = f(0) + f(0) - f(0) \\\\\n0           & = f(0) \\\\\n\\end{split}\n\\end{align}\\] und damit ist alles gezeigt.\n\nWir wollen den Begriff der linearen Abbildung noch an zwei Beispielen verdeutlichen.\n\nFür \\(a \\in \\mathbb{R}\\) ist die Abbildung \\[\\begin{equation}\nf : \\mathbb{R} \\to \\mathbb{R}, x \\mapsto f(x) := ax\n\\end{equation}\\] eine lineare Abbildung, weil gilt, dass \\[\\begin{equation}\nf(x + y) = a(x + y) = ax + ay = f(x) + f(y) \\mbox{ und } f(cx) = acx = cax = cf(x).\n\\end{equation}\\]\nFür \\(a,b \\in \\mathbb{R}\\) ist dagegen die Abbildung \\[\\begin{equation}\nf : \\mathbb{R} \\to \\mathbb{R}, x \\mapsto f(x) := ax + b\n\\end{equation}\\] nicht-linear, weil z.B. für \\(a := b := 1\\) gilt, dass \\[\\begin{equation}\nf(x+y) = 1(x+y)+1 = x + y + 1 \\neq x + 1 + y + 1 = f(x) + f(y).\n\\end{equation}\\]\n\nEine Abbildung der Form \\(f(x) := ax + b\\) heißt linear-affine Abbildung oder linear-affine Funktion. Etwas unsauber werden Funktionen der Form \\(f(x) := ax + b\\) auch manchmal als lineare Funktionen bezeichnet.\nNeben den bisher diskutierten Funktionentypen gibt es noch viele weitere Klassen von Funktionen. In folgender Definition klassifizieren wir Funktionen anhand der Dimensionalität ihrer Definitions- und Zielmengen. Diese Art der Funktionsklassifikation ist oft hilfreich, um sich einen ersten Überblick über ein mathematisches Modell zu verschaffen.\n\nDefinition 4.7 (Funktionenarten) Wir unterscheiden\n\nunivariate reellwertige Funktionen der Form \\[\\begin{equation}\nf : \\mathbb{R} \\to \\mathbb{R}, x \\mapsto f(x),\n\\end{equation}\\]\nmultivariate reellwertige Funktionen der Form \\[\\begin{equation}\nf : \\mathbb{R}^n \\to \\mathbb{R}, x \\mapsto f(x) = f(x_1,...,x_n),\n\\end{equation}\\]\nund multivariate vektorwertige Funktionen der Form \\[\\begin{equation}\nf : \\mathbb{R}^n \\to \\mathbb{R}^m, x \\mapsto\nf(x) =\n\\begin{pmatrix}\nf_1(x_1,...,x_n)    \\\\\n\\vdots              \\\\\nf_m(x_1,...,x_n)\n\\end{pmatrix},\n\\end{equation}\\] wobei \\(f_i, i = 1,...,m\\) die Komponenten(funktionen) von \\(f\\) genannt werden.\n\n\nIn der Physik werden multivariate reellwertige Funktionen Skalarfelder und multivariate vektorwertige Funktionen Vektorfelder genannt. In manchen Anwendungen treten zum Beispiel auch matrixvariate matrixwertige Funktionen auf.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Funktionen</span>"
    ]
  },
  {
    "objectID": "104-Funktionen.html#sec-elementare-funktionen",
    "href": "104-Funktionen.html#sec-elementare-funktionen",
    "title": "4  Funktionen",
    "section": "4.3 Elementare Funktionen",
    "text": "4.3 Elementare Funktionen\nAls elementare Funktionen bezeichnen wir eine kleine Schar von univariaten reellwertigen Funktionen, die häufig als Bausteine komplexerer Funktionen auftreten. Dies sind die Polynomfunktionen, die Exponentialfunktion, die Logarithmusfunktion und die Gammafunktion. Im Folgenden geben wir die Definitionen dieser Funktionen sowie ihre wesentlichen Eigenschaften als Theoreme an und stellen ihre Graphen an. Für Beweise der Eigenschaften der hier vorgestellten Funktionen verweisen wir auf die weiterführende Literatur.\n\nDefinition 4.8 (Polynomfunktionen) Eine Funktion der Form \\[\\begin{equation}\nf : \\mathbb{R} \\to \\mathbb{R}, x \\mapsto f(x) := \\sum_{i=0}^{k} a_i x^i = a_0 + a_1 x^1 + a_2 x^2 + \\cdots + a_k x^k\n\\end{equation}\\] heißt Polynomfunktion \\(k\\)-ten Grades mit Koeffizienten \\(a_0, a_1,...,a_k \\in \\mathbb{R}\\). Typische Polynomfunktionen sind in nachfolgender Tabelle aufgelistet:\n\n\n\n\n\n\n\n\nName\nForm\nKoeffizienten\n\n\n\n\nKonstante Funktion\n\\(f(x) = a\\)\n\\(a_0 := a\\), \\(a_i := 0, i &gt; 0\\)\n\n\nIdentitätsfunktion\n\\(f(x) = x\\)\n\\(a_0 := 0\\), \\(a_1 := 1\\), \\(a_i := 0, i &gt; 1\\)\n\n\nLinear-affine Funktion\n\\(f(x) = ax + b\\)\n\\(a_0 := b\\), \\(a_1 := a\\), \\(a_i := 0, i &gt; 1\\)\n\n\nQuadratfunktion\n\\(f(x) = x^2\\)\n\\(a_0 := 0\\), \\(a_1 := 0\\), \\(a_2 := 1\\), \\(a_i := 0, i &gt; 2\\)\n\n\n\n\n\n\n\n\n\n\nAbbildung 4.6: Ausgewählte Polynomfunktionen\n\n\n\nAbbildung 4.6 zeigt die Graphen der in Definition 4.8 aufgelisteten Polynomfunktionen.\nEin wichtiges Funktionenpaar sind die Exponentialfunktion und die Logarithmusfunktion.\n\nTheorem 4.2 (Exponentialfunktion und ihre Eigenschaften) Die Exponentialfunktion ist definiert als \\[\\begin{equation}\n\\exp : \\mathbb{R} \\to \\mathbb{R},\nx \\mapsto \\exp(x)\n:= e^x\n:= \\sum_{n=0}^\\infty \\frac{x^n}{n!}\n= 1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\frac{x^4}{4!} + \\cdots\n\\end{equation}\\] Die Exponentialfunktion hat folgende Eigenschaften:\n\n\n\n\n\n\n\nEigenschaft\nBedeutung\n\n\n\n\nWertebereich\n\\(x \\in ]-\\infty,0[ \\Rightarrow \\exp(x) \\in ]0,1[\\)\n\n\n\n\\(x \\in ]0,\\infty[\\quad\\,\\,  \\Rightarrow \\exp(x) \\in ]1,\\infty[\\)\n\n\nMonotonie\n\\(x &lt; y \\Rightarrow \\exp(x) &lt; \\exp(y)\\)\n\n\nSpezielle Werte\n\\(\\exp(0) = 1\\) und \\(\\exp(1) = e\\)\n\n\nSummationseigenschaft\n\\(\\exp(x + y) = \\exp(x)\\exp(y)\\)\n\n\nSubtraktionseigenschaft\n\\(\\exp(x - y) = \\frac{\\exp(x)}{\\exp(y)}\\)\n\n\n\n\nInsbesondere nimmt die Exponentialfunktion also nur positive Werte an und schneidet die \\(y\\)-Achse also bei \\(x = 0\\). Die Zahl \\(\\exp(1) := e \\approx 2.71...\\) heißt Eulersche Zahl. Schließlich gilt mit den speziellen Werten der Exponentialfunktion insbesondere auch \\[\\begin{equation}\n\\exp(x)\\exp(-x) = \\exp(x - x) = \\exp(0) = 1.\n\\end{equation}\\]\n\nTheorem 4.3 (Logarithmusfunktion und ihre Eigenschaften) Die Logarithmusfunktion ist definiert als inverse Funktion der Exponentialfunktion, \\[\\begin{equation}\n\\ln : ]0,\\infty[ \\to \\mathbb{R}, x \\mapsto \\ln(x) \\mbox{ mit } \\ln(\\exp(x)) = x \\mbox{ für alle } x \\in \\mathbb{R}.\n\\end{equation}\\] Die Logarithmusfunktion hat folgende Eigenschaften:\n\n\n\n\n\n\n\nEigenschaft\nBedeutung\n\n\n\n\nWertebereich\n\\(x \\in \\, ]0,1[\\,\\,\\, \\Rightarrow \\ln(x) \\in\\,]-\\infty,0[\\)\n\n\n\n\\(x \\in \\, ]1,\\infty[ \\Rightarrow \\ln(x) \\in\\, ]0,\\infty[\\)\n\n\nMonotonie\n\\(x &lt; y \\Rightarrow \\ln(x) &lt; \\ln(y)\\)\n\n\nSpezielle Werte\n\\(\\ln(1) = 0\\) und \\(\\ln(e) = 1\\)\n\n\nProdukteigenschaft\n\\(\\ln(xy) = \\ln(x) + \\ln(y)\\)\n\n\nPotenzeigenschaft\n\\(\\ln(x^c) = c\\ln(x)\\)\n\n\nDivisionseigenschaft\n\\(\\ln\\left(\\frac{1}{x}\\right) = - \\ln (x)\\)\n\n\n\n\nIm Gegensatz zur Exponentialfunktion nimmt die Logarithmusfunktion sowohl negative als auch positive Werte an und Logarithmusfunktion schneidet die \\(x\\)-Achse bei \\(x = 1\\). Die Produkteigenschaft und die Potenzeigenschaften sind beim Rechnen mit der Logarithmusfunktion zentral. Man merkt sie sich intuitiv als “Die Logarithmusfunktion wandelt Produkte in Summen und Potenzen in Produkte um.” Die Graphen der Exponential- und Logarithmusfunktion sind in Abbildung 4.7 abgebildet.\n\n\n\n\n\n\nAbbildung 4.7: Exponentialfunktion und Logarithmusfunktion\n\n\n\nEin häufiger Begleiter in der Wahrscheinlichkeitstheorie ist die Gammafunktion.\n\nDefinition 4.9 (Gammafunktion) Die Gammafunktion ist definiert durch \\[\\begin{equation}\n\\Gamma : \\mathbb{R} \\to \\mathbb{R}, x \\mapsto \\Gamma(x) := \\int_0^\\infty \\xi^{x-1}\\exp(-\\xi)\\,d\\xi\n\\end{equation}\\] Die Gammafunktion hat folgende Eigenschaften:\n\n\n\n\n\n\n\nEigenschaft\nBedeutung\n\n\n\n\nSpezielle Werte\n\\(\\Gamma(1) = 1\\)\n\n\n\n\\(\\Gamma\\left(\\frac{1}{2} \\right) = \\sqrt{\\pi}\\)\n\n\n\n\\(\\Gamma(n) = (n-1)!\\) für \\(n \\in \\mathbb{N}\\)\n\n\nRekursionseigenschaft\nFür \\(x&gt;0\\) gilt \\(\\Gamma(x+1) = x\\Gamma(x)\\)\n\n\n\n\nEin Auschnitt des Graphen der Gammafunktion ist in Abbildung 4.8 dargestellt.\n\n\n\n\n\n\nAbbildung 4.8: Gammafunktion",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Funktionen</span>"
    ]
  },
  {
    "objectID": "104-Funktionen.html#sec-selbstkontrollfragen-funktionen",
    "href": "104-Funktionen.html#sec-selbstkontrollfragen-funktionen",
    "title": "4  Funktionen",
    "section": "4.4 Selbstkontrollfragen",
    "text": "4.4 Selbstkontrollfragen\n\nGeben Sie die Definition einer Funktion wieder.\nGeben Sie die Definition der Begriffe Bildmenge, Wertebereich, und Urbildmenge wieder.\nGeben Sie die Definitionen der Begriffe Surjektivität, Injektivität, und Bijektivität wieder.\nErläutern Sie, warum \\(f:\\mathbb{R} \\to \\mathbb{R}, x \\mapsto f(x) := x^2\\) weder injektiv noch surjektiv ist.\nErläutern Sie, warum \\(f: [0,\\infty[ \\to [0,\\infty[ , x \\mapsto f(x) := x^2\\) bijektiv ist.\nGeben Sie die Definition der Verkettung von Funktionen wieder.\nGeben Sie die Definition des Begriffs der inversen Funktion wieder.\nGeben Sie die inverse Funktion von \\(x^2\\) auf \\([0,\\infty[\\) an.\nGeben Sie die Definition des Begriffs der linearen Abbildung wieder.\nGeben Sie die Definitionen der Begriffe der univariat-reellwertigen, multivariat-reellwertigen und multivariat-vektorwertigen Funktion wieder.\nSkizzieren Sie die Identitätsfunktion und die konstante Funktion für \\(a := 1\\).\nSkizzieren Sie die linear-affine Funktion \\(f(x) = ax + b\\) für \\(a = 2\\) und \\(b = 3\\).\nSkizzieren Sie die Funktionen \\(f(x) := (x-1)^2\\) und \\(g(x) := (x + 3)^2\\).\nSkizzieren Sie die Exponential- und Logarithmusfunktionen.\nGeben Sie die Summations- und Subtraktionseigenschaften der Exponentialfunktion an.\nGeben Sie die Produkt-, Potenz- und Divisionseigenschaften der Logarithmusfunktion an.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Funktionen</span>"
    ]
  },
  {
    "objectID": "105-Folgen-Grenzwerte-Stetigkeit.html",
    "href": "105-Folgen-Grenzwerte-Stetigkeit.html",
    "title": "5  Folgen, Grenzwerte, Stetigkeit",
    "section": "",
    "text": "5.1 Folgen\nDie in diesem Kapitel behandelten Themen sind in der probabilistischen Datenanayse nicht zentral, sondern bilden Grundpfeiler der reellen Analysis. Durch die enge Verschränkung der modernen Wahrscheinlichkeitstheorie mit analytischen Ansätzen dienen sie jedoch dem Verständnis von zum Beispiel dem Zentralen Grenzwertsatz, der eine Hauptgrundlage für die weit verbreitete Normalverteilungsannahme in der probabilistischen Datenanalyse darstellt. In aller Kürze ist der Zentrale Grenzwertsatz eine Aussage über die Grenzfunktion einer Funktionenfolge, nämlich einer Folge von Zufallsvariablen. Das Wissen um das Wesen von Folgen, Funktionenfolgen und ihren Grenzwerten erlaubt also ein tieferes Verständnis wichtiger Grundannahmen der probabilistischen Datenanalyse. Weiterhin ermöglichen die in diesem Kapitel behandelten Themen zumindest einen ersten Einstieg in das Verständnis der Stetigkeit und Glattheit von Funktionen, die insbesondere in der nichtlinearen Optimierung zu Bestimmung von Parameterschätzern in probabilistischen Modellen wichtige Grundkonzepte bilden.\nWir beginnen mit der Definition des Begriffs der reellen Folge.\nMan beachte, dass weil es unendlich viele natürliche Zahlen gibt, eine reelle Folge immer unendlich viele Folgenglieder hat. Dies sollte man sich insbesondere bei der Schreibweise \\((x_1,x_2,...)\\) bewusst machen. Wir wollen zwei Standardbeispiele für reelle Folgen betrachten.\nBeispiele für reelle Folgen\nNeben den reellen Folgen, die Folgen reeller Zahlen sind, kann man auch Folgen anderer mathematischer Objekte betrachten. Eine wichtige Folgenart sind die Funktionenfolgen.\nDie Definition einer Funktionenfolge ist offenbar analog zur Definition einer reellen Folge. Der Unterschied zwischen einer reellen Folge und einer Funktionenfolge ist, dass die Folgenglieder einer reellen Folge reelle Zahlen, die Folgenglieder einer Funktionenfolgen dagegen univariate reellwertige Funktionen sind. Auch hier wollen wir zwei Standardbeispiel diskutieren.\nBeispiele für Funktionenfolgen",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Folgen, Grenzwerte, Stetigkeit</span>"
    ]
  },
  {
    "objectID": "105-Folgen-Grenzwerte-Stetigkeit.html#sec-folgen",
    "href": "105-Folgen-Grenzwerte-Stetigkeit.html#sec-folgen",
    "title": "5  Folgen, Grenzwerte, Stetigkeit",
    "section": "",
    "text": "Definition 5.1 (Reelle Folge) Eine ist eine Funktion der Form \\[\\begin{equation}\nf : \\mathbb{N} \\to \\mathbb{R}, n \\mapsto f(n)\n\\end{equation}\\] Die Funktionswerte \\(f(n)\\) einer reellen Folge werden üblicherweise mit \\(x_n\\) bezeichnet und genannt. Übliche Schreibweisen für Folgen sind \\[\\begin{equation}\n(x_1,x_2,...)\n\\mbox{ oder }\n(x_n)_{n=1}^\\infty\n\\mbox{ oder }\n(x_n)_{n\\in \\mathbb{N}}\n\\mbox{ oder }\n(x_n).\n\\end{equation}\\]\n\n\n\n\nReelle Folgen der Form \\[\\begin{equation}\nf : \\mathbb{N} \\to \\mathbb{R}, n \\mapsto f(n) := \\left(\\frac{1}{n}\\right)^{\\frac{p}{q}} \\mbox{ mit } p,q\\in \\mathbb{N}\n\\end{equation}\\] nennen wir harmonische Folgen. Für \\(p := q := 1\\) hat eine harmonische Folge die Folgengliederform \\[\\begin{equation}\n\\left(\\frac{1}{1}, \\frac{1}{2}, \\frac{1}{3}, ...\\right).\n\\end{equation}\\]\nReelle Folgen der Form \\[\\begin{equation}\nf : \\mathbb{N} \\to \\mathbb{R}, n \\mapsto f(n) := q^n \\mbox{ mit } q \\in ]-1,1[\n\\end{equation}\\] werden geometrische Folgen genannt. Für \\(q := \\frac{1}{2}\\) hat eine geometrische Folge die Folgengliederform \\[\\begin{align}\n\\begin{split}\n\\left(\\left(\\frac{1}{2}\\right)^1,\\left(\\frac{1}{2}\\right)^2,\\left(\\frac{1}{2}\\right)^3, ...\\right)    \n& = \\left(\\left(\\frac{1}{2}\\right)^1,\\left(\\frac{1}{2}\\right)^2,\\left(\\frac{1}{2}\\right)^3, ...\\right)\\\\\n& = \\left(\\frac{1^1}{2^1},\\frac{1^2}{2^2},\\frac{1^3}{2^3} ...\\right)                                  \\\\\n& = \\left(\\frac{1}{2},\\frac{1}{4},\\frac{1}{8} ...\\right)\n\\end{split}\n\\end{align}\\]\n\n\n\nDefinition 5.2 (Funktionenfolge) Es sei \\(\\phi\\) eine Menge univariater reellwertiger Funktionen mit Definitionsmenge \\(D \\subseteq \\mathbb{R}\\). Dann ist eine Funktionenfolge eine Funktion der Form \\[\\begin{equation}\nF: \\mathbb{N} \\to \\phi, n \\mapsto F(n).\n\\end{equation}\\] Die Funktionswerte \\(F(n)\\) einer Funktionenfolgen werden üblicherweise mit \\(f_n\\) bezeichnet und genannt. Übliche Schreibweisen für Funktionenfolgen sind \\[\\begin{equation}\n(f_1,f_2,...)\n\\mbox{ oder }\n(f_n)_{n=1}^\\infty\n\\mbox{ oder }\n(f_n)_{n\\in \\mathbb{N}}\n\\mbox{ oder }\n(f_n).\n\\end{equation}\\]\n\n\n\n\nWir betrachten die Menge \\(\\phi\\) der univariaten reellwertigen Funktionen der Form \\[\\begin{equation}\n\\phi := \\{f_n|f_n : [0,1] \\to \\mathbb{R}, x \\mapsto f_n(x) := x^n \\mbox{ für } n \\in \\mathbb{N}\\}\n\\end{equation}\\] Dann definiert \\[\\begin{equation}\nF : \\mathbb{N} \\to \\phi, n\\mapsto F(n)\n\\end{equation}\\] eine Funktionenfolge. Für die Funktionswerte der Folgenglieder von \\(F\\) gilt \\[\\begin{equation}\nf_1(x) := x^1, f_2(x) := x^2,  f_3(x) := x^3, ...\n\\end{equation}\\]\nWir betrachten die Menge \\(\\phi\\) der univariaten reellwertigen Funktionen der Form \\[\\begin{equation}\n\\phi := \\{f_n|f_n : [-a,a] \\to \\mathbb{R}, x \\mapsto f_n(x) := \\sum_{k=0}^n \\frac{x^k}{k!} \\mbox{ für } n \\in \\mathbb{N}\\}\n\\end{equation}\\] Dann definiert \\[\\begin{equation}\nF : \\mathbb{N} \\to \\phi, n\\mapsto F(n)\n\\end{equation}\\] eine Funktionenfolge. Für die Funktionswerte der Folgenglieder von \\(F\\) gilt \\[\\begin{equation}\nf_1(x) := \\sum_{k=0}^1 \\frac{x^k}{k!}, f_2(x) := \\sum_{k=0}^2 \\frac{x^k}{k!},  f_3(x) := \\sum_{k=0}^3 \\frac{x^k}{k!}, ...\n\\end{equation}\\]",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Folgen, Grenzwerte, Stetigkeit</span>"
    ]
  },
  {
    "objectID": "105-Folgen-Grenzwerte-Stetigkeit.html#sec-grenzwerte",
    "href": "105-Folgen-Grenzwerte-Stetigkeit.html#sec-grenzwerte",
    "title": "5  Folgen, Grenzwerte, Stetigkeit",
    "section": "5.2 Grenzwerte",
    "text": "5.2 Grenzwerte\nWenn man die Folgenglieder einer Folge betrachtet, kann man sich fragen, welche Werte eine Folge wohl annimmt, wenn der Folgenindex \\(n\\) sehr groß wird, also gegen unendlich strebt. Wenn in diesem Fall die Folgenglieder sehr ähnliche Werte annehmen (und nicht etwa auch unendlich groß werden), so ist man auf den Begriff des Grenzwerts für reelle Folgen bzw. der Grenzfunktion für Funktionenfolgen geführt.\n\nDefinition 5.3 (Grenzwert einer Folge) \\(x \\in \\mathbb{R}\\) heißt Grenzwert einer reellen Folge \\((x_n)_{n=1}^\\infty\\), wenn es zu jedem \\(\\epsilon&gt;0\\) ein \\(m \\in \\mathbb{N}\\) gibt, so dass \\[\\begin{equation}\n|x_n - x| &lt; \\epsilon \\mbox{ für alle } n \\ge m.\n\\end{equation}\\] Eine Folge, die einen Grenzwert besitzt, wird genannt, eine Folge die keinen Grenzwert besitzt, wird genannt. Dafür, dass \\(x \\in \\mathbb{R}\\) Grenzwert der Folge \\((x_n)_{n=1}^\\infty\\) ist, schreibt man auch \\[\\begin{equation}\n\\lim_{n \\to \\infty} x_n = x\n\\mbox{ oder }\nx_n \\to x \\mbox{ für } n \\to \\infty\n\\mbox{ oder }\nx_n\\xrightarrow[]{n \\to \\infty} x.\n\\end{equation}\\]\n\nDer Grenzwert einer Folge kann also, aber muss nicht existieren. So hat zum Beispiel die Folge \\[\\begin{equation}\nf : \\mathbb{N} \\to \\mathbb{R}, n \\mapsto f(n) := n\n\\end{equation}\\] keinen Grenzwert, da hier sowohl \\(n\\) als auch \\(f(n)\\) unendlich groß werden. Die oben betrachteten Beispiele für reelle Folgen dagegen haben Grenzwert. Dies ist Inhalt folgender Beispiele\nBeispiele\n\nFür die verallgemeinerten harmonischen Folgen gilt mit \\(p,q \\in \\mathbb{N}\\) \\[\n\\lim_{n\\to \\infty} \\left(\\frac{1}{n}\\right)^{\\frac{p}{q}} = 0.\n\\tag{5.1}\\]\nFür die geometrischen Folgen gilt mit \\(q \\in ]-1,1[\\) \\[\n\\lim_{n\\to \\infty} q^n = 0.\n\\tag{5.2}\\]\n\nMan nennt die harmonischen und geometrischen Folgen entsprechend auch Nullfolgen. Für Beweise von Gleichung 5.1 und Gleichung 5.2 verweisen wir auf die weiterführende Literatur. Tatsächlich sind diese Beweise nicht trivial und rühren an die Grundannahmen über das Wesen der reellen Zahlen. Wir visualisieren die ersten zehn Folgenglieder sowie die Grenzwerte der harmonischen Folge fr \\(p := q := 1\\) und der geometrischen Folge für \\(q := 1/2\\) in Abbildung 5.1.\n\n\n\n\n\n\nAbbildung 5.1: Beispiele für Grenzwerte reeller Folgen.\n\n\n\nFür Funktionenfolgen ist eine Möglichkeit der Erweiterung der Begriffe der Konvergenz und des Grenzwertes folgende.\n\nDefinition 5.4 (Punktweise Konvergenz und Grenzfunktion einer Funktionenfolge) \\(F = (f_n)_{n\\in \\mathbb{N}}\\) sei eine Funktionenfolge von univariaten reellwertigen Funktionen mit Definitionsbereich \\(D\\). \\(F\\) heißt , wenn die reelle Folge \\(\\left(f_n(x)\\right)_{n\\in \\mathbb{N}}\\) für jedes \\(x \\in D\\) eine konvergente Folge ist, also einen Grenzwert besitzt. Die Funktion, die jedem \\(x \\in D\\) diesen Grenzwert von \\(\\left(f_n(x)\\right)_{n\\in \\mathbb{N}}\\) zuordnet, heißt dann die und hat die Form \\[\\begin{equation}\nf : D \\to \\mathbb{R}, x \\mapsto f(x) := \\lim_{n\\to \\infty}f_n(x).\n\\end{equation}\\]\n\nMan beachte, dass die Grenzwerte von konvergenten reellen Folgen reelle Zahlen sind, die Grenzfunktionen von punktweise konvergenten Funktionenfolgen dagegen sind Funktionen. Neben der punktweisen Konvergenz von Funktionenfolgen gibt es noch den mächtigeren Begriff der gleichmäßigen Konvergenz von Funktionenfolgen, für den wir aber auf die weiterführende Literatur verweisen. Als Beispiel betrachten wir die Grenzfunktionen der oben diskutierten Funktionenfolgen, wobei wir für Beweise ebenfalls auf die weiterführende Literatur verweisen.\nBeispiele\n\nWir betrachten die Funktionenfolge \\[\\begin{equation}\nF : \\mathbb{N} \\to \\phi, n\\mapsto F(n)\n\\end{equation}\\] mit \\[\\begin{equation}\n\\phi := \\{f_n|f_n : [0,1] \\to \\mathbb{R}, x \\mapsto f_n(x) := x^n \\mbox{ für } n \\in \\mathbb{N}\\}\n\\end{equation}\\] Dann ist \\(F\\) punktweise konvergent mit Grenzfunktion \\[\\begin{equation}\nf : [0,1] \\to \\mathbb{R}, x \\mapsto f(x)\n:=\n\\begin{cases}\n0, & \\mbox{ für } x \\in [0,1[ \\\\\n1, & \\mbox{ für } x = 1       \\\\\n\\end{cases}\n\\end{equation}\\] da \\(f_n(x) := x^n\\) für \\(x \\in [0,1[\\) eine geometrische Folge und damit eine Nullfolge ist und \\(f_n(x) := x^n\\) für \\(x = 1\\) eine konstante Folge ist, für die alle Folgenglieder den Abstand \\(0\\) von \\(1\\) haben. Die Funktionenfolge \\(F\\) konvergiert also gegen eine Funktion, die auf dem gesamten Intervall \\([0,1]\\) gleich Null ist, außer im Punkt \\(1\\). Diese Funktion hat offenbar einen Sprung.\nWir betrachten die Funktionenfolge \\[\\begin{equation}\nF : \\mathbb{N} \\to \\phi, n\\mapsto F(n)\n\\end{equation}\\] mit \\[\\begin{equation}\n\\phi := \\{f_n|f_n : [-a,a] \\to \\mathbb{R}, x \\mapsto f_n(x) := \\sum_{k=0}^n \\frac{x^k}{k!} \\mbox{ für } n \\in \\mathbb{N}\\}\n\\end{equation}\\] Dann ist \\(F\\) punktweise konvergent mit Grenzfunktion \\[\\begin{equation}\nf : [-a,a] \\to \\mathbb{R}, x \\mapsto f(x)\n:= \\sum_{k=0}^\\infty \\frac{x^k}{k!}\n=: \\exp(x)\n\\end{equation}\\] Die Funktionenfolge \\(F\\) konvergiert also gegen die Exponentialfunktion auf \\([-a,a]\\). Umgekehrt betrachtet ist die Exponentialfunktion gerade durch \\[\\begin{equation}\n\\exp(x) :=  \\sum_{k=0}^\\infty \\frac{x^k}{k!}\n\\end{equation}\\] definiert.\n\n\n\n\n\n\n\nAbbildung 5.2: Beispiele für Grenzwerte von Funktionenfolgen",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Folgen, Grenzwerte, Stetigkeit</span>"
    ]
  },
  {
    "objectID": "105-Folgen-Grenzwerte-Stetigkeit.html#stetigkeit",
    "href": "105-Folgen-Grenzwerte-Stetigkeit.html#stetigkeit",
    "title": "5  Folgen, Grenzwerte, Stetigkeit",
    "section": "5.3 Stetigkeit",
    "text": "5.3 Stetigkeit\nIn diesem Abschnitt versuchen wir uns dem Begriff der Stetigkeit einer Funktion zu nähern. Intuitiv ist eine Funktion stetig, wenn sie keine Sprünge hat oder äquivalent, wenn kleine Änderungen in ihren Argumenten stets nur zu kleinen Änderungen in ihren Funktionswerten (und damit eben keinen Sprüngen) führen. Zur Definition der Stetigkeit benötigen wir zunächst den Begriff des Grenzwertes einer Funktion.\n\nDefinition 5.5 (Grenzwert einer Funktion)  \nFür \\(D\\subseteq\\mathbb{R}\\) und \\(Z\\subseteq\\mathbb{R}\\) sei \\(f : D \\to Z, x \\mapsto f(x)\\) eine Funktion und es seien \\(a,b \\in \\mathbb{R}\\). \\(b\\) heißt , wenn\nWenn \\(b\\) Grenzwert der Funktion \\(f\\) für \\(x\\) gegen \\(a\\) ist, so schreibt man auch \\(\\lim_{x \\to a} f(x) = b\\).\n\nIn Abbildung 5.3 visualisieren wir den Grenzwert der Exponential funktion in \\(a = 1\\) durch Darstellung von Folgenglieder \\(x_n \\to 1\\) und den entsprechenden Folgengliedern \\(f(x_n)\\). Offenbar gilt \\(\\lim_{x\\to 1}\\exp(x) = e\\).\n\n\n\n\n\n\nAbbildung 5.3: Beispiele für einen Grenzwert einer Funktion\n\n\n\nWir können nun den Begriff der Stetigkeit einer Funktion definieren.\n\nDefinition 5.6 (Stetigkeit einer Funktion) Eine Funktion \\(f : D \\to Z\\) mit \\(D \\subseteq \\mathbb{R}, Z \\subseteq \\mathbb{R}\\) heißt , wenn \\[\\begin{equation}\n\\lim_{x\\to a} f(x) = f(a).\n\\end{equation}\\] Ist \\(f\\) in jedem \\(x \\in D\\) stetig, so heißt .\n\nMan beachte, dass für eine in \\(a\\) stetige Funktion folgt, dass \\[\\begin{equation}\n\\lim_{x \\to a} f(x) = f\\left(\\lim_{x\\to a} x\\right)\n\\end{equation}\\] Bei stetigen Funktion können also Grenzwertbildung und Auswertung der Funktion vertauscht werden.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Folgen, Grenzwerte, Stetigkeit</span>"
    ]
  },
  {
    "objectID": "106-Differentialrechnung.html",
    "href": "106-Differentialrechnung.html",
    "title": "6  Differentialrechnung",
    "section": "",
    "text": "6.1 Definitionen und Rechenregeln\nDie Differentialrechnung befasst sich mit der Änderung von Funktionen. Sie bildet einerseits die Grundlage für die mathematische Modellierung mithilfe von Differentialgleichungen, also der Beschreibung von Funktionen anhand ihrer Änderungsraten. Zum anderen bildet die Differentialrechnung die Grundlage der Optimierung, also des Bestimmens von Extremstellen von Funktionen. In Kapitel 6.1 führen wir zunächst den Begriff der Ableitung und mit ihm verbundene elementare Rechenregeln ein. In Kapitel 6.2 widmen wir uns dann der Frage, wie man mithilfe von Ableitungen Extremstellen von Funktionen bestimmen kann.\nWir beginnen mit folgender Definition.\nFür \\(h&gt;0\\) heißt der Ausdruck \\[\\begin{equation}\n\\frac{f(a+h)-f(a)}{h}\n\\end{equation}\\] Newtonscher Differenzquotient. Wie in Abbildung 6.1 dargestellt, misst der Newtonsche Differenzquotient die Änderung \\(f(a+h)-f(a)\\) von \\(f\\) auf der \\(y\\)-Achse pro Strecke \\(h\\) auf der \\(x\\)-Achse. Wenn also zum Beispiel \\(f(a)\\) und \\(f(a+h)\\) die Position eines Objektes zu einem Zeitpunkt \\(a\\) und zu einem späteren Zeitpunkt \\(a+h\\) repräsentieren, dann ist \\(f(a+h)-f(a)\\) die von diesem Objekt in der Zeit \\(h\\) zurückgelegte Strecke, also seine durchschnittliche Geschwindigkeit über den Zeitraum \\(h\\). Für \\(h\\to 0\\) misst der Newtonsche Differenzquotient dann die instantane Änderungsrate von \\(f\\) in \\(a\\), also im Beispiel die Geschwindigkeit des Objektes zu einem Zeitpunkt \\(a\\).\nAus mathematischer Sicht ist es wichtig, bei der Definition der Ableitung zwischen den Symbolen \\(f'(a)\\) und \\(f'\\) zu unterscheiden. Wie üblich bezeichnet \\(f'(a)\\) den Wert einer Funktion, also eine Zahl. \\(f'\\) dagegen bezeichnet eine Funktion, nämlich die Funktion, deren Werte als \\(f'(a)\\) für alle \\(a \\in \\mathbb{R}\\) bestimmt sind.\nEs existieren in der Literatur verschiedene, historisch gewachsene Notationen für Ableitungen, welche alle das identische Konzept der Ableitung repräsentieren.\nWir werden im Folgenden für univariate reellwertige Funktionen vor allem die Lagrange-Notation \\(f'\\) und \\(f'(x)\\) als Bezeichner wählen. In Berechnungen nutzen wir auch eine adapatierte Form der Leibniz-Notation und verstehen dort die Schreibweise \\(\\frac{d}{dx}f(x)\\) als den Auftrag, die Ableitung von \\(f\\) zu berechnen. Die Newton-Notation wird vor allem eingesetzt, wenn das Funktionsargument die Zeit repräsentiert und dann üblicherweise mit \\(t\\) für time bezeichnet wird. \\(\\dot{f}(t)\\) bezeichnet dann die Änderungsrate von \\(f\\) zum Zeitpunkt \\(t\\). Die Euler-Notation ist vor allem im Kontext multivariater reell- oder vektorwertiger Funktionen nützlich.\nWir setzen eine Reihe von Ableitungen elementarer Funktionen als bekannt voraus, diese sind in Tabelle zusammengstellt. Für Beweise verweisen wir wiederum auf die weiterführende Literatur.\nAbleitungen elementarer Funktionen\n\n\n\n\n\n\n\nName\nDefinition\nAbleitung\n\n\n\n\nPolynomfunktion\n\\(f(x) := \\sum_{i=0}^n a_ix^i\\)\n\\(f'(x) = \\sum_{i=1}^n ia_ix^{i-1}\\)\n\n\nKonstante Funktion\n\\(f(x) := a\\)\n\\(f'(x) = 0\\)\n\n\nIdentitätsfunktion\n\\(f(x) := x\\)\n\\(f'(x) = 1\\)\n\n\nLinear-affine Funktion\n\\(f(x) := ax + b\\)\n\\(f'(x) = a\\)\n\n\nQuadratfunktion\n\\(f(x) := x^2\\)\n\\(f'(x) = 2x\\)\n\n\nExponentialfunktion\n\\(f(x) := \\exp(x)\\)\n\\(f'(x) = \\exp(x)\\)\n\n\nLogarithmusfunktion\n\\(f(x) := \\ln(x)\\)\n\\(f'(x) = \\frac{1}{x}\\)\nIn Abbildung 6.2 visualisieren wir die Identitätsfunktion, eine linearen Funktion und die Quadratfunktion zusammen mit ihrer jeweiligen Ableitung. In Abbildung Abbildung 6.3 visualisieren wir die Exponential- und Logarithmusfunktionen zusammen mit ihrer jeweiligen Ableitung.\nBasierend auf der Definition der Ableitung einer univariaten reellwertigen Funktionen lassen sich leicht weitere Ableitungen einer solchen Funktion definieren.\nIn Analogie zu oben Gesagtem schreiben wir in Berechnungen auch \\(\\frac{d^2}{dx^2}f(x)\\) für den Auftrag, die zweite Ableitung einer Funktion \\(f\\) zu bestimmen. Die nullte Ableitung \\(f^{(0)}\\) von \\(f\\) ist \\(f\\) selbst. Der Tradition und Einfachheit halber schreibt man für \\(k &lt; 4\\) gemäß der Lagrange-Notation meist \\(f',f''\\) und \\(f'''\\) anstelle von \\(f^{(1)}, f^{(2)}\\) und \\(f^{(3)}\\).\nBeispiel\nEs sei \\[\\begin{equation}\nf : \\mathbb{R} \\to \\mathbb{R}, x \\mapsto f(x) := x^2\n\\end{equation}\\] Dann gilt \\[\\begin{equation}\nf^{(1)}(x) = f'(x) = \\frac{d}{dx}\\left(x^2\\right) = 2x\n\\end{equation}\\] Weiterhin gelten \\[\\begin{align}\n\\begin{split}\nf^{(2)}(x) = \\left(f^{(2-1)}\\right)'(x)  = \\left(f^{(1)}\\right)'(x) & = \\frac{d}{dx}(2x) = 2, \\\\\nf^{(3)}(x) = \\left(f^{(3-1)}\\right)'(x)  = \\left(f^{(2)}\\right)'(x) & = \\frac{d}{dx}(2)  = 0, \\\\\nf^{(4)}(x) = \\left(f^{(4-1)}\\right)'(x)  = \\left(f^{(3)}\\right)'(x) & = \\frac{d}{dx}(0)  = 0.\n\\end{split}\n\\end{align}\\] Man berechnet also durchgängig lediglich erste Ableitungen.\nZum Bestimmen der Ableitung einer Funktion sind eine Reihe von Rechenregeln hilfreich, die es erlauben, die Ableitung einer Funktion aus den Ableitungen ihrer Unterfunktionen herzuleiten. Für Beweise der in folgendem Theorem eingeführten Rechenregeln verweisen wir auf die weiterführende Literatur.\nBeispiele\nFür ein Beispiel zur Anwendung der Summenregel, sei \\[\\begin{equation}\nf : \\mathbb{R} \\to \\mathbb{R}, x \\mapsto f(x) := 4x^3 + 3x^2\n\\end{equation}\\] Dann hat \\(f\\) die Form \\[\\begin{equation}\nf(x) = \\sum_{i=1}^2 g_i(x) = g_1(x) + g_2(x) \\mbox{ mit } g_1(x) := 4x^3 \\mbox{ und } g_2(x) := 3x^2.\n\\end{equation}\\] Es gelten außerdem \\[\\begin{equation}\ng_1'(x) = \\frac{d}{dx}\\left(4x^3\\right) = 12x^2 \\mbox{ und } g_2'(x) = \\frac{d}{dx}\\left(3x^2\\right) = 6 x.\n\\end{equation}\\] Also gilt mit der Summenregel \\[\\begin{equation}\nf'(x) = \\sum_{i=1}^2 g_i'(x) = g_1'(x) + g_2'(x) =  12x^2 + 6x.\n\\end{equation}\\]\nFür ein Beispiel zur Anwendung der Kettenregel sei \\[\\begin{equation}\nf : \\mathbb{R} \\to \\mathbb{R}, x \\mapsto f(x) := \\exp\\left(-x^2\\right)\n\\end{equation}\\] Dann hat \\(f\\) die Form \\[\\begin{equation}\nf(x) = g_2(g_1(x)) \\mbox{ mit }  g_2(z) := \\exp(z) \\mbox{ und }  g_1(x) := -x^2\n\\end{equation}\\] wobei man beachte, dass für das Funktionsargument \\(z\\) in \\(g_2\\) im Fall von \\(f\\) gerade \\(z = g_1(x) := -x^2\\) eingesetzt wird. Es gelten außerdem \\[\\begin{equation}\ng_1'(x) = \\frac{d}{dx}\\left(-x^2\\right) = -2x  \\mbox{ und }g_2'(z) = \\frac{d}{dz}\\left(\\exp(z)\\right) = \\exp(z)\n\\end{equation}\\] Also gilt mit der Kettenregel \\[\\begin{equation}\nf'(x) = g_2'(g_1(x))g_1'(x) = \\exp\\left(-x^2\\right)\\left(-2x\\right) = -2x\\exp\\left(-x^2 \\right).\n\\end{equation}\\] Man kann sich die resultierende Ableitung der Kettenregel also merken als “Die Ableitung der äußeren Funktion an der Stelle der inneren Funktion mal die Ableitung der inneren Funktion.”",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Differentialrechnung</span>"
    ]
  },
  {
    "objectID": "106-Differentialrechnung.html#sec-definitionen-rechenregeln",
    "href": "106-Differentialrechnung.html#sec-definitionen-rechenregeln",
    "title": "6  Differentialrechnung",
    "section": "",
    "text": "Definition 6.1 (Differenzierbarkeit und Ableitung) Es sei \\(I \\subseteq \\mathbb{R}\\) ein Intervall und \\[\\begin{equation}\nf : I \\to \\mathbb{R}, x \\mapsto f(x)\n\\end{equation}\\] eine univariate reellwertige Funktion. \\(f\\) heißt in \\(a \\in I\\) differenzierbar, wenn der Grenzwert \\[\\begin{equation}\nf'(a) := \\lim_{h\\to 0} \\frac{f(a+h)-f(a)}{h}\n\\end{equation}\\] existiert. \\(f'(a)\\) heißt dann die Ableitung von \\(f\\) an der Stelle \\(a\\). Ist \\(f\\) differenzierbar für alle \\(x \\in I\\), so heißt \\(f\\) differenzierbar und die Funktion \\[\\begin{equation}\nf' : I \\to \\mathbb{R}, x \\mapsto f'(x)\n\\end{equation}\\] heißt Ableitung von \\(f\\).\n\n\n\n\n\n\n\n\nAbbildung 6.1: Newtonscher Differenzquotient\n\n\n\n\n\n\nDefinition 6.2 (Notation für Ableitungen univariater reellwertiger Funktionen) Es sei \\(f\\) eine univariate reellwertige Funktion. Äquivalente Schreibweisen für die Ableitung von \\(f\\) und die Ableitung von \\(f\\) an einer Stelle \\(x\\) sind\n\ndie Lagrange-Notation \\(f'\\) und \\(f'(x)\\),\ndie Leibniz-Notation \\(\\frac{df}{dx}\\) und \\(\\frac{df(x)}{dx}\\),\ndie Newton-Notation \\(\\dot{f}\\) und \\(\\dot{f}(x)\\), sowie\ndie Euler-Notation \\(Df\\) und \\(Df(x)\\),\n\nrespektive.\n\n\n\n\n\n\n\n\n\n\n\nAbbildung 6.2: Ableitungen dreier elementarer Funktionen\n\n\n\n\n\n\n\n\n\nAbbildung 6.3: Ableitungen von Exponentialfunktion und Logarithmusfunktion\n\n\n\n\n\nDefinition 6.3 (Höhere Ableitungen) Es sei \\(f\\) eine univariate reellwertige Funktion und \\[\\begin{equation}\nf^{(1)} := f'\n\\end{equation}\\] sei die Ableitung von \\(f\\). Die \\(k\\)-te Ableitung von \\(f\\) ist rekursiv definiert durch \\[\\begin{equation}\nf^{(k)} := \\left(f^{(k-1)}\\right)' \\mbox{ für } k \\ge 1,\n\\end{equation}\\] unter der Annahme, dass \\(f^{(k-1)}\\) differenzierbar ist. Insbesondere ist die zweite Ableitung von \\(f\\) definiert durch die Ableitung von \\(f'\\), also \\[\\begin{equation}\nf'' := (f')'.\n\\end{equation}\\]\n\n\n\n\n\n\nTheorem 6.1 (Rechenregeln für Ableitungen) Für \\(i = 1,...,n\\) seien \\(g_i\\) reellwertige univariate differenzierbare Funktionen. Dann gelten folgende Rechenregeln:\n\nSummenregel \\[\\begin{equation}\n\\mbox{Für } f(x) := \\sum_{i=1}^n g_i(x) \\mbox{ gilt } f'(x) = \\sum_{i=1}^n g_i'(x).\n\\end{equation}\\]\nProduktregel \\[\\begin{equation}\n\\mbox{Für } f(x) := g_1(x)g_2(x) \\mbox{ gilt } f'(x) = g_1'(x)g_2(x) + g_1(x)g_2'(x).\n\\end{equation}\\]\nQuotientenregel \\[\\begin{equation}\n\\mbox{Für } f(x) := \\frac{g_1(x)}{g_2(x)} \\mbox{ gilt } f'(x) = \\frac{g_1'(x)g_2(x) - g_1(x)g_2'(x)}{g_2^2(x)}.\n\\end{equation}\\]\nKettenregel \\[\\begin{equation}\n\\mbox{Für } f(x) := g_1(g_2(x)) \\mbox{ gilt } f'(x) = g_1'(g_2(x))g'_2(x).\n\\end{equation}\\]",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Differentialrechnung</span>"
    ]
  },
  {
    "objectID": "106-Differentialrechnung.html#sec-analytische-optimierung",
    "href": "106-Differentialrechnung.html#sec-analytische-optimierung",
    "title": "6  Differentialrechnung",
    "section": "6.2 Analytische Optimierung",
    "text": "6.2 Analytische Optimierung\nEine wichtige Anwendung der Differentialrechnung ist das Bestimmen von Extremstellen von Funktionen. Dabei geht es im Kern um die Frage, für welche Werte ihrer Definitionsmenge eine Funktion ein Maximum oder ein Minimum annimmt. Bei einfachen Funktionen ist dies analytisch möglich. Die generelle Vorgehensweise dabei ist oft auch unter dem Stichwort “Kurvendiskussion” bekannt. In der Anwendung ist ein analytisches Vorgehen zur Optimierung von Funktionen meist nicht möglich und es werden Computeralgorithmen zur Bestimmung von Extremstellen genutzt. Ein Verständnis dieser Algorithmen setzt allerdings ein Verständnis der Prinzipien der analytischen Optimierung voraus. In diesem Abschnitt geben wir eine Einführung in die analytische Optimierung von univariaten reellwertigen Funktionen. Wir gehen dabei eher informell vor. Einen formaleren Zugang geben wir an späterer Stelle im Kontext der nichtlinearen Optimierung. Wir beginnen damit, die Begriffe der erwähnten Maxima und Minima von univariaten reellwertigen Funktionen zu präzisieren.\n\nDefinition 6.4 (Extremstellen und Extremwerte) Es seien \\(U \\subseteq \\mathbb{R}\\) und \\(f : U \\to \\mathbb{R}\\) eine univariate reellwertige Funktion. \\(f\\) hat an der Stelle \\(x_0 \\in U\\)\n\nein lokales Minimum, wenn es ein Intervall \\(I := ]a,b[\\) gibt mit \\(x_0 \\in ]a,b[\\) und \\[\\begin{equation}\nf(x_0) \\le f(x) \\mbox{ für alle } x\\in I\\cap U,\n\\end{equation}\\]\nein globales Minimum, wenn gilt, dass \\[\\begin{equation}\nf(x_0) \\le f(x) \\mbox{ für alle } x\\in U,\n\\end{equation}\\]\nein lokales Maximum, wenn es ein Intervall \\(I := ]a,b[\\) gibt mit \\(x_0 \\in ]a,b[\\) und \\[\\begin{equation}\nf(x_0) \\ge f(x) \\mbox{ für alle }  x\\in I\\cap U,\n\\end{equation}\\]\nein lokales Maximum, wenn gilt, dass \\[\\begin{equation}\nf(x_0) \\ge f(x) \\mbox{ für alle } x\\in U.\n\\end{equation}\\]\n\nDer Wert \\(x_0 \\in U\\) der Definitionsmenge von \\(f\\) heißt entsprechend lokale oder globale Minimalstelle oder Maximalstelle, der Funktionswert \\(f(x_0) \\in \\mathbb{R}\\) heißt entsprechend lokales oder globales Minimum oder Maximum. Generell heißt der Wert \\(x_0 \\in U\\) Extremstelle und der Funktionswert \\(f(x_0) \\in \\mathbb{R}\\) Extremwert.\n\nExtremstellen von Funktionen werden häufig mit \\[\\begin{equation}\n\\underset{x \\in I \\cap U}{\\operatorname{argmin}} f(x) \\mbox{ oder }\n\\underset{x \\in I \\cap U}{\\operatorname{argmax}} f(x)\n\\end{equation}\\] bezeichnet und Extremwerte von Funktionen werden häufig mit \\[\\begin{equation}\n\\min_{x \\in I \\cap U} f(x) \\mbox{ oder }\n\\max_{x \\in I \\cap U} f(x)\n\\end{equation}\\] bezeichnet.\nDie analytische Optimierung von univariaten reellwertigen Funktionen basiert auf den sogenannten notwendigen und hinreichenden Bedingungen für Extrema. Erstere macht eine Aussage über das Verhalten der ersten Ableitung einer Funktion an einer Extremstelle, letztere macht eine Aussage über das Verhalten einer Funktion an einer Stelle, die bestimmten Forderungen an ihre erste und zweite Ableitung genügt.\n\nTheorem 6.2 (Notwendige Bedingung für Extrema) \\(f\\) sei eine univariate reellwertige Funktion. Dann gilt \\[\\begin{equation}\nx_0 \\mbox{ ist Extremstelle von } f \\Rightarrow f'(x_0) = 0.\n\\end{equation}\\]\n\nWenn \\(x_0\\) eine Extremstelle von \\(f\\) ist, dann ist also die erste Ableitung von \\(f\\) in \\(x_0\\) gleich null. Anstelle eines Beweises überlegen wir uns, dass zum Beispiel an eine lokaler Maximalstelle \\(x_0\\) von \\(f\\) gilt: links von \\(x_0\\) steigt \\(f\\) an, rechts von \\(x_0\\) fällt \\(f\\) ab. In \\(x_0\\) aber steigt \\(f\\) weder an, noch fällt \\(f\\) ab, es ist also nachvollziehbar, dass \\(f'(x_0) = 0\\) ist.\n\nTheorem 6.3 (Hinreichende Bedingungen für lokale Extrema) \\(f\\) sei eine zweimal differenzierbare univariate reellwertige Funktion.\n\nWenn für \\(x_0 \\in U \\subseteq \\mathbb{R}\\) \\[\\begin{equation}\nf'(x_0) = 0 \\mbox{ und } f''(x_0) &gt; 0\n\\end{equation}\\] gilt, dann hat \\(f\\) an der Stelle \\(x_0\\) ein Minimum.\nWenn für \\(x_0 \\in U \\subseteq \\mathbb{R}\\) \\[\\begin{equation}\nf'(x_0) = 0 \\mbox{ und } f''(x_0) &lt; 0\n\\end{equation}\\] gilt, dann hat \\(f\\) an der Stelle \\(x_0\\) ein Maximum.\n\n\nWir verzichten wiederum auf einen Beweis und verdeutlichen uns die Bedingung an dem in Abbildung 6.4 gezeigtem Beispiel. Hier ist offenbar \\(x_0 = 1\\) eine lokale Minimalstelle von \\(f(x) = (x-1)^2\\). Man erkennt: links von \\(x_0\\) fällt \\(f\\) ab, rechts von \\(x_0\\) steigt \\(f\\) an. In \\(x_0\\) steigt \\(f\\) weder an, noch fällt \\(f\\) ab, also ist \\(f'(x_0) = 0\\). Weiter gilt, dass links und rechts von \\(x_0\\) und in \\(x_0\\) die Änderung \\(f''\\) von \\(f'\\) positiv ist: links von \\(x_0\\) schwächt sich die Negativität von \\(f'\\) zu \\(0\\) ab und rechts von \\(x_0\\) verstärkt sich die Positivität von \\(f'\\).\n\n\n\n\n\n\nAbbildung 6.4: Analytische Optimierung von \\(f(x) := (x-1)^2\\)\n\n\n\nInsbesondere die hinreichende Bedingung für das Vorliegen von Extremstellen legt folgendes Standardverfahren zur Bestimmung von lokalen Extremstellen nahe.\n\nTheorem 6.4 (Standardverfahren der analytischen Optimierung) \\(f\\) sei eine univariate reellwertige Funktion. Lokale Extremstellen von \\(f\\) können mit folgendem Standardverfahren der analytischen Optimierung identifiziert werden:\n\nBerechnen der ersten und zweiten Ableitung von \\(f\\).\nBestimmen von Nullstellen \\(x^*\\) von \\(f'\\) durch Auflösen von \\(f'(x^*) = 0\\) nach \\(x^*\\). Die Nullstellen von \\(f'\\) sind dann Kandidaten für Extremstellen von \\(f\\).\nEvaluation von \\(f''(x^*)\\): Wenn \\(f''(x^*) &gt; 0\\) ist, dann ist \\(x^*\\) lokale Minimumstelle von \\(f\\); wenn \\(f''(x^*) &lt; 0\\) ist, dann ist \\(x^*\\) lokale Maximumstelle von \\(f\\); wenn \\(f''(x^*) = 0\\) ist, dann ist \\(x^*\\) keine Extremstelle von \\(f\\).\n\n\nAnstelle eines Beweises betrachten wir beispielhaft die Funktion \\[\\begin{equation}\nf : \\mathbb{R} \\to \\mathbb{R}, x \\mapsto f(x) := (x - 1)^2.\n\\end{equation}\\] aus Abbildung 6.4. Die erste Ableitung von \\(f\\) ergibt sich mit der Kettenregel zu \\[\\begin{equation}\nf'(x) = \\frac{d}{dx}\\left((x-1)^2 \\right) = 2(x-1)\\cdot \\frac{d}{dx}(x-1) = 2x - 2.\n\\end{equation}\\] Die zweite Ableitung von \\(f\\) ergibt sich zu \\[\\begin{equation}\nf''(x) = \\frac{d}{dx}f'(x) = \\frac{d}{dx}(2x - 2) = 2 &gt; 0 \\mbox{ für alle }\nx \\in \\mathbb{R}.\n\\end{equation}\\] Auflösen von \\(f'(x^*) = 0\\) nach \\(x^*\\) ergibt \\[\\begin{equation}\nf'(x^*) = 0\n\\Leftrightarrow\n2x^* - 2 = 0\n\\Leftrightarrow\n2x^* = 2\n\\Leftrightarrow\nx^* = 1.\n\\end{equation}\\] \\(x^* = 1\\) ist folglich eine Minimalstelle von \\(f\\) mit zugehörigen Minimalwert \\(f(1) = 0\\).",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Differentialrechnung</span>"
    ]
  },
  {
    "objectID": "106-Differentialrechnung.html#sec-differentialrechnung-multivariater-reellwertiger-funktionen",
    "href": "106-Differentialrechnung.html#sec-differentialrechnung-multivariater-reellwertiger-funktionen",
    "title": "6  Differentialrechnung",
    "section": "6.3 Differentialrechnung multivariater reellwertiger Funktionen",
    "text": "6.3 Differentialrechnung multivariater reellwertiger Funktionen\nWir erinnern zunächst an den Begriff der multivariaten reellwertigen Funktion.\n\nDefinition 6.5 (Multivariate reellwertige Funktion) Eine Funktion der Form \\[\\begin{equation}\nf : \\mathbb{R}^n \\to \\mathbb{R}, x \\mapsto f(x) = f(x_1,...,x_n)\n\\end{equation}\\] heißt multivariate reellwertiger Funktion.\n\nDie Argumente multivariater reellwertiger Funktionen sind also reelle \\(n\\)-Tupel der Form \\(x := (x_1,...,x_n)\\) während ihre Funktionswerte reelle Zahlen sind. Ein Beispiel für eine multivariate reellwertige für \\(n:=2\\) ist \\[\\begin{equation}\nf : \\mathbb{R}^2 \\to \\mathbb{R}, x \\mapsto f(x) := x_1^2 + x_2^2\n\\end{equation}\\]\nWir visualisieren diese Funktion in Abbildung 6.5. Dabei zeigt die rechte Abbildung eine Darstellung mithilfe sogenannter Isokonturen, also Linien im Definitionsbereich der Funktion, für die die Funktion identische Werte annimmt. Die entsprechenden Werte sind für ausgewählte Isokonturen in der Abbildung vermerkt.\n\n\n\n\n\n\nAbbildung 6.5: Visualisierungen einer bivariaten Funktion.\n\n\n\nWir wollen nun beginnen, die Begriffe der Differenzierbarkeit und der Ableitung univariater reellwertiger Funktionen auf den Fall multivariater reellwertiger Funktion zu erweitern. Dazu führen wir zunächst die Begriffe der partiellen Differenzierbarkeit und der partiellen Ableitung ein.\n\nDefinition 6.6 (Partielle Differenzierbarkeit und partielle Ableitung) Es sei \\(D \\subseteq \\mathbb{R}^n\\) eine Menge und \\[\\begin{equation}\nf : D \\to \\mathbb{R}, x \\mapsto f(x)\n\\end{equation}\\] eine multivariate reellwertige Funktion. \\(f\\) heißt in \\(a \\in D\\) nach \\(x_i\\) partiell differenzierbar, wenn der Grenzwert \\[\\begin{equation}\n\\frac{\\partial}{\\partial x_i}f(x) := \\lim_{h\\to 0} \\frac{f(a + he_i)-f(a)}{h}\n\\end{equation}\\] existiert. \\(\\frac{\\partial}{\\partial x_i}f(a)\\) heißt dann die partielle Ableitung von \\(f\\) nach \\(x_i\\) an der Stelle \\(a\\). Wenn \\(f\\) für alle \\(x \\in D\\), nach \\(x_i\\) partiell differenzierbar ist, dann heißt \\(f\\) nach \\(x_i\\) partiell differenzierbar und die Funktion \\[\\begin{equation}\n\\frac{\\partial}{\\partial x_i} f: D \\to \\mathbb{R}, x \\mapsto \\frac{\\partial}{\\partial x_i}f(x)\n\\end{equation}\\] heißt partielle Ableitung von \\(f\\) nach \\(x_i\\). \\(f\\) heißt partiell differenzierbar in \\(x \\in D\\), wenn \\(f\\) für alle \\(i = 1,...,n\\) in \\(x \\in D\\) nach \\(x_i\\) partiell differenzierbar ist, und\n\\(f\\) heißt partiell differenzierbar, wenn \\(f\\) für alle \\(i = 1,...,n\\) in allen \\(x \\in D\\) nach \\(x_i\\) partiell differenzierbar ist.\n\nIn Definition 6.6 bezeichnet \\(e_i \\in \\mathbb{R}^n\\) bezeichnet den \\(i\\)ten kanonischen Einheitsvektor, für den gilt, dass \\(e_{i_j} = 1\\) für \\(i=j\\) und \\(e_{i_j} = 0\\) für \\(i \\neq j\\) mit \\(j = 1,...,n\\) (vgl. Definition 8.14). In Analogie und Verallgemeinerung zum Newtonschen Differenzquotienten misst der hier auftretende Differenzquotient \\[\\begin{equation}\n\\frac{f(x + he_i)-f(x)}{h}\n\\end{equation}\\] die Änderung \\(f(x+he_i)-f(x)\\) von \\(f\\) pro Strecke \\(h\\) in Richtung \\(e_i\\). Wir visualisieren die Bestandteile dieses Quotienten im Falle einer bivariaten Funktion in Abbildung 6.6.\n\n\n\n\n\n\nAbbildung 6.6: Partieller Newtonscher Differenzquotient\n\n\n\nFür \\(h\\to 0\\) misst der Differenzquotient entsprechend die Änderungsrate von \\(f\\) in \\(x\\) in Richtung \\(e_i\\). Wie bei der Betrachtung von Ableitungen gilt, dass \\(\\frac{\\partial}{\\partial x_i}f(x)\\) eine Zahl, \\(\\frac{\\partial}{\\partial x_i}f\\) dagegen eine Funktion ist. Praktisch berechnet man \\(\\frac{\\partial}{\\partial x_i}f\\) als die (einfache) Ableitung \\[\\begin{equation}\n\\frac{d}{dx_i}\\tilde{f}_{x_1,...x_{i-1},x_{i+1}, ...,x_n}(x_i)\n\\end{equation}\\] der univariaten reellwertigen Funktion \\[\\begin{equation}\n\\tilde{f} : \\mathbb{R} \\to \\mathbb{R}, x_i \\mapsto \\tilde{f}_{x_1,...x_{i-1},x_{i+1}, ...,x_n}(x_i) := f(x_1,...,x_i, ...,x_n).\n\\end{equation}\\] Man betrachtet für die \\(i\\)te partielle Ableitung also alle \\(x_j\\) mit \\(j \\neq i\\) als Konstanten und ist auf das gewohnte Berechnen von Ableitungen von univariaten reellwertigen Funktionen geführt. Wir wollen das Vorgehen zum Berechnen von partiellen Ableitungen an einem ersten Beispiel verdeutlichen.\nBeispiel (1)\nWir betrachten die Funktion \\[\\begin{equation}\nf:\\mathbb{R}^2\\to \\mathbb{R}, x\\mapsto f(x):=x_1^2+x_2^2.\n\\end{equation}\\] Weil die Definitionsmenge dieser Funktion zweidimensional ist, kann man zwei partielle Ableitungen berechnen \\[\\begin{equation}\\label{eq:pdex_1}\n\\frac{\\partial }{\\partial x_1}f:\\mathbb{R}^2 \\to \\mathbb{R},\nx\\mapsto \\frac{\\partial}{\\partial x_{1}} f(x)\n\\mbox{ und }\n\\frac{\\partial}{\\partial x_2} f:\\mathbb{R}^2\\to \\mathbb{R},\nx\\mapsto \\frac{\\partial }{\\partial x_2}f(x).\n\\end{equation}\\] Um die erste dieser partiellen Ableitungen zu berechnen, betrachtet man die Funktion \\[\\begin{equation}\nf_{x_2}:\\mathbb{R} \\to \\mathbb{R}, x_1 \\mapsto f_{x_2}(x_1):=x_1^2+x_2^2,\n\\end{equation}\\] wobei \\(x_2\\) hier die Rolle einer Konstanten einnimmt. Um explizit zu machen, dass \\(x_2\\) kein Argument der Funktion ist, die Funktion aber weiterhin von \\(x_2\\) abhängt haben wir die Subskriptnotation \\(f_{x_2}(x_1)\\) verwendet. Um nun die partielle Ableitung zu berechnen, berechnen wir die (einfache) Ableitung von \\(f_{x_2}\\), \\[\\begin{equation}\nf_{x_2}'(x)=2x_{1}.\n\\end{equation}\\] Es ergibt sich also \\[\\begin{equation}\n\\frac{\\partial}{\\partial x_1}f:\\mathbb{R}^2\\to \\mathbb{R},\nx\\mapsto \\frac{\\partial}{\\partial x_1}f(x)\n=\\frac{\\partial}{\\partial x_1}(x_1^2+x_2^2)\n=f_{x_2}'(x)=2x_1.\n\\end{equation}\\] Analog gilt mit der entsprechenden Formulierung von \\(f_{x_1}\\), dass \\[\\begin{equation}\n\\frac{\\partial}{\\partial x_2}f:\\mathbb{R}^2\\to \\mathbb{R},\nx\\mapsto \\frac{\\partial}{\\partial x_2}f(x)\n=\\frac{\\partial}{\\partial x_2}(x_1^2+x_2^2)\n=f_{x_1}'(x)=2x_2.\n\\end{equation}\\]\nWie bei der Ableitung einer univariaten reellwertigen Funktion ist es auch für eine multivariate reellwertige Funktion möglich, rekursiv eine höhere Ableitung zu definieren.\n\nDefinition 6.7 (Zweite partielle Ableitungen) \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\) sei eine multivariate reellwertige Funktion und \\(\\frac{\\partial}{\\partial x_i}f\\) sei die partielle Ableitung von \\(f\\) nach \\(x_i\\). Dann ist die zweite partielle Ableitung von \\(f\\) nach \\(x_i\\) und \\(x_j\\) definiert als \\[\\begin{equation}\n\\frac{\\partial^2}{\\partial x_j x_i} f(x)\n:= \\frac{\\partial}{\\partial x_j}\\left(\\frac{\\partial}{\\partial x_i}f\\right).\n\\end{equation}\\]\n\nMan beachte, dass es zu jeder partiellen Ableitung \\(\\frac{\\partial}{\\partial x_i}f\\) für \\(i = 1,...,n\\) insgesamt \\(n\\) zweite partiellen Ableitungen \\(\\frac{\\partial^2}{\\partial x_j\\partial x_i}f\\) für \\(j = 1,...,n\\) gibt. Die so resultierenden \\(n^2\\) zweiten partiellen Ableitungen sind jedoch nicht alle verschieden. Dies ist eine wesentliche Aussage des Satzes von Schwarz\n\nTheorem 6.5 (Satz von Schwarz) \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\) sei eine partiell differenzierbare multivariate reellwertige Funktion. Dann gilt \\[\\begin{equation}\n  \\frac{\\partial^2}{\\partial x_j\\partial x_i}f(x)\n= \\frac{\\partial^2}{\\partial x_i\\partial x_j}f(x)\n    \\mbox{ für alle }  1 \\le i,j \\le n.\n\\end{equation}\\]\n\nFür einen Beweis verweisen wir auf die weiterführende Literatur. Der Satz von Schwarz besagt insbesondere also auch, dass bei Bildung der zweiten partiellen Ableitungen die Reihenfolge des partiellen Ableitens irrelevant ist. Das Theorem erleichtert auf diese Weise die Berechnung von zweiten partiellen Ableitungen und hilft zudem, analytische Fehler bei der Berechnung zweiter partieller Ableitungen aufzudecken. Wir verdeutlichen dies in Fortführung obigen Beispiels.\nBeispiel (1)\nWir wollen die partiellen Ableitungen zweiter Ordnung der Funktion \\[\\begin{equation}\nf:\\mathbb{R}^{2}\\to \\mathbb{R}, x\\mapsto f(x):=x_1^2+x_2^2.\n\\end{equation}\\] berechnen. Mit den Ergebnissen für die partiellen Ableitungen erster Ordnung dieser Funktion ergibt sich \\[\\begin{align}\n\\begin{split}\n\\frac{\\partial^2}{\\partial x_1 x_1} f(x)\n& = \\frac{\\partial}{\\partial x_1}\\left(\\frac{\\partial}{\\partial x_1} f(x)\\right)\n  = \\frac{\\partial}{\\partial x_1}(2x_1)\n  = 2\n\\\\\n\\frac{\\partial^2}{\\partial x_1 x_2} f(x)\n& = \\frac{\\partial}{\\partial x_1}\\left(\\frac{\\partial}{\\partial x_2} f(x)\\right)\n  = \\frac{\\partial}{\\partial x_1}(2x_2)\n  = 0\n\\\\\n\\frac{\\partial^2}{\\partial x_2 x_1} f(x)\n& = \\frac{\\partial}{\\partial x_2}\\left(\\frac{\\partial}{\\partial x_1} f(x)\\right)\n  = \\frac{\\partial}{\\partial x_2}(2x_1)\n  = 0\n  \\\\\n\\frac{\\partial^2}{\\partial x_2 x_2} f(x)\n& = \\frac{\\partial}{\\partial x_2}\\left(\\frac{\\partial}{\\partial x_2} f(x)\\right)\n  = \\frac{\\partial}{\\partial x_2}(2x_2)\n  = 2\n\\end{split}\n\\end{align}\\] Offenbar gilt \\[\\begin{equation}\n\\frac{\\partial^2}{\\partial x_1 x_2} f(x) = \\frac{\\partial^2}{\\partial x_2 x_1} f(x).\n\\end{equation}\\]\nBeispiel (2)\nAls weiteres Beispiel wollen wird die partiellen Ableitungen erster und zweiter Ordnung der Funktion \\[\\begin{equation}\nf:\\mathbb{R}^{3}\\to \\mathbb{R}, x\\mapsto f(x):=x_1^2+x_1x_2+x_2\\sqrt{x_3}.\n\\end{equation}\\] berechnen. Mit den Rechenregeln für Ableitungen ergibt sich für die partiellen Ableitungen erster Ordnung \\[\\begin{align}\n\\begin{split}\n& \\frac{\\partial}{\\partial x_1}f(x)\n= \\frac{\\partial}{\\partial x_1}\\left(x_1^2+x_1x_2+x_2\\sqrt{x_3} \\right) = 2x_1+x_2,                   \\\\\n& \\frac{\\partial}{\\partial x_2}f(x)\n= \\frac{\\partial}{\\partial x_2}\\left(x_1^2+x_1x_2+x_2\\sqrt{x_3} \\right) = x_1+\\sqrt{x_3},             \\\\\n& \\frac{\\partial}{\\partial x_3}f(x)\n= \\frac{\\partial}{\\partial x_3}\\left(x_1^2+x_1x_2+x_2\\sqrt{x_3} \\right) = \\frac{x_{2}}{2\\sqrt{x_3}}.\n\\end{split}\n\\end{align}\\] Für die zweiten partiellen Ableitungen hinsichtlich \\(x_1\\) ergibt sich \\[\\begin{align}\n\\begin{split}\n    \\frac{\\partial^2}{\\partial x_1 \\partial x_1}f(x)\n& = \\frac{\\partial}{\\partial x_1} \\left(\\frac{\\partial}{\\partial x_1} f(x) \\right)\n  = \\frac{\\partial}{\\partial x_1}\\left(2x_1+x_2\\right) = 2, \\\\\n    \\frac{\\partial^2}{\\partial x_2\\partial x_1}f(x)\n& = \\frac{\\partial}{\\partial x_2} \\left(\\frac{\\partial}{\\partial x_1} f(x) \\right)\n  = \\frac{\\partial}{\\partial x_2}\\left(2x_1+x_2 \\right) = 1, \\\\\n    \\frac{\\partial^2}{\\partial x_3\\partial x_1} f(x)\n& = \\frac{\\partial}{\\partial x_3}\\left(\\frac{\\partial}{\\partial x_{1}} f(x) \\right)\n  = \\frac{\\partial}{\\partial x_3}\\left(2x_1+x_2\\right)=0.\n\\end{split}\n\\end{align}\\] Für die zweiten partiellen Ableitungen hinsichtlich \\(x_2\\) ergibt sich \\[\\begin{align}\n\\begin{split}\n    \\frac{\\partial^2}{\\partial x_1\\partial x_2}f(x)\n& = \\frac{\\partial}{\\partial x_1}\\left(\\frac{\\partial}{\\partial x_2}f(x) \\right)\n  = \\frac{\\partial}{\\partial x_{1}}\\left(x_1+ \\sqrt{x_3} \\right) = 1, \\\\\n    \\frac{\\partial^2}{\\partial x_2 \\partial x_2}f(x)\n& = \\frac{\\partial}{\\partial x_2}\\left(\\frac{\\partial}{\\partial x_2}f(x) \\right)\n  = \\frac{\\partial}{\\partial x_2}\\left(x_1 + \\sqrt{x_3} \\right) = 0, \\\\\n    \\frac{\\partial^2}{\\partial x_3\\partial x_2}f(x)\n& = \\frac{\\partial}{\\partial x_3}\\left(\\frac{\\partial}{\\partial x_2}f(x) \\right)\n  = \\frac{\\partial}{\\partial x_3}\\left(x_1+\\sqrt{x_3} \\right) =\\frac{1}{2\\sqrt{x_3}}.\n\\end{split}\n\\end{align}\\] Beispiel (2) Für die zweiten partiellen Ableitungen hinsichtlich \\(x_3\\) ergibt sich \\[\\begin{align}\n\\begin{split}\n    \\frac{\\partial^{2}}{\\partial x_1\\partial x_3}f(x)\n& = \\frac{\\partial}{\\partial x_1}\\left(\\frac{\\partial}{\\partial x_3} f(x) \\right)\n  = \\frac{\\partial}{\\partial x_1}\\left(\\frac{x_2}{2}\\sqrt{x_3}\\right) = 0, \\\\\n    \\frac{\\partial^2}{\\partial x_2\\partial x_3}f(x)\n& = \\frac{\\partial}{\\partial x_2}\\left(\\frac{\\partial}{\\partial x_3}f(x) \\right)\n  = \\frac{\\partial}{\\partial x_2}\\left(\\frac{x_2}{2 \\sqrt{x_3}} \\right)\n  = \\frac{1}{2\\sqrt{x_3}}, \\\\\n    \\frac{\\partial^2}{\\partial x_3 \\partial x_3}f(x)\n& = \\frac{\\partial}{\\partial x_3}\\left(\\frac{\\partial}{\\partial x_3}f(x) \\right)\n  = \\frac{\\partial}{\\partial x_3}\\left(x_2\\frac{1}{2}x_3^{-\\frac{1}{2}}\\right)\n  = -\\frac{1}{4}x_2x_3^{-\\frac{3}{2}}.\n\\end{split}\n\\end{align}\\] Weiterhin erkennt man, dass die Reihenfolge der partiellen Ableitungen irrelevant ist, denn es gilt \\[\\begin{align}\n\\begin{split}\n& \\frac{\\partial^{2}}{\\partial x_{1}\\partial x_{2}}f(x)\n= \\frac{\\partial^{2}}{\\partial x_{2}\\partial x_{1}}f(x) = 1, \\\\\n& \\frac{\\partial^{2}}{\\partial x_{1}\\partial x_{3}}f(x)\n= \\frac{\\partial^{2}}{\\partial x_{3}\\partial x_{1}}f(x) = 0, \\\\\n& \\frac{\\partial^{2}}{\\partial x_{2}\\partial x_{3}}f(x)\n= \\frac{\\partial^{2}}{\\partial x_{3}\\partial x_{2}}f(x) = \\frac{1}{2\\sqrt{x_3}}.\n\\end{split}\n\\end{align}\\]\nWie oben gesehen gibt es für eine multivariate reellwertige Funktion \\(f:\\mathbb{R}^n \\to \\mathbb{R}\\) insgesamt \\(n\\) erste partielle Ableitungen und \\(n^2\\) zweite partielle Ableitungen. Diese werden im Gradienten und der Hesse-Matrix einer multivariaten reellwertigen Funktion zusammengefasst.\n\nDefinition 6.8 (Gradient) \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\) sei eine multivariate reellwertige Funktion. Dann ist der Gradient \\(\\nabla f(x)\\) von \\(f\\) an der Stelle \\(x \\in \\mathbb{R}^n\\) definiert als \\[\\begin{equation}\n\\nabla f(x) :=\n\\begin{pmatrix}\n\\frac{\\partial}{\\partial x_1} f(x)  \\\\\n\\frac{\\partial}{\\partial x_2} f(x)  \\\\\n\\vdots                                            \\\\\n\\frac{\\partial}{\\partial x_n} f(x)  \\\\\n\\end{pmatrix} \\in \\mathbb{R}^n.\n\\end{equation}\\]\n\nMan beachte, dass Gradienten multivariate vektorwertige Funktionen der \\[\\begin{equation}\n\\nabla f: \\mathbb{R}^n \\to \\mathbb{R}^n, x \\mapsto \\nabla f(x)\n\\end{equation}\\] sind. Für \\(n = 1\\) gilt \\(\\nabla f(x) = f'(x)\\). Eine wichtige Eigenschaften des Gradienten ist, dass \\(-\\nabla f(x)\\) die Richtung des steilsten Abstiegs von \\(f\\) in \\(\\mathbb{R}^n\\) anzeigt. Diese Einsicht ist aber nicht trivial und soll an späterer Stelle vertieft werden. Als Beispiele betrachten wir die Gradienten der oben analysierten Funktionen\nBeispiel (1)\nFür die in Beispiel (1) betrachtete Funktion \\(f: \\mathbb{R}^2 \\to \\mathbb{R}\\) gilt \\[\\begin{equation}\n\\nabla f(x) :=\n\\begin{pmatrix}\n\\frac{\\partial}{\\partial x_1} f(x)  \\\\\n\\frac{\\partial}{\\partial x_2} f(x)  \\\\\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n2x_1                    \\\\\n2x_2\n\\end{pmatrix}\n\\in \\mathbb{R}^2.\n\\end{equation}\\]\nIn Abbildung 6.7 visualisieren wir ausgewählte Werte dieses Gradienten für\n\n\n\n\n\n\nAbbildung 6.7: Exemplarische Gradientenwerte der bivariaten Funktion \\(f(x) = x_1^2 + x_2^2\\).\n\n\n\nBeispiel (2)\nFür die in Beispiel (2) betrachtete Funktion \\(f: \\mathbb{R}^3 \\to \\mathbb{R}\\) gilt \\[\\begin{equation}\n\\nabla f(x) :=\n\\begin{pmatrix}\n\\frac{\\partial}{\\partial x_1} f(x)  \\\\\n\\frac{\\partial}{\\partial x_2} f(x)  \\\\\n\\frac{\\partial}{\\partial x_3} f(x)  \\\\\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n2x_1+x_2                    \\\\\nx_1+\\sqrt{x_3}              \\\\\n\\frac{x_{2}}{2\\sqrt{x_3}}   \\\\\n\\end{pmatrix}\n\\in \\mathbb{R}^3.\n\\end{equation}\\]\nSchließlich widmen wir uns der Zusammenfassung der zweiten partiellen Ableitungen einer multivariaten reellwertigen Funktion in der Hesse-Matrix.\n\nDefinition 6.9 (Hesse-Matrix) \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\) sei ein multivariate reellwertige Funktion. Dann ist die Hesse-Matrix \\(\\nabla^2 f(x)\\) von \\(f\\) an der Stelle \\(x \\in \\mathbb{R}^n\\) definiert als \\[\\begin{equation}\n\\nabla^2 f(x) :=\n\\begin{pmatrix}\n    \\frac{\\partial^2}{\\partial x_1 x_1} f(x)\n&   \\frac{\\partial^2}{\\partial x_1 x_2} f(x)\n&   \\cdots\n&   \\frac{\\partial^2}{\\partial x_1 x_n} f(x)  \\\\\n    \\frac{\\partial^2}{\\partial x_2 x_1} f(x)\n&   \\frac{\\partial^2}{\\partial x_2 x_2} f(x)\n&   \\cdots\n&   \\frac{\\partial^2}{\\partial x_2 x_n} f(x)  \\\\\n    \\vdots\n&   \\vdots\n&   \\ddots\n&   \\vdots                                      \\\\\n    \\frac{\\partial^2}{\\partial x_n x_1} f(x)\n&   \\frac{\\partial^2}{\\partial x_n x_2} f(x)\n&   \\cdots\n&   \\frac{\\partial^2}{\\partial x_n x_n} f(x)  \\\\\n\\end{pmatrix} \\in \\mathbb{R}^{n \\times n}.\n\\end{equation}\\]\n\nMan beachte, dass Hesse-Matrizen multivariate matrixwertige Abbildungen der Form \\[\\begin{equation}\n\\nabla^2 f: \\mathbb{R}^n \\to \\mathbb{R}^{n\\times n}, x \\mapsto \\nabla^2 f(x)\n\\end{equation}\\] sind. Für \\(n = 1\\) gilt \\(\\nabla^2 f(x) = f''(x)\\). Weiterhin folgt aus \\[\\begin{equation}\n\\frac{\\partial^2}{\\partial x_i\\partial x_j}f(x) = \\frac{\\partial^2}{\\partial x_j\\partial x_i}f(x) \\mbox{ für } 1 \\le i,j\\le n\n\\end{equation}\\] dass die Hesse-Matrix symmetrisch ist, dass also \\[\\begin{equation}\n\\left(\\nabla^2f(x)\\right)^T = \\nabla^2f(x)\n\\end{equation}\\] gilt.\nBeispiel (1)\nFür die in Beispiel (1) betrachtete Funktion \\(f: \\mathbb{R}^2 \\to \\mathbb{R}\\) gilt \\[\\begin{equation}\n\\nabla^2 f(x)\n:=\n\\begin{pmatrix}\n\\frac{\\partial^2}{\\partial x_1x_1} f(x) & \\frac{\\partial^2}{\\partial x_1x_2}    f(x) \\\\\n\\frac{\\partial^2}{\\partial x_2x_1} f(x) & \\frac{\\partial^2}{\\partial x_2x_2}    f(x) \\\\\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n2 & 0   \\\\\n0 & 2   \\\\\n\\end{pmatrix}\n\\in \\mathbb{R}^{2 \\times 2}\n\\end{equation}\\] Die Hesse-Matrix dieser Funktion ist also eine konstante Funktion, die nicht von \\(x\\) abhängt.\nBeispiel (2)\nFür die in Beispiel (2) betrachtete Funktion \\(f: \\mathbb{R}^3 \\to \\mathbb{R}\\) gilt\n\\[\\begin{equation}\n\\nabla^2 f(x)\n:=\n\\begin{pmatrix}\n  \\frac{\\partial^2}{\\partial x_1x_1}  f(x)\n& \\frac{\\partial^2}{\\partial x_1x_2}    f(x)\n& \\frac{\\partial^2}{\\partial x_1x_3}    f(x)\n\\\\\n  \\frac{\\partial^2}{\\partial x_2x_1}  f(x)\n& \\frac{\\partial^2}{\\partial x_2x_2}    f(x)\n& \\frac{\\partial^2}{\\partial x_2x_3}    f(x)\n\\\\\n  \\frac{\\partial^2}{\\partial x_3x_1}  f(x)\n& \\frac{\\partial^2}{\\partial x_3x_2}    f(x)\n& \\frac{\\partial^2}{\\partial x_3x_3}    f(x)\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n  2\n& 1\n& 0\n\\\\\n  1\n& 0\n& \\frac{1}{2\\sqrt{3}}\n\\\\\n  0\n& \\frac{1}{2\\sqrt{3}}\n& -\\frac{1}{4}x_2x_3^{-3/2}\n\\end{pmatrix}.\n\\end{equation}\\] Im Gegensatz zu Beispiel (1) ist die Hesse-Matrix der hier betrachteten Funktion keine konstante Funktion und ihr Wert hängt vom Wert des Funktionsarguments \\(x \\in \\mathbb{R}^3\\) ab.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Differentialrechnung</span>"
    ]
  },
  {
    "objectID": "106-Differentialrechnung.html#sec-selbskontrollfragen-differentialrechnung",
    "href": "106-Differentialrechnung.html#sec-selbskontrollfragen-differentialrechnung",
    "title": "6  Differentialrechnung",
    "section": "6.4 Selbstkontrollfragen",
    "text": "6.4 Selbstkontrollfragen\n\nGeben Sie die Definition des Begriffs der Ableitung \\(f'(a)\\) einer Funktion \\(f\\) an einer Stelle \\(a\\) wieder.\nGeben Sie die Definition des Begriffs der Ableitung \\(f'\\) einer Funktion \\(f\\).\nErläutern Sie die Symbole \\(f'(x), \\dot{f}(x)\\), \\(\\frac{df(x)}{dx}\\), und \\(\\frac{d}{dx}f(x)\\).\nGeben Sie die Definition des Begriffs der zweiten Ableitung \\(f''\\) einer Funktion \\(f\\) wieder.\nGeben Sie die Summenregel für Ableitungen wieder.\nGeben Sie die Produktregel für Ableitungen wieder.\nGeben Sie die Quotientenregel für Ableitungen wieder.\nGeben Sie die Kettenregel für Ableitungen wieder.\nBestimmen Sie die erste Ableitung der Funktion \\(f(x) := 3x^2 + \\exp\\left(-x^2\\right) - x \\ln(x)\\).\nBestimmen Sie die erste Ableitung der Funktion \\(f(x) :=\\frac{1}{2}\\sum_{i=1}^n (x_i - \\mu)^2\\) für \\(\\mu \\in \\mathbb{R}\\).\nGeben Sie die Definition der Begriffe des globalen und lokalen Maximums/Minimums einer univariaten reellwertigen Funktion wieder.\nGeben Sie die notwendige Bedingung für ein Extremum einer Funktion wieder.\nGeben Sie die hinreichende Bedingung für ein lokales Extremum einer Funktion wieder.\nGeben Sie das Standardverfahren der analytischen Optimierung wieder.\nBestimmen Sie einen Extremwert von \\(f(x) := \\exp\\left(-\\frac{1}{2}(x - \\mu)^2\\right)\\) für \\(\\mu \\in \\mathbb{R}\\).\nBerechnen Sie die partiellen Ableitungen der Funktion \\[\nf : \\mathbb{R}^2 \\to \\mathbb{R}, x \\mapsto f(x) := \\exp\\left(-\\frac{1}{2}\\left(x_1^2 + x_2^2\\right)\\right).\n\\tag{6.1}\\]\nBerechnen Sie die zweiten partiellen Ableitungen obiger Funktion \\(f\\).\nGeben Sie den Satz von Schwarz wieder.\nGeben Sie die Definition des Gradienten einer multivariaten reellwertigen Funktion wieder.\nGeben Sie den Gradienten der Funktion in Gleichung 6.1 an und werten Sie ihn in \\(x = (1,2)^T\\) aus.\nGeben Sie die Definition der Hesse-Matrix einer multivariaten reellwertigen Funktion wieder.\nGeben Sie die Hesse-Matrix der Funktion in Gleichung 6.1 an und werten Sie sie in \\(x = (1,2)^T\\) aus.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Differentialrechnung</span>"
    ]
  },
  {
    "objectID": "107-Integralrechnung.html",
    "href": "107-Integralrechnung.html",
    "title": "7  Integralrechnung",
    "section": "",
    "text": "7.1 Unbestimmte Integrale\nDieses Kapitel gibt einen Überblick über zentrale Begriffe der Integralrechnung. Das Hauptaugenmerk liegt dabei durchgängig auf der Klärung von Begrifflichkeiten, ihrer mathematischen Symbolik und der durch sie vermittelten Intuition und weniger auf der konkreten Berechnung von Integralen.\nWir beginnen mit der Definition des unbestimmen Integrals und dem Begriff der Stammfunktion.\nObige Definition besagt, dass die Ableitung der Stammfunktion einer Funktion \\(f\\) eben \\(f\\) ist. Das unbestimmte Integral einer Funktion \\(f\\) ist darüber hinaus die Menge aller durch Addition verschiedener Konstanten \\(c \\in \\mathbb{R}\\) gegebenen Stammfunktionen von \\(f\\). Eine solche Konstante \\(c \\in \\mathbb{R}\\) heißt auch Integrationskonstante; es gilt natürlich \\(\\frac{d}{dx}c = 0\\). Das Symbol \\(\\int f(x) \\,dx\\) ist als \\(F + c\\) definiert. \\(f(x)\\) wird in diesem Ausdruck Integrand genannt.\n\\(\\int\\) und \\(\\,dx\\) haben keine eigentliche Bedeutung, sondern sind reine Symbole.\nFür die in vorherigen Abschnitten eingeführten elementaren Funktionen ergeben sich die in Tabelle aufgelisteten Stammfunktionen. Man überzeugt sich davon durch Ableiten der jeweiligen Stammfunktion mithilfe der Rechenregeln der Differentialrechnung. Die uneigentlichen Integrale dieser elementaren Funktionen ergeben sich dann direkt aus diesen Stammfunktionen durch Addition einer Integrationskonstanten.\nStammfunktionen elementarer Funktionen\n\n\n\n\n\n\n\nName\nDefinition\nStammfunktion\n\n\n\n\nPolynomfunktion\n\\(f(x) := \\sum_{i=0}^n a_ix^i\\)\n\\(F(x) = \\sum_{i=0}^n \\frac{a_i}{i+1}x^{i+1}\\)\n\n\nKonstante Funktion\n\\(f(x) := a\\)\n\\(F(x) = ax\\)\n\n\nIdentitätsfunktion\n\\(f(x) := x\\)\n\\(F(x) = \\frac{1}{2}x^2\\)\n\n\nLinear-affine Funktion\n\\(f(x) := ax + b\\)\n\\(F(x) = \\frac{1}{2}ax^2 + bx\\)\n\n\nQuadratfunktion\n\\(f(x) := x^2\\)\n\\(F(x) = \\frac{1}{3}x^3\\)\n\n\nExponentialfunktion\n\\(f(x) := \\exp(x)\\)\n\\(F(x) = \\exp(x)\\)\n\n\nLogarithmusfunktion\n\\(f(x) := \\ln(x)\\)\n\\(F(x) = x \\ln x - x\\)\nDie in nachfolgendem Theorem zusammengestellten Rechenregeln sind oft hilfreich, um Stammfunktionen von Funktionen zu bestimmen, die sich aus Funktionen mit bekannten Stammfunktionen zusammensetzen.\nUnbestimmte Integrale nehmen in der Lösung von Differentialgleichungen einen zentralen Platz ein. Naheliegender ist aber zunächst die Anwendung unbestimmter Integrale im Kontext der Auswertung bestimmter Integrale, wie im nächsten Abschnitt eingeführt.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Integralrechnung</span>"
    ]
  },
  {
    "objectID": "107-Integralrechnung.html#sec-unbestimmte-integrale",
    "href": "107-Integralrechnung.html#sec-unbestimmte-integrale",
    "title": "7  Integralrechnung",
    "section": "",
    "text": "Definition 7.1 (Unbestimmtes Integral und Stammfunktion) Für ein Intervall \\(I \\subseteq \\mathbb{R}\\) sei \\(f : I \\to \\mathbb{R}\\) eine univariate reellwertige Funktion. Dann heißt eine differenzierbare Funktion \\(F : I \\to \\mathbb{R}\\) mit der Eigenschaft \\[\\begin{equation}\nF' = f\n\\end{equation}\\] Stammfunktion von \\(f\\). Ist \\(F\\) eine Stammfunktion von \\(f\\), dann heißt \\[\\begin{equation}\n\\int f(x) \\,dx := F + c \\mbox{ mit } c \\in \\mathbb{R}\n\\end{equation}\\] unbestimmtes Integral der Funktion \\(f\\). Das unbestimmte Integral einer Funktion bezeichnet damit die Menge aller Stammfunktionen einer Funktion.\n\n\n\n\n\n\nTheorem 7.1 (Rechenregeln für Stammfunktionen) \\(f\\) und \\(g\\) seien univariate reellwertige Funktion, die Stammfunktionen besitzen, und \\(g\\) sei invertierbar. Dann gelten folgende Rechenregeln für die Bestimmung von Stammfunktionen\n\nSummenregel \\[\\begin{equation}\n\\int a f(x) + bg(x)\\,dx  = a\\int f(x)\\,dx + b\\int g(x)\\,dx \\mbox{ für } a,b \\in \\mathbb{R}\n\\end{equation}\\]\nPartielle Integration \\[\\begin{equation}\n\\int f'(x)g(x)\\,dx = f(x)g(x) - \\int f(x)g'(x)\\,dx\n\\end{equation}\\]\nSubstitionsregel \\[\\begin{equation}\n\\int f(g(x))g'(x)\\,dx = \\int f(t)\\,dt \\mbox{ mit } t  = g(x)\n\\end{equation}\\]\n\n\n\nBeweis. Für einen Beweis der Summenregel verweisen wir auf die weiterführende Literatur. Die Rechenregel der partiellen Integration ergibt sich durch Integration der Produktregel der Differentiation. Wir erinnern uns, dass gilt \\[\\begin{equation}\n(f(x)g(x))' = f'(x)g(x) + f(x)g'(x).\n\\end{equation}\\] Integration beider Seiten der Gleichung und Berücksichtigung der Summenregel für Stammfunktionen ergibt dann \\[\\begin{align}\n\\begin{split}\n\\smallint (f(x)g(x))' \\,dx & = \\smallint f'(x)g(x) + f(x)g'(x) \\,dx     \\\\\n\\Leftrightarrow\nf(x)g(x) & = \\smallint f'(x)g(x)\\,dx + \\smallint f(x)g'(x) \\,dx         \\\\\n\\Leftrightarrow\n\\smallint f'(x)g(x)\\,dx & = f(x)g(x) - \\smallint f(x)g'(x) \\,dx.\n\\end{split}\n\\end{align}\\] Die Substitutionsregel ergibt sich für \\(F' = f\\) durch Anwendung der Kettenregel der Differentiation auf die verkettete Funktion \\(F(g)\\). Speziell gilt zunächst \\[\\begin{align}\n\\begin{split}\n(F(g(x)))' = F'(g(x))g'(x) = f(g(x))g'(x).\n\\end{split}\n\\end{align}\\] Integration beider Seiten der Gleichung \\[\\begin{equation}\n(F(g(x))) ' = f(g(x))g'(x)\n\\end{equation}\\] ergibt dann \\[\\begin{align}\n\\begin{split}\n\\smallint (F(g(x)))' \\,dx  & = \\smallint f(g(x))g'(x) \\,dx              \\\\\n\\Leftrightarrow\nF(g(x)) + c & = \\smallint f(g(x))g'(x) \\,dx                          \\\\\n\\Leftrightarrow\n\\smallint f(g(x))g'(x) \\,dx & = \\smallint f(t)\\,dt  \\mbox{ mit } t := g(x).\n\\end{split}\n\\end{align}\\] Dabei ist die rechte Seite der letzten obigen Gleichung zu verstehen als \\(F(g(x)) + c\\), also als Stammfunktion von \\(f\\) evaluiert an der Stelle \\(t := g(x)\\). Das \\(dt\\) ist nicht durch \\(dg(x)\\) zu ersetzen, sondern rein notationeller Natur.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Integralrechnung</span>"
    ]
  },
  {
    "objectID": "107-Integralrechnung.html#sec-bestimmte-integrale",
    "href": "107-Integralrechnung.html#sec-bestimmte-integrale",
    "title": "7  Integralrechnung",
    "section": "7.2 Bestimmte Integrale",
    "text": "7.2 Bestimmte Integrale\nAnschaulich entspricht ein bestimmtes Integral der vorzeichenbehafteten und auf ein Intervall \\([a,b]\\) beschränkten Fläche zwischen dem Graphen einer Funktion \\(f\\) und der \\(x\\)-Achse (vgl. Abbildung 7.1). Vorzeichenbehaftet heißt dabei, dass Flächen zwischen der \\(x\\)-Achse und positiven Werten von \\(f\\) positiv zur Fläche beitragen, Flächen zwischen der \\(x\\) und negativen Werten von \\(f\\) dagegen negativ. So ergeben sich zum Beispiel der Wert des in Abbildung 7.1 A gezeigten bestimmten Integral zu 0.68, der Wert des in Abbildung Abbildung 7.1 B gezeigten bestimmten Integrals zu 0.95 (die eingezeichnete Fläche ist offensichtlich größer als in Abbildung 7.1 A) und der Wert des in Abbildung 7.1 C gezeigten bestimmten Integrals zu 0 (die eingezeichneten positiven und negativen Flächen gleichen sich genau aus). Letzteres Beispiel legt auch die Interpretation des Integrals als Durchschnittswert einer Funktion \\(f\\) über einem Intervall \\([a,b]\\) nahe.\n\n\n\n\n\n\nAbbildung 7.1: Beispiele bestimmter Integrale\n\n\n\nUm den Begriff des bestimmten Integrals im Sinne des Riemannschen Integrals einführen zu können, müssen wir zunächst etwas Vorarbeit leisten. Wir beginnen damit, einen Begriff für die Aufteilung eines Intervalls in kleinere Abschnitte einzuführen.\n\nDefinition 7.2 (Zerlegung eines Intervalls und Feinheit) Es sei \\([a,b] \\subset \\mathbb{R}\\) ein Intervall und \\(x_0,x_1,x_2,...,x_n\n\\in [a,b]\\) eine Menge von Punkten mit \\[\\begin{equation}\na =: x_0 &lt; x_1 &lt;  x_2 \\cdots &lt; x_n := b\n\\end{equation}\\] und \\[\\begin{equation}\n\\Delta x_i := x_i - x_{i-1} \\mbox{ für } i = 1,...,n.\n\\end{equation}\\] Dann heißt die Menge \\[\\begin{equation}\nZ := \\{[x_0,x_1], [x_1,x_2], ..., [x_{n-1},x_n]\\}\n\\end{equation}\\] der durch \\(x_0,x_1,x_2,...,x_n\\) definierten Teilintervalle von \\([a,b]\\) eine Zerlegung von \\([a,b]\\). Weiterhin heißt \\[\\begin{equation}\nZ_{\\mbox{max}} := \\max_{i \\in n} \\Delta x_i,\n\\end{equation}\\] also die größte der Teilintervalllängen \\(\\Delta x_i\\), die Feinheit von \\(Z\\).\n\nAnschaulich ist \\(\\Delta x_i\\) die Breite der Rechtecke in Abbildung 7.2, wie wir in der Folge sehen werden. Mithilfe der Begriffe der Zerlegung eines Intervalls können wir nun den Begriff der Riemannschen Summen einführen.\n\nDefinition 7.3 (Riemannsche Summen) \\(f : [a,b] \\to \\mathbb{R}\\) sei eine beschränkte Funktion auf \\([a,b]\\), d.h. \\(|f(x)| &lt; c\\) für \\(0 &lt; c &lt; \\infty\\) und alle \\(x \\in [a,b]\\), \\(Z\\) sei eine Zerlegung von \\([a,b]\\) mit Teilintervalllängen \\(\\Delta x_i\\) für \\(i = 1,...,n\\). Weiterhin sei \\(\\xi_{i}\\) für \\(i = 1,...,n\\) ein beliebiger Punkt im Teilintervall \\([x_{i-1}, x_{i}]\\) der Zerlegung \\(Z\\). Dann heißt \\[\\begin{equation}\nR(Z) := \\sum_{i=1}^n f(\\xi_i)\\Delta x_i\n\\end{equation}\\] Riemannsche Summe von \\(f\\) auf \\([a,b]\\) bezüglich der Zerlegung \\(Z\\).\n\nWählt man zum Beispiel in der Riemannschen Summe in jedem Teilintervall das Maximum von \\(f\\), so ergibt sich die sogenannte Riemannsche Obersumme, \\[\\begin{equation}\nR_o(Z) := \\sum_{i=1}^n \\left(\\max_{[x_{i-1}, x_{i}]} f(\\xi_i) \\right)\\Delta x_i.\n\\end{equation}\\] Wählt man dagegen in jedem Teilintervall dagegen das Minimum von \\(f\\), so ergibt sich dies sogenannte Riemannsche Untersumme. \\[\\begin{equation}\nR_u(Z) := \\sum_{i=1}^n \\left(\\min_{[x_{i-1}, x_{i}]} f(\\xi_i) \\right)\\Delta x_i.\n\\end{equation}\\] Abbildung 7.2 verdeutlicht die Definition dieser Riemannschen Summen: die dunkelgrauen Rechtecke haben jeweils die Fläche \\([x_{i-1}, x_{i}] \\cdot \\min_{[x_{i-1}, x_{i}]} f(\\xi)\\) und bilden damit die Summenterme in der Riemannschen Untersumme \\[\\begin{equation}\nR_u(Z) := \\sum_{i=1}^4 \\left(\\min_{[x_{i-1}, x_{i}]} f(\\xi_i) \\right) \\cdot \\Delta x_i.\n\\end{equation}\\] Die vertikale Kombination aus dunkelgrauen und hellgrauen Rechtecken hat jeweils die Fläche \\([x_{i-1}, x_{i}] \\cdot \\max_{[x_{i-1}, x_{i}]} f(\\xi)\\) und bilden damit die Summenterme in der Riemannschen Obersumme \\[\\begin{equation}\nR_o(Z) := \\sum_{i=1}^4 \\left(\\max_{[x_{i-1}, x_{i}]} f(\\xi_i) \\right) \\cdot \\Delta x_i.\n\\end{equation}\\] Stellt man sich nun vor, dass man \\(\\Delta x_i\\) für alle \\(i = 1,...,n\\) gegen Null gehen lässt, verkleinert man die Feinheit der Zerlegung \\(Z\\) also immer weiter, so werden sich die Werte von \\(\\min_{[x_{i-1}, x_{i}]} f(\\xi_i)\\) und \\(\\max_{[x_{i-1}, x_{i}]} f(\\xi_i)\\) und damit auch die Werte von \\(R_u(Z)\\) und \\(R_o(Z)\\) immer weiter annähern. Diesen Grenzprozess macht man sich in der Definition des Riemannschen Integrals zunutze.\n\n\n\n\n\n\nAbbildung 7.2: Riemannsche Summen\n\n\n\n\nDefinition 7.4 (Bestimmtes Riemannsches Integral) \\(f : [a,b] \\to \\mathbb{R}\\) sei eine beschränkte reellwertige Funktion auf \\([a,b]\\). Weiterhin sei für \\(Z_k\\) mit \\(k = 1,2,3...\\) eine Folge von Zerlegungen von \\([a,b]\\) mit zugehörigen Feinheit \\(Z_{\\mbox{max},k}\\). Wenn für jede Folge von Zerlegungen \\(Z_1, Z_2,...\\) mit \\(|Z_{\\mbox{max},k}| \\to 0\\) für \\(k \\to \\infty\\) und für beliebig gewählte Punkte \\(\\xi_{ki}\\) mit \\(i = 1,...,n\\) im Teilintervall \\([x_{k,i-1}, x_{k,i}]\\) der Zerlegung \\(Z_k\\) gilt, dass die Folge der zugehörigen Riemannschen Summen \\(R(Z_1), R(Z_2), ...\\) gegen den gleichen Grenzwert strebt, dann heißt \\(f\\) auf \\([a,b]\\) integrierbar. Der entsprechende Grenzwert der Folge von Riemannschen Summen wird bestimmtes Riemannsches Integral genannt und mit \\[\\begin{equation}\n\\int_a^b f(x)\\,dx := \\lim_{k \\to \\infty} R(Z_k)\n\\mbox{ für } |Z_{\\mbox{max},k}| \\to 0\n\\end{equation}\\] bezeichnet. Die Werte \\(a\\) und \\(b\\) bezeichnet man in diesem Kontext als untere und obere Integrationsgrenzen, respektive, \\(f(x)\\) als Integrand und \\(x\\) als Integrationsvariable.\n\nDie Riemannsche Integrierbarkeit einer Funktion und der Wert eines bestimmten Riemannschen Integrals sind also im Sinne einer Grenzwertbildung definiert. Die Theorie der Riemannschen Integrale lässt sich allerding um die Hauptsätze der Differential- und Integralrechnung erweitern, so dass zur konkreten Berechnung eines bestimmten Integrals die Bildung von Zerlegungen und die Bestimmung eines Grenzwertes nur selten nötig ist. Der Einfachheit halber verzichten wir in der Folge auf die Bezeichungen Riemannsche und sprechen einfach von bestimmten Integralen.\nEin erster Schritt zur Vereinfachung der Berechnung von bestimmten Integralen ist das Feststellen folgender Rechenregeln, für deren Beweis wir auf die weiterführende Literatur verweisen.\n\nTheorem 7.2 (Rechenregeln für bestimmte Integrale) Es seien \\(f\\) und \\(g\\) integrierbare Funktionen auf \\([a,b]\\). Dann gelten folgende Rechenregeln.\n\nLinearität. Für \\(c_1,c_2\\in \\mathbb{R}\\) gilt \\[\\begin{equation}\n\\int_a^b (c_1 f(x) + c_2g(x))\\,dx = c_1 \\int_a^b f(x)\\,dx + c_2 \\int_a^b f(x)\\,dx.\n\\end{equation}\\]\nAdditivität. Für \\(a &lt; c &lt; b\\) gilt \\[\\begin{equation}\n\\int_a^b f(x)\\,dx = \\int_a^c f(x)\\,dx + \\int_c^b f(x)\\,dx.\n\\end{equation}\\]\nVorzeichenwechsel bei Umkehrung der Integralgrenzen \\[\\begin{equation}\n\\int_a^b f(x)\\,dx = - \\int_b^a f(x)\\,dx.\n\\end{equation}\\]\nUnabhängigkeit von der Wahl der Integrationsvariable \\[\\begin{equation}\n\\int_a^b f(x)\\,dx = \\int_a^b f(y)\\,dy.\n\\end{equation}\\]\nUnabhängigkeit des Integrals von Art des Intervalls. Es gilt \\[\\begin{equation}\n\\int_{a}^{b} f(x)\\,dx\n= \\int_{]a,b[}f(x)\\,dx\n= \\int_{[a,b[}f(x)\\,dx\n= \\int_{]a,b]}f(x)\\,dx\n= \\int_{[a,b]}f(x)\\,dx.\n\\end{equation}\\] wobei \\(\\int_I\\) das bestimmte Integral von \\(f\\) auf dem Intervall \\(I \\subseteq \\mathbb{R}\\) bezeichnet.\n\n\nEine graphische Darstellung der Rechenregel der Additivität findet sich in Abbildung 7.3. Die Summe der durch die bestimmten Integrale gegebenen Flächen \\(\\int_a^c f(x)\\,dx\\) und \\(\\int_c^b f(x)\\,dx\\) mit \\(a &lt; c &lt; b\\) ergibt sich dabei zur Fläche von \\(\\int_a^b f(x)\\,dx\\).\n\n\n\n\n\n\nAbbildung 7.3: Additivität bestimmter Integrale\n\n\n\nDie in der Nachfolge vermerkten Hauptsätze der Differential- und Integralrechnung schließlich, ermöglichen es, bestimmte Integrale einer Funktion \\(f\\) direkt mithilfe der Stammfunktion \\(F\\) von \\(f\\) zu berechnen.\n\nTheorem 7.3 (Erster Hauptsatz der Differential- und Integralrechnung) Ist \\(f :  I  \\to \\mathbb{R}\\) eine auf dem Intervall \\(I\n\\subset \\mathbb{R}\\) stetige Funktion, dann ist die Funktion \\[\\begin{equation}\nF :  I  \\to \\mathbb{R}, x \\mapsto F(x) := \\int_a^x f(t)\\,dt \\mbox{ mit } x,\na \\in I\n\\end{equation}\\] eine Stammfunktion von \\(f\\).\n\n\nBeweis. Wir betrachten den Differenzquotienten \\[\\begin{equation}\n\\frac{1}{h}(F(x+h) - F(x))\n\\end{equation}\\] Mit der Definition \\(F(x) := \\smallint_a^x f(t)\\,dt\\) und der Additivität des bestimmten Integrals gilt dann \\[\\begin{equation}\n\\frac{1}{h}(F(x+h) - F(x))\n= \\frac{1}{h}\\left(\\int_a^{x + h} f(t)\\,dt - \\int_a^{x} f(t)\\,dt\\right)\n= \\frac{1}{h} \\int_x^{x + h}f(t)\\,dt\n\\end{equation}\\] Mit dem Mittelwertsatz der Integralrechnung gibt es also ein \\(\\xi \\in ]x,x+h[\\), so dass \\[\\begin{equation}\n\\frac{1}{h}(F(x+h) - F(x)) = f(\\xi)\n\\end{equation}\\] Grenzwertbildung ergibt dann \\[\\begin{equation}\n\\lim_{h \\to 0}\\frac{1}{h}(F(x+h) - F(x)) = \\lim_{h \\to 0} f(\\xi) \\mbox{ für }\n\\xi \\in ]x, x + h[\n\\Leftrightarrow\nF'(x) = f(x).\n\\end{equation}\\]\n\nFür den Beweis des Ersten Hauptsatzes der Differential- und Integralrechnung benötigen wir offenbar den Mittelwertsatz der Integralrechnung, welchen wir hier ohne Beweis wiedergeben und in Abbildung 7.4 veranschaulichen.\n\nTheorem 7.4 (Mittelwertsatz der Integralrechnung) Für eine stetige Funktion \\(f : [a,b] \\to \\mathbb{R}\\) existiert ein \\(\\xi \\in ]a,b[\\) mit \\[\\begin{equation}\n\\int_a^b f(x)\\,dx = f(\\xi)(b-a)\n\\end{equation}\\]\n\nDer Mittelwertsatz der Integralrechnung garantiert die Existenz eines \\(\\xi \\in [a,b]\\), so dass das bestimmte Integral \\(\\int_a^b f(x)\\,dx\\) gleich dem Produkt aus der “Rechteckhöhe” \\(f(\\xi)\\) und und der “Rechteckbreite” \\((b-a)\\) ist. In Abbildung 7.4 liegt dieses \\(\\xi\\) genau mittig zwischen \\(a\\) und \\(b\\). Dass die sich so ergebene grau eingefärbte Rechteckfläche gleich \\(\\int_a^b f(x)\\,dx\\) ist, ergibt sich aus der visuell zumindest nachvollziebaren Tatsache, dass die Flächen zwischen \\(f(x)\\) und \\(f(\\xi)\\) im Intervall \\([a,\\xi]\\) und zwischen \\(f(\\xi)\\) und \\(f(x)\\) im Intervall \\([\\xi,b]\\) den gleichen Betrag haben, erstere aber mit einem negativen Vorzeichen behaftet ist. Der Mittelwertsatz der Integralrechnung garantiert im Allgemeinen aber nur die Existenz eines \\(\\xi \\in [a,b]\\) mit der diskutierten Eigenschaft, gibt aber keine Formel zu Bestimmung von \\(\\xi\\) an.\n\n\n\n\n\n\nAbbildung 7.4: Zum Mittelwertsatz der Integralrechnung\n\n\n\nDer Zweite Hauptsatz der Differential- und Integralrechnung schließlich besagt, wie man mithilfe der Stammfunktion ein bestimmtes Integral berechnet.\n\nTheorem 7.5 (Zweiter Hauptsatz der Differential- und Integralrechnung) Ist \\(F\\) eine Stammfunktion einer stetigen Funktion \\(f : I \\to \\mathbb{R}\\) auf einem Intervall \\(I\\), so gilt für \\(a,b \\in I\\) mit \\(a \\le b\\) \\[\\begin{equation}\n\\int_a^b f(x)\\,dx = F(b) - F(a) =: F(x)\\vert_a^b\n\\end{equation}\\]\n\n\nBeweis. Mit den Rechenregeln für bestimmte Integrale und dem ersten Hauptsatz der Differential- und Integralrechnung ergibt sich \\[\\begin{equation}\nF(b) - F(a) = \\int_\\alpha^b f(t)\\,dt - \\int_\\alpha^a f(t)\\,dt = \\int_a^b f(x)\\,dx\n\\end{equation}\\]\n\nWir wollen den Zweiten Haupsatz der Differential- und Integralrechnung in drei Beispielen anwenden (vgl. Abbildung 7.5).\n\n\n\n\n\n\nAbbildung 7.5: Beispiele zum Zweiten Hauptsatz der Differential- und Integralrechnung\n\n\n\nBeispiel (1)\nWir betrachten die Identitätsfunktion \\[\\begin{equation}\nf : \\mathbb{R} \\to \\mathbb{R}, x \\mapsto f(x) := x\n\\end{equation}\\] und wollen das bestimmte Integral dieser Funktion auf dem Intervall \\([0,1]\\), also \\[\\begin{equation}\n\\int_0^1 f(x)\\,dx = \\int_0^1 x \\,dx\n\\end{equation}\\] berechnen. Dazu erinnern wir uns, dass eine Stammfunktion von \\(f\\) durch \\[\\begin{equation}\nF : \\mathbb{R} \\to \\mathbb{R}, x \\mapsto F(x) := \\frac{1}{2}x^2\n\\end{equation}\\] gegeben ist, weil \\[\\begin{equation}\nF'(x) = \\frac{d}{dx}\\left(\\frac{1}{2}x^2 \\right) = 2 \\cdot \\frac{1}{2} x^{2-1} = x.\n\\end{equation}\\] Einsetzen in den Zweiten Hauptsatz der Differential- und Integralrechnung ergibt dann sofort \\[\\begin{equation}\n\\int_0^1 x \\,dx =\\frac{1}{2}1^2  - \\frac{1}{2}0^2 = \\frac{1}{2}.\n\\end{equation}\\] Dieses Ergebnis ist mit der Intuition, die sich anhand der grauen Fläche in Abbildung 7.5 A, ergibt kongruent.\nBeispiel (2)\nAls nächstes betrachten wir die Quadratfunktion \\[\\begin{equation}\nf : \\mathbb{R} \\to \\mathbb{R}, x \\mapsto f(x) := x^2\n\\end{equation}\\] und wollen das bestimmte Integral auch dieser Funktion auf dem Intervall \\([0,1]\\), also \\[\\begin{equation}\n\\int_0^1 f(x)\\,dx = \\int_0^1 x^2 \\,dx\n\\end{equation}\\] berechnen. Dazu erinnern wir uns, dass eine Stammfunktion von \\(f\\) durch \\[\\begin{equation}\nF : \\mathbb{R} \\to \\mathbb{R}, x \\mapsto F(x) := \\frac{1}{3}x^3\n\\end{equation}\\] gegeben ist, weil \\[\\begin{equation}\nF'(x) = \\frac{d}{dx}\\left(\\frac{1}{3}x^3 \\right) = 3 \\cdot \\frac{1}{3} x^{3-1} = x^2.\n\\end{equation}\\] Einsetzen in den Zweiten Hauptsatz der Differential- und Integralrechnung ergibt dann sofort \\[\\begin{equation}\n\\int_0^1 x^2 \\,dx =\\frac{1}{3}1^3  - \\frac{1}{3}0^3 = \\frac{1}{3}.\n\\end{equation}\\] Dieses Ergebnis ist mit der Intuition, die sich aus dem Vergleich der grauen Flächen in Abbildung 7.5 A und Abbildung 7.5 B ergibt, kongruent.\nBeispiel (3)\nSchließlich betrachten wir die linear-affine Funktion \\[\\begin{equation}\nf : \\mathbb{R} \\to \\mathbb{R}, x \\mapsto f(x) := -x + 1\n\\end{equation}\\] und wollen das bestimmte Integral auch dieser Funktion auf dem Intervall \\([0,2]\\), also \\[\\begin{equation}\n\\int_0^2 f(x)\\,dx = \\int_0^2 -x + 1 \\,dx\n\\end{equation}\\] berechnen. Dazu erinnern wir uns, dass eine Stammfunktion der linearen Funktion mit \\(a = -1\\) und \\(b = 1\\) (vgl. Tablle ) durch \\[\\begin{equation}\nF : \\mathbb{R} \\to \\mathbb{R}, x \\mapsto F(x) := -\\frac{1}{2}x^2 + x\n\\end{equation}\\] gegeben ist, weil \\[\\begin{equation}\nF'(x) = \\frac{d}{dx}\\left(-\\frac{1}{2}x^2 + x \\right) = - 2 \\cdot \\frac{1}{2} x^{2-1} + 1 \\cdot x^{1-1} = -x + 1.\n\\end{equation}\\] Einsetzen in den Zweiten Hauptsatz der Differential- und Integralrechnung ergibt dann sofort \\[\\begin{equation}\n\\int_0^2 -x + 1 \\,dx\n= \\left(-\\frac{1}{2}2^2 + 2 \\right) - \\left(-\\frac{1}{2}0^2 + 0 \\right).\n= -2 + 2 - 0\n= 0.\n\\end{equation}\\] Dieses Ergebnis ist mit der Intuition kongruent, dass sich die “positive” und die “negative” graue Fläche in Abbildung 7.5 C ausgleichen, kongruent.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Integralrechnung</span>"
    ]
  },
  {
    "objectID": "107-Integralrechnung.html#sec-uneigentliche-integrale",
    "href": "107-Integralrechnung.html#sec-uneigentliche-integrale",
    "title": "7  Integralrechnung",
    "section": "7.3 Uneigentliche Integrale",
    "text": "7.3 Uneigentliche Integrale\nUneigentliche Integrale sind bestimmte Integrale bei denen mindestens eine Integrationsgrenze keine reelle Zahl ist, sondern \\(-\\infty\\) oder \\(\\infty\\). Wir beleuchten die Natur uneigentlicher Integrale mit folgender Definition und einem Beispiel.\n\nDefinition 7.5 (Uneigentliche Integrale) \\(f : \\mathbb{R} \\to \\mathbb{R}\\) sei eine univariate reellwertige Funktion. Mit den Definitionen \\[\\begin{equation}\n\\int_{-\\infty}^b f(x)\\,dx := \\lim_{a \\to -\\infty} \\int_a^b f(x)\\,dx\n\\mbox{ und }\n\\int_a^\\infty f(x)\\,dx := \\lim_{b \\to \\infty} \\int_a^b f(x)\\,dx\n\\end{equation}\\] und der Additivität von Integralen \\[\\begin{equation}\n\\int_{-\\infty}^\\infty f(x)\\,dx = \\int_{-\\infty}^b f(x)\\,dx + \\int_b^{\\infty}f(x)\\,dx\n\\end{equation}\\] wird der Begriff des bestimmten Integrals auf die unbeschränkten Integrationsintervalle \\(]-\\infty,b]\\), \\([a,\\infty[\\) und \\(]-\\infty,\\infty[\\) erweitert. Integrale mit unbeschränkten Integrationsintervallen heißen uneigentliche Integrale. Wenn die entsprechenden Grenzwerte existieren, sagt man, dass die uneigentlichen Integrale konvergieren.\n\nAls Beispiel betrachten wir das uneigentliche Integral der Funktion \\[\\begin{equation}\nf : \\mathbb{R} \\to \\mathbb{R}, x \\mapsto f(x) \\frac{1}{x^2}\n\\end{equation}\\] auf dem Intervall \\([1, \\infty[\\), also \\[\\begin{equation}\n\\int_1^{\\infty} \\frac{1}{x^2}\\,dx.\n\\end{equation}\\] Nach den Festlegungen in der Definition uneigentlicher Integrale gilt \\[\\begin{equation}\n\\int_1^{\\infty} \\frac{1}{x^2}\\,dx = \\lim_{b \\to \\infty} \\int_1^b \\frac{1}{x^2}\\,dx.\n\\end{equation}\\] Mit der Stammfunktion \\(F(x) = -x^{-1}\\) von \\(f(x) = x^{-2}\\) ergibt sich für das bestimmte Integral in obiger Gleichung \\[\\begin{equation}\n\\int_1^b \\frac{1}{x^2}\\,dx\n= F(b) - F(1)\n= -\\frac{1}{b} - \\left(-\\frac{1}{1}\\right)\n= -\\frac{1}{b} + 1.\n\\end{equation}\\] Es ergibt sich also \\[\\begin{equation}\n\\int_1^{\\infty} \\frac{1}{x^2}\\,dx\n= \\lim_{b \\to \\infty} \\int_1^b \\frac{1}{x^2}\\,dx\n= \\lim_{b \\to \\infty}\\left(-\\frac{1}{b} + 1\\right)\n= - \\lim_{b \\to \\infty}\\frac{1}{b} + \\lim_{b \\to \\infty} 1\n= 0 + 1\n= 1.\n\\end{equation}\\]",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Integralrechnung</span>"
    ]
  },
  {
    "objectID": "107-Integralrechnung.html#sec-mehrdimensionale-integrale",
    "href": "107-Integralrechnung.html#sec-mehrdimensionale-integrale",
    "title": "7  Integralrechnung",
    "section": "7.4 Mehrdimensionale Integrale",
    "text": "7.4 Mehrdimensionale Integrale\nBisher haben wir nur Integrale univariater reellwertiger Funktionen betrachtet. Der Integralbegriff lässt sich auch auf multivariate reellwertige Funktionen erweitern. Allerdings ist dann der Integrationsbereich der Funktion nicht notwendigerweise so einfach zu beschreiben wie ein Intervall; insbesondere sind zum Beispiel schon im zweidimensionalen arbiträr geformte zweidimensionale Integrationsbereiche möglich. Wir wollen hier nun den einfachsten Fall eines Hyperrechtecks betrachten. In diesem Fall können wir mehrdimensionale bestimmte Integrale wie folgt definieren.\n\nDefinition 7.6 (Mehrdimensionale Integrale) \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\) sei eine multivariate reellwertige Funktion. Dann heißen Integrale der Form \\[\\begin{equation}\n\\int\\limits_{[a_1,b_1]\\times \\cdots \\times [a_n,b_n]} f(x)\\,dx\n= \\int_{a_1}^{b_1} \\cdots \\int_{a_n}^ {b_n} f(x_1,...,x_n)\\,dx_1...\\,dx_n\n\\end{equation}\\] mehrdimensionale bestimmte Integrale auf Hyperrechtecken. Weiterhin heißen Integrale der Form \\[\\begin{equation}\n\\int_{\\mathbb{R}^n} f(x)\\,dx\n= \\int_{-\\infty}^{\\infty}  \\cdots \\int_{-\\infty}^{\\infty}\nf(x_1,...,x_n)\\,dx_1...\\,dx_n\n\\end{equation}\\] mehrdimensionale uneigentliche Integrale.\n\nWie schon erwähnt kann man multivariate reellwertige Funktion nicht nur auf Hyperrechtecken, sondern im Prinzip auf beliebigen Hyperflächen integrieren. Dies kann sich jedoch oft schwierig gestalten.\nAls Beispiel betrachten wir das zweidimensionale bestimmte Integral der Funktion \\[\\begin{equation}\nf : \\mathbb{R}^2 \\to \\mathbb{R}, (x_1,x_2) \\mapsto f(x_1,x_2) := x_1^2 + 4x_2\n\\end{equation}\\] auf dem Rechteck \\([0,1] \\times [0,1]\\). Der Satz von Fubini der Theorie mehrdimensionaler Integrale besagt, dass man mehrdimensionale Integrale in beliebiger Koordinatenfolge auswerten kann. Es gilt also zum Beispiel, dass \\[\\begin{equation}\n\\int_{a_1}^{b_1} \\left(\\int_{a_2}^{b_2} f(x_1,x_2)\\,dx_2\\right) \\,dx_1\n= \\int_{a_2}^{b_2} \\left(\\int_{a_1}^{b_1} f(x_1,x_2)\\,dx_1 \\right) \\,dx_2.\n\\end{equation}\\] In diesem Sinne betrachten wir für das Beispiel \\[\\begin{equation}\n\\int_0^1 \\int_0^1 x_1^2 + 4x_2 \\,dx_1\\,dx_2\n= \\int_0^1 \\left(\\int_0^1 x_1^2 + 4x_2 \\,dx_1\\right)\\,dx_2\n\\end{equation}\\] also zunächst das innere Integral. \\(x_2\\) nimmt dabei die Rolle einer Konstanten ein. Eine Stammfunktion von \\(g(x_1) := x_1^2  +4 x_2\\) ist \\(G(x_1)\n= \\frac{1}{3}x_1^3 + 4x_2x_1\\), wie man sich durch Ableiten von \\(G\\) überzeugt. Es ergibt sich also für das innere Integral \\[\\begin{align}\n\\begin{split}\n\\int_0^1 x_1^2 + 4x_2 \\,dx_1\n& = G(1) - G(0) \\\\\n& = \\frac{1}{3}\\cdot1^3 + 4x_2\\cdot 1 - \\frac{1}{3}\\cdot 0^3 - 4x_2\\cdot 0 \\\\\n& = \\frac{1}{3} + 4x_2.\n\\end{split}\n\\end{align}\\] Betrachten des äußeren Integrals ergibt dann mit der Stammfunktion \\[\\begin{equation}\nH(x_2) = \\frac{1}{3}x_2 + 2x_2^2\n\\end{equation}\\] von \\[\\begin{equation}\nh(x_2) := \\frac{1}{3} + 4x_2,\n\\end{equation}\\] dass \\[\\begin{align}\n\\begin{split}\n\\int_0^1 \\int_0^1 x_1^2 + 4x_2 \\,dx_1\\,dx_2\n& = \\int_0^1 \\frac{1}{3} + 4x_2 \\,dx_2                                          \\\\\n& = H(1) - H(0)                                                                 \\\\\n& = \\frac{1}{3}\\cdot 1 + 4\\cdot 1^2 - \\frac{1}{3}\\cdot 0 + 4\\cdot 0^2           \\\\\n& = \\frac{13}{3}.\n\\end{split}\n\\end{align}\\]\nFolgendes Theorem ist in der Analyse von probabilistischen Modellen oft nützlich, da es die mehrfache Integration einer faktorisierenden Funktion auf das Produkt einfacher Integrationen auf die Funktionenfaktoren zurückführt.\n\nTheorem 7.6 (Vertauschung von Integration und Multiplikation bei Produktfunktionen) Für \\(i = 1,...,n\\) seien \\(f_i: \\mathbb{R} \\to \\mathbb{R}\\) integrierbare univariate reellwertigte Funktionen. Dann gilt \\[\n\\int \\left( \\prod_{i=1}^n f_i(x_i)\\right) \\,dx_1...dx_n = \\prod_{i=1}^n \\left(\\int f_i(x_i)\\,dx_i\\right)\n\\tag{7.1}\\]\n\n\nBeweis. Das Theorem folgt mit den Linearitätseigenschaften von Integralen. Wir zeigen das Theorem durch Induktion nach \\(n\\).\nInduktionsanfang. Es sei \\(n = 2\\). Dann gilt \\[\\begin{align}\n\\begin{split}\n\\int \\left( \\prod_{i=1}^2 f_i(x_i)\\right) \\,dx_1dx_2\n& = \\int \\left( \\int  f_1(x_1)f_2(x_2)  \\,dx_1 \\right) \\,dx_2                   \\\\\n& = \\int f_2(x_2)\\left(\\int f_1(x_1)  \\,dx_1 \\right) \\,dx_2                     \\\\    \n& = \\left(\\int f_1(x_1)  \\,dx_1 \\right) \\left(\\int f_2(x_2) \\,dx_2\\right)       \\\\   \n& = \\prod_{i=1}^2 \\left(\\int f_i(x_i)  \\,dx_i \\right)                           \\\\       \n\\end{split}\n\\end{align}\\]\nInduktionsschritt. Es gelte Gleichung 7.1 für \\(n-1\\), beispielsweise für \\(n-1\\) = 2. Dann gilt \\[\\begin{align}\n\\begin{split}\n\\int \\left( \\prod_{i=1}^n f_i(x_i)\\right) \\,dx_1...dx_n\n& = \\int\\left(\\int\\cdots\\left(\\int f_1(x_1)\\cdots f_{n-1}(x_{n-1}) f_n(x_n)\\,dx_1\\right)\\cdots\\,dx_{n-1}\\right)\\,dx_{n} \\\\    \n& = \\int f_n(x_n) \\left(\\int\\cdots\\left(\\int f_1(x_1)\\cdots f_{n-1}(x_{n-1}) \\,dx_1\\right)\\cdots\\,dx_{n-1}\\right)\\,dx_{n} \\\\\n& = \\int f_n(x_n) \\left(\\int\\left(\\prod_{i=1}^{n-1} f_i(x_i)\\right) \\,dx_{1:n-1} \\right)\\,dx_{n} \\\\\n& = \\left(\\int f_n(x_n) \\,dx_{n}\\right) \\left(\\int\\left(\\prod_{i=1}^{n-1} f_i(x_i)\\right) \\,dx_{1:n-1} \\right)\\\\\n& = \\left(\\int f_n(x_n) \\,dx_{n}\\right) \\left(\\prod_{i=1}^{n-1} \\left(\\int f_i(x_i)\\,dx_i\\right)\\right)\\\\\n& = \\prod_{i=1}^n \\left(\\int f_i(x_i)\\,dx_i\\right),\n\\end{split}\n\\end{align}\\] wobei wir die Annahme, dass Gleichung 7.1 für \\(n-1\\) in der vorletzten Gleichung genutzt haben.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Integralrechnung</span>"
    ]
  },
  {
    "objectID": "107-Integralrechnung.html#sec-selbstkontrollfragen-integralrechnung",
    "href": "107-Integralrechnung.html#sec-selbstkontrollfragen-integralrechnung",
    "title": "7  Integralrechnung",
    "section": "7.5 Selbstkontrollfragen",
    "text": "7.5 Selbstkontrollfragen\n\nGeben Sie die Definition des Begriffs der Stammfunktion wieder.\nGeben Sie die Definition des Begriffs des unbestimmten Integrals wieder.\nErläutern Sie die intuitive Bedeutung des Begriff des Riemannschen Integrals.\nGeben Sie den ersten Hauptsatz der Differential- und Integralrechnung wieder.\nGeben Sie den zweiten Hauptsatz der Differential- und Integralrechnung wieder.\nErläutern Sie den Begriff des uneigentlichen Integrals.\nErläutern Sie den Begriff des mehrdimensionalen Integrals.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Integralrechnung</span>"
    ]
  },
  {
    "objectID": "108-Vektoren.html",
    "href": "108-Vektoren.html",
    "title": "8  Vektoren",
    "section": "",
    "text": "8.1 Reeller Vektorraum\nIn der naturwissenschaftlichen Modellbildung betrachtet man häufig Phänomene, die sich durch das Vorliegen mehrerer quantitativer Merkmale auszeichnen. So ist zum Beispiel die Position eines Objektes im dreidimensionalen Raum durch drei Koordinaten hinsichtlich der drei Achsen eines Kartesischen Koordinatensystems festgelegt. Analog mag der Gesundheitszustand einer Person durch das Vorliegen dreier Messwerte, beispielsweise eines Selbstauskunftscore, eines Biomarkers und einer Expert:inneneinschätzung charakterisiert sein. Zum modellieren und analysieren solcher mehrdimensionalen quantitativen Phänomene stellt die Mathematik mit dem reellen Vektorraum ein vielseitig einsetzbares Hilfsmittel bereit. In diesem Kapitel wollen wir zunächst den Begriff des reellen Vektorraums und das grundlegende Rechnen mit Vektoren einführen (Kapitel 8.1). Eine Vektorraumstruktur, die sich stark an der dreidimensionalen räumlichen Intuition orientiert bietet dann der Euklidische Vektorraum (Kapitel 8.2). Mithilfe der Vektorrechnung können alle Vektoren eines Vektorraums aus einer kleinen Schar ausgezeichneter Vektoren gebildet werden. Die diesem Prinzip zugrundeliegenden Konzepte diskutieren wir in Kapitel 8.3 und Kapitel 8.4.\nWir beginnen mit der allgemeinen Definition eines Vektorraums, die grundlegende Regeln zum Rechnen mit Vektoren festlegt.\nEs fällt auf, dass Definition 8.1 zwar festlegt, wie mit Vektoren gerechnet werden soll, jedoch keine Aussage darüber macht, was ein Vektor, über ein ein Element einer Menge hinaus, eigentlich ist. Dies ist der Tatsache geschuldet, dass es verschiedenste mathematische Objekte gibt, für die Vektorraumstrukturen definiert werden können. Beispiele dafür sind die Menge der reellen \\(m\\)-Tupel, die Menge der Matrizen, die Menge der Polynome, die Menge der Lösungen eines linearen Gleichungssystems, die Menge der reellen Folgen, die Menge der stetigen Funktionen u.v.a.m.\nWir sind hier zunächst nur am Vektorraum der Menge reellen \\(m\\)-Tupel interessiert. Wir erinnern dazu daran, dass wir die reellen \\(m\\)-Tupel mit \\[\\begin{equation}\n\\mathbb{R}^m := \\left\\lbrace \\begin{pmatrix} x_1 \\\\ \\vdots \\\\ x_m \\end{pmatrix} | x_i \\in \\mathbb{R} \\mbox{ für alle } 1 \\le i \\le m \\right\\rbrace\n\\end{equation}\\] bezeichnen und \\(\\mathbb{R}^m\\) als “\\(\\mathbb{R}\\) hoch m” aussprechen. Die Elemente \\(x \\in \\mathbb{R}^m\\) nennen wir reelle Vektoren oder auch einfach Vektoren. Wir wollen nun der Definition eines Vektorraums die Menge \\(\\mathbb{R}^m\\) zugrunde legen. Dazu definieren wir zunächst die Vektoraddition für Elemente von \\(\\mathbb{R}^m\\) und die Skalarmultiplikation für Elemente von \\(\\mathbb{R}\\) und \\(\\mathbb{R}^m\\)\nEs ergibt sich dann folgendes Resultat.\nFür einen Beweis, auf den wir hier verzichten wollen, muss man die Bedingungen (1) bis (8) aus Definition 8.1 für die hier betrachtete Menge und die hier festgelegten Formen der Vektoraddition und der Skalarmultiplikation nachweisen. Diese ergeben sich aber leicht aus den Rechenregeln von Addition und Multiplikation in \\(\\mathbb{R}\\) und der Tatsache, dass Vektoraddition und Skalarmultiplikation für Elemente von \\(\\mathbb{R}^m\\) in Definition 8.2 komponentenweise definiert wurden. Wir definieren damit den Begriff des reellen Vektorraums.\nAuf Grundlage von Definition 8.3 wollen wir uns nun das Rechnen mit reellen Vektoren anhand einiger Beispiele verdeutlichen.\nBeispiele\n(1) Für\n\\[\nx:=\n\\begin{pmatrix}\n1 \\\\ 2 \\\\ 3 \\\\ 4\n\\end{pmatrix}\n\\in \\mathbb{R}^4\n\\mbox{ und }\ny:=\n\\begin{pmatrix}\n2 \\\\ 1 \\\\ 0 \\\\ 1\n\\end{pmatrix}\n\\in \\mathbb{R}^4\n\\] gilt \\[\nx + y\n=\n\\begin{pmatrix}\n1 \\\\ 2 \\\\ 3 \\\\ 4\n\\end{pmatrix}\n+\n\\begin{pmatrix}\n2 \\\\ 1 \\\\ 0 \\\\ 1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 + 2 \\\\ 2 + 1 \\\\ 3 + 0\\\\ 4 + 1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n3 \\\\ 3\\\\ 3 \\\\ 5\n\\end{pmatrix}\n\\in \\mathbb{R}^4.\n\\] In R implementiert dieses Beispiel wie folgt\nx = matrix(c(1,2,3,4), nrow = 4)      # Vektordefinition\ny = matrix(c(2,1,0,1), nrow = 4)      # Vektordefinition\nx + y                                 # Vektoraddition\n\n     [,1]\n[1,]    3\n[2,]    3\n[3,]    3\n[4,]    5\n(2) Für \\[\nx:=\n\\begin{pmatrix}\n2 \\\\ 3\n\\end{pmatrix}\n\\in \\mathbb{R}^2\n\\mbox{ und }\ny:=\n\\begin{pmatrix}\n1 \\\\ 3 \\\n\\end{pmatrix}\n\\in \\mathbb{R}^2\n\\] gilt \\[\nx - y\n=\n\\begin{pmatrix}\n2 \\\\ 3\n\\end{pmatrix}\n-\n\\begin{pmatrix}\n1 \\\\ 3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n2 - 1 \\\\ 3 - 3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 \\\\ 0\n\\end{pmatrix}\n\\in \\mathbb{R}^2.\n\\] In R implementiert man dieses Beispiel wie folgt\nx = matrix(c(2,3), nrow = 2)         # Vektordefinition\ny = matrix(c(1,3), nrow = 2)         # Vektordefinition\nx - y                                # Vektorsubtraktion\n\n     [,1]\n[1,]    1\n[2,]    0\n(3) Für \\[\nx:=\n\\begin{pmatrix}\n2 \\\\ 1 \\\\ 3\n\\end{pmatrix}\n\\in \\mathbb{R}^3\n\\mbox{ und }\na := 3 \\in \\mathbb{R}\n\\] gilt \\[\nax\n=\n3\n\\begin{pmatrix}\n2 \\\\ 1 \\\\ 3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n3 \\cdot 2 \\\\ 3 \\cdot 1 \\\\ 3 \\cdot 3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n6 \\\\ 3 \\\\ 9\n\\end{pmatrix}\n\\in \\mathbb{R}^3.\n\\] In R implementiert man dieses Beispiel wie folgt\nx = matrix(c(2,1,3), nrow = 3)       # Vektordefinition\na = 3                                # Skalardefinition\na*x                                  # Skalarmultiplikation\n\n     [,1]\n[1,]    6\n[2,]    3\n[3,]    9\nFür \\(m \\in \\{1,2,3\\}\\) kann man sich reelle Vektoren und das Rechnen mit ihnen visuell veranschaulichen. Für \\(m &gt; 3\\), wenn also zum Beispiel für eine Person mehr als drei quantitative Merkmale zu ihrem Gesundheitszustand vorliegen, was in der Anwendung regelmäßig der Fall ist, ist dies nicht möglich. Trotzdem mag die visuelle Intuition für \\(m \\le 3\\) einen Einstieg in das Verständnis von Vektorräumen erleichtern. Wir fokussieren hier auf den Fall \\(m := 2\\). In diesem Fall liegen die betrachteten reellen Vektoren in der zweidimensionalen Ebene und werden üblicherweise als Punkte oder Pfeile visualisiert (Abbildung 8.1).\nAbbildung 8.2 visualisiert die Vektoraddition \\[\\begin{equation}\n\\begin{pmatrix}\n1 \\\\ 2\n\\end{pmatrix}\n+\n\\begin{pmatrix}\n3 \\\\ 1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n4 \\\\ 3\n\\end{pmatrix}.\n\\end{equation}\\] Der Summenvektor entspricht dabei der Diagonale des von den beiden Summanden aufgespannten Parallelogramms.\nAbbildung 8.3 visualisiert die Vektorsubtraktion \\[\\begin{equation}\n\\begin{pmatrix}\n1 \\\\ 2\n\\end{pmatrix}\n-\n\\begin{pmatrix}\n3 \\\\ 1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 \\\\ 2\n\\end{pmatrix}\n+\n\\begin{pmatrix}\n-3 \\\\ -1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n-2 \\\\ \\,\\, 1\n\\end{pmatrix}\n\\end{equation}\\] Der resultierende Vektor entspricht dabei der Diagonale des von dem ersten Vektors und dem entgegensetzten Vektor des zweiten Vektors aufgespannten Parallelogramms.\nAbbildung 8.4 schließlich visualisiert die Skalarmultiplikation \\[\\begin{equation}\n3\n\\begin{pmatrix}\n1 \\\\ 1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n3 \\\\ 3\n\\end{pmatrix}\n\\end{equation}\\] Die Multiplikation eines Vektors mit einem Skalar ändert dabei immer nur seine Länge, nicht jedoch seine Richtung.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Vektoren</span>"
    ]
  },
  {
    "objectID": "108-Vektoren.html#sec-reeller-vektorraum",
    "href": "108-Vektoren.html#sec-reeller-vektorraum",
    "title": "8  Vektoren",
    "section": "",
    "text": "Definition 8.1 (Vektorraum) Es seien \\(V\\) eine nichtleere Menge und \\(S\\) eine Menge von Skalaren. Weiterhin sei eine Abbildung \\[\\begin{equation}\n+ : V \\times V \\to V, (v_1,v_2) \\mapsto +(v_1, v_2) =: v_1 + v_2,\n\\end{equation}\\] genannt Vektoraddition, definiert. Schließlich sei eine Abbildung \\[\\begin{equation}\n\\cdot : S \\times V \\to V, (s,v) \\mapsto \\cdot(s,v) =: sv,\n\\end{equation}\\] genannt Skalarmultiplikation definiert. Dann wird das Tupel \\((V,S,+,\\cdot)\\) genau dann Vektorraum genannt, wenn für beliebige Elemente \\(v,w,u\\in V\\) und \\(a,b \\in S\\) folgende Bedingungen gelten:\n(1) Kommutativität der Vektoraddition. \\[\nv + w = w + v.\n\\] (2) Assoziativität der Vektoraddition. \\[\n(v + w) + u = v + (w + u)\n\\] (3) Existenz eines neutralen Elements der Vektoraddition. \\[\n\\mbox{Es gibt einen Vektor } 0 \\in V \\mbox{ mit } v + 0 = 0 + v = v.\n\\] (4) Existenz inverser Elemente der Vektoraddition \\[\n\\mbox{Für alle Vektoren } v \\in V \\mbox{ gibt es einen Vektor } -v \\in V \\mbox{ mit } v + (-v) = 0.\n\\] (5) Existenz eines neutralen Elements der Skalarmultiplikation. \\[\n\\mbox{Es gibt einen Skalar } 1 \\in S \\mbox{ mit } 1 \\cdot v = v.\n\\] (6) Assoziativität der Skalarmultiplikation. \\[\na \\cdot (b \\cdot c) = (a \\cdot b)\\cdot c.\n\\] (7) Distributivität hinsichtlich der Vektoraddition. \\[\na\\cdot (v + w) = a\\cdot v + a \\cdot w.\n\\] (8) Distributivität hinsichtlich der Skalaraddition. \\[\n(a + b)\\cdot v = a\\cdot v + b\\cdot v.\n\\]\n\n\n\n\nDefinition 8.2 (Vektoraddition und Skalarmultiplikation in \\(\\mathbb{R}^m\\)) Für alle \\(x,y \\in \\mathbb{R}^m\\) und \\(a \\in \\mathbb{R}\\) sei die Vektoraddition durch \\[\\begin{equation}\n+ : \\mathbb{R}^m \\times \\mathbb{R}^m \\to \\mathbb{R}^m, (x,y) \\mapsto x + y =\n\\begin{pmatrix}\nx_1 \\\\ \\vdots \\\\ x_m\n\\end{pmatrix}\n+\n\\begin{pmatrix}\ny_1 \\\\ \\vdots \\\\ y_m\n\\end{pmatrix}\n:=\n\\begin{pmatrix}\nx_1 + y_1 \\\\ \\vdots \\\\ x_m + y_m\n\\end{pmatrix}\n\\end{equation}\\] und die Skalarmultiplikation durch \\[\\begin{equation}\n\\cdot : \\mathbb{R} \\times \\mathbb{R}^m \\to \\mathbb{R}^m, (a,x) \\mapsto\nax =\na\n\\begin{pmatrix}\nx_1  \\\\ \\vdots \\\\ x_m\n\\end{pmatrix}\n:=\n\\begin{pmatrix}\nax_1  \\\\ \\vdots \\\\a x_m\n\\end{pmatrix}\n\\end{equation}\\] definiert.\n\n\n\nTheorem 8.1 (Reeller Vektorraum) \\((\\mathbb{R}^m,+,\\cdot)\\) mit den Rechenregeln der Addition und Multiplikation in \\(\\mathbb{R}\\) einen Vektorraum.\n\n\n\nDefinition 8.3 (Reeller Vektorraum) Für \\(\\mathbb{R}^m\\) seien \\(+\\) und \\(\\cdot\\) die in Definition 8.2 definierte Vektoraddition und Skalarmultiplikation. Dann nennen wir auf Grundlage von Theorem 8.1 den Vektorraum \\((\\mathbb{R}^m,+,\\cdot)\\) den reellen Vektorraum\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbbildung 8.1: Visualisierung von Vektoren in \\(\\mathbb{R}^2\\)\n\n\n\n\n\n\n\n\n\n\nAbbildung 8.2: Vektoraddition in \\(\\mathbb{R}^2\\)\n\n\n\n\n\n\n\n\n\n\nAbbildung 8.3: Vektorsubtraktion in \\(\\mathbb{R}^2\\)\n\n\n\n\n\n\n\n\n\n\nAbbildung 8.4: Skalarmultiplikation in \\(\\mathbb{R}^2\\)",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Vektoren</span>"
    ]
  },
  {
    "objectID": "108-Vektoren.html#sec-euklidischer-vektorraum",
    "href": "108-Vektoren.html#sec-euklidischer-vektorraum",
    "title": "8  Vektoren",
    "section": "8.2 Euklidischer Vektorraum",
    "text": "8.2 Euklidischer Vektorraum\nDer reelle Vektorraum kann durch Definition des Skalarprodukts im Sinne eines Euklidischen Vektorraums mit räumlich-geometrischer Intuition versehen werden. Diese ermöglicht es insbesondere, Begriffe wie die Länge eines Vektors, den Abstand zwischen zwei Vektoren, und nicht zuletzt den Winkel zwischen zwei Vektoren zu definieren und zu berechnen. Wir führen zunächst das Skalarprodukt ein.\n\nDefinition 8.4 (Skalarprodukt auf \\(\\mathbb{R}^m\\)) Das Skalarprodukt auf \\(\\mathbb{R}^m\\) ist definiert als die Abbildung \\[\\begin{equation}\n\\langle \\rangle : \\mathbb{R}^m \\times \\mathbb{R}^m \\to \\mathbb{R},\n(x,y) \\mapsto \\langle (x,y) \\rangle := \\langle x,y \\rangle := \\sum_{i=1}^m x_i y_i.\n\\end{equation}\\]\n\nDas Skalarprodukt heißt Skalarprodukt, weil es einen Skalar ergibt, nicht etwa, weil mit Skalaren multipliziert wird. Das Skalarprodukt steht in enger Beziehung zum Matrixprodukt, wie wir an späterer Stelle sehen werden. Wir betrachten zunächst ein Beispiel und seine Implementation in R.\nBeispiel\nEs seien \\[\\begin{equation}\nx :=\n\\begin{pmatrix}\n1 \\\\ 2 \\\\ 3\n\\end{pmatrix}\n\\mbox{ und }\ny :=\n\\begin{pmatrix}\n2 \\\\ 0 \\\\ 1\n\\end{pmatrix}\n\\end{equation}\\] Dann ergibt sich \\[\\begin{equation}\n\\langle x,y \\rangle\n= x_1y_1 + x_2y_2 + x_3y_3\n= 1 \\cdot 2 + 2 \\cdot 0 + 3 \\cdot 1\n= 2 + 0 + 3\n= 5.\n\\end{equation}\\]\nIn R gibt es verschiedene Möglichkeiten, ein Skalarprodukt auszuwerten. Wir führen zwei von ihnen für das gebebene Beispiel untenstehend auf.\n\n# Vektordefinitionen\nx = matrix(c(1,2,3), nrow = 3)\ny = matrix(c(2,0,1), nrow = 3)\n\n# Skalarprodukt mithilfe von R's komponentenweiser Multiplikation und sum() Funktion\nsum(x*y)\n\n[1] 5\n\n# Skalarprodukt mithilfe von R's Matrixtransposition und -multiplikation\nt(x) %*% y\n\n     [,1]\n[1,]    5\n\n\nMithilfe des Skalarprodukts kann der Begriff des reellen Vektorraums zum Begriff des reellen kanonischen Euklidischen Vektorraums erweiter werden.\n\nDefinition 8.5 (Euklidischer Vektorraum) Das Tupel \\(\\left((\\mathbb{R}^m, +, \\cdot), \\langle \\rangle \\right)\\) aus dem reellen Vektorraum \\((\\mathbb{R}^m, +, \\cdot)\\) und dem Skalarprodukt \\(\\langle \\rangle\\) auf \\(\\mathbb{R}^m\\) heißt reeller kanonischer Euklidischer Vektorraum.\n\nGenerell heißt jedes Tupel aus einem Vektorraum und einem Skalarprodukt “Euklidischer Vektorraum”. Informell sprechen wir aber oft auch einfach von \\(\\mathbb{R}^m\\) als “Euklidischer Vektorraum” und insbesondere bei \\(\\left((\\mathbb{R}^m, +, \\cdot), \\langle \\rangle \\right)\\) vom “Euklidischen Vektorraum”. Ein Euklidischer Vektorraum ist ein Vektorraum mit geometrischer Struktur, die durch das Skalarprodukt induziert wird. Insbesondere bekommen im Euklidischen Vektorraum nun die geometrischen Begriffe von Länge, Abstand und Winkel eine Bedeutung. Wir definieren sie wie folgt.\n\nDefinition 8.6 \\(\\left((\\mathbb{R}^m, +, \\cdot), \\langle \\rangle \\right)\\) sei der Euklidische Vektorraum.\n(1) Die Länge eines Vektors \\(x \\in \\mathbb{R}^m\\) ist definiert als \\[\\begin{equation}\n\\Vert x \\Vert := \\sqrt{\\langle x, x \\rangle}.\n\\end{equation}\\] (2) Der Abstand zweier Vektoren \\(x,y \\in \\mathbb{R}^m\\) ist definiert als \\[\\begin{equation}\nd(x,y) := \\Vert x - y \\Vert.\n\\end{equation}\\] (3) Der Winkel \\(\\alpha\\) zwischen zwei Vektoren \\(x,y \\in \\mathbb{R}^m\\) mit \\(x,y \\neq 0\\) ist definiert durch \\[\\begin{equation}\n0 \\le \\alpha \\le \\pi \\mbox{ und } \\cos \\alpha\n:= \\frac{\\langle x, y \\rangle}{\\Vert x \\Vert \\Vert y \\Vert}\n\\end{equation}\\]\n\nDie Länge \\(\\Vert x \\Vert\\) eines Vektors \\(x \\in \\mathbb{R}^m\\) heißt auch Euklidische Norm von \\(x\\) oder \\(\\ell_2\\)-Norm von \\(x\\) oder einfach Norm von \\(x\\). Sie wird häufig auch mit \\(\\Vert x \\Vert_2\\) bezeichnet. Wir betrachten drei Beispiele für die Bestimmung der Länge eines Vektors und ihre entsprechende R Implementation. Wir veranschaulichen diese Beispiele in Abbildung 8.5.\nBeispiel (1)\n\\[\\begin{equation}\n\\left\\lVert \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} \\right\\rVert\n= \\sqrt{\\left\\langle \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} \\right\\rangle}\n= \\sqrt{2^2 + 0^2}\n= \\sqrt{4}\n= 2.00\n\\end{equation}\\]\n\nnorm(matrix(c(2,0),nrow = 2), type = \"2\")             # Vektorlänge = l_2 Norm\n\n[1] 2\n\n\nBeispiel (2)\n\\[\\begin{equation}\n\\left\\lVert \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} \\right\\rVert\n= \\sqrt{\\left\\langle \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}, \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} \\right\\rangle}\n= \\sqrt{2^2 + 2^2}\n= \\sqrt{8}\n\\approx 2.83\n\\end{equation}\\]\n\nnorm(matrix(c(2,2),nrow = 2), type = \"2\")             # Vektorlänge = l_2 Norm\n\n[1] 2.828427\n\n\nBeispiel (3)\n\\[\\begin{equation}\n\\left\\lVert \\begin{pmatrix} 2 \\\\ 4 \\end{pmatrix} \\right\\rVert\n= \\sqrt{\\left\\langle \\begin{pmatrix} 2 \\\\ 4 \\end{pmatrix}, \\begin{pmatrix} 2 \\\\ 4 \\end{pmatrix} \\right\\rangle}\n= \\sqrt{2^2 + 4^2}\n= \\sqrt{20}\n\\approx 4.47\n\\end{equation}\\]\n\nnorm(matrix(c(2,4),nrow = 2), type = \"2\")             # Vektorlänge = l_2 Norm\n\n[1] 4.472136\n\n\n\n\n\n\n\n\nAbbildung 8.5: Vektorlänge in \\(\\mathbb{R}^2\\)\n\n\n\nFür den Abstand \\(d(x,y)\\) zweier Vektoren \\(x,y\\in\\mathbb{R}^m\\) halten wir ohne Beweis fest, dass er zum einen nicht-negativ und symmetrisch ist, also dass \\[\\begin{equation}\nd(x,y) \\ge 0, d(x,x) = 0 \\mbox{ und } d(x,y) = d(y,x)\n\\end{equation}\\] gelten. Zudem erfüllt \\(d(x,y)\\) die sogenannte Dreiecksungleichung, die besagt, dass die direkte Wegstrecke zwischen zwei Punkten im Raum immer kürzer ist als eine indirekte Wegstrecke über einen dritten Punkt, \\[\\begin{equation}\nd(x,y) \\le d(x,z) + d(z,y).\n\\end{equation}\\] Damit erfüllt \\(d(x,y)\\) wichtige Aspekte der räumlichen Anschauung. Wir geben zwei Beispiele für die Bestimmung von Abständen von Vektoren in \\(\\mathbb{R}^2\\), die wir in Abbildung 8.6 visualisieren.\nBeispiel (1)\n\\[\\begin{equation}\nd\\left(\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}\\right)\n= \\left\\lVert \\begin{pmatrix}  1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} \\right\\rVert\n= \\left\\lVert \\begin{pmatrix} -1 \\\\ -1 \\end{pmatrix}  \\right\\rVert\n= \\sqrt{(-1)^2 + (-1)^2}\n= \\sqrt{2}\n\\approx 1.41\n\\end{equation}\\]\n\nnorm(matrix(c(1,1),nrow = 2) - matrix(c(2,2),nrow = 2), type = \"2\")\n\n[1] 1.414214\n\n\nBeispiel (2)\n\\[\\begin{equation}\nd\\left(\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\begin{pmatrix} 4 \\\\ 1 \\end{pmatrix}\\right)\n= \\left\\lVert \\begin{pmatrix}  1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 4 \\\\ 1 \\end{pmatrix} \\right\\rVert\n= \\left\\lVert \\begin{pmatrix} -3 \\\\ 0 \\end{pmatrix} \\right\\rVert\n= \\sqrt{(-3)^2 + 0^2}\n= \\sqrt{9}\n= 3\n\\end{equation}\\]\n\nnorm(matrix(c(1,1),nrow = 2) - matrix(c(1,4),nrow = 2), type = \"2\")\n\n[1] 3\n\n\n\n\n\n\n\n\nAbbildung 8.6: Vektorabstände in \\(\\mathbb{R}^2\\)\n\n\n\nSchließlich halten wir fest, dass für die Berechnung des Winkels zwischen zwei Vektoren anhand obiger Definition gilt, dass die Kosinusfunktion \\(\\cos\\) auf \\([0,\\pi]\\) bijektiv, also invertierbar mit der Umkehrfunktion \\(acos\\), der Arkuskosinusfunktion, ist. Auch für den Begriff des Winkels wollen wir zwei Beispiele betrachten. Man beachte dabei insbesondere, dass die Definition 8.6 den Winkel in Radians angibt. Für eine Angabe in Grad ist eine entsprechende Umrechnung erforderlich.\nBeispiel (1)\n\\[\\begin{equation}\n\\mbox{acos}\n\\left(\\frac{\\left\\langle \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 3 \\\\ 3 \\end{pmatrix} \\right\\rangle}\n           {\\left\\lVert  \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix} \\right\\rVert \\left\\lVert \\begin{pmatrix} 3 \\\\ 3 \\end{pmatrix} \\right\\rVert}\n\\right)\n= \\mbox{acos}\n\\left(\\frac{3\\cdot 3 + 3 \\cdot 0}\n           {\\sqrt{3^2 + 0^2} \\cdot \\sqrt{3^2 + 3^2}}\n\\right)\n= \\mbox{acos}\n\\left(\\frac{9}\n           {3 \\cdot \\sqrt{18}}\n\\right)\n= \\frac{\\pi}{4}\n\\approx 0.785\n\\end{equation}\\] \nDie Umrechnung in Grad ergibt dann \\[\\begin{equation}\n0.785 \\cdot \\frac{180°}{\\pi} = 45°\n\\end{equation}\\] In R implementiert man dies wie folgt.\n\nx = matrix(c(3,0), nrow = 2)                                 # Vektor 1\ny = matrix(c(3,3), nrow = 2)                                 # Vektor 2\nw = acos(sum(x*y)/(sqrt(sum(x*x))*sqrt(sum(y*y)))) * 180/pi  # Winkel in Grad\nprint(w)\n\n[1] 45\n\n\nBeispiel (2)\n\\[\\begin{equation}\n\\alpha\n= \\mbox{acos}\n\\left(\\frac{\\left\\langle \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 3 \\end{pmatrix} \\right\\rangle}\n           {\\left\\lVert  \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix} \\right\\rVert \\left\\lVert \\begin{pmatrix} 0 \\\\ 3 \\end{pmatrix} \\right\\rVert}\n\\right)\n= \\mbox{acos}\n\\left(\\frac{3\\cdot 0 + 0 \\cdot 3}\n           {\\sqrt{3^2 + 0^2} \\cdot \\sqrt{0^2 + 3^2}}\n\\right)\n= \\mbox{acos}\n\\left(\\frac{0}\n           {3 \\cdot 3}\n\\right)\n= \\frac{\\pi}{2}\n\\approx 1.57\n\\end{equation}\\] Die Umrechnung in Grad ergibt dann \\[\\begin{equation}\n\\frac{\\pi}{2} \\cdot \\frac{180°}{\\pi} = 90°\n\\end{equation}\\] Die entsprechende R Implementation lautet wie folgt.\n\nx = matrix(c(3,0), nrow = 2)                                    # Vektor 1\ny = matrix(c(0,3), nrow = 2)                                    # Vektor 2\nw = acos(sum(x*y)/(sqrt(sum(x*x))*sqrt(sum(y*y)))) * 180/pi     # Winkel in Grad\nprint(w)\n\n[1] 90\n\n\n\n\n\n\n\n\nAbbildung 8.7: Winkel in \\(\\mathbb{R}^2\\)\n\n\n\nDie Tatsache, dass zwei Vektoren einen rechten Winkel bilden können, also gewissermaßen maximal nicht-parallel sein können, ist ein wichtiges geometrisches Prinzip und wird deshalb mit folgender Definition speziell ausgezeichnet.\n\nDefinition 8.7 (Orthogonalität und Orthonormalität von Vektoren) \\(\\left((\\mathbb{R}^m, +, \\cdot), \\langle \\rangle \\right)\\) sei der Euklidische Vektorraum.\n(1) Zwei Vektoren \\(x,y \\in \\mathbb{R}^m\\) heißen orthogonal, wenn gilt, dass \\[\\begin{equation}\n\\langle x, y \\rangle = 0\n\\end{equation}\\] (2) Zwei Vektoren \\(x,y \\in \\mathbb{R}^m\\) heißen orthonormal, wenn gilt, dass \\[\\begin{equation}\n\\langle x, y \\rangle = 0 \\mbox{ und } \\Vert x \\Vert = \\Vert y \\Vert = 1.\n\\end{equation}\\]\n\nFür orthogonale und orthonormale Vektoren gilt also insbesondere auch \\[\\begin{equation}\n\\cos \\alpha\n= \\frac{\\langle x, y \\rangle}{\\Vert x \\Vert \\Vert y \\Vert}\n= \\frac{0}{\\Vert x \\Vert \\Vert y \\Vert}\n= 0,\n\\end{equation}\\] also \\[\\begin{equation}\n\\alpha = \\frac{\\pi}{2} = 90°.\n\\end{equation}\\]",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Vektoren</span>"
    ]
  },
  {
    "objectID": "108-Vektoren.html#sec-lineare-unabhaengigkeit",
    "href": "108-Vektoren.html#sec-lineare-unabhaengigkeit",
    "title": "8  Vektoren",
    "section": "8.3 Lineare Unabhängigkeit",
    "text": "8.3 Lineare Unabhängigkeit\nIn diesem Abschnitt führen wir den Begriff der linearen Unabhängigkeit von Vektoren ein. Wir definieren dazu zunächst den Begriff der Linearkombination von Vektoren.\n\nDefinition 8.8 (Linearkombination) \\(\\{v_1, v_2, ..., v_k\\}\\) sei eine Menge von \\(k\\) Vektoren eines Vektorraums \\(V\\) und \\(a_1, a_2,...,a_k\\) seien Skalare. Dann ist die Linearkombination der Vektoren in \\(\\{v_1, v_2, ..., v_k\\}\\) mit den Koeffizienten \\(a_1, a_2,...,a_k\\) definiert als der Vektor \\[\\begin{equation}\nw := \\sum_{i=1}^k a_i v_i \\in V.\n\\end{equation}\\]\n\nBeispiel\nEs seien \\[\\begin{equation}\nv_1 := \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix},\nv_2 := \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix},\nv_3 := \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n\\mbox{ und }\na_1 := 2, a_2 := 3, a_3 := 0.\n\\end{equation}\\] Dann ergibt sich die Linearkombination von \\(v_1,v_2,v_3\\) mit den Koeffizienten \\(a_1,a_2,a_3\\) zu \\[\\begin{align}\n\\begin{split}\nw\n& = a_1v_1 + a_2v_2 + a_3v_3                        \\\\\n& =  2 \\cdot \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}\n     + 3 \\cdot \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n     + 0 \\cdot \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}   \\\\\n& =   \\begin{pmatrix} 4 \\\\ 2 \\end{pmatrix}\n     + \\begin{pmatrix} 3 \\\\ 3 \\end{pmatrix}\n     + \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}          \\\\\n& =   \\begin{pmatrix} 7 \\\\ 5 \\end{pmatrix}.\n\\end{split}\n\\end{align}\\]\nBasierend auf dem Begriff der Linearkombination kann man nun den Begriff der Linearen Unabhängigkeit von Vektoren definieren.\n\nDefinition 8.9 (Lineare Unabhängigkeit) \\(V\\) sei ein Vektorraum. Eine Menge \\(W := \\{w_1, w_2, ...,w_k\\}\\) von Vektoren in \\(V\\) heißt linear unabhängig, wenn die einzige Repräsentation des Nullelements \\(0 \\in V\\) durch eine Linearkombination der \\(w \\in W\\) die sogenannte triviale Repräsentation \\[\\begin{equation}\n0 = a_1 w_1 + a_2 w_2 + \\cdots + a_k w_k \\mbox{ mit } a_1 = a_2 =  \\cdots = a_k = 0\n\\end{equation}\\] ist. Wenn die Menge \\(W\\) nicht linear unabhängig ist, dann heißt sie linear abhängig.\n\nUm zu prüfen, ob eine gegeben Menge von Vektoren linear abhängig oder unabhängig ist muss man prinzipiell für jede mögliche Linearkombination der gegebenen Vektoren, ob sie Null ist. Theorem 8.2 und Theorem 8.3 zeigen, wie dies für zwei bzw. endliche viele Vektoren auch mit weniger Aufwand gelingen kann.\n\nTheorem 8.2 (Lineare Abhängigkeit von zwei Vektoren) \\(V\\) sei ein Vektorraum. Zwei Vektoren \\(v_1, v_2 \\in V\\) sind linear abhängig, wenn einer der Vektoren ein skalares Vielfaches des anderen Vektors ist.\n\n\nBeweis. \\(v_1\\) sei ein skalares Vielfaches von \\(v_2\\), also \\[\\begin{equation}\nv_1 = \\lambda v_2 \\mbox{ mit } \\lambda \\neq 0.\n\\end{equation}\\] Dann gilt \\[\\begin{equation}\nv_1 - \\lambda v_2 = 0.\n\\end{equation}\\] Dies aber entspricht der Linearkombination \\[\\begin{equation}\na_1v_1 + a_2v_2 = 0\n\\end{equation}\\] mit \\(a_1 = 1 \\neq 0\\) und \\(a_2 = -\\lambda \\neq 0\\). Es gibt also eine Linearkombination des Nullelementes, die nicht die triviale Repräsentation ist, und damit sind \\(v_1\\) und \\(v_2\\) nicht linear unabhängig.\n\n\nTheorem 8.3 (Lineare Abhängigkeit einer Menge von Vektoren) \\(V\\) sei ein Vektorraum und \\(w_1,...,w_k \\in V\\) sei eine Menge von Vektoren in \\(V\\). Wenn einer der Vektoren \\(w_i\\) mit \\(i = 1,...,k\\) eine Linearkombination der anderen Vektoren ist, dann ist die Menge der Vektoren linear abhängig.\n\n\nBeweis. Die Vektoren \\(w_1,...,w_k\\) sind genau dann linear abhängig, wenn gilt, dass \\(\\sum_{i=1}^k a_i w_i = 0\\) mit mindestens einem \\(a_i \\neq 0\\) . Es sei also zum Beispiel \\(a_j \\neq 0\\). Dann gilt \\[\\begin{equation}\n0 = \\sum_{i=1}^k a_i w_i = \\sum_{i=1, i \\neq j}^k a_i w_i + a_jw_j\n\\end{equation}\\] Also folgt \\[\\begin{equation}\na_jw_j  = - \\sum_{i=1, i \\neq j}^k a_i w_i\n\\end{equation}\\] und damit \\[\\begin{equation}\nw_j  = - a_j^{-1}\\sum_{i=1, i \\neq j}^k a_i w_i = - \\sum_{i=1, i \\neq j}^k (a_j^{-1}a_i) w_i\n\\end{equation}\\] Also ist \\(w_j\\) eine Linearkombination der \\(w_i\\) für \\(i = 1,...,k\\) mit \\(i \\neq j\\).",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Vektoren</span>"
    ]
  },
  {
    "objectID": "108-Vektoren.html#sec-vektorraumbasen",
    "href": "108-Vektoren.html#sec-vektorraumbasen",
    "title": "8  Vektoren",
    "section": "8.4 Vektorraumbasen",
    "text": "8.4 Vektorraumbasen\nIn diesem Abschnitt wollen wir den Begriff der Vektorraumbasis einführen. Eine Basis eines Vektorraums ist eine Untermenge von Vektoren des Vektorraums, die zur Darstellung aller Vektoren des Vektorraums genutzt werden kann. Im Sinne der linearen Kombination von Vektoren enthält also eine Vektorraumbasis alle nötige Information zur Konstruktion des entsprechenden Vektorraums. Allerdings ist eine Vektorraumbasis in der Regel nicht eindeutig und die viele Vektorräume haben in der Tat unendlich viele Basen. Die folgenden Definition sagt zunächst aus, wie aus einer beschränkten Anzahl von Vektoren mithilfe von Linearkombinationen unendlich viele Vektoren gebildet werden können.\n\nDefinition 8.10 (Lineare Hülle und Aufspannen) \\(V\\) sei ein Vektorraum und es sei \\(W := \\{w_1,...,w_k\\} \\subset V\\). Dann ist die lineare Hülle von \\(W\\) definiert als die Menge aller Linearkombinationen der Elemente von \\(W\\), \\[\\begin{equation}\n\\mbox{Span}(W) := \\left\\lbrace \\sum_{i=1}^k a_iw_i \\vert a_1,...,a_k \\mbox{ sind skalare Koeffizienten } \\right\\rbrace\n\\end{equation}\\] Man sagt, dass eine Menge von Vektoren \\(W \\subseteq V\\) einen Vektorraum \\(V\\) aufspannt, wenn jedes \\(v \\in V\\) als eine Linearkombination von Vektoren in \\(W\\) geschrieben werden kann.\n\nWir definieren nun den Begriff der Basis eines Vektorraums.\n\nDefinition 8.11 (Basis) \\(V\\) sei ein Vektorraum und es sei \\(B \\subseteq V\\). \\(B\\) heißt eine Basis von \\(V\\), wenn\n\ndie Vektoren in \\(B\\) linear unabhängig sind und\ndie Vektoren in \\(B\\) den Vektorraum \\(V\\) aufspannen.\n\n\nBasen von Vektorräumen haben folgende wichtige Eigenschaften.\n\nTheorem 8.4 (Eigenschaften von Basen)  \n\nAlle Basen eines Vektorraums beinhalten die gleiche Anzahl von Vektoren.\nJede Menge von \\(m\\) linear unabhängigen Vektoren ist Basis eines \\(m\\)-dimensionalen Vektorraums.\n\n\nFür einen Beweis dieses sehr tiefen Theorems verweisen wir auf die weiterführende Literatur. Die mit obigem Theorem benannte eindeutige Anzahl der Vektoren einer Basis eines Vektorraums heißt die Dimension des Vektorraums. Da es in der Regel unendliche viele Mengen von m linear unabhängigen Vektoren in einem Vektorraum gibt haben Vektorräume in der Regel unendlich viele Basen.\nBetrachtet man nun einen einzelnen Vektor in einem Vektorraum, so kann man sich fragen, wie man diesen mithilfe einer Vektorraumbasis darstellen kann. Dies führt auf folgende Begriffsbildungen.\n\nDefinition 8.12 (Basisdarstellung und Koordinaten) \\(B := \\{b_1,...,b_m\\}\\) sei eine Basis eines \\(m\\)-dimensionalen Vektorraumes \\(V\\) und es sei \\(v \\in V\\). Dann heißt die Linearkombination \\[\\begin{equation}\nv = \\sum_{i = 1}^m c_i b_i\n\\end{equation}\\] die Darstellung von \\(v\\) bezüglich der Basis \\(B\\) und die Koeffizienten \\(c_1,...,c_m\\) heißen die Koordinaten von \\(v\\) bezüglich der Basis \\(B\\).\n\nBei fester Basis sind auch die Koordinaten eines Vektors bezüglich dieser Basis fest und eindeutig. Dies ist die Aussage folgenden Theorems.\n\nTheorem 8.5 (Eindeutigkeit der Basisdarstellung) Die Basisdarstellung eines \\(v \\in V\\) bezüglich einer Basis \\(B\\) ist eindeutig.\n\n\nBeweis. Ohne Beschränkung der Allgemeinheit nehmen wir an, dass der Vektorraum von Dimension \\(m\\) ist. Nehmen wir an, dass zwei Darstellungen von \\(v\\) bezüglich der Basis \\(B\\) existieren, also dass \\[\\begin{align}\n\\begin{split}\nv & = a_1 b_1 + \\cdots + a_m b_m \\\\\nv & = c_1 b_1 + \\cdots + c_m b_m\n\\end{split}\n\\end{align}\\] Subtraktion der unteren von dern oberen Gleichung ergibt \\[\\begin{equation}\n0 = (a_1 - c_1) b_1 + \\cdots + (a_m - c_m) b_m\n\\end{equation}\\] Weil die \\(b_1,...,b_m\\) linear unabhängig sind, gilt aber, dass \\((a_i - c_i) = 0\\) für alle \\(i = 1,...,m\\) und somit sind die beiden Darstellungen von \\(v\\) bezüglich der Basis \\(B\\) identisch.\n\nZum Abschluss dieses Abschnitts wollen wir eine spezielle Basis des reellen Vektorraums betrachten.\n\nDefinition 8.13 (Orthonormalbasis von \\(\\mathbb{R}^m\\)) Eine Menge von \\(m\\) Vektoren \\(v_1,...,v_m \\in \\mathbb{R}^m\\) heißt Orthonormalbasis von \\(\\mathbb{R}^m\\), wenn \\(v_1,...,v_m\\) jeweils die Länge 1 haben und wechselseitig orthogonal sind, also wenn \\[\\begin{equation}\n\\langle v_i, v_j \\rangle =\n\\begin{cases}\n1   & \\mbox{ für } i = j    \\\\\n0   & \\mbox{ für } i \\neq j\n\\end{cases}.\n\\end{equation}\\]\n\nWir wollen zunächst ein Beispiel für eine Orthonormalbasis betrachten.\nBeispiel (1)\nEs ist \\[\\begin{equation}\nB_1 :=\n\\left\\lbrace\n\\begin{pmatrix}\n1 \\\\ 0\n\\end{pmatrix},\n\\begin{pmatrix}\n0 \\\\ 1\n\\end{pmatrix}\n\\right\\rbrace\n\\end{equation}\\] eine Orthonormalbasis von \\(\\mathbb{R}^2\\), denn \\(B_1\\) besteht aus zwei Vektoren und es gelten \\[\\begin{equation}\n\\left\\langle\n\\begin{pmatrix}\n1 \\\\ 0\n\\end{pmatrix},\n\\begin{pmatrix}\n1 \\\\ 0\n\\end{pmatrix}\n\\right\\rangle\n= 1 \\cdot 1 + 0 \\cdot 0\n= 1 + 0\n= 1\n\\end{equation}\\] sowie \\[\\begin{equation}\n\\left \\langle\n\\begin{pmatrix}\n0 \\\\ 1\n\\end{pmatrix},\n\\begin{pmatrix}\n0 \\\\1\n\\end{pmatrix}\n\\right \\rangle\n= 0 \\cdot 0 + 1 \\cdot 1\n= 0 + 1\n= 1\n\\end{equation}\\] und \\[\\begin{equation}\n\\left \\langle\n\\begin{pmatrix}\n1 \\\\ 0\n\\end{pmatrix},\n\\begin{pmatrix}\n0 \\\\ 1\n\\end{pmatrix}\n\\right \\rangle\n=    1 \\cdot 0 +  0  \\cdot 1\n= 0 + 0\n= 0\n\\end{equation}\\]\nFür allgemeine reelle Vektorräume werden Basen der Form von \\(B_1\\) mit dem Begriff der kanonischen Basis speziell ausgezeichnet.\n\nDefinition 8.14 (Kanonische Basis und kanonische Einheitsvektoren) Die Orthonormalbasis \\[\\begin{equation}\nB :=\n\\left\\lbrace\ne_1,...,e_m\n\\vert\ne_{i_j} = 1 \\mbox{ für } i =  j \\mbox{ und } e_{i_j} =  0 \\mbox{ für } i \\neq j\n\\right\\rbrace\n\\subset \\mathbb{R}^m\n\\end{equation}\\] heißt die kanonische Basis von \\(\\mathbb{R}^m\\) und die \\(e_{i_j}\\) heißen kanonische Einheitsvektoren.\n\n\\(B_1\\) aus Beispiel (1) ist also die kanonische Basis von \\(\\mathbb{R}^2\\).\nDie kanonische Basis von \\(\\mathbb{R}^3\\) ist \\[\\begin{equation}\nB :=\n\\left\\lbrace\n\\begin{pmatrix}\n1 \\\\ 0 \\\\ 0\n\\end{pmatrix},\n\\begin{pmatrix}\n0 \\\\ 1 \\\\ 0\n\\end{pmatrix},\n\\begin{pmatrix}\n0 \\\\ 0 \\\\ 1\n\\end{pmatrix}\n\\right\\rbrace.\n\\end{equation}\\]\nAllerdings gibt es auch nicht kanonische Orthonormalbasen. Dazu betrachten wir ein weiteres Beispiel\nBeispiel (2)\nEs ist auch \\[\\begin{equation}\nB_2 :=\n\\left\\lbrace\n\\begin{pmatrix}\n\\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}}\n\\end{pmatrix},\n\\begin{pmatrix} - \\frac{1}{\\sqrt{2}} \\\\ \\quad \\frac{1}{\\sqrt{2}}\n\\end{pmatrix}\n\\right\\rbrace\n\\end{equation}\\] eine Orthonormalbasis von \\(\\mathbb{R}^2\\), denn \\(B_2\\) besteht aus zwei Vektoren und es gelten \\[\\begin{equation}\n\\left \\langle\n\\begin{pmatrix}\n\\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}}\n\\end{pmatrix},\n\\begin{pmatrix}\n\\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}}\n\\end{pmatrix}\n\\right \\rangle\n= \\frac{1}{\\sqrt{2}} \\cdot \\frac{1}{\\sqrt{2}} + \\frac{1}{\\sqrt{2}} \\cdot \\frac{1}{\\sqrt{2}}\n= \\frac{1}{2} + \\frac{1}{2}\n= 1,\n\\end{equation}\\] sowie \\[\\begin{equation}\n\\left \\langle\n\\begin{pmatrix} - \\frac{1}{\\sqrt{2}} \\\\ \\quad \\frac{1}{\\sqrt{2}}\n\\end{pmatrix},\n\\begin{pmatrix} - \\frac{1}{\\sqrt{2}} \\\\ \\quad \\frac{1}{\\sqrt{2}}\n\\end{pmatrix}\n\\right \\rangle\n= \\left(- \\frac{1}{\\sqrt{2}} \\right)\\cdot \\left(- \\frac{1}{\\sqrt{2}} \\right) + \\frac{1}{\\sqrt{2}} \\cdot \\frac{1}{\\sqrt{2}}\n= \\frac{1}{2} + \\frac{1}{2}\n= 1\n\\end{equation}\\] und \\[\\begin{equation}\n\\left \\langle\n\\begin{pmatrix} - \\frac{1}{\\sqrt{2}} \\\\ \\quad \\frac{1}{\\sqrt{2}}\n\\end{pmatrix},\n\\begin{pmatrix}\n\\frac{1}{\\sqrt{2}} \\\\  \\frac{1}{\\sqrt{2}}\n\\end{pmatrix}\n\\right \\rangle\n= - \\frac{1}{\\sqrt{2}} \\cdot \\frac{1}{\\sqrt{2}} + \\frac{1}{\\sqrt{2}}   \\cdot \\frac{1}{\\sqrt{2}}\n= - \\frac{1}{2} + \\frac{1}{2}\n= 0\n\\end{equation}\\]\nWir visualisieren die beiden Orthonormalbasen \\(B_1\\) und \\(B_2\\) von \\(\\mathbb{R}^2\\) in Abbildung 8.8.\n\n\n\n\n\n\nAbbildung 8.8: Zwei Basen von \\(\\mathbb{R}^2\\)",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Vektoren</span>"
    ]
  },
  {
    "objectID": "108-Vektoren.html#sec-selbstkontrollfragen-vektoren",
    "href": "108-Vektoren.html#sec-selbstkontrollfragen-vektoren",
    "title": "8  Vektoren",
    "section": "8.5 Selbstkontrollfragen",
    "text": "8.5 Selbstkontrollfragen\n\nGeben Sie die Definition eines Vektorraums wieder.\nGeben Sie die Definition des reellen Vektorraums wieder.\nEs seien \\[\\begin{equation}\nx := \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix},\ny := \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n\\mbox{ und }\na := 2.\n\\end{equation}\\] Berechnen Sie \\[\\begin{equation}\nv = a(x+y) \\mbox{ und } w = \\frac{1}{a}(y-x)\n\\end{equation}\\]\nGeben Sie die Definition des Skalarproduktes auf \\(\\mathbb{R}^m\\) wieder.\nFür \\[\nx := \\begin{pmatrix} 2 \\\\ 1 \\\\ 3 \\end{pmatrix},\ny := \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix},\nz := \\begin{pmatrix} 3 \\\\ 1 \\\\ 0 \\end{pmatrix}\n\\tag{8.1}\\] berechnen Sie \\[\\begin{equation}\n\\langle x,y \\rangle, \\langle x, z \\rangle, \\langle y,z \\rangle\n\\end{equation}\\]\nGeben Sie die Definition des Euklidischen Vektorraums wieder.\nGeben Sie die Definition der Länge eines Vektors im Euklidischen Vektorraum wieder,\nBerechnen Sie die Längen der Vektoren \\(x,y,z\\) aus Gleichung 8.1.\nGeben Sie Definition des Abstands zweier Vektoren im Euklidischen Vektorraum wieder.\nBerechnen Sie \\(d(x,y), d(x,z)\\) und \\(d(y,z)\\) für \\(x,y,z\\) aus Gleichung 8.1.\nGeben Sie die Definition des Winkels zwischen zwei Vektoren im Euklidischen Vektorraum wieder.\nBerechnen Sie die Winkel zwischen den Vektoren \\(x\\) und \\(y\\), \\(x\\) und \\(z\\), sowie \\(y\\) und \\(z\\) aus Gleichung 8.1.\nGeben Sie die Definitionen der Orthogonalität und Orthonormalität von Vektoren wieder.\nGeben Sie die Definition der Linearkombination von Vektoren wieder.\nGeben Sie die Definition der linearen Unabhängigkeit von Vektoren wieder.\nWoran kann man erkennen, ob zwei reelle Vektoren linear abhängig sind oder nicht?\nGeben Sie die Definition der linearen Hülle einer Menge von Vektoren wieder.\nGeben Sie die Definition der Basis eines Vektorraums wieder.\nGeben Sie das Theorem zu den Eigenschaften von Vektorraumbasen wieder.\nGeben Sie die Definition der Basisdarstellung eines Vektors wieder.\nGeben Sie die Definition eienr Orthonormalbasis von \\(\\mathbb{R}^m\\) wieder.\nGeben Sie die Definition der kanonischen Basis von \\(\\mathbb{R}^m\\) wieder.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Vektoren</span>"
    ]
  },
  {
    "objectID": "109-Matrizen.html",
    "href": "109-Matrizen.html",
    "title": "9  Matrizen",
    "section": "",
    "text": "9.1 Definition\nMatrizen sind die Worte der Sprache der modernen Datenanalyse. Ein Verständnis moderner datenanalytischer Verfahren und ihrer Implementation ist ohne ein Grundverständnis des Matrixbegriffs und ein Wissen um die grundlegenden Matrixoperationen nicht möglich. Matrizen können dabei sehr unterschiedliche Rollen spielen. So können Matrizen zum Beispiel Daten, experimentelle Designs und Modellparameter repräsentieren. Im Kontext der Linearen Algebra dienen Matrizen zur Repräsentation linearer Abbildungen und von Vektorräumen, hier werden Vektoren dann als spezielle Matrizen aufgefasst.\nIn diesem Kapitel geben wir eine Einführung zum Umgang mit Matrizen, wobei wir auf abstrakte Begrifflichkeiten der Linearen Algebra im Wesentlichen verzichten. Wir führen zunächst den Matrixbegriff ein und diskutieren dann mit der Matrixaddition, Matrixsubtraktion, Skalarmultiplikation und der Matrixtransposition erste grundlegende Matrixoperationen (Kapitel 9.1 und Kapitel 9.2). Wir führen dann die zentralen Begriffe der Matrixmultiplikation und der Matrixinversion ein (Kapitel 9.3 und Kapitel 9.4). Mit der Matrixdeterminante diskutieren wir dann in Kapitel 9.5 eine erste Maßzahl zur Beschreibung von Matrizen. Wir schließen in Kapitel 9.6 mit einer Übersicht zu besonders häufig auftretenden Matrizen.\nWir beginnen mit der Definition einer Matrix.\nMatrizen bestehen aus Zeilen (rows) und Spalten (columns). Die Matrixeinträge \\(a_{ij}\\) werden mit einem Zeilenindex \\(i\\) und einem Spaltenindex \\(j\\) indiziert. Zum Beispiel gilt für \\[\\begin{equation}\nA:=\\begin{pmatrix}\n2 & 7 & 5 & 2 \\\\\n8 & 2 & 5 & 6 \\\\\n6 & 4 & 0 & 9 \\\\\n9 & 2 & 1 & 2\n\\end{pmatrix},\n\\end{equation}\\] dass \\(a_{32} = 4\\). Die Größe oder Dimension einer Matrix ergibt sich aus der Anzahl ihrer Zeilen \\(n \\in \\mathbb{N}\\) und Spalten \\(m \\in \\mathbb{N}\\). Matrizen mit \\(n = m\\) heißen quadratische Matrizen.\nIn der Folge benötigen wir nur Matrizen mit reellen Einträgen, also \\(a_{ij} \\in \\mathbb{R}\\) für alle \\(i = 1,...,n\\) und \\(j = 1,...,m\\). Wir nennen die Matrizen mit reellen Einträge reelle Matrizen und bezeichnen die Menge der reellen Matrizen mit \\(n\\) Zeilen und \\(m\\) Spalten mit \\(\\mathbb{R}^{n \\times m}\\). An dem Ausdruck \\[\\begin{equation}\nA \\in \\mathbb{R}^{n\\times m}\n\\end{equation}\\] können wir also ablesen, dass \\(A\\) eine reelle Matrix mit \\(n\\) Zeilen und \\(m\\) Spalten ist. Wir identifizieren dabei die Menge \\(\\mathbb{R}^{1 \\times 1}\\) mit der Menge \\(\\mathbb{R}\\), die Menge \\(\\mathbb{R}^{n \\times 1}\\) mit der Menge \\(\\mathbb{R}^n\\). Reelle Matrizen mit einer Spalte und \\(n\\) Zeilen entsprechen also \\(n\\)-dimensionalen reellen Vektoren und reelle Matrizen mit einer Spalte und einer Zeile entsprechen reellen Zahlen.\nDefinition von Matrizen in R\nIn R werden Matrizen definiert, indem R Vektoren mithilfe der matrix() Funktion in die Repräsentation einer mathematischen Matrix transformiert werden. Die Einträge eines R Vektors werden dabei anhand der spezifizierten Zeilenanzahl nrow anhand ihrer Gesamtanzahl auf die Matrix verteilt. Wollen wir beispielsweise die Matrix \\[\\begin{equation}\nA :=\n\\begin{pmatrix}\n2 & 3 & 0 \\\\\n1 & 6 & 5\n\\end{pmatrix}\n\\end{equation}\\] in R definieren, so ergibt sich\n# Spaltenweise Definition von A (R default)\nA = matrix(c(2,1,3,6,0,5), nrow = 2)\nprint(A)\n\n     [,1] [,2] [,3]\n[1,]    2    3    0\n[2,]    1    6    5\nR folgt hier per default einer sogenannten column-major-order, das heißt, die Elemente des R Vektors c(2,1,3,6,0,5) werden der Reihe nach von oben nach unten in die Spalten der Matrix von links nach rechts überführt. Einen etwas klareren Zusammenhang zwischen dem visuellen Layout des R Codes und der resultierenden Matrix erhält man, indem man den R Vektor mithilfe von Zeilenumbrüchen anhand des intendierten Matrixlayouts formatiert und dann die column-major-order mithilfe des Arguments byrow = TRUE zu einer row-major-order umstellt. Es wird dann zunächst die erste Zeile der Matrix von links nach rechts mit den Elementen des R Vektors gefüllt wird und dann die zweite Zeile usw. bis alle Elemente des Vektors auf die Matrix verteilt sind.\n# Reihenweise Definition von A (R default)\nA = matrix(c(2,3,0,\n             1,6,5),\n             nrow = 2,\n             byrow = TRUE)\nprint(A)\n\n     [,1] [,2] [,3]\n[1,]    2    3    0\n[2,]    1    6    5\n# Zeilenweise Definition von B\nB = matrix(c(4,1,0,\n            -4,2,0), \n            nrow = 2, \n            byrow = TRUE)\nprint(B)\n\n     [,1] [,2] [,3]\n[1,]    4    1    0\n[2,]   -4    2    0",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Matrizen</span>"
    ]
  },
  {
    "objectID": "109-Matrizen.html#sec-matrix-definition",
    "href": "109-Matrizen.html#sec-matrix-definition",
    "title": "9  Matrizen",
    "section": "",
    "text": "Definition 9.1 Eine Matrix ist eine rechteckige Anordnung von Zahlen, die wie folgt bezeichnet wird \\[\\begin{equation}\nA := \\begin{pmatrix}\na_{11} & a_{12} & \\cdots & a_{1m} \\\\\na_{21} & a_{22} & \\cdots & a_{2m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & \\cdots & a_{nm}\n\\end{pmatrix}\n:= {(a_{ij})}_{1\\le i\\le n,\\, 1\\le j\\le m}.\n\\end{equation}\\]",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Matrizen</span>"
    ]
  },
  {
    "objectID": "109-Matrizen.html#sec-grundlegende-matrixoperationen",
    "href": "109-Matrizen.html#sec-grundlegende-matrixoperationen",
    "title": "9  Matrizen",
    "section": "9.2 Grundlegende Matrixoperationen",
    "text": "9.2 Grundlegende Matrixoperationen\nMan kann mit Matrizen rechnen. Dabei sind folgende Matrixoperationen grundlegend:\n\nDie Addition von Matrizen gleicher Größe, genannt Matrixaddition,\ndie Subtraktion von Matrizen gleicher Größe, genannt Matrixsubtraktion,\ndie Multiplikation einer Matrix mit einem Skalar, genannt Skalarmultiplikation,\ndas Vertauschen der Zeilen- und Spalten einer Matrix, genannt Matrixtransposition.\n\nWir führen diese Operationen in der Folge in Operatorform, also als Funktionen ein. Dies dient insbesondere dazu, bei jeder Operation mit Hilfe ihrer Definitionsmenge zu betonen, von welcher Art die Objekte der jeweiligen Operation sind und mithilfe ihrer Bildmenge zu betonen, von welcher Art das Resultat der jeweiligen Operation ist.\n\n9.2.1 Matrixaddition\n\nDefinition 9.2 Es seien \\(A,B\\in \\mathbb{R}^{n\\times m}\\). Dann ist die Addition von \\(A\\) und \\(B\\) definiert als die Abbildung \\[\\begin{equation}\n+ : \\mathbb{R}^{n\\times m} \\times \\mathbb{R}^{n\\times m} \\to \\mathbb{R}^{n \\times m}, \\,\n(A,B) \\mapsto +(A,B) := A + B\n\\end{equation}\\] mit \\[\\begin{align}\n\\begin{split}\nA + B\n& =\n\\begin{pmatrix}\na_{11} & a_{12} & \\cdots & a_{1m} \\\\\na_{21} & a_{22} & \\cdots & a_{2m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & \\cdots & a_{nm}\n\\end{pmatrix}\n+\n\\begin{pmatrix}\nb_{11} & b_{12} & \\cdots & b_{1m} \\\\\nb_{21} & b_{22} & \\cdots & b_{2m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nb_{n1} & b_{n2} & \\cdots & b_{nm}\n\\end{pmatrix}\n\\\\\n&\n:=\n\\begin{pmatrix}\na_{11} + b_{11} & a_{12} + b_{12} & \\cdots & a_{1m} + b_{1m} \\\\\na_{21} + b_{21} & a_{22} + b_{22} & \\cdots & a_{2m} + b_{2m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} + b_{n1} & a_{n2} + b_{n2} & \\cdots & a_{nm} + b_{nm}\n\\end{pmatrix}.\n\\end{split}\n\\end{align}\\]\n\nDie Definition der Matrixaddition legt insbesondere fest, dass nur Matrizen gleicher Größe addiert werden können und dass die Operation der Matrixaddition elementweise definiert ist.\nBeispiel\nEs seien \\(A,B\\in \\mathbb{R}^{2\\times 3}\\) definiert als \\[\\begin{equation}\nA:=\\begin{pmatrix}\n2 & -3 & 0\\\\\n1 &  6 & 5\\\\\n\\end{pmatrix}\n\\mbox{ und }\nB := \\begin{pmatrix}\n4 & 1 & 0\\\\\n-4 & 2 & 0\\\\\n\\end{pmatrix}.\n\\end{equation}\\] Da \\(A\\) und \\(B\\) gleich groß sind, können wir sie addieren \\[\\begin{align}\n\\begin{split}\nC\n= A+B\n& =\n\\begin{pmatrix}\n2 & -3 & 0\\\\\n1 &  6 & 5\\\\\n\\end{pmatrix}\n+\n\\begin{pmatrix}\n4 & 1 & 0\\\\\n-4 & 2 & 0\\\\\n\\end{pmatrix}\\\\\n& =\n\\begin{pmatrix}\n2 + 4 & -3 + 1 & 0 + 0\\\\\n1 - 4 &  6 + 2 & 5 + 0\\\\\n\\end{pmatrix}\\\\\n& =\n\\begin{pmatrix}\n6 & -2 & 0\\\\\n-3 &  8 & 5 \\\\\n\\end{pmatrix}.\n\\end{split}\n\\end{align}\\]\nIn R führt man obige Rechnung wie folgt aus.\n\n# Definition\nA = matrix(c(2, -3, 0,\n             1,  6, 5),\n             nrow  = 2,\n             byrow = TRUE)\nB = matrix(c( 4, 1, 0,\n             -4, 2, 0),\n              nrow = 2,\n             byrow = TRUE)\n\n# Addition\nC = A + B\nprint(C)\n\n     [,1] [,2] [,3]\n[1,]    6   -2    0\n[2,]   -3    8    5\n\n\n\n\n9.2.2 Matrixsubtraktion\nDie Subtraktion von Matrizen gleicher Größe ist analog zur Addition definiert.\n\nDefinition 9.3 (Matrixsubtraktion) Es seien \\(A,B\\in \\mathbb{R}^{n\\times m}\\). Dann ist die Subtraktion von \\(A\\) und \\(B\\) definiert als die Abbildung \\[\\begin{equation}\n- : \\mathbb{R}^{n\\times m} \\times \\mathbb{R}^{n\\times m} \\to \\mathbb{R}^{n\\times m}, \\,\n(A,B) \\mapsto -(A,B) := A - B\n\\end{equation}\\] mit \\[\\begin{align}\n\\begin{split}\nA - B\n& =\n\\begin{pmatrix}\na_{11} & a_{12} & \\cdots & a_{1m} \\\\\na_{21} & a_{22} & \\cdots & a_{2m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & \\cdots & a_{nm}\n\\end{pmatrix}\n-\n\\begin{pmatrix}\nb_{11} & b_{12} & \\cdots & b_{1m} \\\\\nb_{21} & b_{22} & \\cdots & b_{2m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nb_{n1} & b_{n2} & \\cdots & b_{nm}\n\\end{pmatrix}\n\\\\\n&\n:=\n\\begin{pmatrix}\na_{11} - b_{11} & a_{12} - b_{12} & \\cdots & a_{1m} - b_{1m} \\\\\na_{21} - b_{21} & a_{22} - b_{22} & \\cdots & a_{2m} - b_{2m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} - b_{n1} & a_{n2} - b_{n2} & \\cdots & a_{nm} - b_{nm}\n\\end{pmatrix}.\n\\end{split}\n\\end{align}\\]\n\nWie bei der Matrixaddition legt die Definition der Matrixsubtraktion fest, dass nur Matrizen gleicher Größe voneinander subtrahiert werden können und dass die Subktration zweier gleich großer Matrizen elementweise definiert ist.\nBeispiel\nWir können die im Beispiel zur Matrixaddition definierten Matrizen \\(A\\) und \\(B\\) auch voneinander subtrahieren, \\[\\begin{align}\n\\begin{split}\nD\n= A-B\n& =\n\\begin{pmatrix}\n2 & -3 & 0\\\\\n1 &  6 & 5\\\\\n\\end{pmatrix}\n-\n\\begin{pmatrix}\n4 & 1 & 0\\\\\n-4 & 2 & 0\\\\\n\\end{pmatrix}\\\\\n& =\n\\begin{pmatrix}\n2 - 4 & -3 - 1 & 0 - 0\\\\\n1 + 4 &  6 - 2 & 5 - 0\\\\\n\\end{pmatrix}\\\\\n& =\n\\begin{pmatrix}\n-2 & -4 & 0\\\\\n5 &  4 & 5 \\\\\n\\end{pmatrix}.\n\\end{split}\n\\end{align}\\]\nIn R führt man diese Rechnung wie folgt aus.\n\n# Subtraktion\nD = A - B\nprint(D)\n\n     [,1] [,2] [,3]\n[1,]   -2   -4    0\n[2,]    5    4    5\n\n\n\n\n9.2.3 Skalarmultiplikation\nDie Skalarmultiplikation einer Matrix bezeichnet die Multiplikation eines Skalars mit einer Matrix.\n\nDefinition 9.4 (Skalarmultiplikation) Es sei \\(c \\in \\mathbb{R}\\) ein Skalar und \\(A \\in \\mathbb{R}^{n\\times m}\\). Dann ist die Skalarmultiplikation von \\(c\\) und \\(A\\) definiert als die Abbildung \\[\\begin{equation}\n\\cdot : \\mathbb{R} \\times \\mathbb{R}^{n\\times m} \\to \\mathbb{R}^{n\\times m}, \\,\n(c,A) \\mapsto \\cdot (c,A) := cA\n\\end{equation}\\] mit \\[\\begin{align}\n\\begin{split}\ncA\n=\nc\n\\begin{pmatrix}\na_{11} & a_{12} & \\cdots & a_{1m} \\\\\na_{21} & a_{22} & \\cdots & a_{2m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & \\cdots & a_{nm}\n\\end{pmatrix}\n:=\n\\begin{pmatrix}\nca_{11} & ca_{12} & \\cdots & ca_{1m}  \\\\\nca_{21} & ca_{22} & \\cdots & ca_{2m}  \\\\\n\\vdots  & \\vdots  & \\ddots & \\vdots    \\\\\nca_{n1} & ca_{n2} & \\cdots & ca_{nm}\n\\end{pmatrix}.\n\\end{split}\n\\end{align}\\]\n\nDie Skalarmultiplikation ist mit dieser Definition also elementweise definiert.\nBeispiel\nEs seien \\(c:=-3\\) und \\(A\\in \\mathbb{R}^{4\\times 3}\\) definiert als\n\\[\\begin{equation}\nA := \\begin{pmatrix}\n3 & 1 & 1\\\\\n5 & 2 & 5\\\\\n2 & 7 & 1\\\\\n3 & 4 & 2\n\\end{pmatrix}.\n\\end{equation}\\] Dann ergibt sich \\[\\begin{align}\n\\begin{split}\nB :=\ncA\n= -3\\begin{pmatrix}\n3 & 1 & 1\\\\\n5 & 2 & 5\\\\\n2 & 7 & 1\\\\\n3 & 4 & 2\n\\end{pmatrix}\n= \\begin{pmatrix}\n-3\\cdot3 & -3\\cdot1 & -3\\cdot1\\\\\n-3\\cdot5 & -3\\cdot2 & -3\\cdot5\\\\\n-3\\cdot2 & -3\\cdot7 & -3\\cdot1\\\\\n-3\\cdot3 & -3\\cdot4 & -3\\cdot2\n\\end{pmatrix}\n= \\begin{pmatrix}\n-9  &  -3 & -3  \\\\\n-15 &  -6 & -15 \\\\\n-6  & -21 & -3  \\\\\n-9  & -12 & -6\n\\end{pmatrix}.\n\\end{split}\n\\end{align}\\]\nIn R führt man diese Skalarmultiplikation aus wie folgt. \n\n# Definitionen\nA = matrix(c(3,1,1,\n             5,2,5,\n             2,7,1,\n             3,4,2),\n           nrow = 4,\n           byrow = TRUE)\nc = -3\n\n# Skalarmultiplikation\nB = c*A\nprint(B)\n\n     [,1] [,2] [,3]\n[1,]   -9   -3   -3\n[2,]  -15   -6  -15\n[3,]   -6  -21   -3\n[4,]   -9  -12   -6\n\n\nMithilfe der Definition von Matrixaddition und Skalarmultiplikation ist es möglich, einen Vektorraum zu definieren, dessen Elemente die reellen Matrizen sind. Insbesondere legt diese Definition auch die Rechenregeln beim Umgang mit Matrixaddition und Skalarmultiplikation fest.\n\nTheorem 9.1 (Vektorraum der reellwertigen Matrizen) Das Tripel \\((\\mathbb{R}^{n \\times m}, +, \\cdot)\\) mit der oben definierten Matrixaddition und Skalarmultiplikation ist ein Vektorraum. Insbesondere gelten damit für \\(A,B,C\\in \\mathbb{R}^{n \\times m}\\) und \\(r,s,t\\in \\mathbb{R}\\) folgende Rechenregeln:\n\nKommutativität der Addition: \\(A + B = B + A\\).\nAssoziativität der Addition: \\((A + B) + C = A + (B + C)\\).\nExistenz eines neutralen Elements der Addition: \\(\\exists\\, 0 \\in \\mathbb{R}^{n \\times m}\\) mit \\(A + 0 = 0 + A = A\\).\nExistenz inverser Elemente der Addition: \\(\\forall A\\,\\exists -A \\in \\mathbb{R}^{n \\times m}\\) mit \\(A + (-A) = 0\\).\nExistenz eines neutralen Elements der Skalarmultiplikation: \\(\\exists\\, 1 \\in \\mathbb{R}\\) mit \\(1 \\cdot A = A\\).\nAssoziativität der Skalarmultiplikation: \\(r \\cdot (s \\cdot t) = (r \\cdot s)\\cdot t\\).\nDistributivität hinsichtlich der Matrixaddition: \\(r\\cdot (A + B) = r\\cdot A + r\\cdot B\\).\nDistributivität hinsichtlich der Skalaraddition: \\((r + s)\\cdot A = r\\cdot A + s\\cdot A\\).\n\n\nWir verzichten auf einen Beweis, der sich mit einigem Notationsaufwand direkt aus dem elementweisen Charakter von Matrixaddition und Skalarmultiplikation sowie den aus dem Umgang mit den reellen Zahlen bekannten Rechenregeln ergibt. Das im Theorem erwähnte neutrale Element der Addition wird Nullmatrix genannt, wir werden dazu später eine allgemeine Notation einführen. Die inversen Elemente der Addition sind durch \\[\\begin{equation}\n-A := (-a_{ij})_{1\\le i \\le n, 1 \\le j \\le m}\n\\end{equation}\\] gegeben und erlauben es, die Matrixsubtraktion als Spezialfall der Matrixaddition zu betrachten.\n\n\n9.2.4 Matrixtransposition\nEine weitere häufig auftretende grundlegende Matrixoperation ist das Vertauschen der Zeilen- und Spaltenanordnung einer Matrix, genannt Matrixtransposition.\n\nDefinition 9.5 (Matrixtransposition) Es sei \\(A \\in \\mathbb{R}^{n\\times m}\\). Dann ist die Transposition von \\(A\\) definiert als die Abbildung \\[\\begin{equation}\n\\cdot^{T} : \\mathbb{R}^{n\\times m} \\to \\mathbb{R}^{m \\times n}, \\,\nA \\mapsto \\cdot^{T}(A) := A^T\n\\end{equation}\\] mit \\[\\begin{align}\n\\begin{split}\nA^T\n=\n\\begin{pmatrix}\na_{11} & a_{12} & \\cdots & a_{1m} \\\\\na_{21} & a_{22} & \\cdots & a_{2m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & \\cdots & a_{nm}\n\\end{pmatrix}^T\n:=\n\\begin{pmatrix}\na_{11} & a_{21} & \\cdots & a_{n1} \\\\\na_{12} & a_{22} & \\cdots & a_{n2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{1m} & a_{2m} & \\cdots & a_{nm}\n\\end{pmatrix}.\n\\end{split}\n\\end{align}\\]\n\nFür \\(A \\in \\mathbb{R}^{n \\times m}\\) gilt damit also immer \\(A^T \\in \\mathbb{R}^{m \\times n}\\). Weiterhin gelten folgende Rechenregeln der Matrixtransposition, wie man sich an Beispielen klar macht:\n\nFür \\(A \\in \\mathbb{R}^{1 \\times 1}\\) gilt \\[\\begin{equation}\nA^T = A.\n\\end{equation}\\]\nEs gilt \\[\\begin{equation}\n\\left(A^T\\right)^T = A.\n\\end{equation}\\]\nEs gilt \\[\\begin{equation}\n\\left(a_{ii}\\right)_{1 \\le i \\le \\mbox{min}(n,m)} = \\left(a_{ii}\\right)^T_{1 \\le i \\le \\mbox{min}(n,m)}.\n\\end{equation}\\]\n\nLetztere Eigenschaft der Transposition besagt, dass die Elemente auf der Hauptdiagonalen einer Matrix bei Transposition unberührt bleiben.\nBeispiel\nEs sei \\(A \\in \\mathbb{R}^{2 \\times 3}\\) definiert durch \\[\\begin{equation}\nA:=\\begin{pmatrix}\n2 & 3 & 0 \\\\\n1 & 6 & 5 \\\\\n\\end{pmatrix},\n\\end{equation}\\] Dann gilt \\(A^T \\in \\mathbb{R}^{3 \\times 2}\\) und speziell \\[\\begin{equation}\nA^{T} :=\n\\begin{pmatrix}\n2  & 1 \\\\\n3  & 6 \\\\\n0  & 5 \\\\\n\\end{pmatrix}.\n\\end{equation}\\] Weiterhin gilt offenbar \\(\\min(m,n) = 2\\) und folglich \\[\\begin{equation}\n(a_{11}) = \\left(a_{11}\\right)^T\n\\mbox{ und }\n(a_{22}) = \\left(a_{22}\\right)^T.\n\\end{equation}\\] In R führt man die Transposition einer Matrix wie folgt durch.\n\n# Definition\nA = matrix(c(2,3,0,\n             1,6,5),\n           nrow = 2,\n           byrow = TRUE)\nprint(A)\n\n     [,1] [,2] [,3]\n[1,]    2    3    0\n[2,]    1    6    5\n\n# Transposition\nAT = t(A)\nprint(AT)\n\n     [,1] [,2]\n[1,]    2    1\n[2,]    3    6\n[3,]    0    5\n\n\nSchließlich gelten in der Verbindung mit der Matrixaddition, Matrixsubtraktion und der Skalarmultiplikation folgende Rechenregeln, wie man sich an Beispielen klar macht:\n\nFür \\(A,B\\in \\mathbb{R}^{n \\times m}\\) gilt \\[\\begin{equation}\n(A+B)^T = A^T + B^T.\n\\end{equation}\\]\nFür \\(A,B\\in \\mathbb{R}^{n \\times m}\\) gilt \\[\\begin{equation}\n(A-B)^T = A^T - B^T.\n\\end{equation}\\]\nFür \\(c\\in \\mathbb{R}\\) und \\(A \\in \\mathbb{R}^{n \\times m}\\) gilt \\[\\begin{equation}\n(cA)^T = cA^T.\n\\end{equation}\\]",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Matrizen</span>"
    ]
  },
  {
    "objectID": "109-Matrizen.html#sec-matrixmultiplikation",
    "href": "109-Matrizen.html#sec-matrixmultiplikation",
    "title": "9  Matrizen",
    "section": "9.3 Matrixmultiplikation",
    "text": "9.3 Matrixmultiplikation\nDie Matrixmultiplikation ist die zentrale Operation beim Rechnen mit Matrizen. Sie ist definiert wie folgt.\n\nDefinition 9.6 (Matrixmultiplikation) Es seien \\(A\\in \\mathbb{R}^{n \\times m}\\) und \\(B \\in \\mathbb{R}^{m \\times k}\\). Dann ist die Matrixmultiplikation von \\(A\\) und \\(B\\) definiert als die Abbildung \\[\\begin{equation}\n\\cdot : \\mathbb{R}^{n\\times m} \\times \\mathbb{R}^{m\\times k} \\to \\mathbb{R}^{n \\times k}, \\,\n(A,B) \\mapsto \\cdot(A,B) := AB\n\\end{equation}\\] mit \\[\\begin{align}\n\\begin{split}\nAB\n& =\n\\begin{pmatrix}\na_{11} & a_{12} & \\cdots & a_{1m} \\\\\na_{21} & a_{22} & \\cdots & a_{2m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & \\cdots & a_{nm}\n\\end{pmatrix}\n\\begin{pmatrix}\nb_{11} & b_{12} & \\cdots & b_{1k} \\\\\nb_{21} & b_{22} & \\cdots & b_{2k} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nb_{m1} & b_{m2} & \\cdots & b_{mk}\n\\end{pmatrix}\n\\\\\n&\n:=\n\\begin{pmatrix}\n\\sum_{i=1}^m a_{1i}b_{i1} & \\sum_{i=1}^m a_{1i}b_{i2} & \\cdots & \\sum_{i=1}^m a_{1i}b_{ik}  \\\\\n\\sum_{i=1}^m a_{2i}b_{i1} & \\sum_{i=1}^m a_{2i}b_{i2} & \\cdots & \\sum_{i=1}^m a_{2i}b_{ik}  \\\\\n\\vdots                    & \\vdots                    & \\ddots & \\vdots                     \\\\\n\\sum_{i=1}^m a_{ni}b_{i1} & \\sum_{i=1}^m a_{ni}b_{i2} & \\cdots & \\sum_{i=1}^m a_{ni}b_{ik}\n\\end{pmatrix}\n\\\\\n&\n= \\left(\\sum_{i=1}^m a_{ji}b_{il} \\right)_{1 \\le j \\le n, 1 \\le l \\le k}\n\\end{split}\n\\end{align}\\]\n\nDas Matrixprodukt \\(AB\\) ist also nur dann definiert, wenn \\(A\\) genau so viele Spalten hat wie \\(B\\) Zeilen hat. Informell gilt für die beteiligten Matrixgrößen dabei die Merkregel \\[\\begin{equation}\n(n \\times m)(m \\times k) = (n \\times k).\n\\end{equation}\\] Der Eintrag \\((AB)_{ij}\\) in \\(AB\\) entspricht der Summe der multiplizierten \\(i\\)ten Zeile von \\(A\\) und \\(j\\)ten Spalte von \\(B\\). Zum Berechnen von \\((AB)_{ij}\\) geht man für \\(i = 1,...,n\\) und \\(j = 1,...,k\\) also in Gedanken wie folgt vor:\n\nMan legt die Tranposition der \\(i\\)ten Zeile von \\(A\\) über die \\(j\\)te Spalte von \\(B\\).\nWeil \\(A\\) genau \\(m\\) Spalten hat und \\(B\\) genau \\(m\\) Zeilen hat, gibt es dann zu jedem Element der Zeile aus \\(A\\) ein korrespondierendes Element in der Spalte von \\(B\\).\nMan multipliziert die korrespondierenden Elemente miteinander.\nDie Summe dieser Produkte ist dann der Eintrag mit Index \\(ij\\) in \\(AB\\).\n\nBeispiel\n\\(A\\in \\mathbb{R}^{2\\times 3}\\) und \\(B\\in \\mathbb{R}^{3\\times 2}\\) seien definiert als \\[\\begin{equation}\nA := \\begin{pmatrix}\n2 & -3 &  0   \\\\\n1 &  6 &  5\n\\end{pmatrix}\n\\mbox{ und }\nB := \\begin{pmatrix}\n4 & 2  \\\\\n-1 & 0  \\\\\n1 & 3\n\\end{pmatrix}.\n\\end{equation}\\] Wir wollen \\(C := AB\\) und \\(D := BA\\) berechnen. Mit \\(n = 2, m = 3\\) und \\(k = 2\\) wissen wir schon, dass \\(C \\in \\mathbb{R}^{2 \\times 2}\\) und \\(D \\in \\mathbb{R}^{3 \\times 3}\\), weil \\[\\begin{equation}\n(2 \\times 3)(3 \\times 2) = (2 \\times 2)\n\\end{equation}\\] und \\[\\begin{equation}\n(3 \\times 2)(2 \\times 3) = (3 \\times 3).\n\\end{equation}\\] Es gilt hier also sicher \\(AB \\neq BA\\). Für \\(C\\) ergibt sich dann \\[\\begin{align}\n\\begin{split}\nC\n& = AB\n\\\\\n& = \\begin{pmatrix}\n2 & -3 & 0 \\\\\n1 &  6 & 5 \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n4  & 2 \\\\\n-1 & 0 \\\\\n1  & 3\n\\end{pmatrix}\n\\\\\n& =\n\\begin{pmatrix}\n2\\cdot 4 + (-3)\\cdot (-1) + 0\\cdot 1 & 2\\cdot 2 + (-3)\\cdot 0 + 0\\cdot 3 \\\\\n1\\cdot 4 +    6\\cdot (-1) + 5\\cdot 1 & 1\\cdot 2 +  6\\cdot 0 + 5\\cdot 3 \\\\\n\\end{pmatrix}\n\\\\\n& =\n\\begin{pmatrix}\n8 + 3 + 0 & 4 + 0 + 0 \\\\\n4 - 6 + 5 & 2 + 0 + 15 \\\\\n\\end{pmatrix}\n\\\\\n& =\n\\begin{pmatrix}\n11 & 4 \\\\\n3 & 17 \\\\\n\\end{pmatrix}.\n\\end{split}\n\\end{align}\\]\nIn R nutzt man für die Matrixmultiplikation den %*% Operator.\n\n# Definitionen\nA = matrix(c(2,-3,0,\n             1, 6,5),\n           nrow  = 2,\n           byrow = TRUE)\nB = matrix(c( 4,2,\n             -1,0,\n              1,3),\n           nrow  = 3,\n           byrow = TRUE)\n\n# Matrixmultiplikation\nC = A %*% B\nprint(C)\n\n     [,1] [,2]\n[1,]   11    4\n[2,]    3   17\n\n\nFür \\(D\\) ergibt sich weiterhin \\[\\begin{align}\n\\begin{split}\nD\n& = BA\n\\\\\n& =\n\\begin{pmatrix}\n4  & 2 \\\\\n-1 & 0 \\\\\n1  & 3\n\\end{pmatrix}\n\\begin{pmatrix}\n2 & -3 & 0 \\\\\n1 &  6 & 5 \\\\\n\\end{pmatrix}\n\\\\\n& =\n\\begin{pmatrix}\n  4    \\cdot   2  + 2 \\cdot 1\n& 4    \\cdot (-3) + 2 \\cdot 6\n& 4    \\cdot   0  + 2 \\cdot 5\n\\\\\n  (-1) \\cdot  2  + 0 \\cdot 1\n& (-1) \\cdot(-3) + 0 \\cdot 6\n& (-1) \\cdot  0  + 0 \\cdot 5\n\\\\\n  1    \\cdot  2  + 3 \\cdot 1\n& 1    \\cdot(-3) + 3 \\cdot 6\n& 1    \\cdot  0  + 3 \\cdot 5\n\\end{pmatrix}\n\\\\\n& =\n\\begin{pmatrix}\n    8 + 2\n& -12 + 12\n&   0 + 5\n\\\\\n   -2 + 0\n&   3 + 0\n&   0 + 0\n\\\\\n    2 + 3\n&  -3 + 18\n&   0 + 15\n\\end{pmatrix}\n\\\\\n& =\n\\begin{pmatrix}\n  10\n&  0\n& 10\n\\\\\n  -2\n&  3\n&  0\n\\\\\n   5\n& 15\n& 15\n\\\\\n\\end{pmatrix}\n\\end{split}\n\\end{align}\\]\nIn R überprüft man diese Rechnung wie folgt.\n\n# Definitionen\nA = matrix(c(2,-3,0,\n             1, 6,5),\n           nrow  = 2,\n           byrow = TRUE)\nB = matrix(c( 4,2,\n             -1,0,\n              1,3),\n           nrow  = 3,\n           byrow = TRUE)\n\n# Matrixmultiplikation\nD = B %*% A\nprint(D)\n\n     [,1] [,2] [,3]\n[1,]   10    0   10\n[2,]   -2    3    0\n[3,]    5   15   15\n\n\nIst allerdings eine Matrixmultiplikation aufgrund nicht-adäquater Matrizengrößen nicht definiert, so lässt sich diese auch nicht numerisch auswerten.\n\n# Beispiel für eine undefinierte Matrixmultipliation\nE = t(A) %*% B      # (3 x 2)(3 x 2)\n\nError in t(A) %*% B: non-conformable arguments\n\n\nFolgendes Theorem, das wir nicht beweisen wollen, stellt den Bezug zwischen dem Skalarprodukt zweier Vektoren und der Multiplikation zweier Matrizen her. Dieser ergibt sich im Wesentlichen durch die Identifikation von \\(\\mathbb{R}^{n}\\) und \\(\\mathbb{R}^{n \\times 1}\\) und der Tatsache, dass nach Definition der Eintrag \\((AB)_{ij}\\) im Produkt von \\(A \\in \\mathbb{R}^{n \\times m}\\) und \\(B \\in \\mathbb{R}^{m \\times k}\\) dem Vektorskalarprodukt der \\(i\\)ten Spalte von \\(A^T\\) und der \\(j\\)ten Spalte von \\(B\\) entspricht.\n\nTheorem 9.2 (Matrixmultiplikation und Vektorskalarprodukt) Es seien \\(x,y \\in \\mathbb{R}^n\\). Dann gilt \\[\\begin{equation}\n\\langle x,y \\rangle = x^Ty.\n\\end{equation}\\] Weiterhin seien für \\(A \\in \\mathbb{R}^{n\\times m}\\) für \\(i = 1,...,n\\) \\[\\begin{equation}\n\\bar{a}_i := (a_{ji})_{1 \\le j \\le m} \\in \\mathbb{R}^m\n\\end{equation}\\] die Spalten von \\(A^T\\) und für \\(B \\in \\mathbb{R}^{m \\times k}\\) für \\(i = 1,...,k\\) \\[\\begin{equation}\n\\bar{b}_j := (b_{ij})_{1 \\le j \\le m} \\in \\mathbb{R}^m\n\\end{equation}\\] die Spalten von \\(B\\), also \\[\\begin{equation}\nA^T =\n\\begin{pmatrix}\n\\bar{a}_1 & \\bar{a}_2 & \\cdots & \\bar{a}_n\n\\end{pmatrix}\n\\in \\mathbb{R}^{m \\times n}\n\\mbox{ und }\nB =\n\\begin{pmatrix}\n\\bar{b}_1 & \\bar{b}_2 & \\cdots & \\bar{b}_k\n\\end{pmatrix}\n\\in \\mathbb{R}^{m \\times k}.\n\\end{equation}\\] Dann gilt \\[\\begin{equation}\nAB = \\left(\\langle \\bar{a}_i,\\bar{b}_j \\rangle \\right)_{1 \\le i \\le n, 1 \\le j \\le k}.\n\\end{equation}\\]\n\n\n9.3.1 Rechenregeln der Matrixmultiplikation\nIm Folgenden stellen wir einige grundlegende Rechenregeln der Matrixmultiplikation, insbesondere auch in Kombination mit anderen Matrixoperationen zusammen.\nFür Beweise der folgenden zwei Theoreme zur Assoziativität und Distributivität, die sich im Wesentlichen mit den entsprechenden Rechenregeln für Summen und Produkte der reellen Zahlen ergeben, verweisen wir auf die weiterführende Literatur.\n\nTheorem 9.3 (Assoziativität) Es seien \\(A \\in \\mathbb{R}^{n \\times m}\\), \\(B \\in \\mathbb{R}^{m \\times k}\\), \\(C \\in \\mathbb{R}^{k \\times p}\\) und \\(c \\in \\mathbb{R}\\). Dann gelten\n\nDie Multiplikation von Matrizen ist assoziativ, es gilt \\[\\begin{equation}\nA(BC) = (AB)C.\n\\end{equation}\\]\nDie Kombination von Matrizenmultiplikation und Skalarmultiplikation ist assoziativ, \\[\\begin{equation}\nc(AB) = (cA)B = A(cB).\n\\end{equation}\\]\n\n\nDie Assoziativität von Matrizenmultiplikation und Skalarmultiplikation erkennt man leicht bei Betrachtung des \\(j,l\\)ten Elements von \\(c(AB), (cA)B\\) und \\(A(cB)\\) anhand von \\[\\begin{equation}\nc\\left(\\sum_{i = 1}^m a_{ji}b_{il}\\right)\n= \\sum_{i = 1}^m \\left(c a_{ji}\\right) b_{il}  \n= \\sum_{i = 1}^m a_{ji}\\left(c b_{il}\\right).\n\\end{equation}\\]\n\nTheorem 9.4 (Distributivität) Es seien \\(A \\in \\mathbb{R}^{n \\times m}\\), \\(B \\in \\mathbb{R}^{n \\times m}\\), \\(C \\in \\mathbb{R}^{m \\times p}\\). Dann gelten \\[\\begin{equation}\n(A + B)C = AC + BC\n\\end{equation}\\] und \\[\\begin{equation}\nC^T(A + B) = C^TA + C^TB\n\\end{equation}\\]\n\nIm Gegensatz zur Kommutativität der Multiplikation reeller Zahlen ist die Matrixmultiplikation im Allgemeinen nicht kommutativ.\n\nTheorem 9.5 (Nichtkommutativität) Es seien \\(A \\in \\mathbb{R}^{n \\times m}\\) und \\(B \\in \\mathbb{R}^{m \\times p}\\). Dann gilt im Allgemeinen \\[\\begin{equation}\nAB \\neq BA.\n\\end{equation}\\]\n\n\nBeweis. Im Fall \\(p \\neq n\\) ist \\(BA\\) nicht definiert, wir betrachten also nur den Fall \\(p = n\\). Wir zeigen durch Angabe eines Gegenbeispiels mit \\(A,B\\in \\mathbb{R}^{2 \\times n}\\), dass im Allgemeinen \\(AB = BA\\) nicht gilt. Es seien \\[\\begin{equation}\nA := \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}\n\\mbox{ und }\nB := \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix}.\n\\end{equation}\\] Dann gilt \\[\\begin{equation}\nAB\n=\n\\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}\n\\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix}\n=\n\\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}\n\\neq\n\\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix}\n=\n\\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix}\n\\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}\n=\nBA.\n\\end{equation}\\]\n\n\nTheorem 9.6 (Kombination von Matrixmultiplikation und Transposition) Es seien \\(A \\in \\mathbb{R}^{m \\times n}\\) und \\(B \\in \\mathbb{R}^{n \\times k}\\). Dann gilt \\[\\begin{equation}\n(AB)^T = B^TA^T.\n\\end{equation}\\]\n\n\nBeweis. Ein Beweis ergibt sich wie folgt \\[\\begin{align}\n\\begin{split}\n(AB)^T\n& = \\left(\\left(\\sum_{i=1}^m a_{ji}b_{il} \\right)_{1 \\le j \\le n, 1 \\le l \\le k}\\right)^T \\\\\n& = \\left(\\sum_{i=1}^m a_{ij}b_{li} \\right)_{1 \\le i \\le k, 1 \\le j \\le n}  \\\\\n& = \\left(\\sum_{i=1}^m b_{li}a_{ij} \\right)_{1 \\le j \\le k, 1 \\le l \\le n}  \\\\\n& = B^TA^T.\n\\end{split}\n\\end{align}\\]",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Matrizen</span>"
    ]
  },
  {
    "objectID": "109-Matrizen.html#sec-matrixinversion",
    "href": "109-Matrizen.html#sec-matrixinversion",
    "title": "9  Matrizen",
    "section": "9.4 Matrixinversion",
    "text": "9.4 Matrixinversion\nUm den Begriff der inversen Matrix zu motivieren, betrachten wir zunächst das Problem des Lösens eines linearen Gleichungssystems. Dazu seien \\(A\\in \\mathbb{R}^{n \\times n},\\, x \\in \\mathbb{R}^n\\) und \\(b \\in \\mathbb{R}^n\\) und es gelte \\[\\begin{equation}\nAx = b.\n\\end{equation}\\] \\(A\\) und \\(b\\) seien als bekannt vorausgesetzt, \\(x\\) sei unbekannt. Konkret seien beispielsweise \\[\\begin{equation}\nA := \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix} \\mbox{ und }b := \\begin{pmatrix}  5 \\\\ 11 \\end{pmatrix}.\n\\end{equation}\\]\nDann liegt folgendes lineares Gleichungssystem mit zwei Gleichungen und zwei Unbekannten vor: \\[\\begin{equation}\nAx = b\n\\Leftrightarrow\n\\begin{pmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\\nx_2\n\\end{pmatrix}\n= \\begin{pmatrix}\n5 \\\\\n11\n\\end{pmatrix}\n\\Leftrightarrow\n\\begin{matrix}\n1x_1 + 2x_2 & = 5 \\\\\n3x_1 + 4x_2 & = 11\n\\end{matrix}.\n\\end{equation}\\]\nZiel des Lösens von linearen Gleichungssystemen ist bekanntlich, herauszufinden, für welche \\(x\\) das Gleichungssystem erfüllt ist. Um in diesem Kontext den Begriff der inversen Matrix von \\(A\\) einzuführen, vereinfachen wir die Situation weiter. Wir nehmen an, dass \\(A = a\\) eine \\(1 \\times 1\\) Matrix, also ein Skalar, sei und ebenso \\(x\\) und \\(b\\), dass wir also für \\(a,x,b \\in \\mathbb{R}\\) die Gleichung \\[\\begin{equation}\nax = b\n\\end{equation}\\] haben. Um diese Gleichung nach \\(x\\) aufzulösen würde man natürlich beide Seiten der Gleichung mit dem multiplikativem Inversen von \\(a\\) multiplizieren, wobei das multiplikative Inverse von \\(a\\) den Wert bezeichnet, der mit \\(a\\) multipliziert \\(1\\) ergibt. Dieser ist bekanntlich durch \\[\\begin{equation}\na^{-1} = \\frac{1}{a}\n\\end{equation}\\] gegeben. Dann würde gelten \\[\\begin{equation}\nax = b \\Leftrightarrow a^{-1}ax = a^{-1}b \\Leftrightarrow 1 \\cdot x = a^{-1}b \\Leftrightarrow x = \\frac{b}{a}.\n\\end{equation}\\] Ganz konkret etwa \\[\\begin{equation}\n2x = 6 \\Leftrightarrow 2^{-1} 2x = 2^{-1}6 \\Leftrightarrow \\frac{1}{2}2x = \\frac{1}{2}6 \\Leftrightarrow x = 3.\n\\end{equation}\\] Analog zu dem Fall, dass die Matrizen in \\(Ax = b\\) allesamt Skalare sind, möchte man im Fall eines linearen Gleichungssystems beide Seiten der Gleichung mit dem multiplikativen Inversen \\(A^{-1}\\) von \\(A\\) multiplizieren können, sodass eine Gleichung der Form \\[\\begin{equation}\nA^{-1}A = \"1\".\n\\end{equation}\\] resultiert. Dann hätte man nämlich \\[\\begin{equation}\nAx = b \\Leftrightarrow A^{-1}Ax = A^{-1}b \\Leftrightarrow x = A^{-1}b.\n\\end{equation}\\] Diese intuitive Idee des multiplikativen Inversen einer Matrix \\(A\\) wird im Folgenden unter dem Begriff der inversen Matrix formalisiert. Dazu benötigen wir zunächst den Begriff der Einheitsmatrix.\n\nDefinition 9.7 (Einheitsmatrix) Die Matrix \\[\\begin{equation}\nI_n\n:= (a_{ij})_{1\\le i \\le n, 1 \\le j \\le n}  \\in \\mathbb{R}^{n \\times n}\n:=\n\\begin{pmatrix}\n1      & 0      & \\cdots & 0       \\\\\n0      & 1      & \\cdots & 0       \\\\\n\\vdots & \\vdots & \\ddots & \\vdots  \\\\\n0      & 0      & \\cdots & 1       \\\\\n\\end{pmatrix}\n\\end{equation}\\] mit \\(a_{ij} = 1\\) für \\(i = j\\) und \\(a_{ij} = 0\\) für \\(i \\neq j\\) heißt \\(n\\)-dimensionale Einheitsmatrix.\n\nIn R wird \\(I_n\\) mit dem Befehl diag(n) erzeugt. Die Einheitsmatrix ist für die Matrixmultiplikation das Analog zur 1 bei der Multiplikation reeller Zahlen. Das ist die Aussage folgenden Theorems.\n\nTheorem 9.7 (Neutrales Element der Matrixmultiplikation) \\(I_n\\) ist das neutrale Element der Matrixmultiplikation, das heißt es gilt für \\(A \\in \\mathbb{R}^{n \\times m}\\), dass \\[\\begin{equation}\nI_nA = A \\mbox{ und } AI_m = A.\n\\end{equation}\\]\n\n\nBeweis. Es sei \\(B = (b_{ij}) = I_nA \\in \\mathbb{R}^{n\\times m}\\). Dann gilt für alle \\(1 \\le i \\le n\\) und alle \\(1 \\le j \\le n\\) \\[\\begin{equation}\nd_{ij}\n= 0 \\cdot a_{1j}\n+ 0 \\cdot a_{2j}\n+ \\cdots\n+ 0 \\cdot a_{i-1,j}\n+ 1 \\cdot a_{ij}\n+ \\cdots\n+ 0 \\cdot a_{i+1,j}\n+ 0 \\cdot a_{nj}\n= a_{ij}.\n\\end{equation}\\] Analog zeigt man dies für \\(AI_m\\).\n\nMit dem Begriff der Einheitsmatrix können wir jetzt die Begriffe der inversen Matrix und der invertierbaren Matrix definieren:\n\nDefinition 9.8 (Invertierbare Matrix und inverse Matrix) Eine quadratische Matrix \\(A \\in \\mathbb{R}^{n \\times n}\\) heißt invertierbar, wenn es eine quadratische Matrix \\(A^{-1} \\in \\mathbb{R}^{n \\times n}\\) gibt, so dass \\[\\begin{equation}\nA^{-1}A = AA^{-1} = I_n\n\\end{equation}\\] ist. Die Matrix \\(A^{-1}\\) heißt die inverse Matrix von \\(A\\).\n\nMan beachte, dass sich die Begriffe der inversen Matrix und der Invertierbarkeit nur auf quadratische Matrizen beziehen. Insbesondere können quadratische Matrizen invertierbar sein, müssen es aber nicht sein (lineare Gleichungssysteme können also Lösungen haben, müssen es aber nicht). Nicht invertierbare Matrizen nennt man auch singuläre Matrizen, invertierbare Matrizen manchmal auch nicht-singuläre Matrizen. Schließlich beachte man, dass Definition 9.8 lediglich aussagt, was eine inverse Matrix ist, aber nicht wie man sie berechnet.\nBeispiel für eine invertierbare Matrix\nDie Matrix \\[\\begin{equation}\nA := \\begin{pmatrix} 2.0 & 1.0 \\\\ 3.0 & 4.0 \\end{pmatrix}\n\\end{equation}\\] ist invertierbar und ihre inverse Matrix ist gegeben durch \\[\\begin{equation}\nA^{-1} = \\begin{pmatrix} 0.8 & -0.2 \\\\  -0.6 & 0.4 \\end{pmatrix},\n\\end{equation}\\] denn \\[\\begin{equation}\n\\begin{pmatrix} 2.0 &  1.0 \\\\   3.0 & 4.0 \\end{pmatrix}\n\\begin{pmatrix} 0.8 & -0.2 \\\\  -0.6 & 0.4 \\end{pmatrix}\n=\n\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}\n=\n\\begin{pmatrix} 0.8 & -0.2  \\\\  -0.6 & 0.4 \\end{pmatrix}\n\\begin{pmatrix} 2.0 &  1.0  \\\\   3.0 & 4.0 \\end{pmatrix},\n\\end{equation}\\] wovon man sich durch Nachrechnen überzeugt.\nBeispiel für eine nicht-invertierbare Matrix\nDie Matrix \\[\\begin{equation}\nB := \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}\n\\end{equation}\\] ist nicht invertierbar, denn wäre \\(B\\) invertierbar, dann gäbe es \\[\\begin{equation}\n\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}\n\\end{equation}\\] mit \\[\\begin{equation}\n\\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}\n\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}\n=\n\\begin{pmatrix} a & b \\\\ 0 & 0 \\end{pmatrix}\n=\n\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}.\n\\end{equation}\\] Das würde aber bedeuten, dass \\(0 = 1\\) in \\(\\mathbb{R}\\) und das ist ein Widerspruch. Also kann \\(B\\) nicht invertierbar sein.\nZum Berechnen inverser Matrizen\n\\(2 \\times 2\\) bis etwa \\(5 \\times 5\\) Matrizen kann man prinzipiell per Hand invertieren, dazu stellt die Lineare Algebra verschiedene Verfahren bereit. Wir wollen hier auf eine Einführung in die Matrizeninvertierung per Hand verzichten, da in der Anwendung Matrizen standardmäßig numerisch invertiert werden. Die numerische Matrixinversion ist dann auch ein großes Feld der Forschung zur Numerischen Mathematik, die eine Vielzahl von Algorithmen zu diesem Zweck bereitstellt. In R werden Matrizen per default mit der Funktion solve(), in Anlehnung an das Lösen linearer Gleichungssysteme, invertiert. Für das obige Beispiel einer invertierbaren Matrix ergibt sich dabei folgender R Code.\n\n# Definition\nA = matrix(c(2,1,\n             3,4),\n           nrow  = 2,\n           byrow = TRUE)\n\n# Berechnen von A^{-1}\nprint(solve(A))\n\n     [,1] [,2]\n[1,]  0.8 -0.2\n[2,] -0.6  0.4\n\n# Überprüfen der Eigenschaften einer inversen Matrix\nprint(solve(A) %*% A)\n\n              [,1] [,2]\n[1,]  1.000000e+00    0\n[2,] -1.110223e-16    1\n\n# Bei der umgekehrten Berechnung ergebn sich kleine Rundungsfehler\nprint(A %*% solve(A))\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n\n\nNicht-invertierbare Matrizen sind dabei natürlich auch numerisch nicht-invertierbar, wie folgende Fehlermeldung in R bezüglich obigen Beispiels einer nicht-invertierbaren Matrix demonstriert.\n\nB = matrix(c(1,0,\n             0,0),\n           nrow  = 2,\n           byrow = 2)\nsolve(B)\n\nError in solve.default(B): Lapack routine dgesv: system is exactly singular: U[2,2] = 0",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Matrizen</span>"
    ]
  },
  {
    "objectID": "109-Matrizen.html#sec-determinanten",
    "href": "109-Matrizen.html#sec-determinanten",
    "title": "9  Matrizen",
    "section": "9.5 Determinanten",
    "text": "9.5 Determinanten\nDie Determinante ist eine vielseitig einsetzbare Maßzahl einer quadratischen Matrix. Für das Verständnis der Eigenanalyse und der Matrixzerlegung ist der Begriff der Determinante im Kontext des charakteristischen Polynoms grundlegend.\nAllgemein ist eine Determinante eine nichtlineare Abbildung der Form \\[\\begin{equation}\n\\lvert \\cdot \\rvert: \\mathbb{R}^{n \\times n} \\to \\mathbb{R}, A \\mapsto \\lvert A \\rvert,\n\\end{equation}\\] das heißt, eine Determinante ordnet einer quadratischen Matrix \\(A\\) die reelle Zahl \\(\\lvert A \\rvert\\) zu. Die Zahl \\(\\lvert A \\rvert\\) wird dabei rekursiv anhand folgender Definition bestimmt.\n\nDefinition 9.9 (Determinante) Für \\(A = (a_{ij})_{1 \\le i,j \\le n} \\in \\mathbb{R}^{n \\times n}\\) mit \\(n&gt;1\\) sei \\(A_{ij} \\in \\mathbb{R}^{n-1 \\times n-1}\\) die Matrix, die aus \\(A\\) durch Entfernen der \\(i\\)ten Zeile und der \\(j\\)ten Spalte entsteht. Dann heißt die Zahl \\[\\begin{align}\n\\lvert A \\rvert & := a_{11} \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad   \\mbox{ für } n = 1\\\\\n\\lvert A \\rvert & := \\sum_{j = 1}^n a_{1j}(-1)^{1+j} \\det\\left(A_{1j}\\right)               \\mbox{ für } n &gt; 1\n\\end{align}\\] die Determinante von \\(A\\).\n\nDie Definition führt die Bestimmung der Determinante einer quadratischen Matrix also sukzessive durch Streichen von Zeilen und Spalten auf die Determinante einer \\(1 \\times 1\\) Matrix zurück, die durch ihr einziges Element gegeben ist. Für \\[\\begin{equation}\nA :=\n\\begin{pmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9 \\\\\n\\end{pmatrix}\n\\in \\mathbb{R}^{3 \\times 3}\n\\end{equation}\\] ergeben sich dabei zum Beispiel folgende Matrizen der Form \\(A_{ij} \\in \\mathbb{R}^{3-1 \\times 3-1}\\): \\[\\begin{equation}\nA_{11}\n=\n\\begin{pmatrix}\n5 & 6 \\\\\n8 & 9 \\\\\n\\end{pmatrix},\nA_{12}\n=\n\\begin{pmatrix}\n4 & 6 \\\\\n7 & 9 \\\\\n\\end{pmatrix},\nA_{21}\n=\n\\begin{pmatrix}\n2 & 3 \\\\\n8 & 9 \\\\\n\\end{pmatrix},\nA_{22}\n=\n\\begin{pmatrix}\n1 & 3 \\\\\n7 & 9 \\\\\n\\end{pmatrix}.\n\\end{equation}\\]\nFür die Berechnung der Determinanten von zwei- und dreidimensionalen quadratischen Matrizen gibt es direkte, nicht-rekursive Rechenregeln, die in folgendem Theorem festgehalten sind.\n\nTheorem 9.8 (Determinanten von zwei- und dreidimensionalen Matrizen) \\(\\quad\\)\nEs sei \\(A = (a_{ij})_{1 \\le i,j \\le 2} \\in \\mathbb{R}^{2 \\times 2}\\). Dann gilt \\[\\begin{equation}\n\\lvert A \\rvert = a_{11}a_{22} - a_{12}a_{21}.\n\\end{equation}\\] Es sei \\(A = (a_{ij})_{1 \\le i,j \\le 3} \\in \\mathbb{R}^{3 \\times 3}\\). Dann gilt \\[\\begin{equation}\n\\lvert A \\rvert= a_{11}a_{22}a_{33} + a_{12}a_{23}a_{31} + a_{13}a_{21}a_{32} - a_{12}a_{21}a_{33} - a_{11}a_{23}a_{32} - a_{13}a_{22}a_{31}.\n\\end{equation}\\]\n\n\nBeweis. Für \\(A \\in \\mathbb{R}^{2 \\times 2}\\) gilt nach Definition \\[\\begin{align}\n\\begin{split}\n\\lvert A \\rvert\n& = \\sum_{j = 1}^n a_{1j}(-1)^{1+j} |A_{1j}| \\\\\n& = a_{11}(-1)^{1 + 1}|A_{11}| + a_{12}(-1)^{1 + 2}|A_{12}| \\\\\n& = a_{11}|(a_{22})| - a_{12}|(a_{21})| \\\\\n& = a_{11}a_{22} - a_{12}a_{21}. \\\\\n\\end{split}\n\\end{align}\\] Für \\(A \\in \\mathbb{R}^{3 \\times 3}\\) gilt nach Definition und mit der Formel für Determinanten von \\(2 \\times 2\\) Matrizen \\[\\begin{align}\n\\begin{split}\n\\lvert A \\rvert\n& = \\sum_{j = 1}^n a_{1j}(-1)^{1+j} |(A_{1j}| \\\\\n& =   a_{11}(-1)^{1+1} |A_{1j}| + a_{12}(-1)^{1+2} |A_{12}| +  a_{13}(-1)^{1+3}|A_{13}| \\\\\n& =   a_{11}|A_{11}| - a_{12}|A_{12}| + a_{13}|A_{13}| \\\\\n& =   a_{11}\\left\\vert\\begin{pmatrix} a_{22} & a_{23} \\\\ a_{32} & a_{33}\\end{pmatrix}\\right\\vert\n    - a_{12}\\left\\vert\\begin{pmatrix} a_{21} & a_{23} \\\\ a_{31} & a_{33}\\end{pmatrix}\\right\\vert\n    + a_{13}\\left\\vert\\begin{pmatrix} a_{21} & a_{22} \\\\ a_{31} & a_{32}\\end{pmatrix}\\right\\vert \\\\\n& =   a_{11}(a_{22}a_{33} - a_{23}a_{32})\n    - a_{12}(a_{21}a_{33} - a_{23}a_{31})\n    + a_{13}(a_{21}a_{32} - a_{22}a_{31}) \\\\\n& =   a_{11}a_{22}a_{33} - a_{11}a_{23}a_{32}\n    - a_{12}a_{21}a_{33} + a_{12}a_{23}a_{31}\n    + a_{13}a_{21}a_{32} - a_{13}a_{22}a_{31} \\\\\n& =   a_{11}a_{22}a_{33} + a_{12}a_{23}a_{31} + a_{13}a_{21}a_{32}\n    - a_{12}a_{21}a_{33} - a_{11}a_{23}a_{32} - a_{13}a_{22}a_{31}.\n\\end{split}\n\\end{align}\\]\n\nFür die Bestimmung der Determinanten von \\(2 \\times 2\\) und \\(3 \\times 3\\) Matrizen gilt somit die sogennante Sarrusche Merkregel: \\[\\begin{equation*}\n\\mbox{``Summe der Produkte auf den Diagonalen minus Summe der Produkte auf den Gegendiagonalen.''}\n\\end{equation*}\\] Dabei bezieht sich die Merkregeln bei \\(3 \\times 3\\) Matrizen auf das Schema \\[\\begin{equation}\n\\begin{pmatrix}\na_{11} & a_{12} & a_{13} & \\vert & a_{11} & a_{12} \\\\\na_{21} & a_{22} & a_{23} & \\vert & a_{21} & a_{22} \\\\\na_{31} & a_{32} & a_{33} & \\vert & a_{31} & a_{32}\n\\end{pmatrix}.\n\\end{equation}\\]\nBeispiele für Determinanten von \\(2 \\times 2\\) und \\(3 \\times 3\\) Matrizen\nEs seien \\[\\begin{equation}\nA :=\n\\begin{pmatrix}\n2 & 1 \\\\\n3 & 4\n\\end{pmatrix},\nB :=\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 0\n\\end{pmatrix}\n\\mbox{ und }\nC :=\n\\begin{pmatrix}\n2 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 3\n\\end{pmatrix}\n\\end{equation}\\] Dann ergeben sich \\[\\begin{equation}\n\\lvert A \\rvert\n= 2 \\cdot 4 - 1 \\cdot 3 = 8 - 3 = 5\n\\end{equation}\\] und \\[\\begin{equation}\n\\lvert B \\rvert\n= 1 \\cdot 0 - 0 \\cdot 0 = 0 - 0 = 0\n\\end{equation}\\] und \\[\\begin{equation}\n\\lvert C \\rvert\n= 2 \\cdot 1 \\cdot 3  + 0 \\cdot 0 \\cdot 0 + 0 \\cdot 0 \\cdot 0 - 0 \\cdot 0 \\cdot 3 - 0 \\cdot 0 \\cdot 0  - 0 \\cdot 1 \\cdot 0\n= 2 \\cdot 1 \\cdot 3\n= 6.\n\\end{equation}\\]\nIn R rechnet man dies mithilfe der det() Funktion wie folgt nach.\n\n# Matrixdefinition und Determinantenberechnung\nA = matrix(c(2,1,\n             3,4),\n           nrow = 2,\n           byrow = TRUE)\ndet(A)\n\n[1] 5\n\n# Matrixdefinition und Determinantenberechnung\nB = matrix(c(1,0,\n             0,0),\n           nrow = 2,\n           byrow = TRUE)\ndet(B)\n\n[1] 0\n\n# Matrixdefinition und Determinantenberechnung\nC = matrix(c(2,0,0,\n             0,1,0,\n             0,0,3),\n           nrow = 3,\n           byrow = TRUE)\ndet(C)\n\n[1] 6\n\n\nFür Determinanten bestehen zahlreiche Rechenregeln im Zusammenspiel mit Matrixmultiplikation und Matrixinversion. Ohne Beweis stellen wir diese in folgendem Theorem zusammen.\n\nTheorem 9.9 (Rechenregeln für Determinanten) \\(\\quad\\)\n(Determinantenmultiplikationssatz). Für \\(A,B \\in \\mathbb{R}^{n \\times n}\\) gilt \\[\\begin{equation}\n|AB| = \\lvert A \\rvert\\lvert B \\rvert.\n\\end{equation}\\] (Transposition). Für \\(A \\in \\mathbb{R}^{n \\times n}\\) gilt \\[\\begin{equation}\n\\lvert A \\rvert = \\left\\vert A^T \\right\\vert.\n\\end{equation}\\] (Inversion). Für eine invertierbare Matrix \\(A \\in \\mathbb{R}^{n \\times n}\\) gilt \\[\\begin{equation}\n\\left\\vert A^{-1}\\right\\vert = \\frac{1}{\\lvert A \\rvert}.\n\\end{equation}\\] (Dreiecksmatrizen). Für Matrizen \\(A = (a_{ij})_{1 \\le i,j\\le n} \\in \\mathbb{R}^{n \\times n}\\) mit \\(a_{ij} = 0\\) für \\(i &gt; j\\) oder \\(a_{ij} = 0\\) \\(j &gt; i\\) gilt \\[\\begin{equation}\n\\lvert A \\rvert = \\prod_{i=1}^n a_{ii}.\n\\end{equation}\\]\n\nFolgendes sehr tiefgehendes Theorem, welches wir nicht vollständig beweisen wollen, gibt eine Möglichkeit an, anhand der Determinante einer quadratischen Matrix zu bestimmen, ob sie invertierbar ist.\n\nTheorem 9.10 \\(A \\in \\mathbb{R}^{n \\times n}\\) ist dann und nur dann invertierbar, wenn gilt, dass \\(\\lvert A \\rvert \\neq 0\\). Es gilt also \\[\\begin{equation}\nA \\mbox{ ist invertierbar} \\Leftrightarrow \\lvert A \\rvert \\neq 0\n\\mbox{ und }\nA \\mbox{ ist nicht invertierbar} \\Leftrightarrow \\lvert A \\rvert = 0.\n\\end{equation}\\]\n\n\nBeweis. Wir deuten einen Beweis lediglich an und zeigen, dass aus der Invertierbarkeit von \\(A\\) folgt, dass \\(\\lvert A \\rvert\\) nicht gleich Null sein kann. Nehmen wir also an, dass \\(A\\) invertierbar ist. Dann gibt es eine Matrix \\(B\\) mit \\(AB = I_n\\) und mit dem Determinantenmultiplikationssatz folgt \\[\\begin{equation}\n\\lvert AB \\rvert = \\lvert A \\rvert\\lvert B \\rvert= |I_n| = 1.\n\\end{equation}\\] Also kann \\(\\lvert A \\rvert = 0\\) nicht gelten, denn sonst wäre \\(0 = 1\\).\n\nVisuelle Intuition\nDer abstrakte Begriff der Determinante einer quadratischen Matrix kann mithilfe des Vektorraumbegriffs etwas veranschaulicht werden. Dazu seien \\(a_1,...,a_n \\in \\mathbb{R}^n\\) die Spalten von \\(A \\in \\mathbb{R}^{n \\times n}\\). Dann gilt (wie wir nicht beweisen wollen), dass \\(\\lvert A \\rvert\\) dem signierten Volumen des von \\(a_1,...,a_n\\in \\mathbb{R}^n\\) aufgespannten Parallelotops entspricht. Um dies visuell zu veranschaulichen betrachten wir die Matrizen \\[\\begin{equation}\nA_1 =\n\\begin{pmatrix}\n3 & 1 \\\\\n1 & 2\n\\end{pmatrix},\nA_2 =\n\\begin{pmatrix}\n2 & 0 \\\\\n0 & 2\n\\end{pmatrix},\nA_3 =\n\\begin{pmatrix}\n2 & 2 \\\\\n2 & 2\n\\end{pmatrix}\n\\end{equation}\\] mit den jeweiligen Determinanten \\[\\begin{equation}\n\\lvert A_1 \\rvert = 3\\cdot 2 - 1 \\cdot 1 = 5, \\quad\n\\lvert A_2 \\rvert= 2\\cdot 2 - 0 \\cdot 0 = 4, \\quad\n\\lvert A_3 \\rvert = 2\\cdot 2 - 2 \\cdot 2 = 0.\n\\end{equation}\\]\nAbbildung 9.1 visualisiert die entsprechende Intuition.\n\n\n\n\n\n\nAbbildung 9.1: Determinanten als Parallelotopvolumina.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Matrizen</span>"
    ]
  },
  {
    "objectID": "109-Matrizen.html#sec-spezielle-matrizen",
    "href": "109-Matrizen.html#sec-spezielle-matrizen",
    "title": "9  Matrizen",
    "section": "9.6 Spezielle Matrizen",
    "text": "9.6 Spezielle Matrizen\nIn dieser Sektion stellen wir einige häufig auftretende Typen von Matrizen und ihre Eigenschaften zusammen. Zum Beweis der allermeisten Eigenschaften verweisen wir dabei auf die weiterführende Literatur.\n\n9.6.1 Einheitsmatrizen\nDie Einheitsmatrix und die Einheitsvektoren haben wir bereits kennengelernt. Wir fassen sie hier noch einmal in einer gemeinsamen Definition zusammen.\n\nDefinition 9.10 (Einheitsmatrix und Einheitsvektoren) Wir bezeichnen die Einheitsmatrix mit \\[\\begin{equation}\nI_{n} := (i_{jk})_{1 \\le j \\le n, 1 \\le k \\le n} \\in \\mathbb{R}^{n \\times n} \\mbox{ mit } i_{jk} = 1 \\mbox{ für } j = k \\mbox{ und } i_{jk} = 0 \\mbox{ für } j \\neq k.\n\\end{equation}\\] Wir bezeichnen die Einheitsvektoren \\(e_i, i = 1,...,n\\) mit \\[\\begin{equation}\ne_{i} := (e_{{i}_j})_{1 \\le j \\le n} \\in \\mathbb{R}^{n} \\mbox{ mit } e_{{i}_j} = 1 \\mbox{ für } i = j \\mbox{ und } e_{{i}_j} = 0 \\mbox{ für } i \\neq j.\n\\end{equation}\\]\n\nDie Einheitsmatrix \\(I_n\\) besteht nur aus Nullen und Diagonalelementen gleich Eins, die Einheitsvektoren bestehen nur aus Nullen und einer Eins in der jeweils indizierten Komponente. Es gilt \\[\\begin{equation}\nI_n = \\begin{pmatrix} e_1 & \\cdots & e_n \\end{pmatrix}\n\\in \\mathbb{R}^{n \\times n}\n\\end{equation}\\] Für \\(n = 3\\) gilt also zum Beispiel \\[\\begin{equation}\nI_3 =\n\\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}\n\\mbox{ und }\ne_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix},\ne_2 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix},\ne_3 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}.\n\\end{equation}\\] Weiterhin gelten für die Einheitsvektoren bekanntlich für \\(1 \\le i,j \\le n\\) \\[\\begin{equation}\ne^T_ie_j = 0 \\mbox{ für } i \\neq j,  e^T_ie_i = 1 \\mbox{ und } e^T_iv = v^Te_i = v_i \\mbox{ für } v \\in \\mathbb{R}^n.\n\\end{equation}\\]\n\n\n9.6.2 Einsmatrizen und Nullmatrizen\n\nDefinition 9.11 (Nullmatrizen, Nullvektoren, Einsmatrizen, Einsvektoren) Wir bezeichnen Nullmatrizen und Nullvektoren mit \\[\\begin{equation}\n0_{nm} := (0)_{1 \\le i \\le m, 1 \\le j \\le n} \\in \\mathbb{R}^{n \\times m}\n\\mbox{ und }\n0_{n} := (0)_{1 \\le i \\le n} \\in \\mathbb{R}^{n}.\n\\end{equation}\\] Wir bezeichnen Einsmatrizen und Einsvektoren mit \\[\\begin{equation}\n1_{nm} := (1)_{1 \\le i \\le n, 1 \\le j \\le m} \\in \\mathbb{R}^{n \\times m}\n\\mbox{ und }\n1_n := (1)_{1 \\le i \\le n} \\in \\mathbb{R}^n.\n\\end{equation}\\]\n\n\\(0_{nm}\\) und \\(0_{n}\\) bestehen also nur aus Nullen und \\(1_{nm}\\) und \\(1_{n}\\) bestehen nur aus Einsen. Es gilt also beispielsweise \\[\\begin{equation}\n0_{32} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\\\ 0 & 0 \\end{pmatrix},\n0_{3}  = \\begin{pmatrix} 0  \\\\ 0  \\\\ 0  \\end{pmatrix},\n1_{32} = \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\\\ 1 & 1 \\end{pmatrix} \\mbox{ und }\n1_{3}  = \\begin{pmatrix} 1  \\\\ 1  \\\\ 1  \\end{pmatrix}.\n\\end{equation}\\] Weiterhin gelten zum Beispiel \\[\\begin{equation}\n0_n0_n^T = 0_{nn} \\mbox{ und } 1_n1_n^T = 1_{nn},\n\\end{equation}\\] wovon man sich durch Nachrechnen überzeugt.\n\n\n9.6.3 Diagonalmatrizen\n\nDefinition 9.12 (Diagonalmatrix) Eine Matrix \\(D \\in \\mathbb{R}^{n \\times m}\\) heißt Diagonalmatrix, wenn \\(d_{ij} = 0\\) für \\(1 \\le i \\le n, 1 \\le j \\le m\\) mit \\(i \\neq j\\).\n\nEine quadratische Diagonalmatrix \\(D\\in \\mathbb{R}^{n \\times n}\\) mit den Diagonalelementen \\(d_1,...,d_n \\in \\mathbb{R}\\) schreibt man auch als \\[\\begin{equation}\nD = \\mbox{diag}(d_1,...,d_n).\n\\end{equation}\\] Zum Beispiel gelten \\[\\begin{equation}\nD\n:= \\mbox{diag}(1,2,3)\n= \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 2 & 0 \\\\\n0 & 0 & 3\n\\end{pmatrix}\n\\end{equation}\\] und für \\(\\sigma^2 \\in \\mathbb{R}\\) \\[\\begin{equation}\n\\Sigma\n= \\mbox{diag}(\\sigma^2,\\sigma^2,\\sigma^2)\n= \\begin{pmatrix}\n\\sigma^2 & 0 & 0 \\\\\n0 & \\sigma^2 & 0 \\\\\n0 & 0 & \\sigma^2\n\\end{pmatrix}\n= \\sigma^2I_3.\n\\end{equation}\\]\nIn folgendem Theorem stellen wir einige wichtige Eigenschaften von quadratischen Diagonalmatrizen zusammen.\n\nTheorem 9.11 (Eigenschaften quadratischer Diagonalmatrizen) \\(\\quad\\)\n(Determinante.) \\(D := \\mbox{diag}(d_1,...,d_n) \\in \\mathbb{R}^{n \\times n}\\) sei eine quadratische Diagonalmatrix. Dann gilt \\[\\begin{equation}\n|D| = \\prod_{i=1}^n d_i.\n\\end{equation}\\]\n\n\n\n9.6.4 Symmetrische Matrizen\nSymmetrische Matrizen sind quadratische Matrizen, die bei Transposition unverändert bleiben:\n\nDefinition 9.13 Eine Matrix \\(S \\in \\mathbb{R}^{n \\times n}\\) heißt symmetrisch, wenn \\(S^T = S\\).\n\nEin Beispiel für eine symmetrische Matrix ist \\[\\begin{equation}\nS :=\n\\begin{pmatrix}\n1 & 2 & 3 \\\\\n2 & 1 & 2 \\\\\n3 & 2 & 1\n\\end{pmatrix}.\n\\end{equation}\\]\nIn folgendem Theorem stellen wir einige wichtige Eigenschaften symmetrischer Matrizen zusammen.\n\nTheorem 9.12 (Eigenschaften symmetrischer Matrizen) \\(\\quad\\)\n(Summation.) \\(S_1 \\in \\mathbb{R}^{n \\times n}\\) und \\(S_2 \\in \\mathbb{R}^{n \\times n}\\) seien symmetrische Matrizen. Dann gilt \\[\\begin{equation}\nS_1 + S_2 = (S_1 + S_2)^T.\n\\end{equation}\\] (Inverse.) \\(S\\) sei eine invertierbare symmetrische Matrix und \\(S^{-1}\\) ihre Inverse. Dann ist auch \\(S^{-1}\\) eine symmetrische Matrix, das heißt es gilt \\[\\begin{equation}\n\\left(S^{-1}\\right)^T = S^{-1}.\n\\end{equation}\\]\n\n\n\n9.6.5 Orthogonale Matrizen\n\nDefinition 9.14 Eine Matrix \\(Q \\in \\mathbb{R}^{n \\times n}\\) heißt orthogonal, wenn \\(Q^TQ = I_n\\).\n\nDie Spalten einer orthogonalen Matrix sind also paarweise orthogonal, es gilt für \\[\\begin{equation}\nQ = \\begin{pmatrix} q_1 & \\cdots & q_n \\end{pmatrix} \\mbox{ mit } q_i \\in \\mathbb{R}^n \\mbox{ für } 1 \\le i \\le n,\n\\end{equation}\\] dass \\[\\begin{equation}\nq_i^Tq_j = 0 \\mbox{ für } i \\neq j \\mbox{ und }  q_i^Tq_j = 1 \\mbox{ für } i = j \\mbox{ mit } 1 \\le i,j \\le n.\n\\end{equation}\\]\n\nTheorem 9.13 (Eigenschaften orthogonaler Matrizen) \\(Q \\in \\mathbb{R}^{n \\times n}\\) sei eine orthogonale Matrix. Dann gelten folgende Eigenschaften von \\(Q\\). \\(\\quad\\)\n(Inverse.) Die Inverse von \\(Q\\) ist \\(Q^T\\), es gilt\n\\[\\begin{equation}\nQ^{-1} = Q^T.\n\\end{equation}\\] (Transposition) Die Zeilen von \\(Q\\) sind orthonormal, es gilt \\[\\begin{equation}\nQQ^T = I_n\n\\end{equation}\\]\n\n\nBeweis. (Inverse) Unter der Annahme, dass \\(Q^{-1}\\) existiert, gilt \\[\\begin{equation}\nQ^TQ = I_n \\Leftrightarrow Q^TQQ^{-1} = I_nQ^{-1} \\Leftrightarrow  Q^{-1} = Q^T.\n\\end{equation}\\] (Transposition) Es gilt \\[\\begin{equation}\nQ^TQ = I_n \\Leftrightarrow QQ^TQ = QI_n \\Leftrightarrow  QQ^TQQ^T = QQ^T \\Leftrightarrow  QQ^T = I_n.\n\\end{equation}\\]\n\n\n\n9.6.6 Positiv-definite Matrizen\nPositiv-definite Matrizen sind für die probabilistiche Modellbildung unter Verwendung multivariater Normalverteilungen zentral.\n\nDefinition 9.15 Eine quadratische Matrix \\(C \\in \\mathbb{R}^{n \\times n}\\) heißt positiv-definit (\\(\\mbox{p.d.}\\)), wenn\n\n\\(C\\) eine symmetrische Matrix ist und\nfür alle \\(x \\in \\mathbb{R}^n, x \\neq 0_n\\) gilt, dass \\(x^TCx &gt; 0\\) ist.\n\n\nIn folgendem Theorem stellen wir einige wichtige Eigenschaften positiv-definiter Matrizen zusammen.\n\nTheorem 9.14 (Eigenschaften positiv-definiter Matrizen) \\(\\quad\\)\n(Inverse.) \\(C \\in \\mathbb{R}^{n \\times n}\\) sei eine positiv-definite Matrix. Dann gilt, dass \\(C^{-1}\\) existiert und ebenfalls positiv-definit ist.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Matrizen</span>"
    ]
  },
  {
    "objectID": "109-Matrizen.html#literaturhinweise",
    "href": "109-Matrizen.html#literaturhinweise",
    "title": "9  Matrizen",
    "section": "9.7 Literaturhinweise",
    "text": "9.7 Literaturhinweise\nSearle (1982) gibt eine umfassende Einführung in die Matrixtheorie vor dem Hintergrund der probabilistischen Datenanalyse, Strang (2009) gibt ein umfassende Einführung in die Matrixtheorie im Kontext der linearen Algebra. In ihrer modernen Inkarnation tauchen Matrizen als algebraische Objekte wohl zunächst in den Arbeiten von Arthur Caley (1821-1895) auf, siehe zum Beispiel Caley (1858).",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Matrizen</span>"
    ]
  },
  {
    "objectID": "109-Matrizen.html#selbstkontrollfragen",
    "href": "109-Matrizen.html#selbstkontrollfragen",
    "title": "9  Matrizen",
    "section": "9.8 Selbstkontrollfragen",
    "text": "9.8 Selbstkontrollfragen\n\nGeben Sie die Definition einer Matrix wieder.\nNennen Sie sechs Matrixoperationen.\nGeben Sie die Definitionen der Matrixaddition und der Matrixsubtraktion wieder.\nGeben Sie die Definition der Skalarmultiplikation für Matrizen wieder.\nGeben Sie die Definition der Matrixtransposition wieder.\nEs seien \\[\\begin{equation}\nA :=\n\\begin{pmatrix}\n1 & 2 \\\\\n2 & 1\n\\end{pmatrix},\nB :=\n\\begin{pmatrix}\n3 & 0 \\\\\n1 & 2\n\\end{pmatrix}\n\\mbox{ und }\nc := 2.\n\\end{equation}\\] Berechnen Sie \\[\\begin{equation}\nD := c\\left(A - B^T\\right)\n\\mbox{ und }\nE := \\left(cA\\right)^T + B.\n\\end{equation}\\]\nGeben Sie die Definition der Matrixmultiplikation wieder.\nEs seien \\(A \\in \\mathbb{R}^{3 \\times 2}, B \\in \\mathbb{R}^{2\\times 4}\\) und \\(C \\in \\mathbb{R}^{3 \\times 4}\\). Prüfen Sie, ob folgende Matrixprodukte definiert sind, und wenn ja, geben Sie die Größe der resultierenden Matrix an: \\[\\begin{equation}\nABC, ABC^T, A^TCB^T, BAC.\n\\end{equation}\\]\nEs seien \\[\\begin{equation}\nA :=\n\\begin{pmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n3 & 2 & 0\n\\end{pmatrix}\nB :=\n\\begin{pmatrix}\n1 & 2 & 2 \\\\\n1 & 3 & 1 \\\\\n2 & 0 & 0\n\\end{pmatrix}\n\\mbox{ und }\nC :=\n\\begin{pmatrix}\n1 \\\\ 3 \\\\ 2\n\\end{pmatrix}.\n\\end{equation}\\] Berechnen Sie die Matrixprodukte \\[\\begin{equation}\nAB,\nB^TA^T,\n\\left(B^TA^T\\right)^T,\nAC.\n\\end{equation}\\]\nDefinieren Sie die Begriff der inversen Matrix und der Invertierbarkeit einer Matrix.\nGeben Sie die Formel für die Determinante von \\(A := (A_{ij})_{1 \\le i,j \\le 2} \\in \\mathbb{R}^2\\) wieder.\nGeben Sie die Formel für die Determinante von \\(A := (A_{ij})_{1 \\le i,j \\le 3} \\in \\mathbb{R}^3\\) wieder.\nBerechnen Sie die Determinanten von \\[\\begin{equation}\nA := \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}\nB := \\begin{pmatrix} 3 & 2 & 1 \\\\ 2 & 3 & 2 \\\\ 1 & 2 & 3 \\end{pmatrix} \\mbox{ und }\nC := \\mbox{diag}(1,2,3).\n\\end{equation}\\]\nGeben Sie die Definitionen von Einheitsmatrix und Einheitsvektoren wieder.\nGeben Sie die Definitionen von Nullmatrizen und Einsmatrizen wieder.\nGeben Sie die Definition einer symmetrischen Matrix wieder.\nGeben Sie die Definition einer Diagonalmatrix wieder.\nGeben Sie die Definition einer positiv-definiten Matrix wieder.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Matrizen</span>"
    ]
  },
  {
    "objectID": "109-Matrizen.html#sec-eigenanalyse",
    "href": "109-Matrizen.html#sec-eigenanalyse",
    "title": "9  Matrizen",
    "section": "9.9 Eigenanalyse",
    "text": "9.9 Eigenanalyse\nMit der Eigenanalyse einer quadratischen Matrix, der Orthonormalzerlegung einer symmetrischen Matrix und der Singulärwertzerlegung einer beliebigen Matrix behandeln wir in diesem Abschnitt drei eng zusammenhängende Konzepte der Matrixtheorie, die in vielen Gebieten der datenanalytischen Anwendung zentrale Rollen spielen. Allerdings erschließt sich die Bedeutung dieser Konzepte dann vor allem im jeweiligen Anwendungskontext, so dass dieser Abschnitt notwendigerweise etwas abstrakt anmuten mag.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Matrizen</span>"
    ]
  },
  {
    "objectID": "109-Matrizen.html#eigenvektoren-und-eigenwerte",
    "href": "109-Matrizen.html#eigenvektoren-und-eigenwerte",
    "title": "9  Matrizen",
    "section": "9.10 Eigenvektoren und Eigenwerte",
    "text": "9.10 Eigenvektoren und Eigenwerte\nUnter der Eigenanalyse einer quadratischen Matrix versteht man das bestimmen ihrer Eigenvektoren und Eigenwerte. Diese sind für eine quadratische Matrix wie folgt definiert.\n\nDefinition 9.16 (Eigenvektor und Eigenwert) \\(A \\in \\mathbb{R}^{m \\times m}\\) sei eine quadratische Matrix. Dann heißt jeder vom Nullvektor \\(0_m\\) verschiedene Vektor \\(v \\in \\mathbb{R}^m\\), für den mit einem Skalar \\(\\lambda \\in \\mathbb{R}\\) gilt, dass \\[\\begin{equation}\nAv = \\lambda v\n\\end{equation}\\] ist, ein Eigenvektor von \\(A\\) und \\(\\lambda\\) heißt dann ein Eigenwert von \\(A\\).\n\nNach Definition hat also jeder Eigenvektor einen zugehörigen Eigenwert, allerdings können die Eigenwerte verschiedener Eigenvektoren durchaus identisch sein. Intuitiv bedeutet die Definition von Eigenvektor und Eigenwert, dass ein Eigenvektor einer Matrix durch Multiplikation mit eben dieser Matrix in seiner Länge, nicht aber in seiner Richtung, verändert wird. Der zugehörige Eigenwert des Eigenvektors entspricht dem Faktor der Längenänderung. Allerdings ist die Zuordnung von Eigenvektoren und Eigenwerten nicht eindeutig, wie folgendes Theorem zeigt.\n\nTheorem 9.15 (Multiplikativität von Eigenvektoren) \\(A \\in \\mathbb{R}^{m \\times m}\\) sei eine quadratische Matrix. Wenn \\(v \\in \\mathbb{R}^m\\) Eigenvektor von \\(A\\) mit Eigenwert \\(\\lambda \\in \\mathbb{R}\\) ist, dann ist für \\(c \\in \\mathbb{R}\\) auch \\(cv \\in \\mathbb{R}^m\\) Eigenvektor von \\(A\\) und zwar wiederum mit Eigenwert \\(\\lambda \\in \\mathbb{R}\\).\n\n\nBeweis. Es gilt \\[\\begin{equation}\nAv = \\lambda v   \\Leftrightarrow\ncAv = c\\lambda v \\Leftrightarrow\nA(cv) = \\lambda(cv).\n\\end{equation}\\] Also ist \\(cv\\) ein Eigenvektor von \\(A\\) mit Eigenwert \\(\\lambda\\).\n\nUm nun die Uneindeutigkeit in der Definition des zu einem Eigenwert zugeordneten Eigenvektors aufzulösen, nutzen wir die Konvention, nur diejenigen Vektoren also Eigenvektoren zu einem Eigenwert \\(\\lambda\\) zu betrachten, die die Länge 1 haben, für die also gilt, dass \\[\\begin{equation}\n\\Vert v \\Vert = 1.\n\\end{equation}\\] Sollten wir also einen Eigenvektor \\(v\\) zu einem Eigenwert \\(\\lambda\\) einer Matrix \\(A\\) finden, der nicht von der Länge 1 ist, so können wir ihn immer mit \\(\\Vert v \\Vert^{-1}\\) multiplizieren. Der resultierende Vektor \\(v' = v/\\Vert v \\Vert\\) hat dann die Länge 1 und ist nach Theorem 9.15 ebenso ein Eigenvektor von \\(A\\) zum Eigenwert \\(\\lambda\\). Bevor wir uns der Bestimmung von Eigenwerten und Eigenvektoren widmen, wollen wir die Konzepte von Eigenwert und Eigenvektor für den Fall einer \\(2 \\times 2\\) Matrix an einem Beispiel veranschaulichen\n\nBeispiel\nEs sei \\[\\begin{equation}\nA :=\n\\begin{pmatrix}\n2 & 1 \\\\\n1 & 2\n\\end{pmatrix}\n\\end{equation}\\] Dann ist der Vektor der Länge 1 \\[\\begin{equation}\nv :=\n\\frac{1}{\\sqrt{2}}\n\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n\\end{equation}\\] ein Eigenvektor von \\(A\\) zum Eigenwert \\(\\lambda = 3\\), da gilt, dass \\[\\begin{align}\n\\begin{split}\nAv\n& =\n\\begin{pmatrix}\n2 & 1 \\\\\n1 & 2\n\\end{pmatrix}\n\\left(\n\\frac{1}{\\sqrt{2}}\n\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n\\right)\n\\\\\n& =\n\\frac{1}{\\sqrt{2}}\n\\begin{pmatrix}\n2 & 1 \\\\\n1 & 2\n\\end{pmatrix}\n\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n\\\\\n& =\n\\frac{1}{\\sqrt{2}}\n\\begin{pmatrix} 3 \\\\ 3 \\end{pmatrix}\n\\\\\n& =\n\\frac{1}{\\sqrt{2}}\n3\n\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n\\\\\n& =\n3\n\\left(\n\\frac{1}{\\sqrt{2}}\n\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n\\right)\n\\\\\n& =\n\\lambda v.\n\\end{split}\n\\end{align}\\] Inspektion von Abbildung 9.2 zeigt dementsprechend, dass für die hier definierte Matrix \\(A\\) die Vektoren \\(v\\) und \\(Av\\) in die gleiche Richtung zeigen, dass aber \\(Av\\) um den Faktor \\(\\lambda\\) länger ist als \\(v\\).\nDer Vektor \\[\\begin{equation}\nw := \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n\\end{equation}\\] dagegen hat zwar die Länge 1, ist aber im Gegensatz zu \\(v\\) kein Eigenvektor von \\(A\\), da es im Falle von \\[\\begin{equation}\nAw =\n\\begin{pmatrix}\n2 & 1 \\\\\n1 & 2\n\\end{pmatrix}\n\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n=\n\\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}\n\\end{equation}\\] keinen Skalar \\(\\lambda\\) geben kann, der mit Null, dem zweiten Eintrag von \\(w\\), multipliziert einen Wert ungleich Null ergeben kann. Inspektion von Abbildung 9.2 zeigt dementsprechend, dass der aus der Multiplikation von \\(w\\) mit \\(A\\) resultierende Vektor in eine andere Richtung zeigt als \\(w\\).\n\n\n\n\n\n\nAbbildung 9.2: Eigenvektor einer \\(2 \\times 2\\) Matrix. Für die Matrix \\[\\begin{equation}\n\\begin{pmatrix}\n2 & 1 \\\\\n1 & 2\n\\end{pmatrix}\n\\end{equation}\\] ist \\(v\\) ein Eigenvektor, \\(w\\) jedoch nicht\n\n\n\n\n\nBestimmung von Eigenwerten und Eigenvektoren\nFolgendes Theorem besagt, wie die Eigenwerte und Eigenvektoren einer quadratischen Matrix berechnet werden können.\n\nTheorem 9.16 (Bestimmung von Eigenwerten und Eigenvektoren) \\(A \\in \\mathbb{R}^{m \\times m}\\) sei eine quadratische Matrix. Dann ergeben sich die Eigenwerte von \\(A\\) als die Nullstellen des \\[\\begin{equation}\n\\chi_A(\\lambda) := |A - \\lambda I_m|\n\\end{equation}\\] von \\(A\\). Weiterhin seien \\(\\lambda_i^*, i = 1,2,...\\) die auf diese Weise bestimmten Eigenwerte von \\(A\\). Die entsprechenden Eigenvektoren \\(v_i, i = 1,2,...\\) von \\(A\\) können dann durch Lösen der linearen Gleichungssysteme \\[\\begin{equation}\n(A - \\lambda_i^* I_m)v_i = 0_m \\mbox{ für } i = 1,2,...\n\\end{equation}\\] bestimmt werden.\n\n\nBeweis. (1) Bestimmen von Eigenwerten\nWir halten zunächst fest, dass mit der Definition von Eigenvektoren und Eigenwerten gilt, dass \\[\\begin{equation}\nAv = \\lambda v\n\\Leftrightarrow Av - \\lambda v = 0_m\n\\Leftrightarrow (A - \\lambda I_m)v = 0_m.\n\\end{equation}\\] Für den Eigenwert \\(\\lambda\\) wird der Eigenvektor \\(v\\) also durch Multiplikation mit \\((A - \\lambda I_m)\\) auf den Nullvektor \\(0_m\\) abgebildet. Weil aber per Definition \\(v \\neq 0_m\\) gilt, ist die Matrix \\((A - \\lambda I_m)\\) somit nicht invertierbar: sowohl der Nullvektor als auch \\(v\\) werden durch \\(A\\) auf \\(0_m\\) abgebildet, die Abbildung \\[\\begin{equation}\nf : \\mathbb{R}^m \\to \\mathbb{R}^m, x \\mapsto (A - \\lambda I_m)x\n\\end{equation}\\] ist also nicht bijektiv, und \\((A - \\lambda I_m)^{-1}\\) kann nicht existieren. Die Tatsache, dass \\((A - \\lambda I_m)\\) nicht invertierbar ist, ist aber äquivalent dazu, dass die Determinante von \\((A -\\lambda I_m)\\) gleich Null ist. Also ist \\[\\begin{equation}\n\\chi_A(\\lambda) = |A - \\lambda I_m| = 0\n\\end{equation}\\] eine notwendige und hinreichende Bedingung dafür, dass \\(\\lambda\\) ein Eigenwert von \\(A\\) ist.\n(2) Bestimmen von Eigenvektoren\nEs sei \\(\\lambda_i^*\\) ein Eigenwert von \\(A\\). Dann gilt mit den obigen Überlegungen, dass Auflösen von \\[\\begin{equation}\n(A - \\lambda_i^* I_m)v_i^* = 0_m\n\\end{equation}\\] nach \\(v_i^*\\) einen Eigenvektor zum Eigenwert \\(\\lambda^*\\) ergibt.\n\nAllgemein müssen zur Bestimmung von Eigenwerten und Eigenvektoren also Polynomnullstellen bestimmt und lineare Gleichungssysteme gelöst werden. Dies kann für kleine Matrizen mit \\(m \\le 4\\) durchaus manuell geschehen. Die in der Anwendung auftretetenden Matrizen sind jedoch meist weitaus größer, so dass zur Eigenananalyse numerische Verfahren der Nullstellenbestimmung und des Lösens linearer Gleichungssysteme eingesetzt werden, die zum Beispiel in Funktionen wie R’s eigen(), SciPy’s linalg.eig() oder Julia’s eigvals() und eigvecs() genutzt werden. Für Details zu diesen Verfahren verweisen wir auf die weiterführende Literatur, zum Beispiel Burden et al. (2016) und Richter & Wick (2017). Wir wollen Theorem 9.16 hier lediglich anhand eines Beispiels illustrieren.\n\n\nBeispiel\nDazu sei wiederum \\[\\begin{equation}\nA :=\n\\begin{pmatrix*}[r]\n2 & 1 \\\\\n1 & 2\\end{pmatrix*}\n\\end{equation}\\] Wir wollen zunächst die Eigenwerte von \\(A\\) berechnen. Nach Theorem 9.16 sind dies die Nullstellen des charakteristischen Polynoms von \\(A\\). Wir berechnen also zunächst das charakteristische Polynom von \\(A\\) durch \\[\\begin{equation}\n\\chi_A(\\lambda)\n=\n\\left\\vert\n\\begin{pmatrix*}[r]\n2 & 1 \\\\\n1 & 2\\end{pmatrix*}\n-\n\\begin{pmatrix*}[r]\n\\lambda & 0 \\\\\n0       & \\lambda\n\\end{pmatrix*}\n\\right\\vert\n=\n\\left\\vert\n\\begin{pmatrix*}[r]\n2 - \\lambda & 1 \\\\\n1 & 2 - \\lambda\n\\end{pmatrix*}\n\\right\\vert\n= (2 - \\lambda)^2 - 1.\n\\end{equation}\\] Mithilfe der pq-Formel zur Lösung quadratischer Gleichungen findet man dann \\[\\begin{equation}\n(2 - \\lambda^*_{1/2})^2 - 1 = 0 \\Leftrightarrow \\lambda_1^* = 3 \\mbox{ oder } \\lambda_2^* = 1.\n\\end{equation}\\] Die Eigenwerte von \\(A\\) sind also \\(\\lambda_1 = 3\\) und \\(\\lambda_2 = 1\\). Die zugehörigen Eigenvektoren ergeben sich dann für \\(i = 1,2\\) durch Lösen des linearen Gleichungssystems \\[\\begin{equation}\n(A - \\lambda_i I_2)v_i = 0_2.\n\\end{equation}\\] Speziell ergibt sich hier, dass für \\(\\lambda_1 = 3\\) aus \\[\\begin{equation}\n(A - 3I_2)v_1 = 0_2\n\\Leftrightarrow\n\\begin{pmatrix*}[r]\n-1 & 1 \\\\\n1 & -1\n\\end{pmatrix*}\n\\begin{pmatrix*}[r]\nv_{1_1} \\\\\nv_{1_2}\n\\end{pmatrix*}\n=\n\\begin{pmatrix*}[r]\n0 \\\\\n0\n\\end{pmatrix*}\n\\end{equation}\\] folgt, dass \\[\\begin{equation}\nv_1 =\n\\frac{1}{\\sqrt{2}}\n\\begin{pmatrix*}[r]\n1 \\\\\n1\n\\end{pmatrix*}\n\\end{equation}\\] ein Eigenvektor zum Eigenwert \\(\\lambda_1\\) ist und dass für \\(\\lambda_2 = 1\\) aus \\[\\begin{equation}\n(A - 1I_2)v_2 = 0_2\n\\Leftrightarrow\n\\begin{pmatrix*}[r]\n1 & 1 \\\\\n1 & 1\n\\end{pmatrix*}\n\\begin{pmatrix*}[r]\nv_{2_1} \\\\\nv_{2_2}\n\\end{pmatrix*}\n=\n\\begin{pmatrix*}[r]\n0 \\\\\n0\n\\end{pmatrix*}\n\\end{equation}\\] folgt, dass \\[\\begin{equation}\nv_2 =\n\\frac{1}{\\sqrt{2}}\n\\begin{pmatrix*}[r]\n-1 \\\\\n1\n\\end{pmatrix*}\n\\end{equation}\\] ein Eigenvektor zum Eigenwert \\(\\lambda_2 = 1\\) ist. Weiterhin gelten hier offenbar \\[\\begin{equation}\nv_1^Tv_2 = 0 \\mbox{ und } \\Vert v_1 \\Vert = \\Vert v_2 \\Vert = 1.\n\\end{equation}\\]\nFolgender R Code demonstriert die Bestimmung der Eigenwerte und Eigenvektoren der hier betrachteten Matrix mithilfe der eigen() Funktion.\n\n# Matrixdefinition\nA = matrix(c(2,1,\n             1,2),\n           nrow  = 2,\n           byrow = TRUE)\n\n# Eigenanalyse\neigen(A)\n\neigen() decomposition\n$values\n[1] 3 1\n\n$vectors\n          [,1]       [,2]\n[1,] 0.7071068 -0.7071068\n[2,] 0.7071068  0.7071068\n\n\nZum Abschluss dieses Abschnittes betrachten wir zwei technische Theoreme, die Aussagen zum Zusammenhang spezieller Matrixprodukte und ihrer Eigenwerte und Eigenvektoren machen. Wir benötigen dieses Theoreme im Kontext der Kanonischen Korrelationsanalyse (?sec-kanonische-korrelationsanalyse).\n\nTheorem 9.17 (Eigenwerte und Eigenvektoren von Matrixprodukten) Für \\(A \\in \\mathbb{R}^{n \\times m}\\) und \\(B \\in \\mathbb{R}^{m \\times n}\\) sind die Eigenwerte von \\(AB \\in \\mathbb{R}^{n \\times n}\\) und \\(BA \\in \\mathbb{R}^{m \\times m}\\) gleich. Weiterhin gilt, dass für einen Eigenvektor \\(v\\) zu einem von Null verschiedenen Eigenwert \\(\\lambda\\) von \\(AB\\) \\(w := Bv\\) ein Eigenvektor von \\(BA\\) zum Eigenwert \\(\\lambda\\) ist.\n\nFür einen Beweis verweisen wir auf Mardia et al. (1979), S. 468. Wir demonstrieren die Aussage dieses Theorems anhand untenstehenden R Codes.\n\nA   = matrix(1:6, nrow = 2,  byrow = T)           # Matrix A \\in \\mathbb{R}^{2 x 3}\nB   = matrix(1:6, ncol = 2,  byrow = T)           # Matrix B \\in \\mathbb{R}^{3 x 2}\nEAB = eigen(A %*% B)                              # Eigenanalyse von AB \\in \\mathbb{R}^{2 \\times 2}\nEBA = eigen(B %*% A)                              # Eigenanalyse von BA \\in \\mathbb{R}^{3 \\times 3}\nw   = B %*% EAB$vectors[,1]                       # Eigenvektor von BA\ncat(\"Eigenwerte von AB :\"  , EAB$values[1:2],\n    \"\\nEigenwerte von BA :\", EBA$values[1:2],\n    \"\\nBAw mit w = Bv    :\", B %*% A %*% w,\n    \"\\nlw mit w = Bv     :\", EBA$values[1] * w)\n\nEigenwerte von AB : 85.57934 0.4206623 \nEigenwerte von BA : 85.57934 0.4206623 \nBAw mit w = Bv    : -191.1333 -416.7586 -642.3839 \nlw mit w = Bv     : -191.1333 -416.7586 -642.3839\n\n\n\nTheorem 9.18 Für \\(A \\in \\mathbb{R}^{n \\times m}, B \\in \\mathbb{R}^{p \\times n}, a \\in \\mathbb{R}^m\\) und \\(b \\in \\mathbb{R}^p\\) gilt, dass der einzige von Null verschiedene Eigenwert von \\(Aab^TB \\in \\mathbb{R}^{n \\times n}\\) gleich \\(b^T BAa\\) mit zugehörigem Eigenvektor \\(Aa\\) ist.\n\nFür einen Beweis verweisen wir auf Mardia et al. (1979), S. 468. Wir demonstrieren die Aussage dieses Theorems anhand untenstehenden R Codes.\n\nA      = matrix(1:6, nrow = 2,  byrow = T)     # Matrix A \\in \\mathbb{R}^{2 x 3}\nB      = matrix(1:8, ncol = 2,  byrow = T)     # Matrix B \\in \\mathbb{R}^{4 x 2}\na      = matrix(1:3, nrow = 3,  byrow = T)     # Vektor a \\in \\mathbb{R}^{3 x 1}\nb      = matrix(1:4, nrow = 4,  byrow = T)     # Vektor b \\in \\mathbb{R}^{4 x 1}\nEAabTB = eigen(A %*% a %*% t(b) %*% B)         # Eigenanalyse von Aab^TB \\in \\mathbb{R}^{4 x 4}\ncat(\"Eigenwerte von AabTB :\", EAabTB$values,\n    \"\\nbTBAa                :\", t(b) %*% B %*% A %*% a,\n    \"\\nAa                   :\", A %*% a,\n    \"\\n(AabTB)Aa            :\",(A %*% a %*% t(b) %*% B) %*% A %*% a,            # Mv\n    \"\\n(bTBAa)Aa            :\",as.vector((t(b) %*% B %*% A %*% a)) * (A %*% a)) # = \\lambda v\n\nEigenwerte von AabTB : 2620 0 \nbTBAa                : 2620 \nAa                   : 14 32 \n(AabTB)Aa            : 36680 83840 \n(bTBAa)Aa            : 36680 83840",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Matrizen</span>"
    ]
  },
  {
    "objectID": "109-Matrizen.html#sec-orthonormalzerlegung",
    "href": "109-Matrizen.html#sec-orthonormalzerlegung",
    "title": "9  Matrizen",
    "section": "9.11 Orthonormalzerlegung",
    "text": "9.11 Orthonormalzerlegung\nMit dem Begriff der Zerlegung einer Matrix wird das Aufspalten einer gegebenen Matrix in das Matrixprodukt mehrerer Matrizen bezeichnet. Verschiedenste Matrixzerlegungen spielen in vielen mathematischen Anwendungen eine wichtige Rolle, für einen Überblick siehe beispielsweise Golub & Van Loan (2013). In diesem Abschnitt führen wir mit der Orthonormalzerlegung einer symmetrischen Matrix eine spezielle Matrixzerlegung ein, die direkt auf der Eigenanalyse aufbaut. Wir halten zunächst folgendes grundlegendes Theorem zu den Eigenwerten und Eigenvektoren symmetrischer Matrizen fest.\n\nTheorem 9.19 (Eigenwerte und Eigenvektoren symmetrischer Matrizen)  \n\\(S \\in \\mathbb{R}^{m \\times m}\\) sei eine symmetrische Matrix. Dann gelten\n\n\nBeweis. Wir setzen die Tatsache, dass eine symmetrische Matrix \\(m\\) reelle Eigenwerte hat, als gegeben voraus und zeigen lediglich, dass die Eigenvektoren zu je zwei verschiedenen Eigenwerten einer symmetrischen Matrix orthogonal sind. Ohne Beschränkung der Allgemeinheit seien also \\(\\lambda_i, \\lambda_j \\in \\mathbb{R}\\) mit \\(1 \\le i,j \\le m\\) und \\(\\lambda_i \\neq \\lambda_j\\) zwei verschiedenen Eigenwerte von \\(S\\) mit zugehörigen Eigenvektoren \\(q_i\\) und \\(q_j\\), respektive. Dann ergibt sich wie unten gezeigt, dass \\[\\begin{equation}\n\\lambda_i q_i^Tq_j = \\lambda_j q_i^Tq_j.\n\\end{equation}\\] Mit \\(q_i \\neq 0_m, q_j \\neq 0_m\\) und \\(\\lambda_i \\neq \\lambda_j\\) folgt damit \\(q_i^Tq_j = 0\\), weil weil es keine andere Zahl \\(c\\) als die Null gibt, für die bei \\(a,b\\in \\mathbb{R}\\) und \\(a \\neq b\\) gilt, dass \\[\\begin{equation}\nac = bc.\n\\end{equation}\\] Um abschließend \\[\\begin{equation}\n\\lambda_i q_i^Tq_j = \\lambda_j q_i^Tq_j.\n\\end{equation}\\] zu zeigen, halten wir zunächst fest, dass \\[\\begin{equation}\n                 Sq_i       = \\lambda_i q_i      \n\\Leftrightarrow (Sq_i)^T    = (\\lambda_i q_i)^T  \n\\Leftrightarrow q_i^TS^T    = q_i^T \\lambda_i^T  \n\\Leftrightarrow q_i^T S     = q_i^T \\lambda_i   \n\\Leftrightarrow q_i^T Sq_j  = \\lambda_i q_i^Tq_j\n\\end{equation}\\] und \\[\\begin{equation}\n                 Sq_j           = \\lambda_j q_j      \n\\Leftrightarrow q_j^T S         = q_j^T \\lambda_j   \n\\Leftrightarrow q_j^T Sq_i      = \\lambda_j q_j^Tq_i\n\\Leftrightarrow (q_j^T S q_i)^T = (\\lambda_j q_j^Tq_i)^T\n\\Leftrightarrow q_i^T S q_j     =  \\lambda_j q_i^Tq_j\n\\end{equation}\\] gelten. Sowohl \\(\\lambda_i q_i^Tq_j\\) als auch \\(\\lambda_j q_i^Tq_j\\) sind also mit \\(q_i^T Sq_j\\) und damit auch miteinander identisch.\n\nOffenbar haben wir nur Aussage (2) von Theorem 9.19 bewiesen. Ein vollständiger Beweis des Theorems findet sich zum Beispiel bei Strang (2009). Wir merken außerdem an, dass, weil wir nach Konvention Eigenvektoren der Länge 1 betrachten, die in Theorem 9.19 angesprochenen orthogonalen Eigenvektoren insbesondere auch orthonormal sind. Mithilfe von Theorem 9.19 können wir nun die Orthonormalzerlegung einer symmetrischen Matrix formulieren und ihre Existenz beweisen.\n\nTheorem 9.20 (Orthonormalzerlegung einer symmetrischen Matrix) \\(S \\in \\mathbb{R}^{m \\times m}\\) sei eine symmetrische Matrix mit \\(m\\) verschiedenen Eigenwerten. Dann kannn \\(S\\) geschrieben werden als \\[\\begin{equation}\nS = Q \\Lambda Q^T,\n\\end{equation}\\] wobei \\(Q \\in \\mathbb{R}^{m \\times m}\\) eine orthogonale Matrix ist und \\(\\Lambda \\in \\mathbb{R}^{m\\times m}\\) eine Diagonalmatrix ist.\n\n\nBeweis. Es seien \\(\\lambda_1 &gt; \\lambda_2 &gt; ... &gt; \\lambda_m\\) die der Größe nach geordneten Eigenwerte von \\(S\\) und \\(q_1,...,q_m\\) die zugehörigen orthonormalen Eigenvektoren. Mit \\[\\begin{equation}\nQ :=\n\\begin{pmatrix*}[r]\nq_1 & q_2 & \\cdots & q_m\n\\end{pmatrix*}\n\\in \\mathbb{R}^{m \\times m}\n\\mbox{ und }\n\\Lambda :=\n\\mbox{diag}\\begin{pmatrix*}[r]\n\\lambda_1,\\lambda_2,...,\\lambda_m\n\\end{pmatrix*}\n\\in \\mathbb{R}^{m \\times m},\n\\end{equation}\\] folgt dann mit den Definitionen von Eigenwerten und Eigenvektoren zunächst, dass \\[\\begin{equation}\nSq_i = \\lambda_i q_i \\mbox{ für } i = 1,...,m\n\\Leftrightarrow\nSQ = Q\\Lambda.\n\\end{equation}\\] Rechtseitige Multiplikation mit \\(Q^T\\) ergibt dann mit \\(QQ^T = I_m\\), dass \\[\\begin{equation}\nSQQ^T = Q \\Lambda Q^T\n\\Leftrightarrow SI_m = Q \\Lambda Q^T\n\\Leftrightarrow S    = Q \\Lambda Q^T.\n\\end{equation}\\]\n\nMan nennt das Aufspalten von \\(S\\) in das Matrixprodukt \\(Q\\Lambda Q^T\\) aufgrund der Diagonalität von \\(\\Lambda\\) auch eine Diagonalisierung von \\(S\\). Wie im Beweis gezeigt, wählt man zur Darstellung von \\(S\\) in Diagonaldarstellung für die Diagonalelemente von \\(\\Lambda\\) die der Größe nach geordneten Eigenwerte von \\(S\\) und für die Spalten von \\(Q\\) die jeweils zugehörigen Eigenvektoren von \\(S\\). Wir verdeutlichen dies an einem Beispiel.\nBeispiel\nFür die symmetrische Matrix \\[\\begin{equation}\nA = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}\n\\end{equation}\\] mit den oben bestimmten Eigenwerten \\(\\lambda_1 = 3\\) und \\(\\lambda_2 = 1\\) sowie den zugehörigen orthonormalen Eigenvektoren \\[\\begin{equation}\nv_1 = \\frac{1}{\\sqrt{2}}\n\\begin{pmatrix*}[r]\n1 \\\\\n1\n\\end{pmatrix*},\nv_2 = \\frac{1}{\\sqrt{2}}\n\\begin{pmatrix*}[r]\n-1 \\\\\n1\n\\end{pmatrix*}\n\\end{equation}\\] seien \\[\\begin{equation}\nQ := \\begin{pmatrix*}[r]\nv_1 & v_2\n\\end{pmatrix*}\n\\mbox{ und }\n\\Lambda = \\mbox{diag}(\\lambda_1,\\lambda_2).\n\\end{equation}\\] Dann ergibt sich offenbar \\[\\begin{align*}\nQ\\Lambda Q^T\n& =\n\\begin{pmatrix*}[r]\nv_1 & v_2\n\\end{pmatrix*}\n\\mbox{diag}(\\lambda_1,\\lambda_2)\n\\begin{pmatrix*}[r]\nv_1 & v_2\n\\end{pmatrix*}^T \\\\\n& =\n\\left(\n\\frac{1}{\\sqrt{2}}\n\\begin{pmatrix*}[r]\n1 & -1\\\\\n1 &  1\n\\end{pmatrix*}\n\\begin{pmatrix*}[r]\n3 & 0 \\\\\n0 & 1\n\\end{pmatrix*}\n\\right)\n\\left(\n\\frac{1}{\\sqrt{2}}\n\\begin{pmatrix*}[r]\n1 &  1 \\\\\n-1 &  1\n\\end{pmatrix*}\n\\right)\n\\\\\n& =\n\\left(\n\\frac{1}{\\sqrt{2}}\n\\begin{pmatrix*}[r]\n3 & -1 \\\\\n3 &  1\n\\end{pmatrix*}\n\\right)\n\\left(\n\\frac{1}{\\sqrt{2}}\n\\begin{pmatrix*}[r]\n1 &  1 \\\\\n-1 &  1\n\\end{pmatrix*}\n\\right)\n\\\\\n& =\n\\frac{1}{2}\n\\begin{pmatrix*}[r]\n4 & 2 \\\\\n2 & 4\n\\end{pmatrix*} \\\\\n& =\n\\begin{pmatrix*}[r]\n2 & 1 \\\\\n1 & 2\n\\end{pmatrix*} \\\\\n& = A\n\\end{align*}\\] und wir haben Theorem 9.20 für dieses Beispiel verifiziert.\n\nSymmetrische Quadratwurzel einer Matrix\nDie Definition der Orthonormalzerlegung einer symmetrischen Matrix erlaubt es, den Begriff der symmetrischen Quadratwurzel einer Matrix einzuführen.\n\nDefinition 9.17 (Symmetrische Quadratwurzel einer Matrix) \\(S \\in \\mathbb{R}^{m \\times m}\\) sei eine invertierbare symmetrische Matrix mit positiven Eigenwerten. Dann sind für \\(r \\in \\mathbb{N}^0\\) und \\(s \\in \\mathbb{N}\\) die rationalen Potenzen von \\(S\\) mit der orthonormalen Matrix \\(Q \\in \\mathbb{R}^{m \\times m}\\) der Eigenvektoren von \\(S\\) und der Diagonalmatrix \\(\\Lambda = \\mbox{diag}(\\lambda_i) \\in \\mathbb{R}^{m \\times m}\\) der zugehörigen Eigenwerte \\(\\lambda_1,...,\\lambda_m\\) von \\(S\\) definiert als \\[\\begin{equation}\nS^{r/s} = Q \\Lambda^{r/s} Q^T \\mbox{ mit } \\Lambda^{r/s} = \\mbox{diag}\\left(\\lambda_i^{r/s}\\right).\n\\end{equation}\\] Der Spezialfall \\(r:= 1, s := 2\\) wird als symmetrische Quadratwurzel von \\(S\\) bezeichnet und hat die Form \\[\\begin{equation}\nS^{1/2} = Q\\Lambda^{1/2}Q^T \\mbox{ mit } \\Lambda^{1/2} = \\mbox{diag}\\left(\\lambda_i^{1/2}\\right).\n\\end{equation}\\]\n\nWir halten fest, dass mit Definition 9.17 offenbar gilt, dass \\[\\begin{equation}\n\\left(S^{1/2} \\right)^2\n= Q\\Lambda^{1/2}Q^TQ\\Lambda^{1/2}Q^T\n= Q\\Lambda^{1/2}\\Lambda^{1/2}Q^T\n= Q\\Lambda Q^T\n= S.\n\\end{equation}\\] Weiterhin gilt, dass \\[\\begin{equation}\n\\left(S^{-1/2} \\right)^2\n= Q\\Lambda^{-1/2}Q^TQ\\Lambda^{-1/2}Q^T\n= Q\\Lambda^{-1/2}\\Lambda^{-1/2}Q^T\n= Q\\Lambda^{-1}Q^T\n= S^{-1}.\n\\end{equation}\\] Schließlich gilt, dass \\[\\begin{align}\n\\begin{split}\nS^{-1/2}SS^{-1/2}\n& =  Q\\Lambda^{-1/2}Q^T Q\\Lambda Q^T Q\\Lambda^{-1/2}Q^T   \\\\\n& =  Q\\Lambda^{-1/2}\\Lambda \\Lambda^{-1/2}Q^T             \\\\\n& =  Q\\Lambda \\Lambda^{-1}Q^T                             \\\\\n& =  I_m                                          \n\\end{split}\n\\end{align}\\]",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Matrizen</span>"
    ]
  },
  {
    "objectID": "109-Matrizen.html#sec-singulärwertzerlegung",
    "href": "109-Matrizen.html#sec-singulärwertzerlegung",
    "title": "9  Matrizen",
    "section": "9.12 Singulärwertzerlegung",
    "text": "9.12 Singulärwertzerlegung\nEine vielseitig einsetzbare Matrixzerlegung einer beliebigen Matrix ist die Singulärwertzerlegung. Wir sind an dieser Stelle lediglich an dem Zusammenhang von Singulärwertzerlegung und Eigenanalyse interessiert und verweisen für eine ausführliche Diskussion der Singulärwertzerlegung auf die weiterführende Literatur, beispielsweise Strang (2009). Der Begriff der Singulärwertzerlegung ist wie folgt definiert.\n\nDefinition 9.18 (Singulärwertzerlegung) \\(Y \\in \\mathbb{R}^{m \\times n}\\) sei eine Matrix. Dann heißt die Zerlegung \\[\\begin{equation}\nY = USV^T,\n\\end{equation}\\] wobei \\(U \\in \\mathbb{R}^{m \\times m}\\) eine orthogonale Matrix ist, \\(S \\in \\mathbb{R}^{m \\times n}\\) eine Diagonalmatrix ist und \\(V \\in \\mathbb{R}^{n \\times n}\\) eine orthogonale Matrix ist, Singulärwertzerlegung von \\(Y\\). Die Diagonalelemente von \\(S\\) heißen die Singulärwerte von \\(Y\\).\n\nSingulärwertzerlegungen werden auf Englisch singular value decompositions genannt und entsprechend mit SVD abgekürzt. Wir verzichten auf eine Diskussion der Berechnung einer Singulärwertzerlegung und weisen lediglich daraufhin, dass Singulärwertzerlegungen zum Beispiel in R mit der Funktion svd(), in SciyPy mit scipy.linalg.svd() und in Julia mit svd() berechnet werden können. Folgendes Theorem beschreibt den Zusammenhang zwischen Singulärwertzerlegung und Eigenanalyse und wird an vielen Stellen eingesetzt.\n\nTheorem 9.21 (Singulärwertzerlegung und Eigenanalyse)  \n\\(Y \\in \\mathbb{R}^{m \\times n}\\) sei eine Matrix und \\[\\begin{equation}\nY = USV^T\n\\end{equation}\\] sei ihre Singulärwertzerlegung. Dann gilt:\n\n\nBeweis. Wir halten zunächst fest, dass mit \\[\\begin{equation}\n\\left(YY^T\\right)^T = YY^T \\mbox{ und } \\left(Y^TY\\right)^T = Y^TY,\n\\end{equation}\\] \\(YY^T\\) und \\(Y^TY\\) symmetrische Matrizen sind und somit Orthornomalzerlegungen haben. Wir halten weiterhin fest, dass mit \\(V^TV = I_n\\), \\(U^TU = I_m\\) und \\(S^T = S\\) gilt, dass \\[\\begin{equation}\nYY^T\n= USV^T \\left(USV^T\\right)^T\n= USV^TVS^TU^T\n= USSU^T\n=: U\\Lambda U^T\n\\end{equation}\\] und \\[\\begin{equation}\nY^TY\n= \\left(USV^T\\right)^T USV^T\n= VS^TU^T US^T V^T\n=: V\\Lambda V^T\n\\end{equation}\\] ist, wobei wir \\(\\Lambda := SS\\) definiert haben. Weil das Produkt von Diagonalmatrizen wieder eine Diagonalmatrix ist, ist \\(\\Lambda\\) eine Diagonalmatrix und per Definition sind \\(U\\) und \\(V\\) orthogonale Matrizen. Wir haben also \\(YY^T\\) und \\(Y^TY\\) in Form der Orthonormalzerlegungen \\[\\begin{equation}\nYY^T = U \\Lambda U^T  \\mbox{ und } Y^TY = V \\Lambda V^T\n\\end{equation}\\] geschrieben, wobei für die Diagonalelemente von \\(\\Lambda\\) gilt, dass sie die quadrierten Werte der Diagonalemente von \\(S\\) sind.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Matrizen</span>"
    ]
  },
  {
    "objectID": "109-Matrizen.html#literaturhinweise-1",
    "href": "109-Matrizen.html#literaturhinweise-1",
    "title": "9  Matrizen",
    "section": "9.13 Literaturhinweise",
    "text": "9.13 Literaturhinweise\nDie in diesem Kapitel behandelten Konzepte werden ausführlich zum Beispiel in Searle (1982) und Strang (2009) behandelt. Die Verwendung des Präfix Eigen- für die beschriebenen Vektoren und Skalare in bezug zu einer Matrix beginnt offenbar Hilbert (1904) im Kontext der Analyse von Integralgleichungen und hat sich auch im Englischen durchgesetzt.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Matrizen</span>"
    ]
  },
  {
    "objectID": "109-Matrizen.html#selbstkontrollfragen-1",
    "href": "109-Matrizen.html#selbstkontrollfragen-1",
    "title": "9  Matrizen",
    "section": "9.14 Selbstkontrollfragen",
    "text": "9.14 Selbstkontrollfragen\n\nGeben Sie die Definition eines Eigenvektors einer quadratischen Matrix wieder.\nGeben Sie die Definition eines Eigenwerts einer quadratischen Matrix wieder.\nGeben Sie das Theorem zur Bestimmung von Eigenwerten und Eigenvektoren wieder.\nGeben Sie das Theorem zu den Eigenwerten und Eigenvektoren symmetrischer Matrizen wieder.\nGeben Sie das Theorem zur Orthonormalzerlegung einer symmetrischen Matrix wieder.\nGeben Sie die Definition der symmetrischen Quadratwurzel einer Matrix wieder.\nGeben Sie die Definition einer Singulärwertzerlegung wieder.\nGeben Sie das Theorem zum Zusammenhang von Singulärwertzerlegung und Eigenanalyse wieder.\n\n\n\n\n\nBurden, R. L., Faires, J. D., & Burden, A. M. (2016). Numerical Analysis (Tenth edition). Cengage Learning.\n\n\nCaley, A. (1858). A Memoir on the Theory of Matrices. Philosophical Transactions of the Royal Society of London, 148, 17–37. https://doi.org/10.1098/rstl.1858.0002\n\n\nGolub, G. H., & Van Loan, C. F. (2013). Matrix Computations (Fourth edition). The Johns Hopkins University Press.\n\n\nHilbert, D. (1904). Grundzüge Einer Allgemeinen Theorie Der Linearen Integralgleichungen. Nachrichten von der Gesellschaft der Wissenschaften zu Göttingen, Mathematisch-Physikalische Klasse. https://doi.org/10.1007/978-3-322-84410-1_1\n\n\nMardia, K. V., Kent, J. T., & Bibby, J. M. (1979). Multivariate Analysis. Academic Press.\n\n\nRichter, T., & Wick, T. (2017). Einführung in die Numerische Mathematik: Begriffe, Konzepte und zahlreiche Anwendungsbeispiele. Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-662-54178-4\n\n\nSearle, S. (1982). Matrix Algebra Useful for Statistics. Wiley-Interscience.\n\n\nStrang, G. (2009). Introduction to Linear Algebra. Cambridge University Press.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Matrizen</span>"
    ]
  },
  {
    "objectID": "110-Deskriptivstatistiken.html",
    "href": "110-Deskriptivstatistiken.html",
    "title": "10  Deskriptivstatistiken",
    "section": "",
    "text": "Anwendungsbeispiel\nZiel der Auswertung von Deskriptivstatistiken ist es, Datensätze möglichst effizient der menschlichen Betrachtung zugänglich zu machen. Sobald die Anzahl der Datenpunkte eines Datensatzes eine gewisse Anzahl überschreitet, sind reine Zahlendarstellungen für die menschliche Kognition meist eher schwer zu erfassen. Hier setzt die Deskriptivstatistik an und bietet neben graphischen Darstellungen größerer Datensätze auch Maßzahlen an, die im Sinne der Datenreduktion bestimmte charakteristische Aspekte eines Datensatzes, wie seinen “durchschnittlichen” Wert oder seine Variabilität beschreiben. In diesem Abschnitt wollen wir mit den univariaten Deskriptivstatistiken erste Verfahren und Maße kennenlernen, die es erlauben, Datensätze, in denen pro Studieneinheit eine Zahl vorliegt, zusammenzufassen. Wir betrachten dabei insbesonder Häufigkeitsverteilungen und Histogramme, Verteilungsfunktionen und Quantile, sowie Maße der zentralen Tendenz, die Aussagen über “durchschnittliche” Datenwerte erlauben und Maße der Datenvariabilität, die Aussagen über die Streuung der Datenwerte erlauben.\nIn Abgrenzung zur probabilistischen Modellierung im Rahmen der Frequentistischen und Bayesianischen Inferenz basiert die deskriptive Statistik nicht auf der Wahrscheinlichkeitstheorie, kann also losgelöst von dieser betrachtet werden. Allerdings ergeben viele Aspekte der Deskriptivstatistik letztlich nur vor dem Hintergrund wahrscheinlichkeitstheoretischer Betrachtungen Sinn und umgekehrt sind viele Begriffe der Wahrscheinlichkeitstheorie durch deskriptivstatistische Konzepte motiviert. Wenn also der vorliegende Abschnitt ohne einen wahrscheinlichkeitstheoretischen Hintergrund auskommt, so erschließt sich seine volle Bedeutung sicherlich erst im Kontext der Begriffe der Wahrscheinlichkeitstheorie.\nWir fokussieren in diesem Abschnitt auf Deskriptivstatistiken zur Beschreibung eines Datensatzes der Form \\[\\begin{equation}\ny := (y_1,...,y_n) \\mbox{ mit } y_i \\in \\mathbb{R} \\mbox{ für } i = 1,...,n.\n\\end{equation}\\] Dabei ist es zunächst unerheblich, ob ein solcher Datensatz in Form eines Spaltenvektors \\(y \\in \\mathbb{R}^{n \\times 1}\\) oder eines Zeilenvektors \\(y \\in \\mathbb{R}^{1 \\times n}\\) vorliegt. Weiterhin fokussieren wir auf die praktische Auswertung der betrachteten Deskriptivstatistiken, geben also viele Beispiele für ihre Berechnung mit R.\nAls Anwendungsbeispiel betrachten wir durchgängig einen Datensatz zur evidenzbasierten Evaluation von Psychotherapie bei Depression. Dazu nehmen wir an, dass \\(n = 100\\) Patient:innen zufällig auf eine Treatment- und eine Kontrollstudienbedingung aufgeteilt wurden und zu Beginn der jeweiligen Intervention mithilfe des BDI-II Fragebogens zu ihrer Depressionsschwere befragt wurden. Die nach den Interventionen erhobenen BDI-II Werte wollen wir in diesem Kapitel nicht betrachten. Wir nehmen also an, dass wir einen Datensatz wie durch folgenden R Code in Tabelle 10.1 dargestellt vorliegen haben.\nD  = read.csv(\"./_data/111-deskriptive-statistik.csv\")                          # Laden des Datensatzes\nknitr::kable(D[,1:2], digits = 2, align = \"c\")                                  # RMarkdowntabellenoutput  \n\n\n\nTabelle 10.1: BDI-II Werte (PRE) von n = 100 Patient:innen in einer Treatment (T) und einer Kontrollstudiengruppe (C) vor Beginn einer psychologischen Intervention\n\n\n\n\n\n\nGROUP\nPRE\n\n\n\n\nT\n17\n\n\nT\n20\n\n\nT\n16\n\n\nT\n18\n\n\nT\n21\n\n\nT\n17\n\n\nT\n17\n\n\nT\n17\n\n\nT\n18\n\n\nT\n18\n\n\nT\n20\n\n\nT\n17\n\n\nT\n16\n\n\nT\n18\n\n\nT\n16\n\n\nT\n18\n\n\nT\n17\n\n\nT\n14\n\n\nT\n18\n\n\nT\n18\n\n\nT\n20\n\n\nT\n20\n\n\nT\n21\n\n\nT\n19\n\n\nT\n19\n\n\nT\n17\n\n\nT\n20\n\n\nT\n21\n\n\nT\n17\n\n\nT\n17\n\n\nT\n19\n\n\nT\n20\n\n\nT\n22\n\n\nT\n20\n\n\nT\n21\n\n\nT\n20\n\n\nT\n16\n\n\nT\n15\n\n\nT\n15\n\n\nT\n18\n\n\nT\n21\n\n\nT\n17\n\n\nT\n18\n\n\nT\n21\n\n\nT\n17\n\n\nT\n19\n\n\nT\n16\n\n\nT\n17\n\n\nT\n17\n\n\nT\n18\n\n\nC\n22\n\n\nC\n19\n\n\nC\n21\n\n\nC\n18\n\n\nC\n19\n\n\nC\n17\n\n\nC\n20\n\n\nC\n19\n\n\nC\n19\n\n\nC\n19\n\n\nC\n17\n\n\nC\n20\n\n\nC\n18\n\n\nC\n20\n\n\nC\n18\n\n\nC\n18\n\n\nC\n15\n\n\nC\n18\n\n\nC\n17\n\n\nC\n19\n\n\nC\n19\n\n\nC\n19\n\n\nC\n20\n\n\nC\n19\n\n\nC\n20\n\n\nC\n19\n\n\nC\n21\n\n\nC\n19\n\n\nC\n19\n\n\nC\n18\n\n\nC\n20\n\n\nC\n16\n\n\nC\n21\n\n\nC\n19\n\n\nC\n20\n\n\nC\n18\n\n\nC\n23\n\n\nC\n18\n\n\nC\n19\n\n\nC\n18\n\n\nC\n21\n\n\nC\n17\n\n\nC\n20\n\n\nC\n21\n\n\nC\n21\n\n\nC\n20\n\n\nC\n18\n\n\nC\n18\n\n\nC\n19\n\n\nC\n19",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Deskriptivstatistiken</span>"
    ]
  },
  {
    "objectID": "110-Deskriptivstatistiken.html#sec-haeufigkeitsverteilungen",
    "href": "110-Deskriptivstatistiken.html#sec-haeufigkeitsverteilungen",
    "title": "10  Deskriptivstatistiken",
    "section": "10.1 Häufigkeitsverteilungen",
    "text": "10.1 Häufigkeitsverteilungen\nWir beginnen mit den Begriffen der absoluten und der relativen Häufigkeitsverteilung. Für Datensätze mit Werten, in denen der gleiche Wert mehrfach auftritt, können Häufigkeitsverteilungen einen ersten Eindruck von der Beschaffenheit des Datensatzes liefern.\n\nDefinition 10.1 (Absolute und relative Häufigkeitsverteilungen) \\(y := (y_1,...,y_n)\\) sei ein Datensatz und \\(W := \\{w_1,...,w_k\\}\\) mit \\(k \\le n\\) seien die im Datensatz vorkommenden Werte. Dann heißt die Funktion \\[\\begin{equation}\nh : W \\to \\mathbb{N}, w \\mapsto h(w) := \\mbox{Anzahl der } y_i \\mbox{ aus } y \\mbox{ mit } y_i = w\n\\end{equation}\\] die absolute Häufigkeitsverteilung der Werte von \\(y\\) und die Funktion \\[\\begin{equation}\nr : W \\to [0,1], w \\mapsto r(w) := \\frac{h(w)}{n}\n\\end{equation}\\] heißt die relative Häufigkeitsverteilung der Werte von \\(y\\).\n\nHäufigkeitsverteilungen können in R durch Kombination der table()und barplot() Funktion ausgewertet und dargestellt werden. Folgender R Code erzeugt zunächst die absolute und relative Häufigkeitsverteilungen der Pre-Interventions-BDI-II Daten des Beispieldatensatzes.\n\nD           = read.csv(\"./_data/111-deskriptive-statistik.csv\")                 # Laden des Datensatzes\ny           = D$PRE                                                             # Double vector der PRE Werte\nn           = length(y)                                                         # Anzahl der Datenwerte (100)\nH           = as.data.frame(table(y))                                           # absolute Häufigkeitsverteilung (dataframe)\nnames(H)    = c(\"w\", \"ah\")                                                      # Spaltenbenennung\nH$rh        = H$ah/n                                                            # relative Häufigkeitsverteilung\nprint(H, digits = 1)                                                            # Ausgabe\n\n    w ah   rh\n1  14  1 0.01\n2  15  3 0.03\n3  16  6 0.06\n4  17 17 0.17\n5  18 21 0.21\n6  19 20 0.20\n7  20 17 0.17\n8  21 12 0.12\n9  22  2 0.02\n10 23  1 0.01\n\n\nAn der Ausgabe des Dataframes H können wir ablesen, dass im betrachteten Datensatz 10 verschiedene Werte w zwischen 14 und 23 vorkommen, wobei zum Beispiel die Werte 18 und 22 jeweils 21 bzw. 2 mal auftreten. Sowohl die absolute als auch die relative Häufigkeitsverteilung zeigen, dass Werte um 18 im vorliegenden Datensatz gehäuft auftreten, Extremwerte wie 13 oder 23 dagegen eher selten. Man beachte, dass es sich bei der relativen Häufigkeitsverteilung lediglich um eine skalierte Form der absoluten Häufigkeitsverteilung handelt. Qualitativ, also hinsichtlich des gehäuften oder selteneren Vorkommens bestimmter Werte, unterscheiden sich beide Häufigkeitsverteilungen nicht. Dies wird insbesondere auch deutlich, wenn man wie in Abbildung 10.1 beide Häufigkeitsverteilungen visualisiert. Folgender R nutzt zu diesem Zweck die Funktion barplot(). Die Höhe des über einem Wert \\(w\\) aufgetragenen Balkens entspricht dabei den Funktionswerten \\(h(w)\\) bzw. \\(r(w)\\). Die visuelle Einsicht entspricht natürlich der Inspektion des Dataframew H. Beide Häufigkeitsverteilungen zeigen, dass Werte um 18 im vorliegenden Datensatz gehäuft auftreten, wohingegen niedrigere oder höhere Werte wie 13 bzw. 23 dagegen eher selten sind.\n\npdf(                                                                            # PDF Kopiefunktion\nfile        = \"./_figures/111-haeufigkeitsverteilungen.pdf\",                    # Dateiname\nwidth       = 7,                                                                # Breite (inch)\nheight      = 4)                                                                # Höhe (inch)\npar(                                                                            # Abbildungsparameter    \nmfcol       = c(1,2),                                                           # Zeilen und Spaltenlayout \nlas         = 1,                                                                # x-Tick Orientierung\ncex         = 0.7,                                                              # Annotationsvergrößerung    \ncex.main    = 1.3)                                                              # Titelvergrößerung\nh           = H$ah                                                              # h(w) Werte\nr           = H$rh                                                              # r(w) Werte\nnames(h)    = H$w                                                               # barplot braucht w Werte als names\nnames(r)    = H$w                                                               # barplot braucht w Werte als names\nbarplot(                                                                        # Balkendiagramm\nh,                                                                              # absolute Haeufigkeiten\ncol         = \"gray90\",                                                         # Balkenfarbe\nxlab        = \"w\",                                                              # x Achsenbeschriftung\nylab        = \"h(w)\",                                                           # y Achsenbeschriftung\nylim        = c(0,25),                                                          # y Achsengrenzen\nmain        = \"Absolute Häufigkeitsverteilung\")                                 # Titel\nbarplot(                                                                        # Balkendiagramm\nr,                                                                              # absolute Haeufigkeiten\ncol         = \"gray90\",                                                         # Balkenfarbe\nxlab        = \"w\",                                                              # x Achsenbeschriftung\nylab        = \"r(w)\",                                                           # y Achsenbeschriftung\nylim        = c(0,.25),                                                         # y Achsengrenzen\nmain        = \"Relative Häufigkeitsverteilung\")                                 # Titel\ndev.off()                                                                       # Beenden der Abbildungsbearbeitung\n\n\n\n\n\n\n\nAbbildung 10.1: Absolute und relative Häufigkeitsverteilungen der Pre-Interventions-BDI-II-Werte .",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Deskriptivstatistiken</span>"
    ]
  },
  {
    "objectID": "110-Deskriptivstatistiken.html#sec-histogramme",
    "href": "110-Deskriptivstatistiken.html#sec-histogramme",
    "title": "10  Deskriptivstatistiken",
    "section": "10.2 Histogramme",
    "text": "10.2 Histogramme\nWenn in einem Datensatz keine Werte mehrfach vorkommen, nehmen die absoluten und relativen Häufigkeitsverteilungen für jeden dieser Werte den Wert \\(1\\) bzw. \\(1/n\\) an und bieten dann keinen Mehrwert über die visuelle Inspektion des Datensatzes hinaus. Die in diesem Abschnitt vorgestellten Histogramme bieten jedoch eine Möglichkeit, auch in diesem Fall visuelle Darstellungen von Häufigkeitsverteilungen zu generieren. Dabei werden verschiedene Werte, die in einem wohldefinierten Sinne “nah beieinander” liegen, zu Klassen zusammengefasst und die absoluten oder relativen Häufigkeiten dieser Klassen bestimmt. Da bei Datensätzen, in denen die gleichen Werte mehrfach vorkommen, diese Werte der Natur nach in der gleichen Klasse liegen, können Histogramme auch in dem in Kapitel 10.1 betrachten Fall sinnvoll angewendet werden. Histogramme verallgemeinern also die im vorherigen Abschnitt betrachteten Häufigkeitsverteilungen bei Datensätzen mit mehrfach auftretenen Werten.\nWir geben zunächst eine Definition des Histogrammbegriffs. Man beachte, dass in dieser Definition der Vorgang der Klassenbildung, die Bestimmung ihrer absoluten und relativen Häufigkeiten, sowie die Visualisierung dieser Häufigkeiten eng verzahnt sind. Dies ist der Konvention geschuldet, dass der Begriff des Histogramms sowohl eine bestimmte Form der Datenanalyse als auch ihre Visualisierung beschreibt.\n\nDefinition 10.2 (Histogramm) Ein Histogramm ist ein Diagramm, in dem zu einem Datensatz \\(y = (y_1,...,y_n)\\) mit Werten \\(W := \\{w_1,...,w_m\\}\\) mit \\(m \\le n\\) für \\(j = 1,...,k\\) über benachbarten Intervallen \\([b_{j-1},b_j[\\) mit \\(b_0 := \\min W\\) und \\(b_k := \\max W\\) Rechtecke mit der Breite \\(d_j := b_j - b_{j-1}\\) und der Höhe \\(h(w)\\) oder \\(r(w)\\) für \\(w \\in [b_{j-1}, b_{j}[\\) abgebildet sind. Dabei werden die Intervalle \\([b_{j-1},b_j[\\) Klassen (engl. bins) genannt.\n\nNatürlich ist das visuelle Erscheinungsbild eines Histogramms stark davon abhängig, wie genau die Werte eines Datensatzes zu Klassen zusammengefasst werden. Der entscheidene Parameter dafür ist nach Definition 10.2 die Anzahl der Klassen \\(k\\), die zwischen dem Minimalwert \\(b_0\\) und dem Maximalwert \\(b_k\\) des Datensatzes gewählt werden. Im Folgenden stellen wir einige konventionelle Werte für die Anzahl der Klassen \\(k\\) vor und berechnen und visualisieren sie am Beispiel des Beispieldatensatzes. Dabei gehen wir durchgängig davon aus, dass die benachbarten Intervalle \\([b_{j-1},b_j[\\) mit \\(j = 1,...,k\\) für die Anzahl von Klassen \\(k\\) mit konstanter Breite \\(d = b_j - b_{j-1}\\) gewählt werden.\nZur Berechnung und Visualisierung von Histogrammen nutzen wir die R Funktion hist(). Bei dieser werden die Klassen \\([b_{j-1}, b_{j}[\\) für \\(j = 1,...,k\\) als das Argument breaks festgelegt. Speziell ist breaks der R Vector c(b_0,b_1,...,b_k) mit Länge \\(k + 1\\). Um den Effekt verschiedener Wahlen der Anzahl der Klassen zu veranschaulichen, lesen wir zunächst den Datensatz ein und bestimmen seine minimalen und maximalen Werte \\(b_0\\) und \\(b_k\\).\n\n# Datensatz\nD       = read.csv(\"./_data/111-deskriptive-statistik.csv\")                     # Laden des Datensatzes\ny       = D$PRE                                                                 # Pre-Interventions-BDI-II-Daten\nb_0     = min(y)                                                                # b_0\nb_k     = max(y)                                                                # b_k\n\n\n\n\nMinimum b_0                :  14 \nMaximum b_k                :  23\n\n\nEine erste Möglichkeit der Wahl der Klassen \\(k\\) besteht darin, möglichst genau eine gewünschte konstante Breite \\(d\\) zu erzielen. Hierzu setzt man \\[\\begin{equation}\nk := \\lceil (b_k - b_0)d \\rceil.\n\\end{equation}\\] Man beachte, dass die Anwendung der Aufrundungsfunktion impliziert, dass die so resultierende Klassenbreite höchstens dem gewünschten Wert entspricht, aber auch kleiner sein kann. Folgender R Code verdeutlich diesen Effekt.\n\n# Histogramm mit gewünschter Klassenbreite d = 2\nd       = 2                                                                     # gewünschte Klassenbreite\nb_0     = min(y)                                                                # b_0\nb_k     = max(y)                                                                # b_k\nk       = ceiling((b_k - b_0)/d)                                                # Anzahl der Klassen\nb       = seq(b_0, b_k, length.out = k + 1)                                     # Klassen [b_{j-1}, b_j[ (breaks)\n\n\n\n\nGewünschte Klassenbreite d :  2 \nKlassenanzahl k            :  5 \nIntervalle [b_j-1, b_j]    :  14 15.8 17.6 19.4 21.2 23 \nErzielte Klassenbreite  d  :  1.8\n\n\nDie Wahl einer gewünschten Klassenbreite von \\(d = 1.5\\) dagegen führt für den vorliegenden Datensatz auf eine resultierende Klassenbreite, die dem Wunsch entspricht.\n\n# Histogramm mit gewünschter Klassenbreite d = 1.5\nd       = 1.5                                                                   # gewünschte Klassenbreite\nk       = ceiling((b_k - b_0)/d)                                                # Anzahl der Klassen\nb       = seq(b_0, b_k, length.out = k + 1)                                     # Klassen [b_{j-1}, b_j[ (breaks)\n\n\n\n\nGewünschte Klassenbreite d :  1.5 \nKlassenanzahl k            :  6 \nIntervalle [b_j-1, b_j]    :  14 15.5 17 18.5 20 21.5 23 \nErzielte Klassenbreite  d  :  1.5\n\n\nEine weitere Möglichkeit ist es, die Anzahl der Klassen abhängig von der Anzahl der Datenpunkte im Datensatz zu machen. Die Excelstandardwahl ist dabei\n\\[\\begin{equation}\nk := \\lceil \\sqrt{n} \\rceil.        \n\\end{equation}\\] Folgender R Code implementiert diese Wahl für den den vorliegenden Datensatz. \n\n# Histogramm mit Excelstandard\nn       = length(y)                                                             # Anzahl Datenpunkte\nk       = ceiling(sqrt(n))                                                      # Anzahl der Klassen\nb       = seq(b_0, b_k, length.out = k + 1)                                     # Klassen [b_{j-1}, b_j[ (breaks)\n\n\n\n\nAnzahl Datenpunkte n       :  100 \nKlassenanzahl k            :  10 \nIntervalle [b_j-1, b_j]    :  14 14.9 15.8 16.7 17.6 18.5 19.4 20.3 21.2 22.1 23 \nErzielte Klassenbreite  d  :  0.9\n\n\nÄhnlich gelagert wie der Excelstandard ist die von Sturges (1926) vorgeschlagene Klassenanzahl, welche die Darstellung unter einer impliziten Normalverteilungsannahme optimiert und durch \\[\\begin{equation}\nk := \\lceil \\log_2 n + 1\\rceil      \n\\end{equation}\\] gegeben ist. Im vorliegenden Fall resultiert hier:\n\n# Klassenanzahl nach Sturges (1926) \nn       = length(y)                                                             # Anzahl Datenpunkte\nk       = ceiling(log2(n)+1)                                                    # Anzahl der Klassen\nb       = seq(b_0, b_k, length.out = k + 1)                                     # Klassen [b_{j-1}, b_j[ (breaks)\n\n\n\n\nAnzahl Datenpunkte n       :  100 \nKlassenanzahl k            :  8 \nIntervalle [b_j-1, b_j]    :  14 15.125 16.25 17.375 18.5 19.625 20.75 21.875 23 \nErzielte Klassenbreite  d  :  1.125\n\n\nEine weitere Möglichkeit ist es, Deskriptivstatistiken des Datensatzes in die Wahl der Klassenanzahl einfließen zu lassen. Als Maß für die Streuung der Daten im Datensatz nutzt die Klassenanzahl nach Scott (1979) die empirische Standardabweichung (cf. Kapitel 10.6), um durch das Histogramm eine Dichteschätzung bei Normalverteilung zu realisieren. Wir wollen den nichtparametrischen Hintergrund dieses Vorgehens hier nicht vertiefen, sondern halten lediglich fest, dass die Wahl der Klassenanzahl nach Scott (1979) durch Festlegung der Klassenbreite zu \\[\\begin{equation}\nd := 3.49 S/\\sqrt[3]{n}             \n\\end{equation}\\] und die anschließende Wahl der Klassenanzahl durch \\[\\begin{equation}\nk := \\lceil (b_k - b_0)d \\rceil\n\\end{equation}\\] gegeben ist, wobei \\(S\\) die Standardabweichung des Datensatzes bezeichnet. Im vorliegenden Fall resultieren hier:\n\n# Klassenanzahl nach Scott (1979) \nn       = length(y)                                                             # Anzahl Datenwerte\nS       = sd(y)                                                                 # Stichprobenstandardabweichung\nd       = ceiling(3.49*S/(n^(1/3)))                                             # Klassenbreite\nk       = ceiling((b_k - b_0)/d)                                                # Anzahl der Klassen\nb       = seq(b_0, b_k, length.out = k + 1)                                     # Klassen [b_{j-1}, b_j[ (breaks)\n\n\n\n\nAnzahl Datenpunkte n       :  100 \nStandardabweichung         :  1.740167 \nKlassenbreite nach Scott d :  2 \nKlassenanzahl k            :  5 \nIntervalle [b_j-1, b_j]    :  14 15.8 17.6 19.4 21.2 23 \nErzielte Klassenbreite  d  :  1.8\n\n\nPer Default nutzt die in R implementierte hist() Funktion eine Modifikation der von Sturges (1926) vorgeschlagenen Klassenanzahl und bietet eine Reihe von weiteren Möglichkeiten zur Wahl der Klassen. Einen Überblick dazu liefert ?hist. Folgender R Code visualisiert die absoluten Häufigkeiten der Werte des Beispieldatensatzes anhand der oben spezifierten Klassen.\n\n# Abbildungsparameter\npdf(                                                                            # PDF Kopiefunktion\nfile        = \"./_figures/111-histogramme.pdf\",                                 # Dateiname\nwidth       = 6,                                                                # Breite (inch)\nheight      = 9)                                                                # Höhe (inch)\npar(                                                                            # Abbildungsparameter    \nmfcol       = c(3,2),                                                           # Zeilen und Spaltenlayout \nlas         = 1,                                                                # x-Tick Orientierung\ncex         = .7,                                                               # Annotationsvergrößerung\ncex.main    = .9)                                                               # Titelvergrößerung\nx_min       = 12                                                                # x-Achsenlimit\nx_max       = 25                                                                # x-Achsenlimit\ny_min       = 0                                                                 # y-Achsenlimit\ny_max       = 45                                                                # y-Achsenlimit\nlabel       = c(\"\", \"\", \"Excel\", \"Sturges\", \"Scott\")                            # Titel\n\n# Histogramme mit vordefinierter Klassenanzahl und Klassenbreite\nfor(i in 1:5){\n    hist(                                                                       # Histogramm\n    y,                                                                          # Datensatz\n    breaks  = bs[[i]],                                                          # breaks Argument\n    xlim    = c(x_min, x_max),                                                  # x-Achsenlimits\n    ylim    = c(y_min, y_max),                                                  # y-Achsenlimits\n    ylab    = \"Häufigkeit\",                                                     # y-Achsenbezeichnung\n    xlab    = \"BDI\",                                                            # x-Achsenbezeichnung\n    main    = sprintf(paste(label[i], \"k = %.0f, d = %.2f\"), ks[i],ds[i]))      # Titel\n}\n\n# R Default Histogramm\nhist(                                                                           # Histogramm\ny,                                                                              # Datensatz\nxlim    = c(x_min, x_max),                                                      # x-Achsenlimits\nylim    = c(y_min, y_max),                                                      # y-Achsenlimits\nylab    = \"Häufigkeit\",                                                         # y-Achsenbezeichnung\nxlab    = \"\",                                                                   # x-Achsenbezeichnung\nmain    = \"R Default\")                                                          # Titel\nmtext(LETTERS[6], adj=0, line=2, cex = 1.2, at = 8)                             # Subplot Buchs\ndev.off()                                                                       # Beenden der Abbildungsbearbeitung\n\n\n\n\n\n\n\nAbbildung 10.2: Histogramme der absoluten Häufigkeiten der Pre-Interventions-BDI-II-Werte. Man beachte, dass der qualitative Eindruck, dass im Datensatz Werte um 18 gehäuft auftreten und niedrigere und höhere Werte seltener sind, über alle Wahlen der Klassenanzahl konstant ist. Das genaue quantitative Erscheinungsbild ist dagegen von der exakten Wahl der Klassenanzahl und Klassenbreite abhängig.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Deskriptivstatistiken</span>"
    ]
  },
  {
    "objectID": "110-Deskriptivstatistiken.html#sec-empirische-verteilungsfunktionen",
    "href": "110-Deskriptivstatistiken.html#sec-empirische-verteilungsfunktionen",
    "title": "10  Deskriptivstatistiken",
    "section": "10.3 Empirische Verteilungsfunktionen",
    "text": "10.3 Empirische Verteilungsfunktionen\nNeben der Frage, wie häufig in einem Datensatz bestimmte Werte oder Klassen von Werten vorkommen, kann es interessant sein, zu untersuchen, wie häufig Werte vorkommen, die kleiner als ein bestimmter Wert sind. Obwohl die Antwort auf diese Frage natürlich in den absoluten und relativen Häufigkeiten bzw. den entsprechenden Histogrammen implizit ist, ist es manchmal von Vorteil, diese Information direkt verfügbar zu haben. Die in diesem Abschnitt eingeführten Begriffe der kumulativen absoluten und relativen Häufigkeitsverteilung sowie der empirischen Verteilungsfunktion bezeichnen die entsprechenden Analoga zu den in Kapitel 10.1 und Kapitel 10.2 diskutierten Begriffen. Wir beginnen mit der Definition der kumulativen absoluten und kumulativen relativen Häufigkeitsverteilungen.\n\nDefinition 10.3 (Kumulative absolute und relative Häufigkeitsverteilungen) \\(y = (y_1,...,y_n)\\) sei ein Datensatz, \\(W := \\{w_1,...,w_k\\}\\) mit \\(k \\le n\\) die im Datensatz vorkommenden Zahlenwerte und \\(h\\) und \\(r\\) die absoluten und relativen Häufigkeitsverteilungen der Werte von \\(y\\), respektive. Dann heißt die Funktion \\[\\begin{equation}\nH : W \\to \\mathbb{N}, w \\mapsto H(w) := \\sum_{w' \\le w} h(w')\n\\end{equation}\\] die kumulative absolute Häufigkeitsverteilung von \\(y\\) und die Funktion \\[\\begin{equation}\nR : W \\to [0,1], w \\mapsto R(w) : =\\sum_{w' \\le w} r(w')\n\\end{equation}\\] die kumulative relative Häufigkeitsverteilung der Zahlwerte von \\(y\\).\n\nIm Sinne der Definition Definition 10.1 gilt für die kumulativen Häufigkeitsverteilungen also \\[\\begin{equation}\nH(w) = \\mbox{Anzahl der } y_i \\mbox{ aus } y \\mbox{ mit } y_i \\le w\n\\end{equation}\\] und \\[\\begin{equation}\nR(w) = \\mbox{Anzahl der } y_i \\mbox{ aus } y \\mbox{ mit } y_i \\le w \\mbox{ geteilt durch } n.\n\\end{equation}\\]\nIn R erlaubt die Funktion cumsum() die Berechnung kumulativer Summen und ermöglicht damit die Auswertung kumulativer Häufigkeitsverteilungen, wie folgender R Code anhand des Beispieldatensatzes demonstriert.\n\nD           = read.csv(\"./_data/111-deskriptive-statistik.csv\")                 # Laden des Datensatzes\ny           = D$PRE                                                             # Pre-Interventions-BDI-II-Daten\nn           = length(y)                                                         # Anzahl der Datenwerte\nH           = as.data.frame(table(y))                                           # absolute Häufigkeitsverteilung als Dataframe\nnames(H)    = c(\"w\", \"h\")                                                       # Spaltenbenennung\nH$H         = cumsum(H$h)                                                       # kumulative absolute Häufigkeitsverteilung\nH$r         = H$h/n                                                             # relative Häufigkeitsverteilung\nH$R         = cumsum(H$r)                                                       # kumulative relative Häufigkeitsverteilung\nprint(H, digits = 1)\n\n    w  h   H    r    R\n1  14  1   1 0.01 0.01\n2  15  3   4 0.03 0.04\n3  16  6  10 0.06 0.10\n4  17 17  27 0.17 0.27\n5  18 21  48 0.21 0.48\n6  19 20  68 0.20 0.68\n7  20 17  85 0.17 0.85\n8  21 12  97 0.12 0.97\n9  22  2  99 0.02 0.99\n10 23  1 100 0.01 1.00\n\n\nMan beachte dabei insbesondere die Gegenüberstellung von h und H sowie von r und R, die die kumulativen Summen in Definition 10.3 verdeutlichen. Folgender R Code ermöglicht die Darstellung der so bestimmten kumulativen Häufigkeitsverteilungen wie in Abbildung 10.3 gezeigt. Offensichtlich ist die absolute und die relative Häufigkeit von Werten kleiner als der kleinste Wert im Datensatz Null. Die absolute und relative Häufigkeit von Werten kleiner als der größte Wert im Datensatz ist dagegen \\(n\\) bzw. \\(n/n = 1\\).\n\npdf(                                                                            # PDF Kopiefunktion\nfile        = \"./_figures/111-kumulative-haeufigkeitsverteilungen.pdf\",         # Dateiname\nwidth       = 10,                                                               # Breite (inch)\nheight      = 5)                                                                # Höhe (inch)\nH           = H$H                                                               # H(w) Werte\nR           = H$R                                                               # R(w) Werte\nnames(Hw)   = H$w                                                               # barplot braucht w Werte als names\nnames(Rw)   = H$w                                                               # barplot braucht w Werte als names\npar(                                                                            # Abbildungsparameter    \nmfcol   = c(1,2),                                                               # Zeilen und Spaltenlayout \nlas     = 1,                                                                    # x-Tick Orientierung\ncex     = 1)                                                                    # Annotationsvergrößerung\nbarplot(                                                                        # Balkendiagramm\nH,                                                                              # H(w) Werte\ncol         = \"gray90\",                                                         # Balkenfarbe\nxlab        = \"w\",                                                              # x-Achsenbeschriftung\nylab        = \"H(w)\",                                                           # y-Achsenbeschriftung\nylim        = c(0,110),                                                         # y-Achsenlimits\nlas         = 1,                                                                # Achsenticklabelorientierung \nmain        = \"Absolute kumulative Häufigkeit PRE\")                             # Titel\nbarplot(                                                                        # Balkendiagramm\nR,                                                                              # R(w) Werte\ncol         = \"gray90\",                                                         # Balkenfarbe\nxlab        = \"w\",                                                              # x-Achsenbeschriftung\nylab        = \"R(w)\",                                                           # y-Achsenbeschriftung\nylim        = c(0,1),                                                           # y-Achsenlimits\nlas         = 1,                                                                # Achsenticklabelorientierung \nmain        = \"Relative kumulative Häufigkeit PRE\")                             # Titel\ndev.off()                                                                       # Beenden der Abbildungsbearbeitung\n\n\n\n\n\n\n\nAbbildung 10.3: Absolute und relative kumulative Häufigkeitsverteilungen der Pre-Interventions-BDI-II-Werte .\n\n\n\nDas kumulative Analogon zu einem Histogramm ist die empirische Verteilungsfunktion. Im Unterschied zu einem Histogramm ist für die Bestimmung einer empirischen Verteilungsfunktion allerdings keine Definition von Klassen erforderlich. An die Stelle der Intervalleinteilung der reellen Zahlen zwischen dem Minimum und dem Maximum eines Datensatz treten bei der empirischen Verteilungsfunktion einfach die reellen Zahlen, wie folgende Definition verdeutlicht.\n\nDefinition 10.4 (Empirische Verteilungsfunktion) \\(y = (y_1,...,y_n)\\) sei ein Datensatz. Dann heißt die Funktion \\[\\begin{equation}\nF : \\mathbb{R} \\to [0,1], x \\mapsto F(x)\n:= \\frac{\\mbox{Anzahl der } y_i \\mbox{ aus } y \\mbox{ mit } y_i \\le x}{n}\n\\end{equation}\\] die empirische Verteilungsfunktion (EVF) von \\(y\\).\n\nDie empirische Verteilungsfunktion wird manchmal auch die empirische kumulative Verteilungsfunktion genannt. Allgemein beschränkt man sich bei der empirischen Verteilungsfunktion wie in Definition 10.4 auf die Angabe der relativen Häufigkeiten. Natürlichweise kommen in Datensätzen nicht alle reellen Zahlen vor. Dies impliziert, dass die relative Häufigkeit, die reellen Zahlen zugeordnet wird, die zwischen zwei benachbarten Werten des Datensatzes liegen, derjenigen des kleineren der beiden Werte entspricht. Typischerweise sind empirische Verteilungsfunktionen also Treppenfunktionen. In R kann die Funktion ecdf() zur Evaluation und Visualisierung der empirischen Verteilungsfunktion eingesetzt werden, wie folgender R Code anhand des Beispieldatensatzes demonstriert und in Abbildung 10.4 visualisiert ist. Man liest aus Abbildung 10.4 zum Beispiel ab, dass die relative Häufigkeit von Werten kleiner als 17.9 etwa 0.3 ist. Die gleiche relative Häufigkeit haben Werte kleiner als 17.8.\n\nD       = read.csv(\"./_data/111-deskriptive-statistik.csv\")                     # Laden des Datensatzes\ny       = D$PRE                                                                 # Pre-Interventions-BDI-II-Daten \nevf     = ecdf(y)                                                               # Evaluation der EVF\npdf(                                                                            # PDF Kopiefunktion\nfile    = \"./_figures/111-ecdf.pdf\",                                            # Dateiname\nwidth   = 8,                                                                    # Breite (inch)\nheight  = 5)                                                                    # Höhe (inch)\nplot(                                                                           # plot() weiß mit ecdf object umzugehen\nevf,                                                                            # ecdf Objekt\nxlab    = \"BDI\",                                                                # x-Achsenbeschriftung\nylab    = \"Relative Häufigkeit\",                                                # y-Achsenbeschriftung\nmain    = \"Empirische Verteilungsfunktion\")                                     # Titel\ndev.off()                                                                       # Beenden der Abbildungsbearbeitung      \n\n\n\n\n\n\n\nAbbildung 10.4: Empirische Verteilungsfunktion der PRE Werte.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Deskriptivstatistiken</span>"
    ]
  },
  {
    "objectID": "110-Deskriptivstatistiken.html#sec-quantile-und-boxplots",
    "href": "110-Deskriptivstatistiken.html#sec-quantile-und-boxplots",
    "title": "10  Deskriptivstatistiken",
    "section": "10.4 Quantile und Boxplots",
    "text": "10.4 Quantile und Boxplots\nDie empirische Verteilungsfunktion gibt eine zumindest graphisch dargestellte Antwort auf die Frage, wie groß der relative Anteil der Werte eines Datensatzes ist, die kleiner als ein gegebener Wert sind. Die in diesem Abschnitt eingeführten Quantile können als Inversion dieser Datensatzinterrogation verstanden werden. Speziell gibt man bei Quantilen einen relativen Anteil \\(0 \\le p \\le 1\\) der Werte eines Datensatzes vor und fragt, welcher Wert des Datensatzes so beschaffen ist, dass eben \\(p \\cdot 100 \\%\\) der Werte des Datensatzes kleiner als oder gleich diesem Wert sind. Wir nutzen folgende Definition des Begriffs des \\(p\\)-Quantils.\n\nDefinition 10.5 (\\(p\\)-Quantil) \\(y = (y_1,...,y_n)\\) sei ein Datensatz und \\[\\begin{equation}\ny_s = \\left(y_{(1)}, y_{(2)}, ...,y_{(n)}\\right) \\mbox{ mit }\n\\min_{1 \\le i \\le n} y_i = y_{(1)} \\le y_{(2)} \\le \\cdots \\le y_{(n)} = \\max_{1 \\le i \\le n} y_i\n\\end{equation}\\] sei der zugehörige aufsteigend sortierte Datensatz. Weiterhin bezeichne \\(\\lfloor \\cdot \\rfloor\\) die Abrundungsfunktion. Dann heißt für ein \\(p \\in [0,1]\\) die Zahl \\[\\begin{equation}\ny_p :=\n\\begin{cases}\ny_{(\\lfloor np + 1 \\rfloor)} & \\mbox{ falls } np \\neq \\mathbb{N} \\\\\n\\frac{1}{2}\\left(y_{(np)} +  y_{(np + 1)}\\right) & \\mbox{ falls } np \\in \\mathbb{N} \\\\\n\\end{cases}\n\\end{equation}\\] das \\(p\\)-Quantil des Datensatzes \\(y\\).\n\nMan beachte, dass das \\(p\\)-Quantil \\(y_p\\) nach Definition 10.5 entweder ein Wert des Datensatzes oder ein Wert zwischen zwei Werten des Datensatzes ist. Nach Konstruktion von \\(y_p\\) gilt dabei insbesondere, dass mindestens \\(p \\cdot 100\\%\\) aller Werte des Datensatzes kleiner oder gleich \\(y_p\\) und mindestens \\((1-p) \\cdot 100\\%\\) aller Werte des Datensatzes größer als \\(y_p\\) sind. Das \\(p\\)-Quantil teilt den geordneten Datensatz \\(y_s\\) also im Verhältnis \\(p\\) zu \\((1-p)\\) auf. Je nach Wahl von \\(p\\) erhalten die \\(p\\)-Quantile spezielle Bezeichnungen:\n\n\\(y_{0.25}, y_{0.50}, y_{0.75}\\) heißen unteres Quartil, Median und oberes Quartil, respektive,\n\\(y_{j\\cdot 0.10}\\) für \\(j = 1,...,9\\) heißen Dezile und\n\\(y_{j\\cdot 0.01}\\) für \\(j = 1,...,99\\) heißen Percentile.\n\nUm den Begriff des \\(p\\)-Quantils zu verdeutlichen, betrachten wir ein an Henze (2018), Kapitel 5 orientiertes Beispiel. Dazu seien zunächst der in untenstehender Tabelle dargestellte Datensatz und seine aufsteigend sortierte Version gegeben.\nWir wollen als erstes das untere Quartil, also das \\(0.25\\)-Quantil, dieses Datensatzes bestimmen. Es ist offenbar \\(n = 10\\) und wir haben \\(p := 0.25\\) gewählt. Basierend auf Definition 10.5 finden wir, dass \\[\\begin{equation}\nnp = 10 \\cdot 0.25 = 2.5 \\notin \\mathbb{N}.\n\\end{equation}\\] Also gilt nach Definition 10.5, dass \\[\\begin{equation}\ny_{0.25} = x_{(\\lfloor 2.5 + 1\\rfloor)} = x_{(3)} = 3.0.\n\\end{equation}\\] Das untere Quartil des betrachteten Datensatzes ist also der Wert \\(3.0\\) des Datensatzes.\nAls zweites wollen wir das \\(0.80\\)-Quantil des Datensatzes bestimmten. Es ist offenbar weiterhin \\(n = 10\\) und wir haben nun \\(p := 0.80\\) gewählt. Wir finden in diesem Fall, dass\n\\[\\begin{equation}\nnp = 10 \\cdot 0.80 = 8 \\in \\mathbb{N}.\n\\end{equation}\\] Also folgt nach Definition 10.5 \\[\\begin{equation}\ny_{0.80}\n= \\frac{1}{2} \\left(y_{(8)} + y_{(8 + 1)}\\right)\n= \\frac{1}{2}\\left(y_{(8)} + y_{(9)}\\right)\n= \\frac{8.5 + 9.0}{2}\n= 8.75.\n\\end{equation}\\] Das \\(y_{0.80}\\)-Quantil des betrachteten Datensatzes ist mit \\(8.75\\) also der Wert zwischen den Werten \\(8.5\\) und \\(9\\) des Datensatzes. In R wertet man die so bestimmten Quantile wie folgt aus.\n\ny   = c(8.5,1.5,12,4.5,6.0,3.0,3.0,2.5,6.0,9.0)                                 # Beispieldaten\nn   = length(y)                                                                 # Anzahl Datenwerte\ny_s = sort(y)                                                                   # sortierter Datensatz\np   = 0.25                                                                      # np \\notin \\mathbb{N}\ny_p = y_s[floor(n*p + 1)]                                                       # 0.25 Quantil\ncat(\"0.25 Quantil: \", round(y_p,2))                                             # Ausgabe\n\n0.25 Quantil:  3\n\np   = 0.80                                                                      # np \\in \\mathbb{N}\ny_p = (1/2)*(y_s[n*p] + y_s[n*p + 1])                                           # 0.80 Quantil\ncat(\"0.80 Quantil: \", round(y_p,2))                                             # Ausgabe\n\n0.80 Quantil:  8.75\n\n\nMit quantile() stellt R auch eine Funktion bereit, die die Quantilbestimmung eines Datensatzes automatisiert. Dabei ist allerdings zu beachten, dass die in Definition 10.5 gegebene Definition des Quantilsbegriffs nur eine von mindestens neun verschiedenen Quantildefinitionen ist und die genauen Werte sich von Definition zu Definition unterscheiden können. Die spezielle Quantildefinition, die von quantile() genutzt wird, wird über das Funktionsargument type ausgewählt. Auf theoretischer Ebene geben Hyndman & Fan (1996) dazu einen Überblick, auf Implementationsebene ?quantile. Wir betrachten exemplarisch die Bestimmung des \\(0.25\\)-Quantils Pre-Interventions BDI Beispieldatensatzes anhand von type = 8.\n\nD       = read.csv(\"./_data/111-deskriptive-statistik.csv\")                     # Laden des Datensatzes\ny       = D$PRE                                                                 # Pre-Interventions-BDI-II-Daten \ny_p     = quantile(y, 0.25, type = 8)                                           # 0.25 Quantil, Definition 1\n\n\n\n0.25 Quantil (Type = 8):  17\n\n\nEin Boxplot schließlich visualisiert eine Quantil-basierte Zusammenfassung eines Datensatzes, wobei typischerweise das Minimum und das Maximum eines Datensatzes sowie das untere Quartil, der Median und das obere Quartil visualisiert werden. In R erstellt man einen Boxplot mithilfe der boxplot() Funktion, wie folgender R Code anhand des Pre-Interventions-BDI-II-Datensatzes demonstriert.\n\nD           = read.csv(\"./_data/111-deskriptive-statistik.csv\")                 # Laden des Datensatzes\npdf(                                                                            # PDF Kopiefunktion\nfile        = \"./_figures/111-boxplot.pdf\",                                     # Dateiname\nwidth       = 8,                                                                # Breite (inch)\nheight      = 5)                                                                # Höhe (inch) \nboxplot(                                                                        # Boxplot\nD$PRE,                                                                          # Datensatz\nhorizontal  = T,                                                                # horizontale Darstellung\nrange       = 0,                                                                # Whiskers bis zu min y und max y\nxlab        = \"BDI\",                                                            # x Achsenbeschriftung\nmain        = \"Boxplot\")                                                        # Titel\ndev.off()                                                                       # Beenden der Abbildungsbearbeitung\n\n\n\n\n\n\n\nAbbildung 10.5: Boxplot der Pre-Interventions-BDI-Werte des Beispieldatensatzes.\n\n\n\nIn Abbildung 10.5 werden \\(\\min y\\) und \\(\\max y\\) als sogenannte “Whiskerendpunkte” dargestellt, \\(y_{0.25}\\) und \\(y_{0.75}\\) bilden die unteren und oberen Grenze der zentralen grauen Box und der Median \\(y_{0.50}\\) wird als Strich in der zentralen grauen Box abgebildet. Auch hier gilt, dass es sehr viele Boxplotvariationen gibt, vgl. McGill et al. (1978). Eine genaue Erläuterung der graphisch dargestellten Datensatzeigenschaften ist also folglich immer nötig. Schließlich sei angemerkt, dass die Darstellung von Datensätzes mithilfe von Boxplots in den letzten zwanzig Jahren etwas aus der Mode gekommen ist und man sie nur noch sehr selten in aktuellen wissenschaftlichen Publikationen findet.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Deskriptivstatistiken</span>"
    ]
  },
  {
    "objectID": "110-Deskriptivstatistiken.html#sec-masse-der-zentralen-tendenz",
    "href": "110-Deskriptivstatistiken.html#sec-masse-der-zentralen-tendenz",
    "title": "10  Deskriptivstatistiken",
    "section": "10.5 Maße der Zentralen Tendenz",
    "text": "10.5 Maße der Zentralen Tendenz\nIn diesem Abschnitt betrachten wir einige grundlegende Kennzahlen von Datensätzen, die für die deskriptive Statistik zentral sind und Aussagen zu dem “durchschnittlichen” Wert eines Datensatzes machen. Wir beginnen mit der Definition des Mittelwerts.\n\n10.5.1 Mittelwert\n\nDefinition 10.6 (Mittelwert) \\(y = (y_1,...,y_n)\\) sei ein Datensatz. Dann heißt \\[\\begin{equation}\n\\bar{y} := \\frac{1}{n}\\sum_{i=1}^n y_i\n\\end{equation}\\] der Mittelwert von \\(y\\).\n\nDer Mittelwert eines Datensatzes ist also die Summe der Datenwerte geteilt durch die Anzahl der Datenwerte. Der hier definierte Mittelwert wir auch als arithmetisches Mittel oder Durchschnitt bezeichnet. Im Kontext der Frequentistischen Inferenz werden wir den Mittelwert vor dem Hintergrund des Modells der Zufallsstichprobe als Stichprobenmittel bezeichnen. Insbesondere erlauben die Modelle der Frequentistischen Inferenz auch, die funktionale Form des Mittelwertes tiefer zu begründen, wie wir an gegebener Stelle sehen werden. Mithilfe der Summenfunktion berechnet man den Mittelwert eines Datensatzes in R wie folgt.\n\nD       = read.csv(\"./_data/111-deskriptive-statistik.csv\")                     # Laden des Datensatzes\ny       = D$PRE                                                                 # Pre-Interventions-BDI-II-Daten\nn       = length(y)                                                             # Anzahl der Werte\ny_bar   = (1/n)*sum(y)                                                          # Mittelwertsberechnung\n\n\n\nMittelwert der Pre-BDI-Interventions Daten  18.61\n\n\nMit der mean() Funktion stellt R natürlich auch eine Funktion zur automatisierten Berechnung des Mittelwerts bereit.\n\ncat(\"Mittelwert der PRE-BDI-II-Daten \", mean(y))                                   # Ausgabe\n\nMittelwert der PRE-BDI-II-Daten  18.61\n\n\nIn folgendem Theorem halten wir zunächst zwei Eigenschaften des Mittelwerts eines Datensatzes fest, die seine Einordung als Maß der zentralen Tendenz eines Datensatzes plausibel machen.\n\nTheorem 10.1 (Zentralitätseigenschaften des Mittelwerts) \\(y = (y_1,...,y_n)\\) sei ein Datensatz und \\(\\bar{y}\\) sei der Mittelwert von \\(y\\). Dann gelten\n\nDie Summe der Abweichungen vom Mittelwert ist Null, \\[\\begin{equation}\n\\sum_{i=1}^n (y_i - \\bar{y}) = 0.\n\\end{equation}\\]\nDie absoluten Summen negativer und positiver Abweichungen vom Mittelwert sind gleich, d.h. wenn \\(j = 1,...,n_j\\) die Datenpunktindizes mit \\((y_j - \\bar{y}) &lt; 0\\) und \\(k = 1,...,n_k\\) die Datenpunktindizes mit \\((y_k - \\bar{y}) \\ge\\) bezeichnen, dann gilt mit \\(n_j + n_k\\), dass \\[\\begin{equation}\n\\vert\\sum_{j = 1}^{n_j} (y_j - \\bar{y})\\vert = \\vert\\sum_{k = 1}^{n_k} (y_k - \\bar{y})\\vert.\n\\end{equation}\\]\n\n\n\nBeweis. (1) Es gilt \\[\\begin{align}\n\\begin{split}\n\\sum_{i=1}^n (y_i - \\bar{y})\n& = \\sum_{i=1}^n y_i - \\sum_{i=1}^n \\bar{y}                 \\\\\n& = \\sum_{i=1}^n y_i - n\\bar{y}                             \\\\\n& = \\sum_{i=1}^n y_i - \\frac{n}{n}\\sum_{i=1}^n y_i          \\\\\n& = \\sum_{i=1}^n y_i - \\sum_{i=1}^n y_i                     \\\\\n& = 0.\n\\end{split}\n\\end{align}\\]\n(2) Seien \\(j = 1,...,n_j\\) die Indizes mit \\((y_j - \\bar{y}) &lt; 0\\) und \\(k = 1,...,n_k\\) die Indizes mit \\((y_k - \\bar{y}) \\ge 0\\), so dass \\(n = n_j + n_k\\). Dann gilt \\[\\begin{align}\n\\begin{split}\n\\sum_{i=1}^n (y_i - \\bar{y}) & = 0 \\\\\n\\Leftrightarrow\n\\sum_{j=1}^{n_j} (y_j - \\bar{y})  + \\sum_{k=1}^{n_k} (y_k - \\bar{y}) & = 0 \\\\\n\\Leftrightarrow\n\\sum_{k=1}^{n_k} (y_k - \\bar{y})  & =  - \\sum_{j=1}^{n_j} (y_j - \\bar{y}) \\\\   \n\\Leftrightarrow\n\\vert\\sum_{j = 1}^{n_j} (y_j - \\bar{y})\\vert & = \\vert\\sum_{k = 1}^{n_k} (y_k - \\bar{y})\\vert.\n\\end{split}\n\\end{align}\\]\n\nNach Theorem 10.1 liegt der Mittelwert eines Datensatzes also gerade so, dass sich in Summe die Abweichungen der Datenpunkte von diesem Wert zu Null ausgleichen und dass sich positive und negative Abweichungen vom Mittelwert die Waage halten. In R kann man Theorem 10.1 am Beispieldatensatz wie folgt verifizieren.\n\n# Datensatz\nD       = read.csv(\"./_data/111-deskriptive-statistik.csv\")                     # Laden des Datensatzes\ny       = D$PRE                                                                 # Pre-Interventions-BDI-II-Daten\ns       = sum(y - mean(y))                                                      # Summe der Abweichungen vom Mittelwert\ns_1     = sum(y[y &lt;= mean(y)] - mean(y))                                        # Summe aller negativen Abweichungen\ns_2     = sum(y[y &gt; mean(y)]  - mean(y))                                        # Summe aller positiven Abweichungen\n\n\n\nSumme der Abweichungen vom Mittelwert           :  0 \nSummen der positiven und negativen Abweichungen :  -71.28 71.28\n\n\n\nTheorem 10.2 (Summationsseigenschaften des Mittelwerts) Gegeben seien zwei Datensätze \\(y = (y_1,...,y_n)\\) und \\(z = (z_1,...,z_n)\\) gleichen Umfangs und es sei durch \\[\\begin{equation}\ny + z := (y_1 + z_1,...,y_n + z_n)\n\\end{equation}\\] die Summe dieser Datensätze definiert. Weiterhin seien \\(\\bar{y}\\) und \\(\\bar{z}\\) die Mittelwerte der Datensätze \\(y\\) und \\(z\\), respektive. Dann gilt \\[\\begin{equation}\n\\overline{y + z} = \\bar{y} + \\bar{z}\n\\end{equation}\\]\n\n\nBeweis. Es gilt \\[\\begin{align}\n\\begin{split}\n\\overline{y + z}\n& := \\frac{1}{n}\\sum_{i=1}^n (y_i + z_i)                            \\\\\n& = \\frac{1}{n}\\sum_{i = 1}^n y_i + \\frac{1}{n}\\sum_{i = 1}^n z_i   \\\\\n& =: \\bar{y} + \\bar{z}.\n\\end{split}\n\\end{align}\\]\n\nBetrachtet man die Summe zweier Datensätze, so ist es für die Bestimmung ihres Mittelwerts also unerheblich, ob man zunächst die Datensätze aufaddiert und dann den Mittelwert des summierten Datensatzes bestimmt oder ob man für jeden der beiden Datensätze den Mittelwert bestimmt und diese aufaddiert. Wir verifizieren dieses Ergebnis exemplarisch mithilfe der Pre- und Post-Interventions BDI-II-Daten des Beispieldatensatzes in R wie folgt.\n\nD           = read.csv(\"./_data/111-deskriptive-statistik.csv\")                 # Laden des Datensatzes\ny           = D$PRE                                                             # Pre-Interventions-BDI-II-Daten\ny_bar       = mean(y)                                                           # Mittelwert der Pre-Interventions-BDI-II-Daten\nz           = D$POS                                                             # Post-Interventions BDI-II-Daten\nz_bar       = mean(z)                                                           # Mittelwert der Post-Interventions BDI-II-Daten\nyz_bar_1    = mean(y + z)                                                       # Mittelwert der summierten Pre- und Post-Daten \nyz_bar_2    = y_bar + z_bar                                                     # Summe der Mittelwerte der Pre- und Post-Date\n\n\n\nMittelwert von z                  :  31.68 \nSumme der Mittelwerte von x und y :  31.68\n\n\n\nTheorem 10.3 (Mittelwert bei linear-affiner Transformation) \\(y = (y_1,...,y_n)\\) sei ein Datensatz und \\(\\bar{y}\\) sei der Mittelwert von \\(y\\). Weiterhin sei für \\(a,b \\in \\mathbb{R}\\) mit \\[\\begin{equation}\nay + b := (ay_1 + b,...,ay_n + b)\n\\end{equation}\\] ein linear-affin transformierter Datensatz von \\(y\\) definiert. Dann gilt, dass \\[\\begin{equation}\n\\overline{ay + b} = a\\bar{y} + b\n\\end{equation}\\]\n\n\nBeweis. Es gilt \\[\\begin{align}\n\\begin{split}\n\\overline{ay + b}\n& := \\frac{1}{n}\\sum_{i=1}^n (ay_i + b) \\\\\n& = \\sum_{i=1}^n \\left(\\frac{1}{n}ay_i + \\frac{1}{n}b\\right) \\\\\n& = \\left(\\sum_{i=1}^n \\frac{1}{n}ay_i\\right) + \\left(\\sum_{i=1}^n \\frac{1}{n}b\\right) \\\\\n& = a\\left(\\frac{1}{n}\\sum_{i=1}^n y_i\\right) + \\frac{1}{n} \\sum_{i=1}^n b \\\\\n& = a\\bar{y} + \\frac{1}{n} nb \\\\\n& = a\\bar{y} + b.\n\\end{split}\n\\end{align}\\]\n\nEine linear-affine Transformation eines Datensatz transformiert den Mittelwert des Datensatzes also linear-affin. Dies kann man sich zunutze machen, wenn man sich überlegt, wie der Mittelwert lauten sollte, wenn man einen Datensatz neu skaliert, also z.B. von Gramm in Kilogramm umrechnet. Natürlich kann man zur Sicherheit aber auch einfach den Mittelwert des neu skalierten Datensatzes bestimmen. Wir demonstrieren diese Eigenschaft exemplarisch durch Reskalierung des Pre-Interventions-BDI-II-Datensatzes mit dem Wert \\(a = 2\\) unter gleichzeitiger Addition der Konstante \\(b := 5\\).\n\nD           = read.csv(\"./_data/111-deskriptive-statistik.csv\")                 # Laden des Datensatzes\ny           = D$PRE                                                             # Pre-Interventions-BDI-II-Daten\ny_bar       = mean(y)                                                           # Mittelwert der Pre-Interventions-BDI-II-Daten\na           = 2                                                                 # Multiplikationskonstante\nb           = 5                                                                 # Additionskonstante\nz           = a*y + b                                                           # linear-affine Transformation der PRE Daten\nz_bar       = mean(z)                                                           # Mittelwert der transfomierten PRE Daten\nay_bar_b    = a*y_bar + b                                                       # Transformation des PRE Daten Mittelwerts\n\n\n\nMittelwert des transformierten Datensatzes :  42.22 \nTransformation des Mittelwertes            : 42.22\n\n\n\n\n10.5.2 Median\nIn den oben definierten Mittelwert geht jeder Wert eines Datensatzes summativ mit gleichem Gewicht \\(1/n\\) ein. Dies betrifft insbesondere auch solche Werte, die im Vergleich zu den übrigen Werten des Datensatzes sehr untypisch sind. Eine Möglichkeit, die “Mitte” eines Datensatzes unabhängig von solchen untypischen Werten zu bestimmen, besteht in der Bestimmung seines Medians, den wir nachfolgend definieren. Wir haben den Median bereits in Abschnitt Kapitel 10.4 als das 0.5-Quantil kennengelernt, mit dem er identisch ist.\n\nDefinition 10.7 (Median) \\(y = (y_1,...,y_n)\\) sei ein Datensatz und \\(y_{s} = (y_{(1)},...,y_{(n)})\\) der zugehörige aufsteigend sortierte Datensatz. Dann ist der Median von \\(y\\) definiert als \\[\\begin{equation}\n\\tilde{y} :=\n\\begin{cases}\ny_{((n+1)/2)}\n& \\mbox{ falls } n \\mbox{ ungerade},\n\\\\\n\\frac{1}{2}\\left(y_{(n/2)} + y_{(n/2 + 1)}\\right)\n& \\mbox{ falls } n \\mbox{ gerade}.\n\\end{cases}\n\\end{equation}\\]\n\nDefinition 10.7 impliziert, dass mindestens 50% aller Datenwerte \\(y_i\\) des Datensatzes kleiner oder gleich \\(\\tilde{y}\\) sind und dass gleichzeitig mindestens 50% aller Datenwerte \\(y_i\\) des Datensatzes größer oder gleich \\(\\tilde{y}\\) sind. Anstelle eines formalen Beweises dieser Aussagen verweisen wir auf Abbildung 10.6. Wie in der Definition des Medians unterscheiden wir dort die Fälle, dass die Anzahl der Datenpunkte ungerade ist (hier \\(n = 5\\), Abbildung 10.6 A) oder gerade ist (\\(n = 6\\), Abbildung 10.6 B). Im Fall der ungeraden Anzahl an Datenpunkten ist der Median der Datenwert mit der Indexzahl, die in der Mitte der Indexzahlen des sortierten Datensatzes liegt, im Beispiel Abbildung 10.6 A also der Wert \\(y_{(3)}\\). Damit sind im vorliegenden Fall \\(3/5\\), also \\(60\\%\\) der Datenwerte (speziell \\(y_{(1)}\\), \\(y_{(2)}\\) und \\(y_{(3)}\\)) kleiner oder gleich dem Median, gleichzeitig aber auch \\(3/5\\), also \\(60\\%\\) der Datenwerte (speziell \\(y_{(3)}\\), \\(y_{(4)}\\) und \\(y_{(5)}\\)) größer oder gleich dem Median. Im Fall einer geraden Anzahl von Datenpunkten ist die Lage des Medians etwas intuitiver: Der Median liegt in der Mitte der beiden mittleren Werte des sortierten Datensatzes (in Abbildung 10.6 B speziell in der Mitte der Werte \\(_{(3)}\\) und \\(y_{(4)}\\)). Damit sind dann \\(3/6\\), also \\(50\\%\\) der Datenwerte (speziell \\(y_{(1)}\\), \\(y_{(2)}\\) und \\(y_{(3)}\\)) kleiner als der Median und \\(3/6\\), also \\(50\\%\\) der Datenwerte (speziell \\(y_{(4)}\\), \\(y_{(5)}\\) und \\(y_{(6)}\\)) größer als der Median.\n\n\n\n\n\n\nAbbildung 10.6: Bestimmung und Eigenschaften des Medians.\n\n\n\nFolgender R Code bestimmt den Median des Beispieldatensatzes anhand von Definition 10.7\n\nD           = read.csv(\"./_data/111-deskriptive-statistik.csv\")                 # Laden des Datensatzes\ny           = D$PRE                                                             # Pre-Interventions-BDI-II-Daten\nn           = length(y)                                                         # Anzahl der Werte\ny_s         = sort(y)                                                           # aufsteigend sortierter Vektor\nif(n %% 2 == 1){                                                                # n ungerade, n mod 2 == 1    \n   y_tilde = y_s[(n+1)/2]                                                       # Medianformel, Fall n ungerade\n} else {                                                                        # n gerade, n mod 2 == 0\n   y_tilde = (y_s[n/2] + y_s[n/2 + 1])/2}                                       # Medianformel, Fall n gerade\n\n\n\nMedian der PRE-BDI-II-Daten:  19\n\n\nNatürlich stellt R mit median() auch eine Funktion zur direkten Bestimmung, wie folgender R Code demonstriert.\n\ncat(\"Median der PRE-BDI-II-Daten: \", median(y))                                  # Ausgabe\n\nMedian der PRE-BDI-II-Daten:  19\n\n\nFolgende Simulation zeigt beispielhaft, dass der Median als Maß der Mitte eines Datensatzes weniger anfällig für Ausreißer ist als der Mittelwert.\n\nD           = read.csv(\"./_data/111-deskriptive-statistik.csv\")                  # Laden des Datensatzes\ny           = D$PRE                                                              # Pre-Interventions-BDI-II-Daten\n\n\n\nMittelwert und Median für Datensatz ohne Ausreißer:  18.61 19\n\n\n\nz           = y                                                                  # neuer simulierter Datensatz \nz[1]        = 10000                                                              # ... mit einem Extremwert\n\n\n\nMittelwert und Median für Datensatz mit Ausreißer:  118.44 19\n\n\nObwohl der Median also als robusteres Maß für die zentrale Tendenz eines Datensatzes als der Mittelwert erscheint, ist die Bestimmung von Mittelwerten sicherlich das üblichere Vorgehen. Dies ist einerseits dadurch begründet, dass sich der Mittelwert aufgrund seiner mathematisch weniger komplexen Definition einfacher im analytischen Kontext zur Gestaltung von probabilistischen Modellen eignet. Andererseits ist das Auftreten von Extremwerten üblicherweise ein Anzeichen von Datenerhebungsfehlern (z.B. sollte eine BDI-II Wert von 10.000 schlicht nicht vorkommen) oder großen unterschieden zwischen den betrachteten experimentellen Einheiten, die durch visuelle Inspektion des Datensatzes meist vor der Bestimmung von Maßen der zentralen Tendenz auffallen. Es gibt allerdings ein ganzes Unterfeld der Statistik, dass sich mit - im weitesten Sinne - Ausreißer unabhängigen Maßen beschäftigt, vgl. z.B. Huber (1981).\n\n\n10.5.3 Modalwert\nWenn in einem Datensatz manche Werte wiederholt auftreten, ist mit dem Modalwert als dem in einem Datensatz am häufigsten vorkommenden Wert ein drittes Maß der zentralen Tendenz gegeben. Wir nutzen folgende Definition.\n\nDefinition 10.8 (Modalwert) \\(y := (y_1,...,y_n)\\) mit \\(y_i \\in \\mathbb{R}\\) sei ein Datensatz, \\(W := \\{w_1,...,w_k\\}\\) mit \\(k \\le n\\) seien die im Datensatz vorkommenden verschiedenen Zahlenwerte und \\(h : W \\to \\mathbb{N}\\) sei die absolute Häufigkeitsverteilung der Zahlwerte von \\(y\\). Dann ist der Modalwert (oder Modus) von \\(y\\) definiert als \\[\\begin{equation}\n\\mbox{argmax}_{w \\in W} h(w),\n\\end{equation}\\] also als der im Datensatz am häufigsten vorkommende Wert.\n\nIn R kann man den Modalwert eines entsprechenden Datensatzes wie folgt bestimmen.\n\nD           = read.csv(\"./_data/111-deskriptive-statistik.csv\")                 # Laden des Datensatzes\ny           = D$PRE                                                             # Pre-Interventions-BDI-II-Daten\nn           = length(y)                                                         # Anzahl der Datenwerte (100)\nH           = as.data.frame(table(y))                                           # absolute Haeufigkeitsverteilung (dataframe)\nnames(H)    = c(\"a\", \"w\")                                                       # konsistente Benennung\nmode        = H$w[which.max(H$w)]                                               # Modalwert\n\n\ncat(\"Modalwert der PRE-BDI-II-Daten: \", as.numeric(as.vector(mode)))            # Ausgabe als numeric vector, nicht factor\n\nModalwert der PRE-BDI-II-Daten:  21\n\n\n\n\n10.5.4 Datenverteilungsformen und Maße zentraler Tendenz\nWie sinnhaft es ist, mit dem Mittelwert, dem Median oder dem Modalwert eine Angabe über die zentrale Tendenz eines Datensatzes zu geben, hängt stark von der Beschaffenheit des Datensatzes ab. Hat man beispielsweise einen Datensatz hochaufgelöster Daten vorliegen, in dem kein Datenwert mehrfach auftaucht, eignet sich der Modalwert sicherlich nicht, um eine quantitative Aussage über die zentrale Tendenz des Datensatzes zu machen. Man sollte die Angabe eines Maßes der zentralen Tendenz also immer im Kontext der Gesamtbeschaffenheit des Datensatzes sowie, wenn zutreffend, des inferenzstatistischen Modells, das man zur Interrogation des Datensatzes nutzt, betrachten. In Abbildung 10.7 geben wir vier Beispiele, wie sich die oben betrachteten Maße der zentralen Tendenz in verschiedenen Szenarien der Datenwertverteilung verhalten können, Diese vier Szenarien sind natürlich nicht erschöpfend und jeder Datensatz ist in Hinblick auf ein sinnvolles Maß der zentralen Tendenz kritisch zu betrachten.\nAbbildung 10.7 A visualisiert die Lage von Mittelwert, Median und Modalwert in einem Datenzatz, der sich symmetrisch um einen Wert von etwa \\(y = 50\\) verteilt und für den Werte nah bei diese zentralen Wert häufig vorkommen und Werte fern von diesem zentralen Wert eher selten. Wie wir später sehen, entspricht dies den typischen Charakteristika normalverteilter Datenwerte. In diesem Fall liegen Mittelwert, Median und Modalwert recht nahe beieinander und jedes dieser Maße beschreibt in der Tat die zentrale Tendenz des Datensatzes.\nAbbildung 10.7 B visualisiert die Lage von Mittelwert, Median und Modalwert in einem Datenzatz, in dem die Datenwerte um zwei Werte von von etwa \\(y = 25\\) und \\(y = 75\\) gehäuft auftreten und jeweils in positive und negative Richtung von diesen Werten entfernt eher selten auftreten. So liegen zum Beispiel um \\(y = 50\\) recht wenige Datenpunkte dieses Datensatzes. Mittelwert und Median nehmen nun aber gerade Werte um \\(y = 50\\) an. Als solche tragen sie bei diesem Datensatz also keine Bedeutungdarüber, welche Werte besonders häufig vorkommen, sondern lediglich darüber, wo sich der mittlere “physikalische Schwerpunkt” des Datensatzes befindet. Der Modalwert hingegen liegt, aufgrund des etwas häufigeren Auftretens von Werten um \\(y = 75\\), bei dem größeren der beiden Datenhäufungspunkte. Insgesamt betrachtet beschreibt keines der drei Maße bei einem solchen Datensatz, den man auch bimodal nennt, die zentrale Tendenz der Datenwerte wirklich zufriedenstellend.\nAbbildung 10.7 C visualisiert die Lage von Mittelwert, Median und Modalwert in einem Datenzatz, in dem die Datenwerte in der betrachteten Breite etwa durchgängig gleichhäufig auftreten. Hier liegt der Modalwert relativ arbiträr bei etwa \\(y = 15\\), da sich dort eine minimale Häufung von Datenwerten zeigt. Mittelwert und Median liegen wiederrum im Sinne des “physikalischen Schwerpunkts” des Datensatzes bei etwa \\(y = 50\\), auch wenn die Werte dieses Datensatzes dort keinerlei Tendenz haben, häufiger als anderorts aufzutreten. Wirklich zufriedenstellend ist auch hier die Beschreibung der zentralen Tendenz des Datensatzes durch die betrachteten Maße nicht. Zusammen betrachtet kann man anhand von Abbildung 10.7 B und Abbildung 10.7 C vielleicht festhalten, dass wenn ein Datensatz keine klare zentrale Tendenz hat, die Maße der zentralen Tendenz diese auch nicht abbilden können.\nAbbildung 10.7 D schließlich zeigt die Lage von Mittelwert, Median, und Modalwert für einen Datensatz, der schiefsymmetrisch um Werte von etwa \\(y = 0.5\\) erteilt ist. Die Daten dieses Datensatzes nehmen nur positive Werte an, viele Datenwerte liegen in der Tat um \\(y = 0.5\\), aber es kommen auch höhere Datenwerte vor. Wir werden an späterer Stelle sehen, dass dieses Erscheinungsbild für quadrierte normalverteilte Datenwerte typisch ist. Im psychologischen Kontext haben insbesondere Reaktionszeiten oft eine ähnliche Verteilung, da sie nicht negativ sein können, in den meisten Fällen etwa im Bereich von 0.3 bis 0.6 Sekunden liegen, aber in manchen Fällen durch Unaufmerksamkeit oder Stimulusvariabilität auch sehr viel länger sein können. In diesem Fall beschreibt der Median die tatsächliche zentrale Tendenz der Datenwerte etwas besser als der Mittelwert, da dieser durch die selten vorkommenden Ausreißerwerte etwas zu höheren Werten verschoben ist.\nInsgesamt ist die Angabe eines oder mehrerer Maße der zentralen Tendenz also immer kritisch vor dem Hintergrund der Verteilung der Werte eines Datensatzes zu betrachten und qualitativ einzuordnen.\n\n\n\n\n\n\nAbbildung 10.7: Visuelle Intuition zu Maßen zentraler Tendenz bei verschiedenen Datenverteilungsformen",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Deskriptivstatistiken</span>"
    ]
  },
  {
    "objectID": "110-Deskriptivstatistiken.html#sec-masse-der-variabilitaet",
    "href": "110-Deskriptivstatistiken.html#sec-masse-der-variabilitaet",
    "title": "10  Deskriptivstatistiken",
    "section": "10.6 Maße der Variabilität",
    "text": "10.6 Maße der Variabilität\nNeben der Frage, welchen “durchschnittlichen Wert” ein Datensatz annimmt, ist es oft von Interesse zu quantifizieren, wie sehr die Werte im Datensatz streuen oder anders ausgedrückt, wie “variabel” sie sind. Als quantitative Maße für die Variabilität eines Datensatzes betrachten wir hier die Spannbreite, die empirische Varianz und die empirische Standardabweichung. Wie auch für den Mittelwert erschließt sich die volle Bedeutung insbesondere letztere Maße vor allem im Kontext inferenzstatistischer Verfahren.\n\n10.6.1 Spannbreite\nDie Spannbreite eines Datensatzes ist die Differenz zwischen seinem größten und seinem kleinsten Wert.\n\nDefinition 10.9 (Spannbreite) \\(y = (y_1,...,y_n)\\) sei ein Datensatz. Dann ist die von \\(y_1,...,y_n\\) definiert als \\[\\begin{equation}\nr := \\max(y_1,...,y_n) - \\min(y_1,...,y_n).\n\\end{equation}\\]\n\nIn R bestimmt man die Spannbreite entweder durch Evaluation des Maximums und des Minimums des Datensatzes mithilfe der max() und min() Funktionen oder direkt mithilfe der range() Funktion. Folgender R Code demonstriert die Anwendung der max() und min() Funktionen.\n\nD           = read.csv(\"./_data/111-deskriptive-statistik.csv\")                 # Laden des Datensatzes\ny           = D$PRE                                                             # Pre-Interventions-BDI-II-Daten\ny_max       = max(y)                                                            # Maximum der PRE Werte\ny_min       = min(y)                                                            # Mininum der PRE Werte\nr           = y_max - y_min                                                     # Spannbreite\n\n\n\nSpannbreite der PRE-BDI-II-Daten:  9\n\n\nFolgender R Code demonstriert die Anwendung der range() Funktion.\n\nMinMax  = range(y)                                                              # \"automatische\" Berechnung von min(x), max(x)\nr       = MinMax[2] - MinMax[1]                                                 # Spannbreite\n\n\n\nSpannbreite der PRE-BDI-II-Daten:  9\n\n\n\n\n10.6.2 Empirische Varianz\nEin typisches statistisches Maß für die Variabilität von Datensätzen ist die standardisierte Summe der quadrierten Differenzen zwischen den einzelnen Datensatzwerten und ihrem Mittelwert. Die quadrierten Differenzen zwischen den einzelnen Datensatzwerten und ihrem Mittelwert werden auch als Abweichungsquadrate bezeichnet. Je nach Form der Standardisierung, also der Bildung eines Durchschnitts der Abweichungsquadrate, unterscheidet man eine unkorrigierte empirische Varianz und eine empirische Varianz, wie folgende Definition festsetzt.\n\nDefinition 10.10 (Unkorrigierte empirische Varianz und empirische Varianz) \\(y = (y_1,...,y_n)\\) sei ein Datensatz mit \\(n&gt;1\\) und \\(\\bar{y}\\) sei sein Mittelwert. Dann ist die unkorrigierte empirische Varianz von \\(y\\) definiert als \\[\\begin{equation}\n\\tilde{s}^2 := \\frac{1}{n}\\sum_{i=1}^n (y_i - \\bar{y})^2\n\\end{equation}\\] und die empirische Varianz von \\(y\\) ist definiert als \\[\\begin{equation}\ns^2 := \\frac{1}{n-1}\\sum_{i=1}^n (y_i - \\bar{y})^2.\n\\end{equation}\\]\n\nWir haben in Theorem 10.1 gesehen, dass die Summe der Differenzen zwischen den Datenwerten und ihrem Mittelwert immer gleich Null ist. Die Quadrierung der Differenzen \\[\\begin{equation}\ny_i - \\bar{y} \\mbox{ für } i = 1,...,n\n\\end{equation}\\] in Definition 10.10 führt nun gerade dazu, dass sich negative und positive Differenzen von Datenpunkten nicht ausgleichen und, je nach Höhe der positiven und negativen Differenzen, mit \\[\\begin{equation}\n\\sum_{i=1}^n (y_i - \\bar{y})^2\n\\end{equation}\\] ein höheres oder geringeres Maß für die Gesamtabweichung der Datenwerte von ihrem Mittelwert bestimmt werden kann. Dabei ist die Summe der Abweichungsquadrate gerade dann Null, wenn alle Datenwerte identisch und damit auch identisch mit ihrem Mittelwert sind. Alternativ wäre zum Beispiel auch die Bestimmung der Absolutbeträge \\(|y_i - \\bar{y}|\\) denkbar, allerdings erfüllt das so entstehende Maß der Variabilität andere Eigenschaften als das Maß der quadrierten Abweichungen in Definition 10.10, weshalb dieses, auch in Hinblick auf seine Nähe zu probabilistischen Normalverteilungsmodellen in der Regel bevorzugt wird. Die unkorrigierte empirische Varianz \\(\\tilde{s}^2\\) und die (korrigierte) empirische Varianz \\(s^2\\) unterscheiden sich dann hinsichtlich der Form der Durchschnittsbildung der quadrierten Abweichungsquadrate. Für \\(\\tilde{s}^2\\) wird die Summe der Abweichungsquadrate durch \\(n\\), für \\(s^2\\) durch \\(n-1\\) geteilt. \\(\\tilde{s}^2\\) ist also immer etwas kleiner als \\(s^2\\). Allerdings wird dieser Unterschied für großes \\(n\\) eher gering ausfallen, man denke beispielsweise an den numerischen Unterschied zwischen \\(\\frac{1}{3}\\) und \\(\\frac{1}{2}\\) bei kleinem \\(n\\) sowie zwischen \\(\\frac{1}{1001}\\) und \\(\\frac{1}{1000}\\) bei großem \\(n\\). Man beachte, dass die empirische Varianz für \\(n = 1\\) nicht definiert ist, die unkorrigierte empirische Varianz jedoch auch in diesem Spezialfall prinzipiell bestimmt werden kann. Der Begriff unkorrigiert (und implizit korrigiert) kann sich an dieser Stelle noch nicht erschließen, sondern ergibt nur vor dem Hintegrund eines Frequentischen Inferenzmodells Sinn und wird in ?sec-parameterschaetzung erläutert werden.\nWir fassen die obige Diskussion in folgendem Theorem zusammen\n\nTheorem 10.4 (Verhältnis von unkorrigierter empirischer Varianz und empirischer Varianz) \\(y = (y_1,...,y_n)\\) sei ein Datensatz und \\(\\tilde{s}^2\\) und \\(s^2\\) seien seine unkorrigierte empirische Varianz und empirische Varianz, respektive. Dann gelten\n\n(Umrechnung) \\[\\begin{equation}\n\\tilde{s}^2 = \\frac{n-1}{n}s^2 \\mbox{ und } s^2 = \\frac{n}{n-1}\\tilde{s}^2.\n\\end{equation}\\]\n(Ungleichung) \\[\\begin{equation}\n0 \\le \\tilde{s}^2 \\le s^2.\n\\end{equation}\\]\n\n\n\nBeweis. (1) Nach Definition 10.10 gilt zum einen \\[\\begin{align}\n\\begin{split}\n\\tilde{s}^2 & = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\bar{y})^2 \\\\\n\\Leftrightarrow\n\\frac{n-1}{n-1}\\tilde{s}^2 & = \\frac{n-1}{n-1}\\frac{1}{n}\\sum_{i=1}^n (y_i - \\bar{y})^2 \\\\\n\\Leftrightarrow\n\\tilde{s}^2 & = \\frac{n-1}{n}\\frac{1}{n-1}\\sum_{i=1}^n (y_i - \\bar{y})^2\\\\\n\\Leftrightarrow\n\\tilde{s}^2 & = \\frac{n-1}{n}s^2\n\\end{split}\n\\end{align}\\] und zum anderen \\[\\begin{align}\n\\begin{split}\ns^2 & = \\frac{1}{n-1}\\sum_{i=1}^n (y_i - \\bar{y})^2 \\\\\n\\Leftrightarrow\n\\frac{n}{n}s^2 & = \\frac{n}{n}\\frac{1}{n-1}\\sum_{i=1}^n (y_i - \\bar{y})^2 \\\\\n\\Leftrightarrow\ns^2 & = \\frac{n}{n-1}\\frac{1}{n}\\sum_{i=1}^n (y_i - \\bar{y})^2\\\\\n\\Leftrightarrow\ns^2 & = \\frac{n}{n-1}\\tilde{s}^2.\n\\end{split}\n\\end{align}\\]\n(2) Mit der Nichtnegativität der Quadratfunktion sowie \\(\\frac{1}{n} &gt; 0\\) und \\(\\frac{1}{n-1} &gt; 0\\) für \\(n &gt; 1\\) ergeben sich \\(\\tilde{s}^2 \\ge 0\\) und \\(\\tilde{s}^2 &gt; 0\\) direkt. Schließlich gilt für \\(\\sum_{i=1}^n (y_i - \\bar{y})^2 \\neq 0\\) \\[\\begin{equation}\n\\frac{1}{n} &lt; \\frac{1}{n-1} \\Leftrightarrow\n\\frac{1}{n}\\sum_{i=1}^n (y_i - \\bar{y})^2 &lt; \\frac{1}{n-1}\\sum_{i=1}^n (y_i - \\bar{y})^2 \\Leftrightarrow\n\\tilde{s}^2 &lt; s^2.\n\\end{equation}\\] und für \\(\\sum_{i=1}^n (y_i - \\bar{y})^2 = 0\\) offenbar \\(\\tilde{s}^2 = s^2\\). Insgesamt gilt also \\[\\begin{equation}\n0 \\le \\tilde{s}^2 \\le s^2.\n\\end{equation}\\]\n\nFolgender R Code demonstriert die Bestimmung der unkorrigierten empirischen Varianz und der empirischen Varianz anhand der in Definition 10.10 festgelegten Formeln.\n\nD           = read.csv(\"./_data/111-deskriptive-statistik.csv\")                 # Laden des Datensatzes\ny           = D$PRE                                                             # Pre-Interventions-BDI-II-Daten                                        \nn           = length(y)                                                         # Anzahl der Werte\nS2_tilde    = (1/n)*sum((y - mean(y))^2)                                        # unkorrigierte empirische Varianz\nS2          = (1/(n-1))*sum((y - mean(y))^2)                                    # empirische Varianzz\n\n\n\nUnkorrigierte empirische Varianz der PRE-BDI-II-Daten :  2.998 \nEmpirische Varianz der PRE-BDI-II-Daten               :  3.028\n\n\nDie R Funktion var() bestimmt per default die empirische Varianz. Mithilfe von Aussage (1) in Theorem 10.4 kann basierend auf dieser Funktion auch die unkorrigierte empirische Varianz bestimmt werden.\n\ns2          = var(y)                                                            # empirische Varianzz\ns2_tilde    = ((n-1)/n)*var(y)                                                  # unkorrigierte empirische Varianz\n\n\n\nUnkorrigierte empirische Varianz der PRE-BDI-II-Daten:  2.998 \nEmpirische Varianz der PRE-BDI-II-Daten              :  3.028\n\n\nFolgendes Theorem besagt, wie sich die empirische Varianz bei linear-affiner Transformation eines Datensatzes verhält.\n\nTheorem 10.5 (Varianz bei linear-affinen Transformationen) \\(y = (y_1,...,y_n)\\) sei ein Datensatz mit empirischer Varianz \\(S_y^2\\) und \\(z = (ay_1+b, ..., ay_n+b)\\) sei der mit \\(a,b \\in \\mathbb{R}\\) linear-affin transformierte Datensatz mit empirischer Varianz \\(S_z^2\\). Dann gilt \\[\\begin{equation}\ns^2_z = a^2 s^2_y.\n\\end{equation}\\]\n\n\nBeweis. \\[\\begin{align}\n\\begin{split}\ns^2_z\n&  = \\frac{1}{n-1}\\sum_{i=1}^n (z_i - \\bar{z})^2                \\\\\n&  = \\frac{1}{n-1}\\sum_{i=1}^n (ay_i + b - (a\\bar{y} + b))^2    \\\\\n&  = \\frac{1}{n-1}\\sum_{i=1}^n (ay_i + b - a\\bar{y} - b)^2      \\\\\n&  = \\frac{1}{n-1}\\sum_{i=1}^n (a(y_i - \\bar{y}))^2             \\\\\n&  = \\frac{1}{n-1}\\sum_{i=1}^n a^2(y_i - \\bar{y})^2             \\\\     \n&  = a^2\\frac{1}{n-1}\\sum_{i=1}^n (y_i - \\bar{y})^2             \\\\\n&  = a^2S_y^2.                                                   \\\\\n\\end{split}\n\\end{align}\\]\n\nBeispielsweise ändert also das Umrechnen eines Datensatzes von Meter in Zentimeter die Varianz des Ausgangsdatensatzes um den Faktor \\(100^2\\). Wir reproduzieren Theorem 10.5 beispielhaft mithilfe folgenden R Codes.\n\ny       = D$PRE                                                                  # Pre-Interventions-BDI-II-Daten\ns2y     = var(y)                                                                 # Empirische Varianz von y_1,...,y_n\na       = 2                                                                      # Multiplikationskonstante\nb       = 5                                                                      # Additionskonstante\nz       = a*y + b                                                                # z_i = az_i + b\ns2z     = var(z)                                                                 # Empirische Varianz von z_1,...,z_n\ncat(\"Empirische Varianz von z_1,...,z_n durch direkte Berechung : \", round(s2z,3))                       # Ausgabe\n\nEmpirische Varianz von z_1,...,z_n durch direkte Berechung :  12.113\n\ns2z     = a^2*s2y                                                                # Empirische Varianz von z_1,...,z_n\ncat(\"Empirische Varianz von z_1,...,z_n nach Theorem            : \", round(s2z,3))      # Ausgabe\n\nEmpirische Varianz von z_1,...,z_n nach Theorem            :  12.113\n\n\nFolgendes Theorem zeigt eine Möglichkeit auf, die unkorrigierte empirische Varianz eines Datensatzes allein durch die Bestimmung von Mittelwerten zu bestimmen. Das Theorem findet seine analytische Entsprechung im Verschiebungssatz der Varianz, ?thm-varianzverschiebungssatz.\n\nTheorem 10.6 (Verschiebungssatz zur unkorrigierten empirischen Varianz) \\(y = (y_1,...,y_n)\\) sei ein Datensatz, \\(y^2 := (y_1^2, ..., y_n^2)\\) sei sein elementweises Quadrat und \\(\\bar{y}\\) und \\(\\overline{y^2}\\) seien die respektiven Mittelwerte. Dann gilt \\[\\begin{equation}\n\\tilde{s}^2 = \\overline{y^2} - \\bar{y}^2\n\\end{equation}\\]\n\n\nBeweis. \\[\\begin{align}\n\\begin{split}\n\\tilde{s}^2\n& := \\frac{1}{n}\\sum_{i=1}^n (y_i - \\bar{y})^2 \\\\\n&  = \\frac{1}{n}\\sum_{i=1}^n \\left(y_i^2 - 2y_i \\bar{y} +  \\bar{y}^2\\right) \\\\\n&  = \\frac{1}{n}\\sum_{i=1}^n y_i^2  - 2  \\bar{y} \\frac{1}{n}\\sum_{i=1}^n y_i + \\frac{1}{n}\\sum_{i=1}^n \\bar{y}^2 \\\\\n&  = \\overline{y^2} - 2\\bar{y}\\bar{y} + \\frac{1}{n}n\\bar{y}^2 \\\\\n&  = \\overline{y^2} - 2\\bar{y}^2 + \\bar{y}^2 \\\\\n&  = \\overline{y^2} - \\bar{y}^2.\n\\end{split}\n\\end{align}\\]\n\nWir reproduzieren ?thm-verschiebungssatz-der-empirischen-varianz beispielhaft mithilfe folgenden R Codes.\n\ny           = D$PRE                                                              # Pre-Interventions-BDI-II-Daten\ns2_tilde    = mean(y^2) - (mean(y))^2                                            # \\bar{y^2} - \\bar{y}^2\ncat(\"Korrigierte empirische Varianz nach Transformation : \", round(s2z,3))       # Ausgabe\n\nKorrigierte empirische Varianz nach Transformation :  12.113\n\n\nMan beachte, dass dass der Verschiebungssatz der unkorrigierten empirischen Varianz für die empirische Varianz nicht zutrifft, da in diesem Fall der Multiplikationsfactor \\(\\frac{1}{n-1}\\) nicht auf die entsprechenden Mittelwerte führt. Insbesondere haben wir oben schon gesehen, dass für den betreffenden Beispieldatensatz gilt, dass \\(s^2  = 3.028\\).\n\n\n10.6.3 Empirische Standardabweichung\nDie empirische Varianz ergibt sich wie oben gesehen als durchschnittliches Abweichungsquadrat eines Datensatzwerte von seinem Mittelwert. Ist die Einheit des Datensatzes dabei zum Beispiel in einer Reaktionsszeitaufgabe die Millisekunde, so ergibt sich durch Differenzbildung und anschließende Quadrierung die Einheit Millisekunde\\(^2\\), eine nicht sehr intuitive Einheit. Um ein Streuungsmaß zu erhalten, dessen Einheit der Einheit des Ausgangsdatensatzes entspricht, wird auf die empirische Varianz häufig noch die Quadratwurzelfunktion angewendet. Dies führt auf den Begriff der empirischen Standardabweichung.\n\nDefinition 10.11 (Empirische Standardabweichung) \\(y = (y_1,...,y_n)\\) sei ein Datensatz. Die empirische Standardabweichung von \\(y\\) ist definiert als\n\\[\\begin{equation}\ns := \\sqrt{s^2}\n\\end{equation}\\] und die unkorrigierte empirische Stichprobenstandardabweichung von \\(y\\) ist definiert als \\[\\begin{equation}\n\\tilde{s} := \\sqrt{\\tilde{s}^2}.\n\\end{equation}\\]\n\nIn Analogie zu den Eigenschaften der empirischen Varianz und ihrer unkorrigierten Version kann auch die unkorrigierte empirischen Standardabweichung leicht in die empirischen Standardabweichung umgerechnet werden und umgekehrt, es gelten hier offenbar\n\\[\\begin{equation}\n\\tilde{s} = \\sqrt{\\frac{n-1}{n}}s \\mbox{ und } s = \\sqrt{\\frac{n}{n-1}}\\tilde{s}.\n\\end{equation}\\]\nFolgender R Code demonstriert die Bestimmung der unkorrigierten empirischen Varianz und der empirischen Varianz anhand der in Definition 10.11 festgelegten Formeln.\n\ny   = D$PRE                                                                     # Pre-Interventions-BDI-II-Daten\nn   = length(y)                                                                 # Anzahl der Werte\ns   = sqrt((1/(n-1))*sum((y - mean(y))^2))                                      # Standardabweichung\n\n\n\nEmpirische Standardabweichung :  1.74\n\n\nDie R Funktion sd() bestimmt per default die empirische Varianz.\n\ns   = sd(y)   \n\n\n\nEmpirische Standardabweichung :  1.74\n\n\nTransformiert man einen Datensatz linear-affin, so geht die Multiplikationskonstante der Transformation lediglich im Sinne ihres Betrages in die Transformation der empirischen Standardabweichung ein. Dies ist die Aussage des folgenden Theorems.\n\nTheorem 10.7 (Empirische Standardabweichung bei linear-affinen Transformationen) \\(y = (y_1,...,y_n)\\) sei ein Datensatz mit empirischer Standardabweichung \\(s_y\\) und \\(z = (az_1+b, ..., az_n+b)\\) sei der mit \\(a,b \\in \\mathbb{R}\\) linear-affin transformierte Datensatz mit empirischer Standardabweichung \\(s_z\\). Dann gilt \\[\\begin{equation}\ns_z = |a| s_y.\n\\end{equation}\\]\n\n\nBeweis. \\[\\begin{align}\n\\begin{split}\nS_y\n& := \\left(\\frac{1}{n-1}\\sum_{i=1}^n (y_i - \\bar{y})^2\\right)^{1/2}                         \\\\\n&  = \\left(\\frac{1}{n-1}\\sum_{i=1}^n \\left(ay_i + b - (a\\bar{x} + b)\\right)^2\\right)^{1/2}  \\\\\n&  = \\left(\\frac{1}{n-1}\\sum_{i=1}^n \\left(a(y_i - \\bar{x})\\right)^2\\right)^{1/2}           \\\\\n&  = \\left(\\frac{1}{n-1}\\sum_{i=1}^n a^2(y_i - \\bar{x})^2\\right)^{1/2}                      \\\\\n&  = \\left(a^2\\right)^{1/2}\\left(\\frac{1}{n-1}\\sum_{i=1}^n (y_i - \\bar{x})^2\\right)^{1/2}.\n\\end{split}\n\\end{align}\\] Also gilt \\(S_z = aS_y\\), wenn \\(a \\ge 0\\) und \\(S_z = -aS_y\\), wenn \\(a &lt; 0\\). Dies aber entspricht \\(S_z = |a|S_y\\).\n\nWir reproduzieren Theorem 10.7 beispielhaft mithilfe folgenden R Codes einmal für den Fall \\(a \\ge 0\\) und einmal für den Fall \\(a &lt; 0\\)\n\n# a &gt;= 0\nD       = read.csv(\"./_data/111-deskriptive-statistik.csv\")                     # Laden des Datensatzes\ny       = D$PRE                                                                 # Pre-Interventions-BDI-II-Daten\na       = 2                                                                     # Multiplikationskonstante\nb       = 5                                                                     # Additionskonstante\nz       = a*y + b                                                               # y_i = ax_i + b\nsz      = sd(z)                                                                 # Korrigierte empirische Standardabweichung von y\n\n\n\nEmpirische Standardabweichung von z_1,...z_n durch direkte Berechnung :  3.48\n\n\n\nsz      = a*sd(y)                                                               # Korrigierte empirische Standardabweichung von y\n\n\n\nEmpirische Standardabweichung von z_1,...z_n nach Theorem :  3.48\n\n\n\n# a &lt; 0\nD       = read.csv(\"./_data/111-deskriptive-statistik.csv\")                     # Laden des Datensatzes\ny       = D$PRE                                                                 # Pre-Interventions-BDI-II-Daten\na       = -3                                                                    # Multiplikationskonstante\nb       = 10                                                                    # Additionskonstante\nz       = a*y + b                                                               # y_i = ax_i + b\nsz      = sd(z)                                                                 # Korrigierte empirische Standardabweichung von y\n\n\n\nEmpirische Standardabweichung nach Transformation :  5.221\n\n\n\nsz      = -a*sd(y)                                                              # Korrigierte empirische Standardabweichung von y\n\n\n\nEmpirische Standardabweichung nach Transformation :  5.221\n\n\n\n\n\n\nHenze, N. (2018). Stochastik für Einsteiger. Springer Fachmedien Wiesbaden. https://doi.org/10.1007/978-3-658-22044-0\n\n\nHuber, P. J. (1981). Robust Statistics. Wiley.\n\n\nHyndman, R. J., & Fan, Y. (1996). Sample Quantiles in Statistical Packages. The American Statistician, 50(4), 361. https://doi.org/10.2307/2684934\n\n\nMcGill, R., Tukey, J. W., & Larsen, W. A. (1978). Variations of Box Plots. The American Statistician, 32(1), 12. https://doi.org/10.2307/2683468\n\n\nScott, D. W. (1979). On Optimal and Data-Based Histograms. 6.\n\n\nSturges, H. A. (1926). The Choice of a Class Interval. Journal of the American Statistical Association, 21(153), 65–66. https://doi.org/10.1080/01621459.1926.10502161",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Deskriptivstatistiken</span>"
    ]
  }
]