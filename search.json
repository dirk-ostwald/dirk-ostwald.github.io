[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Probabilistische Datenwissenschaft für die Psychologie",
    "section": "",
    "text": "Willkommen\nHerzlich willkommen zur Arbeitsversion von Probabilistische Datenwissenschaft für die Psychologie (PDWP), einem Online-Lehrbuch zur datenwissenschaftlichen Methodenlehre am Institut für Psychologie der Otto-von-Guericke-Universität Magdeburg. Eine Druckversion ist unter dem PDF Symbol oben rechts erhältlich.\nDieses Werk ist lizenziert unter einer Creative Commons Namensnennung 4.0 International Lizenz.",
    "crumbs": [
      "Willkommen"
    ]
  },
  {
    "objectID": "100-Mathematische-Grundlagen.html",
    "href": "100-Mathematische-Grundlagen.html",
    "title": "Mathematische Grundlagen",
    "section": "",
    "text": "Vorbemerkungen\nDie Mathematik ist die zentrale Sprache der probabilistischen Datenwissenschaft. Ihre Funktion ist dabei die einer Vermittlerin zwischen intuitiver Alltagssprache auf der einen Seite und der Vielfalt von Programmiersprachen, die zur Implementation datenwissenschaftlicher Analysen eingesetzt werden, auf der anderen. Mathematische Begriffsbildungen erlauben es, Konzepte sehr genau und allgemein verständlich zu greifen und unabhängig von den Ungenauigkeiten der Alltagsprache oder den Idiosynkratien verschiedener Programmiersprachen darzustellen. Natürlich kann die mathematische Sprache diese Rolle nicht unabhängig von alltagssprachlicher Intuition oder praktischer programmiertechnischer Anwendung einnehmen. Die Mathematik ist in der probabilistischen Datenanalyse deshalb kein Selbstzweck, sondern als angewandte Mathematik zu verstehen.\nIn diesem Teil stellen wir nach einigen Gedanken zu den grundlegenden Konzepten der Mathematik (1  Sprache und Logik) mit den Mengen (2  Mengen) und den Funktionen (2  Mengen) die beiden Grundpfeiler der modernen Mathematik in stark verkürzter Form dar. Wir ergänzen diese durch einige im wesentlichen notationelle Aspekte in (?sec-sec-summen-produkte-potenzen). Eng verwoben mit dem Begriff der Funktion sind bekanntlich die Differential- und Integralrechnung (5  Differentialrechnung und 7  Integralrechnung). Auch hier geht es uns keinesfalls um eine erschöpfende Darstellung, sondern insbesondere um die Erläuterung einiger Grundlagen, die für die im Kontext der probabilistischen Datenwissenschaft zentralen Konzepte der Optimierung von Funktionen sowie dem Umgang mit Wahrscheinlichkeitsfunktionen zentral sind. Wir ergänzen diese Abschnitte mit einigen oberflächlichen Gedanken zu den analytischen Grundlagen der Differential- und Integralrechnung in 6  Folgen, Grenzwerte, Stetigkeit. Im Umgang mit den großen Datenmengen der zeitgenössischen Wissenschaft hat sich die Matrizenschreibweise als hilfreich erwiesen. Wir widmen uns diesem Teilbereich der Linearen Algebra in 8  Vektoren, 9  Matrizen und 10  Eigenanalyse, wiederrum mit dem Ziel, notationelle und rechnerische Grundlagen für spätere Abschnitte bereitzustellen.\nInsgesamt dient dieser Teil also vor allem der Kurzeinführung von Grundlagen, die in allen anderen Teilen von PDWP wieder aufgegriffen werden und dient dabei vor allem auch der Einführung einer einheitlichen Notation über die verschiedenen Teilbereiche von PDWP. Je nach Bedarf und Interesse ist hier natürlich ein vertieftes Studium der verschiedenen Inhalte angezeigt. Im Folgenden legen wir überblicksartig die primären Quellen und Literaturempfehlungen zu den Inhalten dieses Teils dar.\n\n\nLiteraturhinweise",
    "crumbs": [
      "Mathematische Grundlagen"
    ]
  },
  {
    "objectID": "101-Sprache-und-Logik.html",
    "href": "101-Sprache-und-Logik.html",
    "title": "1  Sprache und Logik",
    "section": "",
    "text": "1.1 Mathematik ist eine Sprache\nMathematik ist die Sprache der naturwissenschaftlichen Modellbildung. So entspricht zum Beispiel der Ausdruck \\[\\begin{equation}\nF = ma\n\\end{equation}\\] im Sinne des zweiten Newtonschen Axioms einer Theorie zur Bewegung von Objekten unter der Einwirkung von Kräften (Newton (1687)). Gleichermaßen entspricht der Ausdruck \\[\\begin{equation}\n\\max_{q(z)} \\int q(z) \\ln \\left(\\frac{p(y,z)}{q(z)}\\right)\\,dz\n\\end{equation}\\] im Sinne der Variational Inference der zeitgenössischen Theorie zur Funktionsweise des Gehirns (Friston (2005), Friston et al. (2023), Ostwald et al. (2014), Blei et al. (2017)). Mathematische Symbolik dient dabei insbesondere der genauen Kommunikation wissenschaftlicher Erkenntnisse und zielt darauf ab, komplexe Sachverhalte exakt und effizient zu beschreiben. Wie beim reflektierten Umgang mit jeder Form von Sprache steht also die Frage “Was soll das heißen?” als Leitfrage im Umgang mit mathematischen Inhalten und Symbolismen immer im Vordergrund.\nAls Sprachgebäude weist die Mathematik einige Besonderheiten auf. Zum einen sind ihre Inhalte oft abstrakt. Dies rührt daher, dass sich die Mathematik um eine möglichst breite Allgemeinverständlichkeit und Anwendbarkeit bemüht. Mathematische Zugänge zu den Phänomenen der Welt sind dabei an einer möglichst einfache Transferierbarkeit von Erkenntnissen in andere Kontexte interessiert. Um dies zu ermöglichen, versucht die Mathematik möglichst genau und verständlich, also im Sinne präziser Begriffsbildungen zu arbeiten. Sie geht dabei insbesondere streng hierarchisch vor, so dass an späterer Stelle eingeführte Begrifflichkeiten oft ein gutes Verständnis der ihnen zugrundeliegenden und an früherer Stelle eingeführten Begrifflichkeiten voraussetzen.\nDie Genauigkeit der mathematischen Sprache impliziert dabei eine hohe Informationsdichte. Sie ist daher eher nüchtern und lässt überflüssiges weg, so dass in mathematischen Texten im besten Fall alles für die Kommunikation einer Idee relevant ist. Als Rezipient:in mathematischer Texte nimmt man die Informationsdichte mathematischer Texte anhand des hohen Verbrauchs an kognitiver Energie beim Lesen eines Textes wahr. Dieser hohe Energieverbrauch gebietet insbesondere Ruhe und Langsamkeit bei einem auf ein gutes Verständnis abzielenden Lesen. Als Leitsatz im Umgang mit mathematischen Texten mag dabei folgendes Zitat dienen: “Einen mathematischen Text kann man nicht lesen wie einen Roman, man muss ihn sich erarbeiten” (Unger (2000)). Nach dem Lesen eines kurzen mathematischen Textes sollte man sich immer kritisch fragen, ob man das Gelesene wirklich verstanden hat oder ob man zur Klärung des Sachverhaltes weitere Quellen heranziehen sollte. Auch ist es hilfreich, sich im Sinne des berühmten Zitats “What I cannot create, I do not understand” von Richard Feynman eigene Aufzeichnungen anzufertigen und mathematische Sprachgebäude selbst nachzubauen.\nMöchte man sich also die Welt der naturwissenschaftliche Modellbildung erschließen, so ist es hilfreich, beim Umgang mit ihrer mathematischen Ausdrucksweise und Symbolik die gleichen Strategien wie beim Erlernen einer Fremdsprache anzuwenden. Hierzu gehört neben dem Eintauchen in den entsprechenden Sprachraum, also der ständige Exposition mit mathematischen Ausdrucksweisen, sicherlich auch zunächst einmal das Auswendiglernen von Begriffen und das aktive Lesen und das Übersetzen von Texten in die Alltagssprache. Ein tiefes und sicheres Verständnis mathematischer Modellbildung ergibt sich dann insbesondere durch die Anwendung mathematischer Herangehensweisen in schriftlicher und mündlicher Form.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sprache und Logik</span>"
    ]
  },
  {
    "objectID": "101-Sprache-und-Logik.html#sec-grundbausteine-mathematischer-kommunikation",
    "href": "101-Sprache-und-Logik.html#sec-grundbausteine-mathematischer-kommunikation",
    "title": "1  Sprache und Logik",
    "section": "1.2 Grundbausteine mathematischer Kommunikation",
    "text": "1.2 Grundbausteine mathematischer Kommunikation\nIn diesem Abschnitt stellen wir mit den Begriffen der Definition, des Theorems und des Beweises drei Grundbausteine mathematischer Kommunikation vor, die uns durchgängig begleiten.\nDefinition\nEine Definition ist eine Grundannahme eines mathematischen Systems, die innerhalb dieses Systems weder begründet noch deduktiv abgeleitet wird. Definitionen können nur nach ihrer Nützlichkeit innerhalb eines mathematischen Systems bewertet werden. Eine Definition lernt man am besten erst einmal auswendig und hinterfragt sie erst dann, wenn man ihren Nutzen in der Anwendung verstanden hat oder von diesem nicht überzeugt ist. Etwas Entspannung und Ruhe beim Umgang mit auf den ersten Blick komplexen Definitionen ist generell hilfreich. Um zu kennzeichnen, dass wir ein Symbol als etwas definieren, nutzen wir die Schreibweise “\\(:=\\)”. Zum Beispiel definiert der Ausdruck “\\(a := 2\\)” das Symbol \\(a\\) als die Zahl Zwei. Definitionen enden in diesem Text immer mit dem Symbol \\(\\bullet\\).\nTheorem\nEin Theorem ist eine mathematische Aussage, die mittels eines Beweises als wahr (richtig) erkannt werden kann. Dass heißt, ein Theorem wird immer aus Definitionen und/oder anderen Theoremen hergeleitet. Theoreme sind in diesem Sinne die empirischen Ergebnisse der Mathematik. Im Deutschen werden Theoreme auch oft als Sätze bezeichnet. In der angewandten, datenanalytischen Mathematik sind Theoreme oft für Berechnungen hilfreich. Es lohnt sich also, sie auswendig zu lernen, da sie meist die Grundlage für Datenauswertung und Dateninterpretation bilden. Oft tauchen in Theoremen Gleichungen auf. Diese ergeben sich dabei aus den Voraussetzungen des Theorems. Um Gleichungen zu kennzeichnen nutzen wir das Gleichheitszeichen “\\(=\\)”. So besagt also zum Beispiel der Ausdruck “\\(a = 2\\)” in einem gegebenen Kontext, dass aufgrund bestimmter Voraussetzungen das Symbol oder die Variable \\(a\\) den Wert zwei hat. Theoreme enden in diesem Text immer mit dem Symbol \\(\\circ\\).\nBeweis\nEin Beweis ist eine logische Argumentationskette, die auf bekannte Definitionen und Theoreme zurückgreift, um die Wahrheit (Richtigkeit) eines Theorems zu belegen. Kurze Beweise tragen dabei oft zum Verständnis eines Theorems bei, lange Beweise eher nicht. Beweise sind also insbesondere die Antwort auf die Frage, warum eine mathematische Aussage gilt (“Warum ist das so?”). Beweise lernt man nicht auswendig. Wenn Beweise kurz sind, ist es sinnvoll, sie durchzuarbeiten, da sie meist als bekannt vorausgesetzte Inhalte wiederholen. Wenn sie lang sind, ist es sinnvoller sie zunächst zu übergehen, um sich nicht in Details zu verlieren und vom eigentlichen Weg durch das entsprechende mathematische Gebäude abzukommen. Beweise enden in diesem Text immer mit dem Symbol \\(\\Box\\).\nNeben den oben vorgestellten Begriffen gibt es mit Axiomen, Lemmata, Korollaren und Vermutungen noch weitere typische Grundbausteine mathematischer Texte. Wir werden diese Begriff nicht verwenden und geben deshalb für sie nur einen kurzen Überblick.\nAxiome sind unbeweisbare Theoreme, in dem Sinne, als dass sie als Grundannahmen zum Aufbau mathematischer Systeme dienen. Der Übergang zwischen Definitionen und Axiomen ist dabei oft fließend. Da wir mathematisch nicht besonders tief arbeiten, bevorzugen wir in den allermeisten Fällen den Begriff der Definition.\nEin Lemma ist ein “Hilfstheorem”, also eine mathematische Aussage, die zwar bewiesen wird, aber nicht so bedeutend ist wie ein Theorem. Da wir einerseits auf bedeutende Inhalte fokussieren und andererseits mathematische Aussagen nicht diskriminieren wollen, verzichten wir auf diesen Begriff und nutzen stattdessen den Begriff des Theorems.\nEin Korollar ist eine mathematische Aussage, die sich durch einen einfachen Beweis aus einem Theorem ergibt. Da die “Einfachheit” mathematischer Beweise eine relative Eigenschaft ist, verzichten wir auf diesen Begriff und nutzen stattdessen auch hier den Begriff des Theorems.\nVermutungen sind mathematische Aussagen von denen unbekannt ist, ob sie beweisbar oder widerlegbar sind. Da wir im Bereich der angewandten Mathematik arbeiten, treffen wir nicht auf Vermutungen.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sprache und Logik</span>"
    ]
  },
  {
    "objectID": "101-Sprache-und-Logik.html#sec-aussagenlogik",
    "href": "101-Sprache-und-Logik.html#sec-aussagenlogik",
    "title": "1  Sprache und Logik",
    "section": "1.3 Aussagenlogik",
    "text": "1.3 Aussagenlogik\nNachdem wir nun einige Grundbausteine mathematischer Modellbildung kennengelernt haben, wollen wir uns mit der Aussagenlogik einem einfachem System nähern, das es erlaubt, Beziehungen zwischen mathematischen Aussagen herzustellen und zu formalisieren. Im Folgenden spielt die Aussagenlogik zum Beispiel in der Definition von Mengenoperationen, bei Optimierungsbedingungen von Funktionen und in vielen Beweisen einen tragende Rolle. In der mathematischen Anwendung ist Aussagenlogik die Grundlage der Booleschen Logik der Programmierung. In der mathematischen Psychologie ist die Aussagenlogik zum Beispiel die Grundlage der Repräsentationstheorie des Messens.\nWir beginnen mit der Definition des Begriffs der mathematischen Aussage.\n\nDefinition 1.1 (Aussage) Eine ist ein Satz, dem eindeutig die Eigenschaft oder zugeordnet werden kann.\n\nDas Adjektiv wahr kann auch als richtig verstanden werden. Wir kürzen wahr mit “w” und falsch mit “f” ab. Im Körper der reellen Zahlen ist zum Beispiel die Aussage \\(1 + 1 = 2\\) wahr und die Aussage \\(1 + 1 = 3\\) falsch. Man beachte, dass die Binärität des Wahrheitsgehalts von Aussagen eine Grundannahme der Aussagenlogik und damit formal wissenschaftlich und nicht empirisch zu verstehen ist. Wahrheitsgehalte beziehen sich nicht auf Definitionen, Definitionen sind immer wahr. Eine erste Möglichkeit, mit Aussagen zu arbeiten, ist, sie zu negieren. Dies führt auf folgende Definition.\n\nDefinition 1.2 (Negation) \\(A\\) sei eine Aussage. Dann ist die die Aussage, die falsch ist, wenn \\(A\\) wahr ist und die wahr ist, wenn \\(A\\) falsch ist. Die Negation von \\(A\\) wird mit \\(\\neg A\\), gesprochen als “nicht \\(A\\)”, bezeichnet.\n\nBeispielsweise ist die Negation der Aussage “Die Sonne scheint” die Aussage “Die Sonne scheint nicht”. Die Negation der Aussage \\(1 + 1 = 2\\) ist die Aussage \\(1 + 1 \\neq 2\\) und die Negation der Aussage \\(x&gt;1\\) ist die Aussage \\(x \\le 1\\). Tabellarisch stellt man die Definition der Negation einer Aussage \\(A\\) wie folgt dar.\nTabellen dieser Form nennt man Wahrheitstafeln. Sie sind ein beliebtes Hilfsmittel in der Aussagenlogik. Möchte man zwei Aussagen logisch verbinden, so bieten sich zunächst die Begriffe der Konjunktion und Disjunktion an.\n\nDefinition 1.3 (Konjunktion) \\(A\\) und \\(B\\) seien Aussagen. Dann ist die die Aussage, die dann und nur dann wahr ist, wenn \\(A\\) und \\(B\\) beide wahr sind. Die Konjunktion von \\(A\\) und \\(B\\) wird mit \\(A \\land B\\), gesprochen als “\\(A\\) und \\(B\\)”, bezeichnet.\n\nDie Definition der Konjunktion impliziert folgende Wahrheitstafel.\nAls Beispiel sei \\(A\\) die Aussage \\(2\\ge1\\) und \\(B\\) die Aussage \\(2&gt;1\\). Da sowohl \\(A\\) und \\(B\\) wahr sind, ist auch die Aussage \\(2 \\ge 1 \\land 2 &gt; 1\\) wahr. Als weiteres Beispiel sei \\(A\\) die Aussage \\(1\\ge 1\\) und \\(B\\) die Aussage \\(1&gt;1\\). Hier ist nun \\(A\\) wahr und \\(B\\) falsch. Also ist die Aussage \\(1 \\ge 1 \\land 1 &gt; 1\\) falsch.\n\nDefinition 1.4 (Disjunktion) \\(A\\) und \\(B\\) seien Aussagen. Dann ist die die Aussage, die dann und nur dann wahr ist, wenn mindestens eine der beiden Aussagen \\(A\\) und \\(B\\) wahr ist. Die Disjunktion von \\(A\\) und \\(B\\) wird mit \\(A \\lor B\\), gesprochen als “\\(A\\) oder \\(B\\)”, bezeichnet.\n\nDie Definition der Disjunktion impliziert folgende Wahrheitstafel\n\\(A \\lor B\\) ist also insbesondere auch dann wahr, wenn \\(A\\) und \\(B\\) beide wahr sind. Damit ist das hier betrachtete “oder” genauer ein “und/oder”. Man nennt die Disjunktion daher auch ein “nicht-exklusives oder”. Als Beispiel sei \\(A\\) die Aussage \\(2\\ge1\\) und \\(B\\) die Aussage \\(2&gt;1\\). \\(A\\) ist wahr und \\(B\\) ist wahr. Also ist die Aussage \\(2 \\ge 1 \\lor 2 &gt; 1\\) wahr. Sei nun wiederrum \\(A\\) die Aussage \\(1\\ge 1\\) wahr und \\(B\\) die Aussage \\(1&gt;1\\). Dann ist \\(A\\) wahr und \\(B\\) falsch. Also ist die Aussage \\(1 \\ge 1 \\lor 1 &gt; 1\\) wahr.\nEine Möglichkeit, Aussagen in einen mechanischen logischen Zusammenhang zu stellen, ist die Implikation. Diese ist wie folgt definiert.\n\nDefinition 1.5 (Implikation) \\(A\\) und \\(B\\) seien Aussagen. Dann ist die , bezeichnet mit \\(A \\Rightarrow B\\), die Aussage, die dann und nur dann falsch ist, wenn \\(A\\) wahr und \\(B\\) falsch ist. \\(A\\) heißt dabei die und \\(B\\) der der Implikation. \\(A \\Rightarrow B\\) spricht man als “aus \\(A\\) folgt \\(B\\)”, “\\(A\\) impliziert \\(B\\)”, oder “wenn \\(A\\), dann \\(B\\)”.\n\nMan mag \\(\\Rightarrow\\) auch als “daraus folgt” lesen. Die Definition der Implikation impliziert folgende Wahrheitstafel.\nEin Verständnis der Definition der Implikation im Sinne obiger Wahrheitstafel ergibt sich am ehesten, indem man sie als Versuch liest, die intuitive Vorstellung einer Folgerung im Kontext der Aussagenlogik abzubilden und zu formalisieren. Betrachtet man obige Wahrheitstafel unter diesem Gesichtspunkt, so sieht man, dass wenn \\(A\\) wahr ist und \\(A \\Rightarrow B\\) wahr ist, \\(B\\) wahr ist. Konstruiert man basierend auf einer wahren Aussage also (zum Beispiel durch das Umformen von Gleichungen) eine wahre Implikation so folgt, dass auch \\(B\\) wahr ist. Ist dies nicht möglich (dass also gilt, wenn \\(A\\) wahr ist, dass \\(A \\Rightarrow B\\) immer falsch ist), dann ist auch \\(B\\) falsch. So mag man Aussagen widerlegen. Schließlich sieht man, dass wenn \\(A\\) falsch ist und \\(A \\Rightarrow B\\) wahr ist, \\(B\\) wahr oder falsch sein kann. Aus einer wahren Voraussetzung folgt also nur bei wahrer Implikation eine wahre Konklusion. Insbesondere genügt die Definition der Implikation damit der Forderung “Aus Falschem folgt beliebiges (ex falso sequitur quodlibet)”. Aus falschen Aussagen kann man also mithilfe der Implikation nichts richtiges folgern.\nIm Kontext der Implikation ergeben sich die Begriffe der hinreichenden und der notwendigen Aussagen (Bedingungen). Diese sind definiert wie folgt: wenn \\(A \\Rightarrow B\\) wahr ist, sagt man “\\(A\\) ist hinreichend für \\(B\\)” und “\\(B\\) ist notwendig für \\(A\\)”. Diese Sprachregelung erklärt sich folgendermaßen. Wenn \\(A \\Rightarrow B\\) wahr ist, gilt dass, wenn \\(A\\) wahr ist auch \\(B\\) wahr ist. Die Wahrheit von \\(A\\) reicht also für die Wahrheit von \\(B\\) aus. \\(A\\) ist also hinreichend (ausreichend) für \\(B\\). Weiterhin gilt, dass wenn \\(A \\Rightarrow B\\) wahr ist, dass wenn \\(B\\) falsch ist, dann auch \\(A\\) falsch ist. Die Wahrheit von \\(B\\) ist also für die Wahrheit von \\(A\\) notwendig.\nEine sehr häufig autretender Zusammenhang zwischen zwei Aussagen ist ihre Äquivalenz.\n\nDefinition 1.6 (Äquivalenz) \\(A\\) und \\(B\\) seien Aussagen. Die ist die Aussage, die dann und nur dann wahr ist,wenn \\(A\\) und \\(B\\) beide wahr sind oder wenn \\(A\\) und \\(B\\) beide falsch sind. Die Äquivalenz von \\(A\\) und \\(B\\) wird mit \\(A \\Leftrightarrow B\\) bezeichnet und gesprochen als “\\(A\\) genau dann wenn \\(B\\)” oder “\\(A\\) ist äquivalent zu \\(B\\)”.\n\nDie Definition der Äquivalenz impliziert folgende Wahrheitstafel\nDie Definition des Begriffes der logischen Äquivalenz erlaubt es unter anderem, die Äquivalenz zweier Aussagen mithilfe von Implikationen nachzuweisen.\n\nDefinition 1.7 (Logische Äquivalenz) Zwei Aussagen heißen , wenn ihre Wahrheitstafeln gleich sind.\n\nAls Beispiele für logische Äquivalenzen, die häufig in Beweisargumentationen genutzt werden, zeigen wir folgendes Theorem.\n\nTheorem 1.1 (Logische Äquivalenzen)  \n\\(A\\) und \\(B\\) seien zwei Aussagen. Dann sind folgende Aussagen logisch äquivalent\n\n\nBeweis. Nach Definition des Begriffs der logischen Äquivalenz müssen wir zeigen, dass die Wahrheitstafeln der betrachteten Aussagen gleich sind. Wir zeigen erst (1), dann (2).\n(1) Wir erinnern an die Wahrheitstafel von \\(A \\Leftrightarrow B\\):\nWir betrachten weiterhin die Wahrheitstafel von \\((A \\Rightarrow B) \\land (B \\Rightarrow A)\\):\nDer Vergleich der Wahrheitstafel von \\(A \\Leftrightarrow\\) mit den ersten beiden und der letzten Spalte der Wahrheitstafel von \\((A \\Rightarrow B) \\land (B \\Rightarrow A)\\) zeigt ihre Gleichheit.\n(2) Wir erinnern an die Wahrheitstafel von \\(A \\Rightarrow B\\):\nWir betrachten weiterhin die Wahrheitstafel von \\((\\neg B) \\Rightarrow (\\neg A)\\):\nDer Vergleich der Wahrheitstafel von \\(A \\Rightarrow B\\) mit den ersten beiden und der letzten Spalte der Wahrheitstafel von \\((\\neg B) \\Rightarrow (\\neg A)\\) zeigt ihre Gleichheit.\n\nDie erste Aussage von Theorem 1.1 besagt, dass die Aussage “\\(A\\) und”\\(B\\) sind äquivalent” logisch äquivalent zur Aussage “Aus \\(A\\) folgt \\(B\\)” und aus “\\(B\\) folgt \\(A\\)” ist. Dies ist die Grundlage für viele sogenannte direkte Beweise mithilfe von Äquivalenzumformungen. Die zweite Aussage von Theorem 1.1 besagt, dass die Aussage “Aus \\(A\\) folgt”\\(B\\)” logisch äquivalent zur Aussage “Aus nicht \\(B\\) folgt nicht \\(A\\)” ist. Dies ist die Grundlage für die Technik des indirekten Beweises.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sprache und Logik</span>"
    ]
  },
  {
    "objectID": "101-Sprache-und-Logik.html#sec-beweistechniken",
    "href": "101-Sprache-und-Logik.html#sec-beweistechniken",
    "title": "1  Sprache und Logik",
    "section": "1.4 Beweistechniken",
    "text": "1.4 Beweistechniken\nIm letzten Abschnitt wollen wir mit den Begriffen der direkten und indirekten Beweise sowie des Beweises durch Widerspruch kurz drei Beweistechniken skizzieren, von denen vor allem die erste in diesem Text immer wieder zur Begründung von Theoremen herangezogen wird. Dabei haben typische Theoreme die Form \\(A \\Rightarrow B\\) für Aussagen \\(A\\) und \\(B\\).\nEs gilt dabei\n\nDirekte Beweise nutzen Äquivalenzumformungen, um \\(A \\Rightarrow B\\) zu zeigen.\nIndirekte Beweise nutzen die logische Äquivalenz von \\(A \\Rightarrow B\\) und \\((\\neg B) \\Rightarrow (\\neg A)\\).\nBeweise durch Widerspruch zeigen, dass \\((\\neg B) \\land A\\) falsch ist.\n\nUm diese Techniken an einem Beispiel zu erläutern, erinnern wir kurz an folgende Äquivalenzumformungen von Gleichungen:\n\nAddition oder Subtraktion einer Zahl auf beiden Seiten der Gleichung, zum Beispiel \\[\\begin{equation}\n2x + 4 = 10 \\Leftrightarrow 2x = 6,\n\\end{equation}\\]\nMultiplikation mit einer oder Division durch eine von Null verschiedene Zahl auf beiden Seiten der Gleichung, zum Beispiel \\[\\begin{equation}\n2x = 6 \\Leftrightarrow x = 3,\n\\end{equation}\\]\nAnwendung einer injektiven Funktion auf beiden Seiten der Gleichung, zum Beispiel \\[\\begin{equation}\n\\exp(x) = 2 \\Leftrightarrow x = \\ln(2),\n\\end{equation}\\]\n\nsowie an folgende elementaren Äquivalenzumformungen von Ungleichungen:\n\nAddition oder Subtraktion einer Zahl auf beiden Seiten der Ungleichung, zum Beispiel \\[\\begin{equation}\n-2x + 4 \\ge 10 \\Leftrightarrow -2x \\ge 6,\n\\end{equation}\\]\nMultiplikation mit einer Zahl oder Division durch eine von Null verschiedene Zahl auf beiden Seiten der Ungleichung, wobei die Multiplikation oder Division mit einer negativen Zahl die Umkehrung der Ungleichung impliziert, zum Beispiel \\[\\begin{equation}\n-2x \\ge 6 \\Leftrightarrow x \\le -3,\n\\end{equation}\\]\nAnwendung monotoner Funktionen auf beiden Seiten der Ungleichung\n\\[\\begin{equation}\n\\exp(x) \\ge 2 \\Leftrightarrow x \\ge \\ln(2).\n\\end{equation}\\]\n\nDamit ausgestattet wollen wir nun folgendes Theorem mithilfe eines direkten Beweises, eines indirekten Beweises und eines Beweises durch Widerspruch beweisen (vgl. Arens et al. (2018)).\n\nTheorem 1.2 (Quadrate positiver Zahlen) Es seien \\(a\\) und \\(b\\) zwei positive Zahlen. Dann gilt \\(a^2 &lt; b^2 \\Rightarrow a &lt; b\\).\n\n\nBeweis. Wir geben zunächst einen direkten Beweis. Dazu sei \\(a^2 &lt; b^2\\) die Aussage \\(A\\) und \\(a &lt; b\\) die Aussage \\(B\\). Dann gilt \\[\\begin{equation}\na^2 &lt; b^2\n\\Leftrightarrow 0 &lt; b^2 - a^2\n\\Leftrightarrow 0 &lt; (b+a)(b-a)\n\\Leftrightarrow 0 &lt; (b-a)\n\\Leftrightarrow a &lt; b.\n\\end{equation}\\] Wir geben nun einen indirekten Beweis. Es sei \\(a^2 \\ge b^2\\) die Aussage \\(\\neg A\\). Weiterhin sei \\(a \\ge b\\) die Aussage \\(\\neg B\\). Dann gilt \\[\\begin{equation}\na \\ge b\n\\Leftrightarrow a^2 \\ge ab \\land ab \\ge b^2\n\\Leftrightarrow a^2 \\ge b^2.\n\\end{equation}\\] Schließlich geben wir einen Beweis durch Widerspruch. Wir zeigen, dazu, dass die Annahme \\((\\neg B) \\land A\\) auf eine falsche Aussage führt. Es gilt \\[\\begin{equation}\na \\ge b \\land a^2 &lt; b^2 \\Leftrightarrow  a^2  \\ge ab \\land a^2 &lt; b^2   \\Leftrightarrow ab \\le a^2 &lt; b^2.\n\\end{equation}\\] Weiterhin gilt \\[\\begin{equation}\na \\ge b \\land a^2 &lt; b^2 \\Leftrightarrow  ab  \\ge b^2 \\land a^2 &lt; b^2   \\Leftrightarrow a^2 &lt; b^2 \\le ab.\n\\end{equation}\\] Insgesamt gilt dann also die falsche Aussage \\[\\begin{equation}\nab \\le a^2 &lt; b^2 \\le ab \\Leftrightarrow ab &lt; ab.\n\\end{equation}\\]",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sprache und Logik</span>"
    ]
  },
  {
    "objectID": "101-Sprache-und-Logik.html#sec-selbstkontrollfragen-sprache-und-logik",
    "href": "101-Sprache-und-Logik.html#sec-selbstkontrollfragen-sprache-und-logik",
    "title": "1  Sprache und Logik",
    "section": "1.5 Selbstkontrollfragen",
    "text": "1.5 Selbstkontrollfragen\n\nErläutern Sie die Besonderheiten der mathematischen Sprache.\nWas sind wesentliche Tätigkeiten zum Erlernen einer Sprache?\nErläutern Sie den Begriff der Definition.\nErläutern Sie den Begriff des Theorems.\nErläutern Sie den Begriff des Beweises.\nGeben Sie die Definition einer mathematischen Aussage wieder.\nGeben Sie die Definition der Negation einer mathematischen Aussage wieder.\nGeben Sie die Definition der Konjunktion zweier mathematischer Aussagen wieder.\nGeben Sie die Definition der Disjunktion zweier mathematischer Aussagen wieder.\nGeben Sie die Definition der Implikation wieder.\nGeben Sie die Definition der Äquivalenz wieder.\nGeben Sie die Definition der logischen Äquivalenz wieder.\nErläutern Sie die Begriffe des direkten Beweises, des indirekten Beweises und des Beweises durch Widerspruch.\n\n\n\n\n\nArens, T., Hettlich, F., Karpfinger, C., Kockelkorn, U., Lichtenegger, K., & Stachel, H. (2018). Mathematik. Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-662-56741-8\n\n\nBlei, D. M., Kucukelbir, A., & McAuliffe, J. D. (2017). Variational Inference: A Review for Statisticians. Journal of the American Statistical Association, 112(518), 859–877. https://doi.org/10.1080/01621459.2017.1285773\n\n\nFriston, K. (2005). A Theory of Cortical Responses. Philosophical Transactions of the Royal Society B: Biological Sciences, 360(1456), 815–836. https://doi.org/10.1098/rstb.2005.1622\n\n\nFriston, K., Da Costa, L., Sakthivadivel, D. A. R., Heins, C., Pavliotis, G. A., Ramstead, M., & Parr, T. (2023). Path Integrals, Particular Kinds, and Strange Things. Physics of Life Reviews, 47, 35–62. https://doi.org/10.1016/j.plrev.2023.08.016\n\n\nNewton, I. (1687). Philosophiae Naturalis Principia Mathematica. Royal Society.\n\n\nOstwald, D., Kirilina, E., Starke, L., & Blankenburg, F. (2014). A Tutorial on Variational Bayes for Latent Linear Stochastic Time-Series Models. Journal of Mathematical Psychology, 60, 1–19. https://doi.org/10.1016/j.jmp.2014.04.003\n\n\nUnger, L. (2000). Grundkurs Mathematik.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sprache und Logik</span>"
    ]
  },
  {
    "objectID": "102-Mengen.html",
    "href": "102-Mengen.html",
    "title": "2  Mengen",
    "section": "",
    "text": "2.1 Grundlegende Definitionen\nMengen fassen mathematische Objekte wie beispielsweise Zahlen zusammen und bilden die Grundlage der modernen Mathematik. Wir beginnen mit folgender Definition.\nZur Definition von Mengen gibt es mindestens folgende Möglichkeiten:\nDie Schreibweise \\(\\{x \\in \\mathbb{N}|x &lt; 4\\}\\) wird gelesen als “\\(x \\in \\mathbb{N}\\), für die gilt, dass \\(x &lt; 4\\) ist”, wobei die Bedeutung von \\(\\mathbb{N}\\) im Folgenden noch zu erläutern sein wird. Es ist wichtig zu erkennen, dass Mengen ungeordnete mathematische Objekte sind, dass heißt die Reihenfolge der Auflistung der Elemente einer Menge spielt keine Rolle. Zum Beispiel bezeichnen \\(\\{1,2,3\\}\\), \\(\\{1,3,2\\}\\) und \\(\\{2,3,1\\}\\) dieselbe Menge, nämlich die Menge der ersten drei natürlichen Zahlen.\nGrundlegende Beziehungen zwischen mehreren Mengen werden in der nächsten Definition festgelegt.\nBetrachten wir zum Beispiel die Mengen \\(A := \\{1\\}\\), \\(B := \\{1,2\\}\\), und \\(C := \\{1,2\\}\\). Dann gilt mit obigen Definitionen, dass \\(A \\subset B\\), weil \\(1 \\in A\\) und \\(1 \\in B\\), aber \\(2 \\in B\\) und \\(2 \\notin A\\). Weiterhin gilt, dass \\(B \\subseteq C\\), weil \\(1 \\in B\\) und \\(1 \\in C\\) sowie \\(2 \\in B\\) und \\(2 \\in C\\) und es kein Element von \\(C\\) gibt, welches nicht in \\(B\\) ist. Ebenso gilt \\(C \\subseteq B\\), weil \\(1 \\in C\\) und \\(1 \\in B\\) sowie \\(2 \\in C\\) und \\(2 \\in B\\) und es kein Element von \\(B\\) gibt, welches nicht in \\(C\\) ist. Schließlich gilt sogar \\(B = C\\), weil für jedes Element \\(b \\in B\\) gilt, dass auch \\(b \\in C\\), und gleichzeitig für jedes Element \\(c \\in C\\) gilt, dass auch \\(c\\in B\\).\nEine wichtige Eigenschaft einer Menge ist die Anzahl der in ihr enthaltenen Elemente. Diese wird als Kardinalität der Menge bezeichnet.\nEine besondere Menge ist die Menge ohne Elemente.\nAls Beispiele seien \\(A := \\{1,2,3\\}\\), \\(B = \\{a,b,c,d\\}\\) und \\(C := \\{\\,\\}\\). Dann gelten \\(|A| = 3\\), \\(B = 4\\) und \\(|C| = 0\\).\nZu jeder Menge kann man die Menge aller Teilmengen dieser Menge betrachten. Dies führt auf den wichtigen Begriff der Potenzmenge.\nMan beachte, dass die leere Untermenge von \\(M\\) und \\(M\\) selbst auch immer Elemente von \\(\\mathcal{P}(M)\\) sind. Wir betrachten vier Beispiele zum Begriff der Potenzmenge.\nHinsichtlich der Kardinalitäten einer Menge und ihrer Potenzmenge kann man beweisen, dass aus \\(|M| = n\\) mit \\(n&gt;0\\) folgt, dass die Kardinalität der Potenzmenge \\(|\\mathcal{P}(M)| = 2^n\\) ist. In den obigen Beispielen haben wir die Fälle \\(|M_1| = 1\\) und somit \\(|\\mathcal{P}(M_1)| = 2^1 = 2\\), \\(|M_2| = 2\\) und somit \\(|\\mathcal{P}(M_3)| = 2^2 = 4\\) und schließlich \\(|M_3| = 3\\) und somit \\(|\\mathcal{P}(M_3)| = 2^3 = 8\\), wovon man sich durch Nachzählen schnell überzeugt.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Mengen</span>"
    ]
  },
  {
    "objectID": "102-Mengen.html#grundlegende-definitionen",
    "href": "102-Mengen.html#grundlegende-definitionen",
    "title": "2  Mengen",
    "section": "",
    "text": "Definition 2.1 (Mengen) Nach Cantor (1895) ist eine Menge definiert als “eine Zusammenfassung \\(M\\) von bestimmten wohlunterschiedenen Objekten \\(m\\) unsere Anschauung oder unseres Denken (welche die Elemente der Menge genannt werden) zu einem Ganzen”. Wir schreiben \\[\\begin{equation}\nm \\in M \\mbox{ bzw. } m \\notin M\n\\end{equation}\\] um auszudrücken, dass \\(m\\) ein Element bzw. kein Element von \\(M\\) ist.\n\n\n\nAuflisten der Elemente in geschweiften Klammern, z.B. \\(M := \\{1,2,3\\}\\).\nAngabe der Eigenschaften der Elemente, z.B. \\(M := \\{x \\in \\mathbb{N}|x &lt; 4\\}\\).\nGleichsetzen mit einer anderen eindeutig definieren Menge, z.B. \\(M := \\mathbb{N}_3\\).\n\n\n\n\nDefinition 2.2 (Teilmengen und Mengengleichheit) \\(A\\) und \\(B\\) seien zwei Mengen.\n\nEine Menge \\(A\\) heißt Teilmenge einer Menge \\(B\\), wenn für jedes Element \\(a \\in A\\) gilt, dass auch \\(a\\in B\\). Ist \\(A\\) eine Teilmenge von \\(B\\), so schreibt man \\[\\begin{equation}\nA \\subseteq B\n\\end{equation}\\] und nennt \\(A\\) Untermenge von \\(B\\) und \\(B\\) Obermenge von \\(A\\).\nEine Menge \\(A\\) heißt echte Teilmenge einer Menge \\(B\\), wenn für jedes Element \\(a \\in A\\) gilt, dass auch \\(a\\in B\\), es aber zumindest ein Element \\(b \\in B\\) gibt, für das gilt \\(b \\notin A\\). Ist \\(A\\) eine echte Teilmenge von \\(B\\), so schreibt man \\[\\begin{equation}\nA \\subset B.\n\\end{equation}\\]\nZwei Mengen \\(A\\) und \\(B\\) heißen gleich, wenn für jedes Element \\(a \\in A\\) gilt, dass auch \\(a \\in B\\), und wenn für jedes Element \\(b \\in B\\) gilt, dass auch \\(b\n\\in A\\). Sind die Mengen \\(A\\) und \\(B\\) gleich, so schreibt man \\[\\begin{equation}\nA = B.\n\\end{equation}\\]\n\n\n\n\n\nDefinition 2.3 (Kardinalität) Die Anzahl der Elemente einer Menge \\(M\\) heißt Kardinalität und wird mit \\(|M|\\) bezeichnet.\n\n\n\nDefinition 2.4 Eine Menge mit Kardinalität Null heißt leere Menge und wird mit \\(\\emptyset\\) bezeichnet.\n\n\n\n\nDefinition 2.5 (Potenzmenge) Die Menge aller Teilmengen einer Menge \\(M\\) heißt Potenzmenge von \\(M\\) und wird mit \\(\\mathcal{P}(M)\\) bezeichnet.\n\n\n\n\\(M_0 := \\emptyset\\) sei die leere Menge. Dann gilt \\[\\begin{equation}\n\\mathcal{P}(M_0) = \\emptyset.\n\\end{equation}\\]\n\\(M_1\\) sei die einelementige Menge \\(M_1 := \\{a\\}\\). Dann gilt \\[\\begin{equation}\n\\mathcal{P}(M_1) = \\{\\emptyset,\\{a\\}\\}.\n\\end{equation}\\]\nEs sei \\(M_2 := \\{a,b\\}\\). Dann hat \\(M_2\\) sowohl ein- als auch zweielementige Teilmengen und es gilt \\[\\begin{equation}\n\\mathcal{P}(M_2) = \\{\\emptyset,\\{a\\}\\, \\{b\\}, \\{a,b\\}\\}.\n\\end{equation}\\]\nSchließlich sei \\(M_3 := \\{a,b,c\\}\\). Dann hat \\(M\\) ein-, zwei-, als auch dreielementige Teilmengen und es gilt \\[\\begin{equation}\n\\mathcal{P}(M_3) = \\{\\emptyset, \\{a\\},\\{b\\},\\{c\\},\\{a,b\\},\\{a,c\\},\\{b,c\\},\\{a,b,c\\}\\}.\n\\end{equation}\\]",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Mengen</span>"
    ]
  },
  {
    "objectID": "102-Mengen.html#sec-verknuepfung-von-mengen",
    "href": "102-Mengen.html#sec-verknuepfung-von-mengen",
    "title": "2  Mengen",
    "section": "2.2 Verknüpfungen von Mengen",
    "text": "2.2 Verknüpfungen von Mengen\nZwei Mengen können auf unterschiedliche Weise miteinander verknüpft werden. Das Ergebnis einer solchen Verknüpfung ist eine weitere Menge. Wir bezeichnen die Verknüpfung zweier Mengen als Mengenoperation und geben folgende Definitionen.\n\nDefinition 2.6 (Mengenoperationen) \\(M\\) und \\(N\\) seien zwei Mengen.\n\nDie Vereinigung von \\(M\\) und \\(N\\) ist definiert als die Menge \\[\\begin{equation}\nM \\cup N := \\{x | x \\in M \\lor x \\in N\\},\n\\end{equation}\\] wobei \\(\\lor\\) wie immer im inklusiven Sinne als und/oder zu verstehen ist.\nDer Durchschnitt von \\(M\\) und \\(N\\) ist definiert als die Menge \\[\\begin{equation}\nM \\cap N := \\{x | x \\in M \\land x \\in N\\}.\n\\end{equation}\\] Wenn für \\(M\\) und \\(N\\) gilt, dass \\(M \\cap N= \\emptyset\\), dann heißen \\(M\\) und \\(N\\) disjunkt.\nDie Differenz von \\(M\\) und \\(N\\) ist definiert als die Menge \\[\\begin{equation}\nM\\setminus N := \\{x | x \\in M \\land x \\notin N\\}.\n\\end{equation}\\] Die Differenz \\(M\\) und \\(N\\) heißt, insbesondere bei \\(M \\subseteq N\\), auch das Komplement von \\(N\\) bezüglich \\(M\\) und wird mit \\(N^c\\) bezeichnet.\nDie symmetrische Differenz von \\(M\\) und \\(N\\) ist definiert als die Menge \\[\\begin{equation}\nM \\Delta N := \\{x|(x \\in M \\lor x \\in N ) \\land x \\notin M \\cap N\\},\n\\end{equation}\\] Die symmetrische Differenz kann also als exklusives oder verstanden werden.\n\n\nAls Beispiel betrachten wir die Mengen \\(M := \\{1,2,3\\}\\)und \\(N := \\{2,3,4,5\\}\\). Dann gelten\n\n\\(M \\cup N = \\{1,2,3,4,5\\}\\), weil \\(1 \\in M\\), \\(2 \\in M\\), \\(3 \\in M\\), \\(4 \\in N\\) und \\(5 \\in N\\).\n\\(M \\cap N = \\{2,3\\}\\), weil nur für \\(2\\) und \\(3\\) gilt, dass \\(2\\in M, 3 \\in M\\) und auch \\(2\\in N, 3 \\in N\\). Für \\(1\\) gilt lediglich, dass \\(1 \\in M\\) und für \\(4\\) und \\(5\\) gelten lediglich, dass \\(4 \\in N\\) und \\(5 \\in N\\).\n\\(M \\setminus N = \\{1\\}\\), weil \\(1 \\in M\\), aber \\(1 \\notin N\\) und \\(2 \\in M\\), aber auch \\(2 \\in N\\).\n\\(N \\setminus M = \\{4,5\\}\\), weil \\(2 \\in N\\) und \\(3 \\in N\\), aber auch \\(2\\in M\\) und \\(3 \\in M\\). Dies zeigt insbesondere, dass die Differenz von \\(M\\) Und \\(N\\) nicht symmetrisch ist, also dass nicht zwangsläufig gilt, dass \\(M\\setminus N\\) gleich \\(N \\setminus M\\) ist.\n\\(M \\Delta N = \\{1,4,5\\}\\), weil \\(1 \\in M\\), aber \\(1 \\notin \\{2,3\\}\\), \\(2 \\in M\\), aber \\(2 \\in \\{2,3\\}\\), \\(3 \\in M\\), aber \\(3 \\in \\{2,3\\}\\), \\(4 \\in N\\), aber \\(4 \\notin \\{2,3\\}\\) und \\(5 \\in N\\), aber \\(5 \\notin \\{2,3\\}\\).\n\nSchließlich wollen wir noch den Begriff der Partition einer Menge einführen.\n\nDefinition 2.7 (Partition) \\(M\\) sei eine Menge und \\(P := \\{N_i\\}\\) sei eine Menge von Mengen \\(N_i\\) mit \\(i = 1,...,n\\), so dass gilt \\[\\begin{equation}\nM = \\cup_{i=1}^n N_i \\land N_i \\cap N_j = \\emptyset \\mbox{ für } i = 1,...,n, j = 1,...,n, i \\neq j.\n\\end{equation}\\] Dann heißt \\(P\\) eine .\n\nIntuitiv entspricht die Partition einer Menge also dem Aufteilen der Menge in disjunkte Teilmengen. Partitionen sind generell nicht eindeutig, d.h. es gibt meist verschiedene Möglichkeiten eine gegebene Menge zu partitionieren. Betrachten wir zum Beispiel die Menge \\(M := \\{1,2,3,4,5,6\\}\\). Dann sind \\(P_1 := \\{\\{1\\}, \\{2,3,4,5,6\\}\\}\\), \\(P_2 := \\{\\{1,2,3\\}, \\{4,5,6\\}\\}\\) und \\(P_3 := \\{\\{1,2\\},\\{3,4\\}, \\{5,6\\}\\}\\) drei mögliche Partitionen von \\(M\\).",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Mengen</span>"
    ]
  },
  {
    "objectID": "102-Mengen.html#spezielle-mengen",
    "href": "102-Mengen.html#spezielle-mengen",
    "title": "2  Mengen",
    "section": "2.3 Spezielle Mengen",
    "text": "2.3 Spezielle Mengen\nIn der Naturwissenschaft versucht man, Phänomene der Welt mit Zahlen zu beschreiben. Je nach Phänomen bieten sich dazu diskrete oder kontinuierliche Zahlenmengen an. Die Mathematik stellt dazu unter anderem die in folgender Definition gegebenen Zahlenmengen bereit.\n\nDefinition 2.8 (Zahlenmengen) Es bezeichnen\n\n\\(\\mathbb{N}\\,\\,\\, := \\{1,2,3,...\\}\\) die natürlichen Zahlen,\n\\(\\mathbb{N}_n     := \\{1,2,3,...,n\\}\\) die natürlichen Zahlen der Ordnung \\(n\\),\n\\(\\mathbb{N}^0     := \\mathbb{N} \\cup \\{0\\}\\) die natürlichen Zahlen und Null,\n\\(\\mathbb{Z}\\,\\,\\, := \\{...,-3,-2,-1,0,1,2,3...\\}\\) die ganzen Zahlen,\n\\(\\mathbb{Q}\\,\\,\\, := \\{\\frac{p}{q}|p,q \\in \\mathbb{Z}, q \\neq 0\\}\\) die rationalen Zahlen,\n\\(\\mathbb{R}\\,\\,\\,\\) die reellen Zahlen, und\n\\(\\mathbb{C}\\,\\,\\, := \\{a + ib|a,b\\in \\mathbb{R}, i := \\sqrt{-1} \\}\\) die komplexen Zahlen.\n\n\nDie natürlichen und ganzen Zahlen eignen sich insbesondere zum Quantifizieren diskreter Phänomene. Die rationalen und insbesondere die reellen Zahlen eignen sich zum Quantifizieren kontinuierlicher Phänomene. \\(\\mathbb{R}\\) umfasst dabei die rationalen Zahlen und die sogenannten irrationalen Zahlen \\(\\mathbb{R}\\setminus\n\\mathbb{Q}\\). Rationale Zahlen sind Zahlen, die sich, wie oben definiert, durch Brüche ganzer Zahlen ausdrücken lassen. Dies sind alle ganzen Zahlen sowie die negativen und positiven Dezimalzahlen wie z.B. \\(-\\frac{9}{10} = -0.9\\), \\(\\frac{1}{3} = 1.3\\bar{3}\\), und \\(\\frac{196}{100} = 1.96\\). Irrationale Zahlen sind Zahlen, die sich nicht als rationale Zahlen ausdrücken lassen. Beispiele für irrationale Zahlen sind die Eulersche Zahl \\(e \\approx 2.71\\), die Kreiszahl \\(\\pi \\approx 3.14\\) und die Quadratwurzel von \\(2\\), \\(\\sqrt{2} \\approx 1.41\\).\nDie reellen Zahlen enthalten als Teilmengen die natürlichen, ganzen, und die rationalen Zahlen. Es gibt also sehr viele reelle Zahlen. Tatsächlich kann man beweisen (Cantor (1892)), dass es mehr reelle Zahlen als natürliche Zahlen gibt, obwohl es sowohl unendlich viele reelle Zahlen als auch unendlich viele natürliche Zahlen gibt. Diese Eigenschaft der reellen Zahlen bezeichnet man als die Überabzählbarkeit der reellen Zahlen. Insbesondere gilt \\[\\begin{equation}\n\\mathbb{N} \\subset \\mathbb{Z} \\subset \\mathbb{Q} \\subset \\mathbb{R}.\n\\end{equation}\\] Zwischen zwei reellen Zahlen gibt es unendlich viele weitere reelle Zahlen. Positiv-Unendlich (\\(\\infty\\)) und Negativ-Unendlich (\\(-\\infty\\)) sind keine Zahlen, mit denen in der Standardmathematik gerechnet werden kann. Sie gehören auch nicht zu den in obiger Definition gegebenen Zahlenmengen, es gilt also sowohl \\(\\infty \\notin \\mathbb{R}\\) als auch \\(-\\infty \\notin \\mathbb{R}\\).\nKomplexe Zahlen eignen sich zur Beschreibung zweidimensionaler kontinuierlicher Phänomene. Dabei werden die Werte der ersten Dimension im reellen Teil \\(a\\) und die Werte der zweiten Dimension im komplexen Teil \\(b\\) einer komplexen Zahl repräsentiert. Komplexe Zahlen kommen insbesondere bei der Modellierung physikalischer Phänomene und im Bereich der Fourieranalyse zum Einsatz. Wir vertiefen die Theorie komplexer Zahlen an dieser Stelle nicht.\nWichtige Teilmengen der reellen Zahlen sind die sogenannten Intervalle. Wir geben folgende Definitionen.\n\nDefinition 2.9 Zusammenhängende Teilmengen der reellen Zahlen heißen Intervalle. Für \\(a,b\\in \\mathbb{R}\\) unterscheidet man\n\ndas abgeschlossene Intervall \\[\\begin{equation}\n[a,b] := \\{x \\in \\mathbb{R}|a \\le x \\le b\\},\n\\end{equation}\\]\ndas offene Interval \\[\\begin{equation}\n]a,b[ := \\{x \\in \\mathbb{R}|a &lt; x &lt; b\\},\n\\end{equation}\\]\nund die halboffenen Intervalle \\[\\begin{equation}\n]a,b] := \\{x \\in \\mathbb{R}| a &lt; x \\le b\\} \\mbox{ und }\n[a,b[ := \\{x \\in \\mathbb{R}| a \\le x &lt; b\\}.\n\\end{equation}\\]\n\n\nWie oben erwähnt sind Positiv-Unendlich (\\(\\infty\\)) und Negativ-Unendlich (\\(-\\infty\\)) keine Elemente von \\(\\mathbb{R}\\). Es gilt also immer \\(]-\\infty,b]\\) oder \\(]-\\infty,b[\\) bzw. \\(]a,\\infty[\\) oder \\([a,\\infty[\\), sowie \\(\\mathbb{R} = ]-\\infty, \\infty[\\).\nOft möchte man mehrere Eigenschaften eines Phänomens gleichzeitig quantitativ beschreiben. Zu diesem Zweck können die oben definierten eindimensionalen Zahlenmenge durch Bildung Kartesischer Produkte auf mehrdimensionale Zahlenmengen erweitert werden. Die Elemente Kartesischer Produkte nennt man geordnete Tupel oder auch Vektoren.\n\nDefinition 2.10 (Kartesische Produkte) \\(M\\) und \\(N\\) seien zwei Mengen. Dann ist das Kartesische Produkt der Mengen \\(M\\) und \\(N\\) die Menge aller geordneten Tupel \\((m,n)\\) mit \\(m \\in M\\) und \\(n \\in N\\), formal \\[\\begin{equation}\nM \\times N := \\{(m,n)|m\\in M, n \\in N \\}.\n\\end{equation}\\]\nDas Kartesische Produkt einer Menge \\(M\\) mit sich selbst wird bezeichnet mit \\[\\begin{equation}\nM^2 := M \\times M.\n\\end{equation}\\] Seien weiterhin \\(M_1, M_2, ..., M_n\\) Mengen. Dann ist das Kartesische Produkt der Mengen \\(M_1,...,M_n\\) die Menge aller geordneten \\(n\\)-Tupel \\((m_1,...,m_n)\\) mit \\(m_i \\in M_i\\) für \\(i = 1,...,n\\), formal \\[\\begin{equation}\n\\prod_{i=1}^n M_i := M_1 \\times \\cdots \\times M_n := \\{(m_1,...,m_n)\n                |m_i \\in M_i \\mbox{ für } i = 1,...,n\\}.\n\\end{equation}\\] Das \\(n\\)-fache Kartesische Produkt einer Menge \\(M\\) mit sich selbst wird bezeichnet mit \\[\\begin{equation}\nM^n := \\prod_{i=1}^n M := \\{(m_1,,...,m_n)|m_i \\in M\\}.\n\\end{equation}\\]\n\nIm Gegensatz zu Mengen sind die in Definition 2.10 eingeführten Tupel geordnet. Das heißt, für Mengen gilt zum Beispiel \\(\\{1,2\\} = \\{2,1\\}\\), aber für Tupel gilt \\((1,2) \\neq (2,1)\\).\nWie oben beschrieben eignen sich insbesondere die reellen Zahlen zur Beschreibung kontinuierlicher Phänomene. Zur simultanen Beschreibung mehrere Aspekte eines kontinuierlichen Phänomens bietet sich entsprechend die Menge der reellen Tupel \\(n\\)-ter Ordnung an.\n\nDefinition 2.11 (Menge der reellen Tupel \\(n\\)-ter Ordnung) Das \\(n\\)-fache Kartesische Produkt der reellen Zahlen mit sich selbst wird bezeichnet mit \\[\\begin{equation}\n\\mathbb{R}^n := \\prod_{i=1}^n \\mathbb{R} := \\{x := (x_1,,...,x_n)|x_i \\in \\mathbb{R}\\}\n\\end{equation}\\] und wird “\\(\\mathbb{R}\\) hoch \\(n\\)” gesprochen. Wir schreiben die Elemente von \\(\\mathbb{R}^n\\) als Spalten \\[\\begin{equation}\nx :=\n\\begin{pmatrix}\nx_1\n\\\\\n\\vdots\n\\\\\nx_n\n\\end{pmatrix}\n\\end{equation}\\] und nennen sie \\(n\\)-dimensionale Vektoren. Zu Abgrenzung nennen wir die Elemente von \\(\\mathbb{R}^1 = \\mathbb{R}\\) auch Skalare.\n\nEin Beispiel für \\(x \\in \\mathbb{R}^4\\) ist \\[\\begin{equation}\nx = \\begin{pmatrix} 0.16 \\\\ 1.76 \\\\ 0.23 \\\\ 7.11 \\end{pmatrix}.\n\\end{equation}\\]",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Mengen</span>"
    ]
  },
  {
    "objectID": "102-Mengen.html#sec-selbstkontrollfragen-mengen",
    "href": "102-Mengen.html#sec-selbstkontrollfragen-mengen",
    "title": "2  Mengen",
    "section": "2.4 Selbstkontrollfragen",
    "text": "2.4 Selbstkontrollfragen\n\nGeben Sie die Definition einer Menge nach Cantor (1895) wieder.\nNennen Sie drei Möglichkeiten zur Definition einer Menge.\nErläutern Sie die Ausdrücke \\(m \\in M, m \\notin N, M \\subseteq N, M \\subset N\\) für zwei Mengen \\(M\\) und \\(N\\).\nGeben Sie die Definition der Kardinalität einer Menge wieder.\nGeben Sie die Definition der Potenzmenge einer Menge wieder.\nEs sei \\(M := \\{1,2\\}\\). Bestimmen Sie \\(\\mathcal{P}(M)\\).\nEs seien \\(M := \\{1,2\\}, N := \\{1,4,5\\}\\). Bestimmen Sie \\(M \\cup N, M \\cap N, M\\setminus N, M \\Delta N\\).\nErläutern Sie die Symbole \\(\\mathbb{N}\\), \\(\\mathbb{N}_n\\), und \\(\\mathbb{N}^0\\).\nErläutern Sie die Unterschiede zwischen \\(\\mathbb{N}\\) und \\(\\mathbb{Z}\\) und zwischen \\(\\mathbb{R}\\) und \\(\\mathbb{Q}\\).\nGeben Sie die Definition abgeschlossener, offener, und halboffener Intervalle wieder.\nEs seien \\(M\\) und \\(N\\) Mengen. Erläutern Sie die Notation \\(M \\times N\\).\nGeben Sie die Definition von \\(\\mathbb{R}^n\\) wieder.\n\n\n\n\n\nCantor, G. (1892). Über Eine Eigenschaft Des Inbegriffes Aller Reellen Algebraischen Zahlen. Jahresbericht der Deutschen Mathematiker-Vereinigung, 1.\n\n\nCantor, G. (1895). Beiträge Zur Begründung Der Transfiniten Mengenlehre. Mathematische Annalen, 46(4), 481–512. https://doi.org/10.1007/BF02124929",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Mengen</span>"
    ]
  },
  {
    "objectID": "103-Summen-Produkte-Potenzen.html",
    "href": "103-Summen-Produkte-Potenzen.html",
    "title": "3  Summen, Produkte, Potenzen",
    "section": "",
    "text": "3.1 Summen\nDiese Einheit führt einige Schreibweisen für die Grundrechenarten ein.\nFür die sinnvolle Benutzung des Summenzeichens ist es essentiell, dass mit mithilfe des Subskripts und des Superskripts Anfang und Ende der Summation festgelegt werden. Die genaue Bezeichnung des Laufindexes ist dagegen für den Wert der Summe irrelevant, es gilt \\[\\begin{equation}\n\\sum_{i=1}^n x_i = \\sum_{j=1}^n x_j.\n\\end{equation}\\] Manchmal wird der Laufindex auch als Element einer Indexmenge angegeben. Ist z.B. die Indexmenge \\(I := \\{1,5,7\\}\\) definiert, so ist \\[\\begin{equation}\n\\sum_{i \\in I}x_i := x_1 + x_5 + x_7.\n\\end{equation}\\] Im Folgenden wollen wir kurz einige Beispiele für die Benutzung des Summenzeichens betrachten.\nDer Umgang mit dem Summenzeichen wird oft durch die Anwendung folgender Rechenregeln vereinfacht.\nAls Beispiel für die Anwendung einer Rechenregel betrachten wir die Auswertung eines Mittelwertes (manchmal auch Durchschnitt genannt). Dazu seien \\(x_1, x_2,...,x_n\\) reelle Zahlen. Der Mittelwert dieser Zahlen entspricht der Summe von \\(x_1, x_2,...,x_n\\) geteilt durch die Anzahl der Zahlen \\(n\\). Dabei ist es nach obiger Rechenregel (3) irrelevant, ob zunächst die Zahlen aufaddiert werden und dann die resultierende Summe durch \\(n\\) geteilt wird, oder die Zahlen jeweils einzeln durch \\(n\\) geteilt werden und die entsprechenden Ergebenisse dann aufaddiert werden. Genauer gilt durch Anwendung von Rechenregel (3) mit \\(a =1/n\\), dass \\[\\begin{equation}\n\\frac{1}{n}\\sum_{i=1}^n x_i = \\sum_{i=1}^n \\frac{x_i}{n}.\n\\end{equation}\\] So ist zum Beispiel der Mittelwert von \\(x_1 := 1, x_2 := 4, x_3 := 2\\) \\(x_4 := 1\\) gegeben durch \\[\\begin{equation}\n\\frac{1}{4}\\sum_{i=1}^4 x_i\n= \\frac{1}{4}(1 + 4 + 2 + 1)\n= \\frac{8}{4}\n= 2\n= \\frac{8}{4}\n= \\frac{1}{4} + \\frac{4}{4} + \\frac{2}{4} + \\frac{1}{4}\n= \\sum_{i=1}^4 \\frac{x_i}{4}.\n\\end{equation}\\]",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Summen, Produkte, Potenzen</span>"
    ]
  },
  {
    "objectID": "103-Summen-Produkte-Potenzen.html#sec-summen",
    "href": "103-Summen-Produkte-Potenzen.html#sec-summen",
    "title": "3  Summen, Produkte, Potenzen",
    "section": "",
    "text": "Definition 3.1 (Summenzeichen) Es bezeichnet \\[\\begin{equation}\n\\sum_{i=1}^{n} x_i = x_1 + x_{2} + \\cdots + x_{n}.\n\\end{equation}\\] Dabei stehen\n\n\\(\\Sigma\\) für das griechische Sigma, mnemonisch fürSumme,\ndas Subskript \\(i = 1\\) für den Laufindex und den Startindex,\ndas Superskript \\(n\\) für den Endindex und\n\\(x_1, x_2, ..., x_n\\) für die Summanden.\n\n\n\n\nSummation vordefinierter Summanden. Es seien \\(x_1 := 2\\), \\(x_2 := 10\\), \\(x_3 := -4\\). Dann gilt \\[\\begin{equation}\n\\sum_{i=1}^3 x_i = x_1 + x_2 + x_3 = 2 + 10 - 4 = 8.\n\\end{equation}\\]\nSummation natürlicher Zahlen. Es gilt \\[\\begin{equation}\n\\sum_{i=1}^5 i = 1 + 2 + 3 + 4 + 5 = 15.  \n\\end{equation}\\]\nSummation gerader natürlicher Zahlen. Es gilt \\[\\begin{equation}\n\\sum_{i=1}^5 2i = 2\\cdot 1  + 2\\cdot 2 + 2\\cdot 3 + 2\\cdot 4 + 2\\cdot 5 = 2 + 4 + 6 + 8 + 10 = 30.\n\\end{equation}\\]\nSummation ungerader natürlicher Zahlen. Es gilt \\[\\begin{equation}\n\\sum_{i=1}^5 (2i - 1) = 2\\cdot 1 - 1  + 2\\cdot 2 - 1 + 2\\cdot 3 - 1 + 2\\cdot 4 - 1 + 2\\cdot 5 - 1 = 1 + 3 + 5 + 7 + 9 = 25.\n\\end{equation}\\]\n\n\n\nTheorem 3.1 (Rechenregeln für Summen)  \n\nSummen gleicher Summanden \\[\\begin{equation}\n\\sum_{i=1}^n x = nx\n\\end{equation}\\]\nAssoziativität bei Summen gleicher Länge \\[\\begin{equation}\n\\sum_{i=1}^n x_i + \\sum_{i=1}^n y_i = \\sum_{i=1}^n (x_i + y_i)\n\\end{equation}\\]\nDistributivität bei Multiplikation mit einer Konstante \\[\\begin{equation}\n\\sum_{i=1}^n ax_i = a\\sum_{i=1}^n x_i\n\\end{equation}\\]\nAufspalten von Summen mit \\(1 &lt; m &lt; n\\) \\[\\begin{equation}\n\\sum_{i = 1}^n x_i = \\sum_{i=1}^m x_i + \\sum_{i=m+1}^n x_i\n\\end{equation}\\]\nUmindizierung \\[\\begin{equation}\n\\sum_{i=0}^n x_i = \\sum_{j = m}^{n+m} x_{j - m}\n\\end{equation}\\]\n\n\n\nBeweis. Man überzeugt sich von diesen Rechenregeln durch Ausschreiben der Summen und Anwenden der Rechenregeln von Addition und Multiplikation. Wir zeigen hier exemplarisch die Assoziativität bei Summen gleicher Länge und die Distributivität bei Multiplikation mit einer Konstante. Hinsichtlich ersterer haben wir \\[\\begin{align}\n\\begin{split}\n\\sum_{i=1}^n x_i + \\sum_{i=1}^n y_i\n& = x_1 + x_2 + \\cdots + x_n +  y_1 + y_2 + \\cdots + y_n    \\\\\n& = x_1 + y_1 + x_2 + y_2 + \\cdots + x_n + y_n               \\\\\n& = \\sum_{i=1}^n (x_i + y_i).\n\\end{split}\n\\end{align}\\] Hinsichtlich letzterer gilt \\[\\begin{align}\n\\begin{split}\n\\sum_{i=1}^n ax_i\n& = ax_1 + ax_2 + \\cdots + ax_n                             \\\\\n& = a(x_1 + x_2 + \\cdots + x_n)                             \\\\\n& = a\\sum_{i=1}^n x_i.\n\\end{split}\n\\end{align}\\]",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Summen, Produkte, Potenzen</span>"
    ]
  },
  {
    "objectID": "103-Summen-Produkte-Potenzen.html#sec-produkte",
    "href": "103-Summen-Produkte-Potenzen.html#sec-produkte",
    "title": "3  Summen, Produkte, Potenzen",
    "section": "3.2 Produkte",
    "text": "3.2 Produkte\nEine analoge Schreibweise zum Summenzeichen bietet das Produktzeichen für Produkte.\n\nDefinition 3.2 (Produktzeichen) Es bezeichnet \\[\\begin{equation}\n\\prod_{i=1}^{n} x_i = x_1 \\cdot x_{2} \\cdot \\cdots \\cdot x_{n}.\n\\end{equation}\\] Dabei stehen\n\n\\(\\prod\\) für das griechische i, mnemonisch für Produkt,\ndas Subskript \\(i = 1\\) für den Laufindex und den Startindex,\ndas Superskript \\(n\\) für den Endindex,\n\\(x_1, x_2, ..., x_n\\) für die Produktterme\n\n\nAnalog zum Summenzeichen gilt, dass das Produktzeichen nur mit Subskript und Superskripten zu Lauf- und Endindex Sinn ergibt. Die genaue Bezeichnung des Laufindizes ist wiederum irrelevant, es gilt \\[\\begin{equation}\n\\prod_{i=1}^n x_i = \\prod_{j=1}^n x_j.\n\\end{equation}\\] Auch hier wird in seltenen Fällen der Laufindex als Element einer Indexmenge angegeben. Ist z.B. die Indexmenge \\(J := \\mathbb{N}_2^0\\) definiert, so ist \\[\\begin{equation}\n\\prod_{j \\in J}x_j := x_0 \\cdot x_1 \\cdot x_2.\n\\end{equation}\\]",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Summen, Produkte, Potenzen</span>"
    ]
  },
  {
    "objectID": "103-Summen-Produkte-Potenzen.html#sec-potenzen",
    "href": "103-Summen-Produkte-Potenzen.html#sec-potenzen",
    "title": "3  Summen, Produkte, Potenzen",
    "section": "3.3 Potenzen",
    "text": "3.3 Potenzen\nProdukte von Zahlen mit sich selbst können mithilfe der Potenzschreibweise abgekürzt werden.\n\nDefinition 3.3 (Potenz) Für \\(a \\in \\mathbb{R}\\) und \\(n \\in \\mathbb{N}^0\\) ist die \\(n\\)-te Potenz von \\(a\\) definiert durch \\[\\begin{equation}\na^0 := 1 \\mbox{ und } a^{n+1} := a^n \\cdot a.\n\\end{equation}\\] Weiterhin ist für \\(a\\in \\mathbb{R} \\setminus 0\\) und \\(n \\in \\mathbb{N}^0\\) die negative \\(n\\)-te Potenz von \\(a\\) definiert durch \\[\\begin{equation}\na^{-n} := (a^n)^{-1} := \\frac{1}{a^n}.\n\\end{equation}\\] \\(a\\) wird dabei Basis und \\(n\\) wird Exponent genannt.\n\nDie Art der Definition von \\(a^{n+1}\\) mit Rückbezug auf die Potenz \\(a^n\\) in obiger Definition nennt man rekursiv. Die Definition \\(a^0 := 1\\) nennt man dabei den Rekursionsanfang; er macht die rekursive Definition von \\(a^{n+1}\\) erst möglich. Die Definition \\(a^{n+1} := a^n \\cdot a\\) nennt man auch Rekursionsschritt. Folgende Rechenregeln vereinfachen das Rechnen mit Potenzen.\n\nTheorem 3.2 (Rechenregeln für Potenzen) Für \\(a,b\\in \\mathbb{R}\\) und \\(n,m \\in \\mathbb{Z}\\) mit \\(a\\neq 0\\) bei negativen Exponenten gelten folgende Rechenregeln: \\[\\begin{align}\na^n a^m & = a^{n+m} \\\\\n(a^n)^m & = a^{nm}  \\\\\n(ab)^n  & = a^nb^n\n\\end{align}\\]\n\nWir verzichten auf einen Beweis. Beispielsweise gelten also \\[\\begin{equation}\n2^2 \\cdot 2^3 = (2\\cdot 2) \\cdot (2 \\cdot 2 \\cdot 2) = 2^5 = 2^{2 + 3},\n\\end{equation}\\] \\[\\begin{equation}\n(3^2)^3 = (3\\cdot 3)^3 = (3\\cdot 3)\\cdot(3\\cdot 3)\\cdot(3\\cdot 3)= 3^6 = 3^{2\\cdot3},\n\\end{equation}\\] und \\[\\begin{equation}\n(2 \\cdot 4)^2 = (2\\cdot 4)\\cdot (2 \\cdot 4) = (2 \\cdot 2)\\cdot(4\\cdot 4) = 2^2 \\cdot 4^2.\n\\end{equation}\\]\nIn enger Beziehung zur Potenz steht die Definition der \\(n\\)ten Wurzel:\n\nDefinition 3.4 (\\(n\\)-te Wurzel) Für \\(a \\in \\mathbb{R}\\) und \\(n \\in \\mathbb{N}\\) ist die \\(n\\)-te Wurzel von \\(a\\) definiert als die Zahl \\(r\\), so dass \\[\\begin{equation}\nr^n = a.\n\\end{equation}\\]\n\nBeim Rechnen mit Wurzeln ist die Potenzschreibweise von Wurzeln oft hilfreich, da sie die direkte Anwendung der Rechenregeln für Potenzen ermöglicht.\n\nTheorem 3.3 (Potenzschreibweise der \\(n\\)-ten Wurzel) Es sei \\(a \\in \\mathbb{R}\\), \\(n \\in \\mathbb{N}\\), und \\(r\\) die \\(n\\)-te Wurzel von \\(a\\). Dann gilt \\[\\begin{equation}\nr = a^{\\frac{1}{n}}\n\\end{equation}\\]\n\n\nBeweis. Es gilt \\[\\begin{equation}\n\\left(a^{\\frac{1}{n}}\\right)^n\n= a^{\\frac{1}{n}}\\cdot a^{\\frac{1}{n}}\\cdot \\cdots \\cdot a^{\\frac{1}{n}}\n= a^{\\sum_{i=1}^n \\frac{1}{n}}\n= a^1\n= a.\n\\end{equation}\\] Also gilt mit der Definition der \\(n\\)-ten Wurzel, dass \\(r = a^\\frac{1}{n}\\).\n\nDas Rechnen mit Quadratwurzeln wird durch die Potenzschreibweise \\(\\sqrt{x} = x^{\\frac{1}{2}}\\) sehr erleichtert. Zum Beispiel gilt \\[\\begin{equation}\n\\frac{2\\pi}{\\sqrt{2\\pi}}\n= \\frac{2\\pi}{(2\\pi)^{\\frac{1}{2}}}\n= (2\\pi)^{1} \\cdot (2\\pi)^{-\\frac{1}{2}}\n= (2\\pi)^{1-\\frac{1}{2}}\n= (2\\pi)^{\\frac{1}{2}}\n= \\sqrt{2\\pi}.\n\\end{equation}\\]",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Summen, Produkte, Potenzen</span>"
    ]
  },
  {
    "objectID": "103-Summen-Produkte-Potenzen.html#sec-selbstkontrollfragen-summen-produkte-potenzen",
    "href": "103-Summen-Produkte-Potenzen.html#sec-selbstkontrollfragen-summen-produkte-potenzen",
    "title": "3  Summen, Produkte, Potenzen",
    "section": "3.4 Selbstkontrollfragen",
    "text": "3.4 Selbstkontrollfragen\n\nGeben Sie die Definition des Summenzeichens wieder.\nBerechnen Sie die Summen \\(\\sum_{i=1}^3 2\\), \\(\\sum_{i=1}^3 i^2\\), und \\(\\sum_{i=1}^3 \\frac{2}{3}i\\).\nSchreiben Sie die Summe \\(1 + 3 + 5 + 7 + 9 + 11\\) mithilfe des Summenzeichens.\nSchreiben Sie die Summe \\(0 + 2 + 4 + 6 + 8 + 10\\) mithilfe des Summenzeichens.\nGeben Sie die Definition des Produktzeichens wieder.\nGeben Sie die Definition der \\(n\\)-ten Potenz von \\(a \\in \\mathbb{R}\\) wieder.\nBerechnen Sie \\(2^2\\cdot 2^3\\) und \\(2^5\\) und geben Sie die zugehörige Potenzregel wieder.\nBerechnen Sie \\(6^2\\) und \\(2^2\\cdot 3^2\\) und geben Sie die zugehörige Potenzregel wieder.\nBegründen Sie, warum die \\(n\\)-te Wurzel von \\(a\\) als \\(a^{\\frac{1}{n}}\\) geschrieben werden kann.\nBerechnen Sie \\((\\sqrt{2})^{\\frac{2}{3}}, 9^{\\frac{1}{2}}\\), und \\(4^{-\\frac{1}{2}}\\).",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Summen, Produkte, Potenzen</span>"
    ]
  },
  {
    "objectID": "104-Funktionen.html",
    "href": "104-Funktionen.html",
    "title": "4  Funktionen",
    "section": "",
    "text": "4.1 Definition und Eigenschaften\nEs ist zentral, zwischen der Funktion \\(f\\) als Zuordnungsvorschrift und einem Wert der Funktion \\(f(x)\\) als Element von \\(Z\\) zu unterscheiden. \\(x\\) ist das Argument der Funktion (der Input der Funktion), \\(f(x)\\) der Wert, den die Funktion \\(f\\) für das Argument \\(x\\) annimmt (der Output der Funktion). Üblicherweise folgt in der Definition einer Funktion \\(f(x)\\) die Definition der funktionalen Form von \\(f\\), also einer Regel, wie aus \\(x\\) der Wert \\(f(x)\\) zu bilden ist. Zum Beispiel wird in folgender Definition einer Funktion \\[\\begin{equation}\nf : \\mathbb{R} \\to \\mathbb{R}_{\\ge 0}, x \\mapsto f(x) := x^2\n\\end{equation}\\] die Definition der Potenz genutzt.\nFunktionen sind immer eindeutig, in dem Sinne dass sie jedem \\(x \\in D\\) bei jeder Anwendung der Funktion immer dasselbe \\(f(x) \\in Z\\) zuordnen. Funktionen setzen dabei Elemente von Mengen miteinander in Beziehung. Die Mengen dieser Elemente erhalten spezielle Bezeichnungen.\nMan beachte, dass der Wertebereich \\(f(D)\\) von \\(f\\) und die Zielmenge \\(Z\\) von \\(f\\) sind nicht notwendigerweise identisch sein müssen. Grundlegende Eigenschaften von Funktionen werden in folgender Definition festgelegt.\nAbbildung 4.1 verdeutlicht diese Definitionen anhand dreier (Gegen)beispiele.\nAbbildung 4.1 A visualisiert die nicht-injektive Funktion \\[\\begin{equation}\nf : \\{1,2,3\\} \\to \\{A,B\\}, x \\mapsto f(x) := \\begin{cases} f(1) & := A \\\\ f(2) & := A \\\\ f(3) & := B \\end{cases}.\n\\end{equation}\\] Die Funktion ist nicht-injektiv, weil es zum Element \\(A\\) in der Bildmenge von \\(f\\) mehr als ein Urbild in der Definitionsmenge von \\(f\\) gibt, nämlich \\(1\\) und \\(2\\).\nAbbildung 4.1 B visualisiert die nicht-surjektive Funktion \\[\\begin{equation}\ng : \\{1,2,3\\} \\to \\{A,B,C,D\\}, x \\mapsto g(x) := \\begin{cases} g(1) & := A \\\\ g(2) & := B \\\\ g(3) & := D \\end{cases}.\n\\end{equation}\\] Die Funktion ist nicht surjektiv, weil das Element \\(D\\) in der Zielmenge von \\(f\\) kein Urbild in der Definitionsmenge von \\(f\\) hat. Abbildung 4.1 C schließlich visualisiert die bijektive Funktion \\[\\begin{equation}\nh : \\{1,2,3\\} \\to \\{A,B,C\\}, x \\mapsto g(x) := \\begin{cases} h(1) & := A \\\\ h(2) & := B \\\\ h(3) & := C \\end{cases}.\n\\end{equation}\\] Zu jedem Element in der Zielmenge von \\(h\\) gibt es genau ein Urbild, die Funktion ist also injektiv und surjektiv und damit bijektiv.\nAls weiteres Beispiel betrachten wir die Funktion \\[\\begin{equation}\nf : \\mathbb{R} \\to \\mathbb{R}, x \\mapsto f(x) := x^2\n\\end{equation}\\] Diese Funktion ist nicht injektiv, weil z.B. für \\(x_1 = 2 \\neq -2 = x_2\\) gilt, dass \\(f(x_1) = 2^2 = 4 = (-2)^2 = f(x_2)\\). Weiterhin ist \\(f\\) auch nicht surjektiv, weil z.B. \\(-1 \\in \\mathbb{R}\\) kein Urbild unter \\(f\\) hat. Schränkt man die Definitionsmenge von \\(f\\) allerdings auf die nicht-negativen reellen Zahlen ein, definiert man also die Funktion \\[\\begin{equation}\n\\tilde{f} : [0,\\infty[ \\to [0,\\infty[, x \\mapsto \\tilde{f}(x) := x^2,\n\\end{equation}\\] so ist \\(\\tilde{f}\\) im Gegensatz zu \\(f\\) injektiv und surjektiv, also bijektiv.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Funktionen</span>"
    ]
  },
  {
    "objectID": "104-Funktionen.html#definition-und-eigenschaften",
    "href": "104-Funktionen.html#definition-und-eigenschaften",
    "title": "4  Funktionen",
    "section": "",
    "text": "Definition 4.1 (Funktion) Eine Funktion oder Abbildung \\(f\\) ist eine Zuordnungsvorschrift, die jedem Element einer Menge \\(D\\) genau ein Element einer Zielmenge \\(Z\\) zuordnet. \\(D\\) wird dabei Definitionsmenge von \\(f\\) und \\(Z\\) wird Zielmenge von \\(f\\) genannt. Wir schreiben \\[\\begin{equation}\nf : D \\to Z, x \\mapsto f(x),\n\\end{equation}\\] wobei \\(f : D \\to Z\\) gelesen wird als “die Funktion \\(f\\) bildet alle Elemente der Menge \\(D\\) eindeutig auf Elemente in \\(Z\\) ab” und \\(x \\mapsto f(x)\\) gelesen wird als “\\(x\\), welches ein Element von \\(D\\) ist, wird durch die Funktion \\(f\\) auf \\(f(x)\\) abgebildet, wobei \\(f(x)\\) ein Element von \\(Z\\) ist”. Der Pfeil \\(\\to\\) steht für die Abbildung zwischen den Mengen \\(D\\) und \\(Z\\), der Pfeil \\(\\mapsto\\) steht für die Abbildung zwischen einem Element von \\(D\\) und einem Element von \\(Z\\).\n\n\n\n\nDefinition 4.2 (Bildmenge und Urbildmenge) Es sei \\(f : D \\to Z, x \\mapsto f(x)\\) eine Funktion und es seien \\(D' \\subseteq D\\) und \\(Z' \\subseteq Z\\). Die Menge \\[\\begin{equation}\nf(D') := \\{z \\in Z| \\mbox{Es gibt ein } x \\in D' \\mbox{ mit } z = f(x)\\}\n\\end{equation}\\] heißt die Bildmenge von \\(D'\\)  und \\(f(D) \\subseteq Z\\) heißt der Wertebereich von \\(f\\). Weiterhin heißt die Menge \\[\\begin{equation}\nf^{-1}(Z') := \\{x \\in D | f(x) \\in Z'\\}\n\\end{equation}\\] die Urbildmenge von \\(Z'\\). \\(x \\in D\\) mit \\(z = f(x) \\in Z\\) heißt auch Urbild von \\(z\\).\n\n\n\nDefinition 4.3 (Injektivität, Surjektivität, Bijektivität) \\(f : D \\to Z, x \\mapsto f(x)\\) sei eine Funktion. \\(f\\) heißt injektiv, wenn es zu jedem Bild \\(z \\in f(D)\\) genau ein Urbild \\(x \\in D\\) gibt. Äquivalent gilt, dass \\(f\\) injektiv ist, wenn aus \\(x_1,x_2 \\in D\\) mit \\(x_1 \\neq x_2\\) folgt, dass \\(f(x_1) \\neq f(x_2)\\) ist. \\(f\\) heißt surjektiv, wenn \\(f(D) = Z\\) gilt, wenn also jedes Element der Zielmenge \\(Z\\) ein Urbild in der Definitionsmenge \\(D\\) hat. Schließlich heißt \\(f\\) bijektiv, wenn \\(f\\) injektiv und surjektiv ist. Bijektive Funktionen werden auch eineindeutige Funktionen (engl. one-to-one mappings) genannt.\n\n\n\n\n\n\n\n\nAbbildung 4.1: Injektivität, Surjektivität, Bijektivität.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Funktionen</span>"
    ]
  },
  {
    "objectID": "104-Funktionen.html#funktionentypen",
    "href": "104-Funktionen.html#funktionentypen",
    "title": "4  Funktionen",
    "section": "4.2 Funktionentypen",
    "text": "4.2 Funktionentypen\nDurch Verkettung lassen sich aus Funktionen weitere Funktionen bilden.\n\nDefinition 4.4 (Verkettung von Funktionen) Es seien \\(f : D \\to Z\\) und \\(g : Z \\to S\\) zwei Funktionen, wobei die Wertemenge von \\(f\\) mit der Definitionsmenge von \\(g\\) übereinstimmen sollen. Dann ist durch \\[\\begin{equation}\ng \\circ f : D \\to S, x \\mapsto (g \\circ f)(x) := g(f(x))\n\\end{equation}\\] eine Funktion definiert, die die Verkettung von \\(f\\) und \\(g\\) genannt wird.\n\nDie Schreibweise für verkettete Funktionen ist etwas gewöhnungsbedürftig. Wichtig ist es zu erkennen, dass \\(g \\circ f\\) die verkette Funktion und \\((g \\circ f)(x)\\) ein Element in der Zielmenge der verketten Funktion bezeichnen. Intuitiv wird bei der Auswertung von \\((g \\circ f)(x)\\) zunächst die Funktion \\(f\\) auf \\(x\\) angewendet und dann die Funktion \\(g\\) das Element auf \\(f(x)\\) von \\(R\\) angewendet. Dies ist in der funktionalen Form \\(g(f(x))\\) festgehalten. Der Einfachheit halber benennt man die Verkettung zweier Funktionen auch oft mit einem einzelnen Buchstaben und schreibt beispielsweise, \\(h := g \\circ f\\) mit \\(h(x) = g(f(x))\\).\nLeicht zur Verwirrung kann es führen, wenn Elemente in der Zielmenge von \\(f\\) mit \\(y\\) bezeichnet werden, also die Schreibweise \\(y = f(x)\\) und \\(h(x) = g(y)\\) genutzt wird. Allerdings ist diese Schreibweise manchmal zur notationellen Vereinfachung nötig.\nAls Beispiel für die Verkettung zweier Funktionen betrachten wir \\[\\begin{equation}\nf : \\mathbb{R} \\to \\mathbb{R}, x \\mapsto f(x) := -x^2\n\\end{equation}\\] und \\[\\begin{equation}\ng : \\mathbb{R} \\to \\mathbb{R}, x \\mapsto g(x) := \\exp(x).\n\\end{equation}\\] Die Verkettung von \\(f\\) und \\(g\\) ergibt sich in diesem Fall zu \\[\\begin{equation}\ng \\circ f : \\mathbb{R} \\to \\mathbb{R}, x \\mapsto (g \\circ f)(x) := g(f(x)) = \\exp\\left(-x^2\\right).\n\\end{equation}\\]\nEine erste Anwendung der Verkettung von Funktionen findet sich in folgender Definition.\n\nDefinition 4.5 (Inverse Funktion) Es sei \\(f : D \\to Z, x \\mapsto f(x)\\) eine bijektive Funktion. Dann heißt die Funktion \\(f^{-1}\\) mit \\[\\begin{equation}\nf^{-1} \\circ f : D \\to D, x \\mapsto (f^{-1} \\circ f)(x) := f^{-1}(f(x)) = x\n\\end{equation}\\] inverse Funktion, Umkehrfunktion oder einfach Inverse von \\(f\\).\n\nInverse Funktionen sind immer bijektiv. Dies folgt, weil \\(f\\) bijektiv ist und damit jedem \\(x \\in D\\) genau ein \\(f(x) = z \\in Z\\) zugeordnet wird. Damit wird aber auch jedem \\(z \\in Z\\) genau ein \\(x \\in D\\), nämlich \\(f^{-1}(f(x)) = x\\) zugeordnet.\nIntuitiv macht die inverse Funktion von \\(f\\) den Effekt von \\(f\\) auf ein Element \\(x\\) rückgängig. Betrachtet man den Graphen einer Funktion in einem Kartesischen Koordinatensystem, so führt die Anwendung von einem Wert auf der \\(x\\)-Achse zu einem Wert auf der \\(y\\)-Achse. Die Anwendung der inversen Funktion führt dementsprechend von einem Wert auf der \\(y\\)-Achse zu einem Wert auf der \\(x\\)-Achse. Betrachten wir zum Beispiel die Funktion \\[\\begin{equation}\nf : \\mathbb{R} \\to \\mathbb{R}, x \\mapsto f(x) := 2x =:y.\n\\end{equation}\\] Dann ist die inverse Funktion von \\(f\\) gegeben durch \\[\\begin{equation}\nf^{-1} : \\mathbb{R} \\to \\mathbb{R}, y \\mapsto f^{-1}(y) := \\frac{1}{2}y,\n\\end{equation}\\] weil für jedes \\(x \\in \\mathbb{R}\\) gilt, dass \\[\\begin{equation}\n(f^{-1} \\circ f)(x) := f^{-1}(f(x)) = f^{-1}(2x) = \\frac{1}{2}\\cdot 2x = x.\n\\end{equation}\\]\nEine wichtige Klasse von Funktionen sind lineare Abbildungen.\n\nDefinition 4.6 (Lineare Abbildung) Eine Abbildung \\(f : D \\to Z, x \\mapsto f(x)\\) heißt lineare Abbildung, wenn für \\(x,y \\in D\\) und einen Skalar \\(c\\) gelten, dass \\[\\begin{equation}\nf(x + y) = f(x) + f(y)  f(cx) = cf(x) \\tag*{(Additivität)}\n\\end{equation}\\] und \\[\\begin{equation}\nf(cx) = cf(x) \\tag*{(Homogenität)}\n\\end{equation}\\] Eine Abbildung, für die obige Eigenschaften nicht gelten, heißt nicht-lineare Abbildung.\n\nLineare Abbildungen sind oft als “gerade Linien” bekannt. Die allgemeine Definition linearer Abbildungen ist mit dieser Intuition nicht komplett kongruent. Insbesondere sind lineare Abbildungen nur solche Funktionen, die den Nullpunkt auf den Nullpunkt abbilden. Wir zeigen dazu folgendes Theorem.\n\nTheorem 4.1 (Lineare Abbildung der Null) \\(f : D \\to Z\\) sei eine lineare Abbildung. Dann gilt \\[\\begin{equation}\nf(0) = 0.\n\\end{equation}\\]\n\n\nBeweis. Wir halten zunächst fest, dass mit der Additivität von \\(f\\) gilt, dass \\[\\begin{equation}\nf(0) = f(0 + 0) = f(0) + f(0).\n\\end{equation}\\] Addition von \\(-f(0)\\) auf beiden Seiten obiger Gleichung ergibt dann \\[\\begin{align}\n\\begin{split}\nf(0) - f(0) & = f(0) + f(0) - f(0) \\\\\n0           & = f(0) \\\\\n\\end{split}\n\\end{align}\\] und damit ist alles gezeigt.\n\nWir wollen den Begriff der linearen Abbildung noch an zwei Beispielen verdeutlichen.\n\nFür \\(a \\in \\mathbb{R}\\) ist die Abbildung \\[\\begin{equation}\nf : \\mathbb{R} \\to \\mathbb{R}, x \\mapsto f(x) := ax\n\\end{equation}\\] eine lineare Abbildung, weil gilt, dass \\[\\begin{equation}\nf(x + y) = a(x + y) = ax + ay = f(x) + f(y) \\mbox{ und } f(cx) = acx = cax = cf(x).\n\\end{equation}\\]\nFür \\(a,b \\in \\mathbb{R}\\) ist dagegen die Abbildung \\[\\begin{equation}\nf : \\mathbb{R} \\to \\mathbb{R}, x \\mapsto f(x) := ax + b\n\\end{equation}\\] nicht-linear, weil z.B. für \\(a := b := 1\\) gilt, dass \\[\\begin{equation}\nf(x+y) = 1(x+y)+1 = x + y + 1 \\neq x + 1 + y + 1 = f(x) + f(y).\n\\end{equation}\\]\n\nEine Abbildung der Form \\(f(x) := ax + b\\) heißt linear-affine Abbildung oder linear-affine Funktion. Etwas unsauber werden Funktionen der Form \\(f(x) := ax + b\\) auch manchmal als lineare Funktionen bezeichnet.\nNeben den bisher diskutierten Funktionentypen gibt es noch viele weitere Klassen von Funktionen. In folgender Definition klassifizieren wir Funktionen anhand der Dimensionalität ihrer Definitions- und Zielmengen. Diese Art der Funktionsklassifikation ist oft hilfreich, um sich einen ersten Überblick über ein mathematisches Modell zu verschaffen.\n\nDefinition 4.7 (Funktionenarten) Wir unterscheiden\n\nunivariate reellwertige Funktionen der Form \\[\\begin{equation}\nf : \\mathbb{R} \\to \\mathbb{R}, x \\mapsto f(x),\n\\end{equation}\\]\nmultivariate reellwertige Funktionen der Form \\[\\begin{equation}\nf : \\mathbb{R}^n \\to \\mathbb{R}, x \\mapsto f(x) = f(x_1,...,x_n),\n\\end{equation}\\]\nund multivariate vektorwertige Funktionen der Form \\[\\begin{equation}\nf : \\mathbb{R}^n \\to \\mathbb{R}^m, x \\mapsto\nf(x) =\n\\begin{pmatrix}\nf_1(x_1,...,x_n)    \\\\\n\\vdots              \\\\\nf_m(x_1,...,x_n)\n\\end{pmatrix},\n\\end{equation}\\] wobei \\(f_i, i = 1,...,m\\) die Komponenten(funktionen) von \\(f\\) genannt werden.\n\n\nIn der Physik werden multivariate reellwertige Funktionen Skalarfelder und multivariate vektorwertige Funktionen Vektorfelder genannt. In manchen Anwendungen treten zum Beispiel auch matrixvariate matrixwertige Funktionen auf.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Funktionen</span>"
    ]
  },
  {
    "objectID": "104-Funktionen.html#sec-elementare-funktionen",
    "href": "104-Funktionen.html#sec-elementare-funktionen",
    "title": "4  Funktionen",
    "section": "4.3 Elementare Funktionen",
    "text": "4.3 Elementare Funktionen\nAls elementare Funktionen bezeichnen wir eine kleine Schar von univariaten reellwertigen Funktionen, die häufig als Bausteine komplexerer Funktionen auftreten. Dies sind die Polynomfunktionen, die Exponentialfunktion, die Logarithmusfunktion und die Gammafunktion. Im Folgenden geben wir wesentliche Eigenschaften dieser Funktionen und ihre Graphen an. Für Beweise der Eigenschaften der hier vorgestellten F unktionen verweisen wir auf die weiterführende Literatur.\n\nDefinition 4.8 (Polynomfunktionen) Eine Funktion der Form \\[\\begin{equation}\nf : \\mathbb{R} \\to \\mathbb{R}, x \\mapsto f(x)\n:= \\sum_{i=0}^{k} a_i x^i = a_0 + a_1 x^1 + a_2 x^2 + \\cdots + a_k x^k\n\\end{equation}\\] heißt Polynomfunktion \\(k\\)-ten Grades mit Koeffizienten \\(a_0, a_1,...,a_k \\in \\mathbb{R}\\).\n\nEinige ausgewählte Polynomfunktionen sind in Tabelle aufgelistet, Abbildung 4.2 zeigt die enstprechende Graphen.\n\n\n\nAusgewählte Polynomfunktionen\n\n\n\n\n\n\n\nName\nFunktionale Form\nKoeffizienten\n\n\n\n\nKonstante Funktion\n\\(f(x) = a\\)\n\\(a_0 := a, a_i := 0, i &gt; 0\\)\n\n\nIdentitätsfunktion\n\\(f(x) = x\\)\n\\(a_0 := 0, a_1 := 1, a_i := 0, i &gt; 1\\)\n\n\nLinear-affine Funktion\n\\(f(x) = ax + b\\)\n\\(a_0 := b, a_1 := a, a_i := 0, i &gt; 1\\)\n\n\nQuadratfunktion\n\\(f(x) = x^2\\)\n\\(a_0 := 0, a_1 := 0, a_2 := 1, a_i := 0, i &gt; 2\\)\n\n\n\n\n\n\n\n\n\n\n\nAbbildung 4.2: Ausgewählte Polynomfunktionen\n\n\n\nEin wichtiges Funktionenpaar sind die Exponentialfunktion und die Logarithmusfunktion. Die Graphen der Exponential- und Logarithmusfunktion sind in Abbildung 4.3 abgebildet.\n\n\n\n\n\n\nAbbildung 4.3: Exponentialfunktion und Logarithmusfunktion\n\n\n\n\nDefinition 4.9 (Exponentialfunktion) Die Exponentialfunktion ist definiert als \\[\\begin{equation}\n\\exp : \\mathbb{R} \\to \\mathbb{R},\nx \\mapsto \\exp(x)\n:= e^x\n:= \\sum_{n=0}^\\infty \\frac{x^n}{n!}\n= 1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\frac{x^4}{4!} + \\cdots.\n\\end{equation}\\]\n\nDie Exponentialfunktion hat unter anderem folgende Eigenschaften.\nWertebereich der Exponentialfunktion\n\n\\(x \\in ]-\\infty,0[\\, \\Rightarrow \\exp(x) \\in ]0,1[\\)\n\\(x \\in ]0,\\infty[\\quad\\,\\,  \\Rightarrow \\exp(x) \\in ]1,\\infty[\\)\n\nInsbesondere nimmt die Exponentialfunktion also nur positive Werte an.\nMonotonieeigenschaft der Exponentialfunktion\n\n\\(x &lt; y \\Rightarrow \\exp(x) &lt; \\exp(y)\\)\n\nSpezielle Werte der Exponentialfunktion\n\n\\(\\exp(0) = 1\\)\n\\(\\exp(1) = e \\approx 2.71\\)\n\nDie Logarithmusfunktion schneidet die \\(y\\)-Achse also bei 0. Die Zahl \\(e\\) heißt Eulersche Zahl.\nSummationseigenschaft und Subtraktionseigenschaft der Exponentialfunktion\n\n\\(\\exp(x + y) = \\exp(x)\\exp(y)\\)\n\\(\\exp(x - y) = \\frac{\\exp(x)}{\\exp(y)}\\)\n\nMit den speziellen Werten der Exponentialfunktion gilt dann insbesondere auch \\[\\begin{equation}\n\\exp(x)\\exp(-x) = \\exp(x - x) = \\exp(0) = 1.\n\\end{equation}\\]\n\nDefinition 4.10 (Logarithmusfunktion) Die Logarithmusfunktion ist definiert als inverse Funktion der Exponentialfunktion, \\[\\begin{equation}\n\\ln : ]0,\\infty[ \\to \\mathbb{R}, x \\mapsto \\ln(x)\n\\mbox{ mit } \\ln(\\exp(x)) = x \\mbox{ für alle } x \\in \\mathbb{R}.\n\\end{equation}\\]\n\nDie Logarithmusfunktion hat unter anderem folgende Eigenschaften.\nWertebereich der Logarithmusfunktion\n\n\\(x \\in \\, ]0,1[\\,\\,\\, \\Rightarrow \\ln(x) \\in\\,]-\\infty,0[\\)\n\\(x \\in \\, ]1,\\infty[ \\Rightarrow \\ln(x) \\in\\, ]0,\\infty[\\)\n\nDie Logarithmusfunktion nimmt also sowohl negative als auch positive Werte an.\nMonotonie der Logarithmusfunktion\n\n\\(x &lt; y \\Rightarrow \\ln(x) &lt; \\ln(y)\\)\n\nSpezielle Werte der Logarithmusfunktion\n\n\\(\\ln(1) = 0\\) und \\(\\ln(e) = 1\\).\n\nDie Logarithmusfunktion schneidet die \\(x\\)-Achse also bei 1.\nProdukteigenschaft, Potenzeigenschaft und Divisionseigenschaft der Logarithmusfunktion\n\n\\(\\ln(xy) = \\ln(x) + \\ln(y)\\)\n\\(\\ln(x^c) = c\\ln(x)\\)\n\\(\\ln\\left(\\frac{1}{x}\\right) = -\\ln(x)\\)\n\nLetztere Eigenschaft sind beim Rechnen Logarithmusfunktionen zentral. Man merkt sie sich intuitiv als “Die Logarithmusfunktion wandelt Produkte in Summen und Potenzen in Produkte um.”\nEin häufiger Begleiter in der Wahrscheinlichkeitstheorie ist die Gammafunktion. Ein Auschnitt des Graphen der Gammafunktion ist in Abbildung 4.4 dargestellt.\n\n\n\n\n\n\nAbbildung 4.4: Gammafunktion\n\n\n\n\nDefinition 4.11 (Gammafunktion) Die Gammafunktion ist definiert durch \\[\\begin{equation}\n\\Gamma : \\mathbb{R} \\to \\mathbb{R}, x \\mapsto\n\\Gamma(x) := \\int_0^\\infty \\xi^{x-1}\\exp(-\\xi)\\,d\\xi\n\\end{equation}\\]\n\nDie Gammafunktion hat folgende Eigenschaften:\nSpezielle Werte der Gammafunktion\n\n\\(\\Gamma(1) = 1\\)\n\\(\\Gamma\\left(\\frac{1}{2} \\right) = \\sqrt{\\pi}\\)\n\\(\\Gamma(n) = (n-1)!\\) für \\(n \\in \\mathbb{N}\\).\n\nRekursionseigenschaft der Gammafunktion\n\nFür \\(x&gt;0\\) gilt \\(\\Gamma(x+1) = x\\Gamma(x)\\)",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Funktionen</span>"
    ]
  },
  {
    "objectID": "104-Funktionen.html#sec-selbstkontrollfragen-funktionen",
    "href": "104-Funktionen.html#sec-selbstkontrollfragen-funktionen",
    "title": "4  Funktionen",
    "section": "4.4 Selbstkontrollfragen",
    "text": "4.4 Selbstkontrollfragen\n\nGeben Sie die Definition einer Funktion wieder.\nGeben Sie die Definition der Begriffe Bildmenge, Wertebereich, und Urbildmenge wieder.\nGeben Sie die Definitionen der Begriffe Surjektivität, Injektivität, und Bijektivität wieder.\nErläutern Sie, warum \\(f:\\mathbb{R} \\to \\mathbb{R}, x \\mapsto f(x) := x^2\\) weder injektiv noch surjektiv ist.\nErläutern Sie, warum \\(f: [0,\\infty[ \\to [0,\\infty[ , x \\mapsto f(x) := x^2\\) bijektiv ist.\nGeben Sie die Definition der Verkettung von Funktionen wieder.\nGeben Sie die Definition des Begriffs der inversen Funktion wieder.\nGeben Sie die inverse Funktion von \\(x^2\\) auf \\([0,\\infty[\\) an.\nGeben Sie die Definition des Begriffs der linearen Abbildung wieder.\nGeben Sie die Definitionen der Begriffe der univariat-reellwertigen, multivariat-reellwertigen und multivariat-vektorwertigen Funktion wieder.\nSkizzieren Sie die Identitätsfunktion und die konstante Funktion für \\(a := 1\\).\nSkizzieren Sie die linear-affine Funktion \\(f(x) = ax + b\\) für \\(a = 2\\) und \\(b = 3\\).\nSkizzieren Sie die Funktionen \\(f(x) := (x-1)^2\\) und \\(g(x) := (x + 3)^2\\).\nSkizzieren Sie die Exponential- und Logarithmusfunktionen.\nGeben Sie die Summations- und Subtraktionseigenschaften der Exponentialfunktion an.\nGeben Sie die Produkt-, Potenz- und Divisionseigenschaften der Logarithmusfunktion an.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Funktionen</span>"
    ]
  },
  {
    "objectID": "105-Differentialrechnung.html",
    "href": "105-Differentialrechnung.html",
    "title": "5  Differentialrechnung",
    "section": "",
    "text": "5.1 Definitionen und Rechenregeln\nWir beginnen mit folgender Definition.\nFür \\(h&gt;0\\) heißt der Ausdruck \\[\\begin{equation}\n\\frac{f(a+h)-f(a)}{h}\n\\end{equation}\\] Newtonscher Differenzquotient. Der Newtonsche Differenzquotient misst die Änderung \\(f(a+h)-f(a)\\) von \\(f\\) pro Strecke \\(h\\) auf der \\(x\\)-Achse. Wenn also zum Beispiel \\(f(a)\\) und \\(f(a+h)\\) die Position eines Objektes zu einem Zeitpunkt \\(a\\) und zu einem späteren Zeitpunkt \\(a+h\\) repräsentieren, dann ist \\(f(a+h)-f(a)\\) die von diesem Objekt in der Zeit \\(h\\) zurückgelegte Strecke, also seine durchschnittliche Geschwindigkeit über den Zeitraum \\(h\\). Für \\(h\\to 0\\) misst der Newtonsche Differenzquotient die instantane Änderungsrate von \\(f\\) in \\(a\\), also im Beispiel die Geschwindigkeit des Objektes zu einem Zeitpunkt \\(a\\).\nAus mathematischer Sicht ist es wichtig, bei der Definition der Ableitung zwischen den Symbolen \\(f'(a)\\) und \\(f'\\) zu unterscheiden. Wie üblich bezeichnet \\(f'(a)\\) den Wert einer Funktion, also eine Zahl. \\(f'\\) dagegen bezeichnet eine Funktion, nämlich die Funktion, deren Werte als \\(f'(a)\\) für alle \\(a \\in \\mathbb{R}\\) bestimmt sind.\nEs existieren in der Literatur verschiedene, historisch gewachsene Notationen für Ableitungen, welche alle das identische Konzept der Ableitung repräsentieren.\nWir werden im Folgenden für univariate reellwertige Funktionen vor allem die Lagrange-Notation \\(f'\\) und \\(f'(x)\\) als Bezeichner wählen. In Berechnungen nutzen wir auch eine adapatierte Form der Leibniz-Notation und verstehen dort die Schreibweise \\(\\frac{d}{dx}f(x)\\) als den Auftrag, die Ableitung von \\(f\\) zu berechnen. Die Newton-Notation wird vor allem eingesetzt, wenn das Funktionsargument die Zeit repräsentiert und dann üblicherweise mit \\(t\\) für time bezeichnet wird. \\(\\dot{f}(t)\\) bezeichnet dann die Änderungsrate von \\(f\\) zum Zeitpunkt \\(t\\). Die Euler-Notation ist vor allem im Kontext multivariater reell- oder vektorwertiger Funktionen nützlich.\nBasierend auf der Definition der Ableitung einer univariaten reellwertigen Funktionen lassen sich leicht weitere Ableitungen einer solchen Funktion definieren.\nIn Analogie zu oben Gesagtem schreiben wir in Berechnungen auch \\(\\frac{d^2}{dx^2}f(x)\\) für den Auftrag, die zweite Ableitung einer Funktion \\(f\\) zu bestimmen. Die nullte Ableitung \\(f^{(0)}\\) von \\(f\\) ist \\(f\\) selbst. Der Tradition und Einfachheit halber schreibt man für \\(k &lt; 4\\) gemäß der Lagrange-Notation meist \\(f',f''\\) und \\(f'''\\) anstelle von \\(f^{(1)}, f^{(2)}\\) und \\(f^{(3)}\\).\nZum Bestimmen der Ableitung einer Funktion sind eine Reihe von Rechenregeln hilfreich, die es erlauben, die Ableitung einer Funktion aus den Ableitungen ihrer Unterfunktionen herzuleiten. Für Beweise der in folgendem Theorem eingeführten Rechenregeln verweisen wir auf die weiterführende Literatur.\nErste Beispiele für die Anwendung obiger Rechenregeln lernen wir im Abschnitt Kapitel 5.2 kennen. Wir setzen eine Reihe von Ableitungen elementarer Funktionen als bekannt voraus, diese sind in Tabelle zusammengstellt. Für Beweise verweisen wir wiederum auf die weiterführende Literatur.\nAbleitungen elementarer Funktionen\n\n\n\n\n\n\n\nName\nDefinition\nAbleitung\n\n\n\n\nPolynomfunktion\n\\(f(x) := \\sum_{i=0}^n a_ix^i\\)\n\\(f'(x) = \\sum_{i=1}^n ia_ix^{i-1}\\)\n\n\nKonstante Funktion\n\\(f(x) := a\\)\n\\(f'(x) = 0\\)\n\n\nIdentitätsfunktion\n\\(f(x) := x\\)\n\\(f'(x) = 1\\)\n\n\nLinear-affine Funktion\n\\(f(x) := ax + b\\)\n\\(f'(x) = a\\)\n\n\nQuadratfunktion\n\\(f(x) := x^2\\)\n\\(f'(x) = 2x\\)\n\n\nExponentialfunktion\n\\(f(x) := \\exp(x)\\)\n\\(f'(x) = \\exp(x)\\)\n\n\nLogarithmusfunktion\n\\(f(x) := \\ln(x)\\)\n\\(f'(x) = \\frac{1}{x}\\)\nIn Abbildung 5.1 visualisieren wir die Identitätsfunktion, eine linearen Funktion und die Quadratfunktion zusammen mit ihrer jeweiligen Ableitung. In Abbildung Abbildung 5.2 visualisieren wir die Exponential- und Logarithmusfunktionen zusammen mit ihrer jeweiligen Ableitung.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Differentialrechnung</span>"
    ]
  },
  {
    "objectID": "105-Differentialrechnung.html#sec-definitionen-rechenregeln",
    "href": "105-Differentialrechnung.html#sec-definitionen-rechenregeln",
    "title": "5  Differentialrechnung",
    "section": "",
    "text": "Definition 5.1 (Differenzierbarkeit und Ableitung) Es sei \\(I \\subseteq \\mathbb{R}\\) ein Intervall und \\[\\begin{equation}\nf : I \\to \\mathbb{R}, x \\mapsto f(x)\n\\end{equation}\\] eine univariate reellwertige Funktion. \\(f\\) heißt in \\(a \\in I\\) differenzierbar, wenn der Grenzwert \\[\\begin{equation}\nf'(a) := \\lim_{h\\to 0} \\frac{f(a+h)-f(a)}{h}\n\\end{equation}\\] existiert. \\(f'(a)\\) heißt dann die Ableitung von \\(f\\) an der Stelle \\(a\\). Ist \\(f\\) differenzierbar für alle \\(x \\in I\\), so heißt \\(f\\) differenzierbar und die Funktion \\[\\begin{equation}\nf' : I \\to \\mathbb{R}, x \\mapsto f'(x)\n\\end{equation}\\] heißt Ableitung von \\(f\\).\n\n\n\n\n\nDefinition 5.2 (Notation für Ableitungen univariater reellwertiger Funktionen) Es sei \\(f\\) eine univariate reellwertige Funktion. Äquivalente Schreibweisen für die Ableitung von \\(f\\) und die Ableitung von \\(f\\) an einer Stelle \\(x\\) sind\n\ndie Lagrange-Notation \\(f'\\) und \\(f'(x)\\),\ndie Leibniz-Notation \\(\\frac{df}{dx}\\) und \\(\\frac{df(x)}{dx}\\),\ndie Newton-Notation \\(\\dot{f}\\) und \\(\\dot{f}(x)\\), sowie\ndie Euler-Notation \\(Df\\) und \\(Df(x)\\),\n\nrespektive.\n\n\n\n\nDefinition 5.3 (Höhere Ableitungen) Es sei \\(f\\) eine univariate reellwertige Funktion und \\[\\begin{equation}\nf^{(1)} := f'\n\\end{equation}\\] sei die Ableitung von \\(f\\). Die \\(k\\)-te Ableitung von \\(f\\) ist rekursiv definiert durch \\[\\begin{equation}\nf^{(k)} := \\left(f^{(k-1)}\\right)' \\mbox{ für } k \\ge 0,\n\\end{equation}\\] unter der Annahme, dass \\(f^{(k-1)}\\) differenzierbar ist. Insbesondere ist die zweite Ableitung von \\(f\\) definiert durch die Ableitung von \\(f'\\), also \\[\\begin{equation}\nf'' := (f')'.\n\\end{equation}\\]\n\n\n\n\nTheorem 5.1 (Rechenregeln für Ableitungen) Für \\(i = 1,...,n\\) seien \\(g_i\\) reellwertige univariate differenzierbare Funktionen. Dann gelten folgende Rechenregeln:\n\nSummenregel \\[\\begin{equation}\n\\mbox{Für } f(x) := \\sum_{i=1}^n g_i(x) \\mbox{ gilt } f'(x) = \\sum_{i=1}^n g_i'(x).\n\\end{equation}\\]\nProduktregel \\[\\begin{equation}\n\\mbox{Für } f(x) := g_1(x)g_2(x) \\mbox{ gilt } f'(x) = g_1'(x)g_2(x) + g_1(x)g_2'(x).\n\\end{equation}\\]\nQuotientenregel \\[\\begin{equation}\n\\mbox{Für } f(x) := \\frac{g_1(x)}{g_2(x)} \\mbox{ gilt } f'(x) = \\frac{g_1'(x)g_2(x) - g_1(x)g_2'(x)}{g_2^2(x)}.\n\\end{equation}\\]\nKettenregel \\[\\begin{equation}\n\\mbox{Für } f(x) := g_1(g_2(x)) \\mbox{ gilt } f'(x) = g_1'(g_2(x))g'_2(x).\n\\end{equation}\\]\n\n\n\n\n\n\n\n\n\n\n\nAbbildung 5.1: Ableitungen dreier elementarer Funktionen\n\n\n\n\n\n\n\n\n\nAbbildung 5.2: Ableitungen von Exponentialfunktion und Logarithmusfunktion",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Differentialrechnung</span>"
    ]
  },
  {
    "objectID": "105-Differentialrechnung.html#sec-analytische-optimierung",
    "href": "105-Differentialrechnung.html#sec-analytische-optimierung",
    "title": "5  Differentialrechnung",
    "section": "5.2 Analytische Optimierung",
    "text": "5.2 Analytische Optimierung\nEine wichtige Anwendung der Differentialrechnung ist das Bestimmen von Extremstellen von Funktionen. Dabei geht es im Kern um die Frage, für welche Werte ihrer Definitionsmenge eine Funktion ein Maximum oder ein Minimum annimmt. Bei einfachen Funktionen ist dies analytisch möglich. Die generelle Vorgehensweise dabei ist oft auch unter dem Stichwort “Kurvendiskussion” bekannt. In der Anwendung ist ein analytisches Vorgehen zur Optimierung von Funktionen meist nicht möglich und es werden Computeralgorithmen zur Bestimmung von Extremstellen genutzt. Ein Verständnis dieser Algorithmen setzt allerdings ein Verständnis der Prinzipien der analytischen Optimierung voraus. In diesem Abschnitt geben wir eine Einführung in die analytische Optimierung von univariaten reellwertigen Funktionen. Wir gehen dabei eher informell vor. Einen formaleren Zugang geben wir an späterer Stelle im Kontext der nichtlinearen Optimierung. Wir beginnen damit, die Begriffe der erwähnten Maxima und Minima von univariaten reellwertigen Funktionen zu präzisieren.\n\nDefinition 5.4 (Extremstellen und Extremwerte) Es seien \\(U \\subseteq \\mathbb{R}\\) und \\(f : U \\to \\mathbb{R}\\) eine univariate reellwertige Funktion. \\(f\\) hat an der Stelle \\(x_0 \\in U\\)\n\nein lokales Minimum, wenn es ein Intervall \\(I := ]a,b[\\) gibt mit \\(x_0 \\in ]a,b[\\) und \\[\\begin{equation}\nf(x_0) \\le f(x) \\mbox{ für alle } x\\in I\\cap U,\n\\end{equation}\\]\nein globales Minimum, wenn gilt, dass \\[\\begin{equation}\nf(x_0) \\le f(x) \\mbox{ für alle } x\\in U,\n\\end{equation}\\]\nein lokales Maximum, wenn es ein Intervall \\(I := ]a,b[\\) gibt mit \\(x_0 \\in ]a,b[\\) und \\[\\begin{equation}\nf(x_0) \\ge f(x) \\mbox{ für alle }  x\\in I\\cap U,\n\\end{equation}\\]\nein lokales Maximum, wenn gilt, dass \\[\\begin{equation}\nf(x_0) \\ge f(x) \\mbox{ für alle } x\\in U.\n\\end{equation}\\]\n\nDer Wert \\(x_0 \\in U\\) der Definitionsmenge von \\(f\\) heißt entsprechend lokale oder globale Minimalstelle oder Maximalstelle, der Funktionswert \\(f(x_0) \\in \\mathbb{R}\\) heißt entsprechend lokales oder globales Minimum oder Maximum. Generell heißt der Wert \\(x_0 \\in U\\) Extremstelle und der Funktionswert \\(f(x_0) \\in \\mathbb{R}\\) Extremwert.\n\nExtremstellen von Funktionen werden häufig mit \\[\\begin{equation}\n\\underset{x \\in I \\cap U}{\\operatorname{argmin}} f(x) \\mbox{ oder }\n\\underset{x \\in I \\cap U}{\\operatorname{argmax}} f(x)\n\\end{equation}\\] bezeichnet und Extremwerte von Funktionen werden häufig mit \\[\\begin{equation}\n\\min_{x \\in I \\cap U} f(x) \\mbox{ oder }\n\\max_{x \\in I \\cap U} f(x)\n\\end{equation}\\] bezeichnet.\nDie analytische Optimierung von univariaten reellwertigen Funktionen basiert auf den sogenannten notwendigen und hinreichenden Bedingungen für Extrema. Erstere macht eine Aussage über das Verhalten der ersten Ableitung einer Funktion an einer Extremstelle, letztere macht eine Aussage über das Verhalten einer Funktion an einer Stelle, die bestimmten Forderungen an ihre erste und zweite Ableitung genügt.\n\nTheorem 5.2 (Notwendige Bedingung für Extrema) \\(f\\) sei eine univariate reellwertige Funktion. Dann gilt \\[\\begin{equation}\nx_0 \\mbox{ ist Extremstelle von } f \\Rightarrow f'(x_0) = 0.\n\\end{equation}\\]\n\nWenn \\(x_0\\) eine Extremstelle von \\(f\\) ist, dann ist also die erste Ableitung von \\(f\\) in \\(x_0\\) gleich null. Anstelle eines Beweises überlegen wir uns, dass zum Beispiel an eine lokaler Maximalstelle \\(x_0\\) von \\(f\\) gilt: links von \\(x_0\\) steigt \\(f\\) an, rechts von \\(x_0\\) fällt \\(f\\) ab. In \\(x_0\\) aber steigt \\(f\\) weder an, noch fällt \\(f\\) ab, es ist also nachvollziehbar, dass \\(f'(x_0) = 0\\) ist.\n\nTheorem 5.3 (Hinreichende Bedingungen für lokale Extrema) \\(f\\) sei eine zweimal differenzierbare univariate reellwertige Funktion.\n\nWenn für \\(x_0 \\in U \\subseteq \\mathbb{R}\\) \\[\\begin{equation}\nf'(x_0) = 0 \\mbox{ und } f''(x_0) &gt; 0\n\\end{equation}\\] gilt, dann hat \\(f\\) an der Stelle \\(x_0\\) ein Minimum.\nWenn für \\(x_0 \\in U \\subseteq \\mathbb{R}\\) \\[\\begin{equation}\nf'(x_0) = 0 \\mbox{ und } f''(x_0) &lt; 0\n\\end{equation}\\] gilt, dann hat \\(f\\) an der Stelle \\(x_0\\) ein Maximum.\n\n\nWir verzichten wiederum auf einen Beweis und verdeutlichen uns die Bedingung an dem in Abbildung 5.3 gezeigtem Beispiel. Hier ist offenbar \\(x_0 = 1\\) eine lokale Minimalstelle von \\(f(x) = (x-1)^2\\). Man erkennt: links von \\(x_0\\) fällt \\(f\\) ab, rechts von \\(x_0\\) steigt \\(f\\) an. In \\(x_0\\) steigt \\(f\\) weder an, noch fällt \\(f\\) ab, also ist \\(f'(x_0) = 0\\). Weiter gilt, dass links und rechts von \\(x_0\\) und in \\(x_0\\) die Änderung \\(f''\\) von \\(f'\\) positiv ist: links von \\(x_0\\) schwächt sich die Negativität von \\(f'\\) zu \\(0\\) ab und rechts von \\(x_0\\) verstärkt sich die Positivität von \\(f'\\).\n\n\n\n\n\n\nAbbildung 5.3: Analytische Optimierung von \\(f(x) := (x-1)^2\\)\n\n\n\nInsbesondere die hinreichende Bedingung für das Vorliegen von Extremstellen legt folgendes Standardverfahren zur Bestimmung von lokalen Extremstellen nahe.\n\nTheorem 5.4 (Standardverfahren der analytischen Optimierung) \\(f\\) sei eine univariate reellwertige Funktion. Lokale Extremstellen von \\(f\\) können mit folgendem Standardverfahren der analytischen Optimierung identifiziert werden:\n\nBerechnen der ersten und zweiten Ableitung von \\(f\\).\nBestimmen von Nullstellen \\(x^*\\) von \\(f'\\) durch Auflösen von \\(f'(x^*) = 0\\) nach \\(x^*\\). Die Nullstellen von \\(f'\\) sind dann Kandidaten für Extremstellen von \\(f\\).\nEvaluation von \\(f''(x^*)\\): Wenn \\(f''(x^*) &gt; 0\\) ist, dann ist \\(x^*\\) lokale Minimumstelle von \\(f\\); wenn \\(f''(x^*) &lt; 0\\) ist, dann ist \\(x^*\\) lokale Maximumstelle von \\(f\\); wenn \\(f''(x^*) = 0\\) ist, dann ist \\(x^*\\) keine Extremstelle von \\(f\\).\n\n\nAnstelle eines Beweises betrachten wir beispielhaft die Funktion \\[\\begin{equation}\nf : \\mathbb{R} \\to \\mathbb{R}, x \\mapsto f(x) := (x - 1)^2.\n\\end{equation}\\] aus Abbildung 5.3. Die erste Ableitung von \\(f\\) ergibt sich mit der Kettenregel zu \\[\\begin{equation}\nf'(x) = \\frac{d}{dx}\\left((x-1)^2 \\right) = 2(x-1)\\cdot \\frac{d}{dx}(x-1) = 2x - 2.\n\\end{equation}\\] Die zweite Ableitung von \\(f\\) ergibt sich zu \\[\\begin{equation}\nf''(x) = \\frac{d}{dx}f'(x) = \\frac{d}{dx}(2x - 2) = 2 &gt; 0 \\mbox{ für alle }\nx \\in \\mathbb{R}.\n\\end{equation}\\] Auflösen von \\(f'(x^*) = 0\\) nach \\(x^*\\) ergibt \\[\\begin{equation}\nf'(x^*) = 0\n\\Leftrightarrow\n2x^* - 2 = 0\n\\Leftrightarrow\n2x^* = 2\n\\Leftrightarrow\nx^* = 1.\n\\end{equation}\\] \\(x^* = 1\\) ist folglich eine Minimalstelle von \\(f\\) mit zugehörigen Minimalwert \\(f(1) = 0\\).",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Differentialrechnung</span>"
    ]
  },
  {
    "objectID": "105-Differentialrechnung.html#sec-differentialrechnung-multivariater-reellwertiger-funktionen",
    "href": "105-Differentialrechnung.html#sec-differentialrechnung-multivariater-reellwertiger-funktionen",
    "title": "5  Differentialrechnung",
    "section": "5.3 Differentialrechnung multivariater reellwertiger Funktionen",
    "text": "5.3 Differentialrechnung multivariater reellwertiger Funktionen\nWir erinnern zunächst an den Begriff der multivariaten reellwertigen Funktion.\n\nDefinition 5.5 (Multivariate reellwertige Funktion) Eine Funktion der Form \\[\\begin{equation}\nf : \\mathbb{R}^n \\to \\mathbb{R}, x \\mapsto f(x) = f(x_1,...,x_n)\n\\end{equation}\\] heißt Funktion.\n\nDie Argumente multivariater reellwertiger Funktionen sind also reelle \\(n\\)-Tupel der Form \\(x := (x_1,...,x_n)\\) während ihre Funktionswerte reelle Zahlen sind. Ein Beispiel für eine multivariate reellwertige für \\(n:=2\\) ist \\[\\begin{equation}\nf : \\mathbb{R}^2 \\to \\mathbb{R}, x \\mapsto f(x) := x_1^2 + x_2^2\n\\end{equation}\\]\nWir visualisieren diese Funktion in Abbildung 5.4. Dabei zeigt die rechte Abbildung eine Darstellung mithilfe sogenannter Isokonturen, also Linien im Definitionsbereich der Funktion, für die die Funktion identische Werte annimmt. Die entsprechenden Werte sind für ausgewählte Isokonturen in der Abbildung vermerkt.\n\n\n\n\n\n\nAbbildung 5.4: Visualisierungen einer bivariaten Funktion.\n\n\n\nWir wollen nun beginnen, die Begriffe der Differenzierbarkeit und der Ableitung univariater reellwertiger Funktionen auf den Fall multivariater reellwertiger Funktion zu erweitern. Dazu führen wir zunächst die Begriffe der partiellen Differenzierbarkeit und der partiellen Ableitung ein.\n\nDefinition 5.6 (Partielle Differenzierbarkeit und partielle Ableitung) Es sei \\(D \\subseteq \\mathbb{R}^n\\) eine Menge und \\[\\begin{equation}\nf : D \\to \\mathbb{R}, x \\mapsto f(x)\n\\end{equation}\\] eine multivariate reellwertige Funktion. \\(f\\) heißt in \\(a \\in D\\) , wenn der Grenzwert \\[\\begin{equation}\n\\frac{\\partial}{\\partial x_i}f(x) := \\lim_{h\\to 0} \\frac{f(a + he_i)-f(a)}{h}\n\\end{equation}\\] existiert. \\(\\frac{\\partial}{\\partial x_i}f(a)\\) heißt dann die . Wenn \\(f\\) für alle \\(x \\in D\\), nach \\(x_i\\) partiell differenzierbar ist, dann heißt \\(f\\) und die Funktion \\[\\begin{equation}\n\\frac{\\partial}{\\partial x_i} f: D \\to \\mathbb{R}, x \\mapsto \\frac{\\partial}{\\partial x_i}f(x)\n\\end{equation}\\] heißt . \\(f\\) heißt , wenn \\(f\\) für alle \\(i = 1,...,n\\) in \\(x \\in D\\) nach \\(x_i\\) partiell differenzierbar ist, und \\(f\\) heißt partiell differenzierbar, wenn \\(f\\) für alle \\(i = 1,...,n\\) in allen \\(x \\in D\\) nach \\(x_i\\) partiell differenzierbar ist.\n\nIn Definition 5.6 bezeichnet \\(e_i \\in \\mathbb{R}^n\\) bezeichnet den \\(i\\)ten kanonischen Einheitsvektor, für den gilt, dass \\(e_{i_j} = 1\\) für \\(i=j\\) und \\(e_{i_j} = 0\\) für \\(i \\neq j\\) mit \\(j = 1,...,n\\) (vgl. Definition 8.14). In Analogie und Verallgemeinerung zum Newtonschen Differenzquotienten misst der hier auftretende Differenzquotient \\[\\begin{equation}\n\\frac{f(x + he_i)-f(x)}{h}\n\\end{equation}\\] die Änderung \\(f(x+he_i)-f(x)\\) von \\(f\\) pro Strecke \\(h\\) in Richtung \\(e_i\\). Für \\(h\\to 0\\) misst der Differenzquotient entsprechend die Änderungsrate von \\(f\\) in \\(x\\) in Richtung \\(e_i\\). Wie bei der Betrachtung von Ableitungen gilt, dass \\(\\frac{\\partial}{\\partial x_i}f(x)\\) eine Zahl, \\(\\frac{\\partial}{\\partial x_i}f\\) dagegen eine Funktion ist. Praktisch berechnet man \\(\\frac{\\partial}{\\partial x_i}f\\) als die (einfache) Ableitung \\[\\begin{equation}\n\\frac{d}{dx_i}\\tilde{f}_{x_1,...x_{i-1},x_{i+1}, ...,x_n}(x_i)\n\\end{equation}\\] der univariaten reellwertigen Funktion \\[\\begin{equation}\n\\tilde{f} : \\mathbb{R} \\to \\mathbb{R}, x_i \\mapsto \\tilde{f}_{x_1,...x_{i-1},x_{i+1}, ...,x_n}(x_i) := f(x_1,...,x_i, ...,x_n).\n\\end{equation}\\] Man betrachtet für die \\(i\\)te partielle Ableitung also alle \\(x_j\\) mit \\(j \\neq i\\) als Konstanten und ist auf das gewohnte Berechnen von Ableitungen von univariaten reellwertigen Funktionen geführt. Wir wollen das Vorgehen zum Berechnen von partiellen Ableitungen an einem ersten Beispiel verdeutlichen.\nBeispiel (1)\nWir betrachten die Funktion \\[\\begin{equation}\nf:\\mathbb{R}^2\\to \\mathbb{R}, x\\mapsto f(x):=x_1^2+x_2^2.\n\\end{equation}\\] Weil die Definitionsmenge dieser Funktion zweidimensional ist, kann man zwei partielle Ableitungen berechnen \\[\\begin{equation}\\label{eq:pdex_1}\n\\frac{\\partial }{\\partial x_1}f:\\mathbb{R}^2 \\to \\mathbb{R},\nx\\mapsto \\frac{\\partial}{\\partial x_{1}} f(x)\n\\mbox{ und }\n\\frac{\\partial}{\\partial x_2} f:\\mathbb{R}^2\\to \\mathbb{R},\nx\\mapsto \\frac{\\partial }{\\partial x_2}f(x).\n\\end{equation}\\] Um die erste dieser partiellen Ableitungen zu berechnen, betrachtet man die Funktion \\[\\begin{equation}\nf_{x_2}:\\mathbb{R} \\to \\mathbb{R}, x_1 \\mapsto f_{x_2}(x_1):=x_1^2+x_2^2,\n\\end{equation}\\] wobei \\(x_2\\) hier die Rolle einer Konstanten einnimmt. Um explizit zu machen, dass \\(x_2\\) kein Argument der Funktion ist, die Funktion aber weiterhin von \\(x_2\\) abhängt haben wir die Subskriptnotation \\(f_{x_2}(x_1)\\) verwendet. Um nun die partielle Ableitung zu berechnen, berechnen wir die (einfache) Ableitung von \\(f_{x_2}\\), \\[\\begin{equation}\nf_{x_2}'(x)=2x_{1}.\n\\end{equation}\\] Es ergibt sich also \\[\\begin{equation}\n\\frac{\\partial}{\\partial x_1}f:\\mathbb{R}^2\\to \\mathbb{R},\nx\\mapsto \\frac{\\partial}{\\partial x_1}f(x)\n=\\frac{\\partial}{\\partial x_1}(x_1^2+x_2^2)\n=f_{x_2}'(x)=2x_1.\n\\end{equation}\\] Analog gilt mit der entsprechenden Formulierung von \\(f_{x_1}\\), dass \\[\\begin{equation}\n\\frac{\\partial}{\\partial x_2}f:\\mathbb{R}^2\\to \\mathbb{R},\nx\\mapsto \\frac{\\partial}{\\partial x_2}f(x)\n=\\frac{\\partial}{\\partial x_2}(x_1^2+x_2^2)\n=f_{x_1}'(x)=2x_2.\n\\end{equation}\\]\nWie bei der Ableitung einer univariaten reellwertigen Funktion ist es auch für eine multivariate reellwertige Funktion möglich, rekursiv eine höhere Ableitung zu definieren.\n\nDefinition 5.7 (Zweite partielle Ableitungen) \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\) sei eine multivariate reellwertige Funktion und \\(\\frac{\\partial}{\\partial x_i}f\\) sei die partielle Ableitung von \\(f\\) nach \\(x_i\\). Dann ist die zweite partielle Ableitung von \\(f\\) nach \\(x_i\\) und \\(x_j\\) definiert als \\[\\begin{equation}\n\\frac{\\partial^2}{\\partial x_j x_i} f(x)\n:= \\frac{\\partial}{\\partial x_j}\\left(\\frac{\\partial}{\\partial x_i}f\\right).\n\\end{equation}\\]\n\nMan beachte, dass es zu jeder partiellen Ableitung \\(\\frac{\\partial}{\\partial x_i}f\\) für \\(i = 1,...,n\\) insgesamt \\(n\\) zweite partiellen Ableitungen \\(\\frac{\\partial^2}{\\partial x_j\\partial x_i}f\\) für \\(j = 1,...,n\\) gibt. Die so resultierenden \\(n^2\\) zweiten partiellen Ableitungen sind jedoch nicht alle verschieden. Dies ist eine wesentliche Aussage des Satzes von Schwarz\n\nTheorem 5.5 (Satz von Schwarz) \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\) sei eine partiell differenzierbare multivariate reellwertige Funktion. Dann gilt \\[\\begin{equation}\n  \\frac{\\partial^2}{\\partial x_j\\partial x_i}f(x)\n= \\frac{\\partial^2}{\\partial x_i\\partial x_j}f(x)\n    \\mbox{ für alle }  1 \\le i,j \\le n.\n\\end{equation}\\]\n\nFür einen Beweis verweisen wir auf die weiterführende Literatur. Der Satz von Schwarz besagt insbesondere also auch, dass bei Bildung der zweiten partiellen Ableitungen die Reihenfolge des partiellen Ableitens irrelevant ist. Das Theorem erleichtert auf diese Weise die Berechnung von zweiten partiellen Ableitungen und hilft zudem, analytische Fehler bei der Berechnung zweiter partieller Ableitungen aufzudecken. Wir verdeutlichen dies in Fortführung obigen Beispiels.\nBeispiel (1)\nWir wollen die partiellen Ableitungen zweiter Ordnung der Funktion \\[\\begin{equation}\nf:\\mathbb{R}^{2}\\to \\mathbb{R}, x\\mapsto f(x):=x_1^2+x_2^2.\n\\end{equation}\\] berechnen. Mit den Ergebnissen für die partiellen Ableitungen erster Ordnung dieser Funktion ergibt sich \\[\\begin{align}\n\\begin{split}\n\\frac{\\partial^2}{\\partial x_1 x_1} f(x)\n& = \\frac{\\partial}{\\partial x_1}\\left(\\frac{\\partial}{\\partial x_1} f(x)\\right)\n  = \\frac{\\partial}{\\partial x_1}(2x_1)\n  = 2\n\\\\\n\\frac{\\partial^2}{\\partial x_1 x_2} f(x)\n& = \\frac{\\partial}{\\partial x_1}\\left(\\frac{\\partial}{\\partial x_2} f(x)\\right)\n  = \\frac{\\partial}{\\partial x_1}(2x_2)\n  = 0\n\\\\\n\\frac{\\partial^2}{\\partial x_2 x_1} f(x)\n& = \\frac{\\partial}{\\partial x_2}\\left(\\frac{\\partial}{\\partial x_1} f(x)\\right)\n  = \\frac{\\partial}{\\partial x_2}(2x_1)\n  = 0\n  \\\\\n\\frac{\\partial^2}{\\partial x_2 x_2} f(x)\n& = \\frac{\\partial}{\\partial x_2}\\left(\\frac{\\partial}{\\partial x_2} f(x)\\right)\n  = \\frac{\\partial}{\\partial x_2}(2x_2)\n  = 2\n\\end{split}\n\\end{align}\\] Offenbar gilt \\[\\begin{equation}\n\\frac{\\partial^2}{\\partial x_1 x_2} f(x) = \\frac{\\partial^2}{\\partial x_2 x_1} f(x).\n\\end{equation}\\]\nBeispiel (2)\nAls weiteres Beispiel wollen wird die partiellen Ableitungen erster und zweiter Ordnung der Funktion \\[\\begin{equation}\nf:\\mathbb{R}^{3}\\to \\mathbb{R}, x\\mapsto f(x):=x_1^2+x_1x_2+x_2\\sqrt{x_3}.\n\\end{equation}\\] berechnen. Mit den Rechenregeln für Ableitungen ergibt sich für die partiellen Ableitungen erster Ordnung \\[\\begin{align}\n\\begin{split}\n& \\frac{\\partial}{\\partial x_1}f(x)\n= \\frac{\\partial}{\\partial x_1}\\left(x_1^2+x_1x_2+x_2\\sqrt{x_3} \\right) = 2x_1+x_2,                   \\\\\n& \\frac{\\partial}{\\partial x_2}f(x)\n= \\frac{\\partial}{\\partial x_2}\\left(x_1^2+x_1x_2+x_2\\sqrt{x_3} \\right) = x_1+\\sqrt{x_3},             \\\\\n& \\frac{\\partial}{\\partial x_3}f(x)\n= \\frac{\\partial}{\\partial x_3}\\left(x_1^2+x_1x_2+x_2\\sqrt{x_3} \\right) = \\frac{x_{2}}{2\\sqrt{x_3}}.\n\\end{split}\n\\end{align}\\] Für die zweiten partiellen Ableitungen hinsichtlich \\(x_1\\) ergibt sich \\[\\begin{align}\n\\begin{split}\n    \\frac{\\partial^2}{\\partial x_1 \\partial x_1}f(x)\n& = \\frac{\\partial}{\\partial x_1} \\left(\\frac{\\partial}{\\partial x_1} f(x) \\right)\n  = \\frac{\\partial}{\\partial x_1}\\left(2x_1+x_2\\right) = 2, \\\\\n    \\frac{\\partial^2}{\\partial x_2\\partial x_1}f(x)\n& = \\frac{\\partial}{\\partial x_2} \\left(\\frac{\\partial}{\\partial x_1} f(x) \\right)\n  = \\frac{\\partial}{\\partial x_2}\\left(2x_1+x_2 \\right) = 1, \\\\\n    \\frac{\\partial^2}{\\partial x_3\\partial x_1} f(x)\n& = \\frac{\\partial}{\\partial x_3}\\left(\\frac{\\partial}{\\partial x_{1}} f(x) \\right)\n  = \\frac{\\partial}{\\partial x_3}\\left(2x_1+x_2\\right)=0.\n\\end{split}\n\\end{align}\\] Für die zweiten partiellen Ableitungen hinsichtlich \\(x_2\\) ergibt sich \\[\\begin{align}\n\\begin{split}\n    \\frac{\\partial^2}{\\partial x_1\\partial x_2}f(x)\n& = \\frac{\\partial}{\\partial x_1}\\left(\\frac{\\partial}{\\partial x_2}f(x) \\right)\n  = \\frac{\\partial}{\\partial x_{1}}\\left(x_1+ \\sqrt{x_3} \\right) = 1, \\\\\n    \\frac{\\partial^2}{\\partial x_2 \\partial x_2}f(x)\n& = \\frac{\\partial}{\\partial x_2}\\left(\\frac{\\partial}{\\partial x_2}f(x) \\right)\n  = \\frac{\\partial}{\\partial x_2}\\left(x_1 + \\sqrt{x_3} \\right) = 0, \\\\\n    \\frac{\\partial^2}{\\partial x_3\\partial x_2}f(x)\n& = \\frac{\\partial}{\\partial x_3}\\left(\\frac{\\partial}{\\partial x_2}f(x) \\right)\n  = \\frac{\\partial}{\\partial x_3}\\left(x_1+\\sqrt{x_3} \\right) =\\frac{1}{2\\sqrt{x_3}}.\n\\end{split}\n\\end{align}\\] Beispiel (2) Für die zweiten partiellen Ableitungen hinsichtlich \\(x_3\\) ergibt sich \\[\\begin{align}\n\\begin{split}\n    \\frac{\\partial^{2}}{\\partial x_1\\partial x_3}f(x)\n& = \\frac{\\partial}{\\partial x_1}\\left(\\frac{\\partial}{\\partial x_3} f(x) \\right)\n  = \\frac{\\partial}{\\partial x_1}\\left(\\frac{x_2}{2}\\sqrt{x_3}\\right) = 0, \\\\\n    \\frac{\\partial^2}{\\partial x_2\\partial x_3}f(x)\n& = \\frac{\\partial}{\\partial x_2}\\left(\\frac{\\partial}{\\partial x_3}f(x) \\right)\n  = \\frac{\\partial}{\\partial x_2}\\left(\\frac{x_2}{2 \\sqrt{x_3}} \\right)\n  = \\frac{1}{2\\sqrt{x_3}}, \\\\\n    \\frac{\\partial^2}{\\partial x_3 \\partial x_3}f(x)\n& = \\frac{\\partial}{\\partial x_3}\\left(\\frac{\\partial}{\\partial x_3}f(x) \\right)\n  = \\frac{\\partial}{\\partial x_3}\\left(x_2\\frac{1}{2}x_3^{-\\frac{1}{2}}\\right)\n  = -\\frac{1}{4}x_2x_3^{-\\frac{3}{2}}.\n\\end{split}\n\\end{align}\\] Weiterhin erkennt man, dass die Reihenfolge der partiellen Ableitungen irrelevant ist, denn es gilt \\[\\begin{align}\n\\begin{split}\n& \\frac{\\partial^{2}}{\\partial x_{1}\\partial x_{2}}f(x)\n= \\frac{\\partial^{2}}{\\partial x_{2}\\partial x_{1}}f(x) = 1, \\\\\n& \\frac{\\partial^{2}}{\\partial x_{1}\\partial x_{3}}f(x)\n= \\frac{\\partial^{2}}{\\partial x_{3}\\partial x_{1}}f(x) = 0, \\\\\n& \\frac{\\partial^{2}}{\\partial x_{2}\\partial x_{3}}f(x)\n= \\frac{\\partial^{2}}{\\partial x_{3}\\partial x_{2}}f(x) = \\frac{1}{2\\sqrt{x_3}}.\n\\end{split}\n\\end{align}\\]\nWie oben gesehen gibt es für eine multivariate reellwertige Funktion \\(f:\\mathbb{R}^n \\to \\mathbb{R}\\) insgesamt \\(n\\) erste partielle Ableitungen und \\(n^2\\) zweite partielle Ableitungen. Diese werden im Gradienten und der Hesse-Matrix einer multivariaten reellwertigen Funktion zusammengefasst.\n\nDefinition 5.8 (Gradient) \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\) sei eine multivariate reellwertige Funktion. Dann ist der \\(\\nabla f(x)\\) von \\(f\\) an der Stelle \\(x \\in \\mathbb{R}^n\\) definiert als \\[\\begin{equation}\n\\nabla f(x) :=\n\\begin{pmatrix}\n\\frac{\\partial}{\\partial x_1} f(x)  \\\\\n\\frac{\\partial}{\\partial x_2} f(x)  \\\\\n\\vdots                                            \\\\\n\\frac{\\partial}{\\partial x_n} f(x)  \\\\\n\\end{pmatrix} \\in \\mathbb{R}^n.\n\\end{equation}\\]\n\nMan beachte, dass Gradienten multivariate vektorwertige Funktionen der \\[\\begin{equation}\n\\nabla f: \\mathbb{R}^n \\to \\mathbb{R}^n, x \\mapsto \\nabla f(x)\n\\end{equation}\\] sind. Für \\(n = 1\\) gilt \\(\\nabla f(x) = f'(x)\\). Eine wichtige Eigenschaften des Gradienten ist, dass \\(-\\nabla f(x)\\) die Richtung des steilsten Abstiegs von \\(f\\) in \\(\\mathbb{R}^n\\) anzeigt. Diese Einsicht ist aber nicht trivial und soll an späterer Stelle vertieft werden. Als Beispiele betrachten wir die Gradienten der oben analysierten Funktionen\nBeispiel (1)\nFür die in Beispiel (1) betrachtete Funktion \\(f: \\mathbb{R}^2 \\to \\mathbb{R}\\) gilt \\[\\begin{equation}\n\\nabla f(x) :=\n\\begin{pmatrix}\n\\frac{\\partial}{\\partial x_1} f(x)  \\\\\n\\frac{\\partial}{\\partial x_2} f(x)  \\\\\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n2x_1                    \\\\\n2x_2\n\\end{pmatrix}\n\\in \\mathbb{R}^2.\n\\end{equation}\\]\nIn Abbildung 5.5 visualisieren wir ausgewählte Werte dieses Gradienten für\n\n\n\n\n\n\nAbbildung 5.5: Exemplarische Gradientenwerte der bivariaten Funktion \\(f(x) = x_1^2 + x_2^2\\).\n\n\n\nBeispiel (2)\nFür die in Beispiel (2) betrachtete Funktion \\(f: \\mathbb{R}^3 \\to \\mathbb{R}\\) gilt \\[\\begin{equation}\n\\nabla f(x) :=\n\\begin{pmatrix}\n\\frac{\\partial}{\\partial x_1} f(x)  \\\\\n\\frac{\\partial}{\\partial x_2} f(x)  \\\\\n\\frac{\\partial}{\\partial x_3} f(x)  \\\\\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n2x_1+x_2                    \\\\\nx_1+\\sqrt{x_3}              \\\\\n\\frac{x_{2}}{2\\sqrt{x_3}}   \\\\\n\\end{pmatrix}\n\\in \\mathbb{R}^3.\n\\end{equation}\\]\nSchließlich widmen wir uns der Zusammenfassung der zweiten partiellen Ableitungen einer multivariaten reellwertigen Funktion in der Hesse-Matrix.\n\nDefinition 5.9 (Hesse-Matrix) \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\) sei ein multivariate reellwertige Funktion. Dann ist die \\(\\nabla^2 f(x)\\) von \\(f\\) an der Stelle \\(x \\in \\mathbb{R}^n\\) definiert als \\[\\begin{equation}\n\\nabla^2 f(x) :=\n\\begin{pmatrix}\n    \\frac{\\partial^2}{\\partial x_1 x_1} f(x)\n&   \\frac{\\partial^2}{\\partial x_1 x_2} f(x)\n&   \\cdots\n&   \\frac{\\partial^2}{\\partial x_1 x_n} f(x)  \\\\\n    \\frac{\\partial^2}{\\partial x_2 x_1} f(x)\n&   \\frac{\\partial^2}{\\partial x_2 x_2} f(x)\n&   \\cdots\n&   \\frac{\\partial^2}{\\partial x_2 x_n} f(x)  \\\\\n    \\vdots\n&   \\vdots\n&   \\ddots\n&   \\vdots                                      \\\\\n    \\frac{\\partial^2}{\\partial x_n x_1} f(x)\n&   \\frac{\\partial^2}{\\partial x_n x_2} f(x)\n&   \\cdots\n&   \\frac{\\partial^2}{\\partial x_n x_n} f(x)  \\\\\n\\end{pmatrix} \\in \\mathbb{R}^{n \\times n}.\n\\end{equation}\\]\n\nMan beachte, dass Hesse-Matrizen multivariate matrixwertige Abbildungen der Form \\[\\begin{equation}\n\\nabla^2 f: \\mathbb{R}^n \\to \\mathbb{R}^{n\\times n}, x \\mapsto \\nabla^2 f(x)\n\\end{equation}\\] sind. Für \\(n = 1\\) gilt \\(\\nabla^2 f(x) = f''(x)\\). Weiterhin folgt aus \\[\\begin{equation}\n\\frac{\\partial^2}{\\partial x_i\\partial x_j}f(x) = \\frac{\\partial^2}{\\partial x_j\\partial x_i}f(x) \\mbox{ für } 1 \\le i,j\\le n\n\\end{equation}\\] dass die Hesse-Matrix symmetrisch ist, dass also \\[\\begin{equation}\n\\left(\\nabla^2f(x)\\right)^T = \\nabla^2f(x)\n\\end{equation}\\] gilt.\nBeispiel (1)\nFür die in Beispiel (1) betrachtete Funktion \\(f: \\mathbb{R}^2 \\to \\mathbb{R}\\) gilt \\[\\begin{equation}\n\\nabla^2 f(x)\n:=\n\\begin{pmatrix}\n\\frac{\\partial^2}{\\partial x_1x_1} f(x) & \\frac{\\partial^2}{\\partial x_1x_2}    f(x) \\\\\n\\frac{\\partial^2}{\\partial x_2x_1} f(x) & \\frac{\\partial^2}{\\partial x_2x_2}    f(x) \\\\\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n2 & 0   \\\\\n0 & 2   \\\\\n\\end{pmatrix}\n\\in \\mathbb{R}^{2 \\times 2}\n\\end{equation}\\] Die Hesse-Matrix dieser Funktion ist also eine konstante Funktion, die nicht von \\(x\\) abhängt.\nBeispiel (2)\nFür die in Beispiel (2) betrachtete Funktion \\(f: \\mathbb{R}^3 \\to \\mathbb{R}\\) gilt\n\\[\\begin{equation}\n\\nabla^2 f(x)\n:=\n\\begin{pmatrix}\n  \\frac{\\partial^2}{\\partial x_1x_1}  f(x)\n& \\frac{\\partial^2}{\\partial x_1x_2}    f(x)\n& \\frac{\\partial^2}{\\partial x_1x_3}    f(x)\n\\\\\n  \\frac{\\partial^2}{\\partial x_2x_1}  f(x)\n& \\frac{\\partial^2}{\\partial x_2x_2}    f(x)\n& \\frac{\\partial^2}{\\partial x_2x_3}    f(x)\n\\\\\n  \\frac{\\partial^2}{\\partial x_3x_1}  f(x)\n& \\frac{\\partial^2}{\\partial x_3x_2}    f(x)\n& \\frac{\\partial^2}{\\partial x_3x_3}    f(x)\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n  2\n& 1\n& 0\n\\\\\n  1\n& 0\n& \\frac{1}{2\\sqrt{3}}\n\\\\\n  0\n& \\frac{1}{2\\sqrt{3}}\n& -\\frac{1}{4}x_2x_3^{-3/2}\n\\end{pmatrix}.\n\\end{equation}\\] Im Gegensatz zu Beispiel (1) ist die Hesse-Matrix der hier betrachteten Funktion keine konstante Funktion und ihr Wert hängt vom Wert des Funktionsarguments \\(x \\in \\mathbb{R}^3\\) ab.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Differentialrechnung</span>"
    ]
  },
  {
    "objectID": "105-Differentialrechnung.html#sec-selbskontrollfragen-differentialrechnung",
    "href": "105-Differentialrechnung.html#sec-selbskontrollfragen-differentialrechnung",
    "title": "5  Differentialrechnung",
    "section": "5.4 Selbstkontrollfragen",
    "text": "5.4 Selbstkontrollfragen\n\nGeben Sie die Definition des Begriffs der Ableitung \\(f'(a)\\) einer Funktion \\(f\\) an einer Stelle \\(a\\) wieder.\nGeben Sie die Definition des Begriffs der Ableitung \\(f'\\) einer Funktion \\(f\\).\nErläutern Sie die Symbole \\(f'(x), \\dot{f}(x)\\), \\(\\frac{df(x)}{dx}\\), und \\(\\frac{d}{dx}f(x)\\).\nGeben Sie die Definition des Begriffs der zweiten Ableitung \\(f''\\) einer Funktion \\(f\\) wieder.\nGeben Sie die Summenregel für Ableitungen wieder.\nGeben Sie die Produktregel für Ableitungen wieder.\nGeben Sie die Quotientenregel für Ableitungen wieder.\nGeben Sie die Kettenregel für Ableitungen wieder.\nBestimmen Sie die erste Ableitung der Funktion \\(f(x) := 3x^2 + \\exp\\left(-x^2\\right) - x \\ln(x)\\).\nBestimmen Sie die erste Ableitung der Funktion \\(f(x) :=\\frac{1}{2}\\sum_{i=1}^n (x_i - \\mu)^2\\) für \\(\\mu \\in \\mathbb{R}\\).\nGeben Sie die Definition der Begriffe des globalen und lokalen Maximums/Minimums einer univariaten reellwertigen Funktion wieder.\nGeben Sie die notwendige Bedingung für ein Extremum einer Funktion wieder.\nGeben Sie die hinreichende Bedingung für ein lokales Extremum einer Funktion wieder.\nGeben Sie das Standardverfahren der analytischen Optimierung wieder.\nBestimmen Sie einen Extremwert von \\(f(x) := \\exp\\left(-\\frac{1}{2}(x - \\mu)^2\\right)\\) für \\(\\mu \\in \\mathbb{R}\\).\nBerechnen Sie die partiellen Ableitungen der Funktion \\[\nf : \\mathbb{R}^2 \\to \\mathbb{R}, x \\mapsto f(x) := \\exp\\left(-\\frac{1}{2}\\left(x_1^2 + x_2^2\\right)\\right).\n\\tag{5.1}\\]\nBerechnen Sie die zweiten partiellen Ableitungen obiger Funktion \\(f\\).\nGeben Sie den Satz von Schwarz wieder.\nGeben Sie die Definition des Gradienten einer multivariaten reellwertigen Funktion wieder.\nGeben Sie den Gradienten der Funktion in Gleichung 5.1 an und werten Sie ihn in \\(x = (1,2)^T\\) aus.\nGeben Sie die Definition der Hesse-Matrix einer multivariaten reellwertigen Funktion wieder.\nGeben Sie die Hesse-Matrix der Funktion in Gleichung 5.1 an und werten Sie sie in \\(x = (1,2)^T\\) aus.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Differentialrechnung</span>"
    ]
  },
  {
    "objectID": "106-Folgen-Grenzwerte-Stetigkeit.html",
    "href": "106-Folgen-Grenzwerte-Stetigkeit.html",
    "title": "6  Folgen, Grenzwerte, Stetigkeit",
    "section": "",
    "text": "6.1 Folgen\nWir beginnen mit der Definition des Begriffs der reellen Folge.\nMan beachte, dass weil es unendlich viele natürliche Zahlen gibt, eine reelle Folge immer unendlich viele Folgenglieder hat. Dies sollte man sich insbesondere bei der Schreibweise \\((x_1,x_2,...)\\) bewusst machen. Wir wollen zwei Standardbeispiele für reelle Folgen betrachten.\nBeispiele für reelle Folgen\nNeben den reellen Folgen, die Folgen reeller Zahlen sind, kann man auch Folgen anderer mathematischer Objekte betrachten. Eine wichtige Folgenart sind die Funktionenfolgen.\nDie Definition einer Funktionenfolge ist offenbar analog zur Definition einer reellen Folge. Der Unterschied zwischen einer reellen Folge und einer Funktionenfolge ist, dass die Folgenglieder einer reellen Folge reelle Zahlen, die Folgenglieder einer Funktionenfolgen dagegen univariate reellwertige Funktionen sind. Auch hier wollen wir zwei Standardbeispiel diskutieren.\nBeispiele für Funktionenfolgen",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Folgen, Grenzwerte, Stetigkeit</span>"
    ]
  },
  {
    "objectID": "106-Folgen-Grenzwerte-Stetigkeit.html#sec-folgen",
    "href": "106-Folgen-Grenzwerte-Stetigkeit.html#sec-folgen",
    "title": "6  Folgen, Grenzwerte, Stetigkeit",
    "section": "",
    "text": "Definition 6.1 (Reelle Folge) Eine ist eine Funktion der Form \\[\\begin{equation}\nf : \\mathbb{N} \\to \\mathbb{R}, n \\mapsto f(n)\n\\end{equation}\\] Die Funktionswerte \\(f(n)\\) einer reellen Folge werden üblicherweise mit \\(x_n\\) bezeichnet und genannt. Übliche Schreibweisen für Folgen sind \\[\\begin{equation}\n(x_1,x_2,...)\n\\mbox{ oder }\n(x_n)_{n=1}^\\infty\n\\mbox{ oder }\n(x_n)_{n\\in \\mathbb{N}}\n\\mbox{ oder }\n(x_n).\n\\end{equation}\\]\n\n\n\n\nReelle Folgen der Form \\[\\begin{equation}\nf : \\mathbb{N} \\to \\mathbb{R}, n \\mapsto f(n) := \\left(\\frac{1}{n}\\right)^{\\frac{p}{q}} \\mbox{ mit } p,q\\in \\mathbb{N}\n\\end{equation}\\] nennen wir harmonische Folgen. Für \\(p := q := 1\\) hat eine harmonische Folge die Folgengliederform \\[\\begin{equation}\n\\left(\\frac{1}{1}, \\frac{1}{2}, \\frac{1}{3}, ...\\right).\n\\end{equation}\\]\nReelle Folgen der Form \\[\\begin{equation}\nf : \\mathbb{N} \\to \\mathbb{R}, n \\mapsto f(n) := q^n \\mbox{ mit } q \\in ]-1,1[\n\\end{equation}\\] werden geometrische Folgen genannt. Für \\(q := \\frac{1}{2}\\) hat eine geometrische Folge die Folgengliederform \\[\\begin{align}\n\\begin{split}\n\\left(\\left(\\frac{1}{2}\\right)^1,\\left(\\frac{1}{2}\\right)^2,\\left(\\frac{1}{2}\\right)^3, ...\\right)    \n& = \\left(\\left(\\frac{1}{2}\\right)^1,\\left(\\frac{1}{2}\\right)^2,\\left(\\frac{1}{2}\\right)^3, ...\\right)\\\\\n& = \\left(\\frac{1^1}{2^1},\\frac{1^2}{2^2},\\frac{1^3}{2^3} ...\\right)                                  \\\\\n& = \\left(\\frac{1}{2},\\frac{1}{4},\\frac{1}{8} ...\\right)\n\\end{split}\n\\end{align}\\]\n\n\n\nDefinition 6.2 (Funktionenfolge) Es sei \\(\\phi\\) eine Menge univariater reellwertiger Funktionen mit Definitionsmenge \\(D \\subseteq \\mathbb{R}\\). Dann ist eine Funktionenfolge eine Funktion der Form \\[\\begin{equation}\nF: \\mathbb{N} \\to \\phi, n \\mapsto F(n).\n\\end{equation}\\] Die Funktionswerte \\(F(n)\\) einer Funktionenfolgen werden üblicherweise mit \\(f_n\\) bezeichnet und genannt. Übliche Schreibweisen für Funktionenfolgen sind \\[\\begin{equation}\n(f_1,f_2,...)\n\\mbox{ oder }\n(f_n)_{n=1}^\\infty\n\\mbox{ oder }\n(f_n)_{n\\in \\mathbb{N}}\n\\mbox{ oder }\n(f_n).\n\\end{equation}\\]\n\n\n\n\nWir betrachten die Menge \\(\\phi\\) der univariaten reellwertigen Funktionen der Form \\[\\begin{equation}\n\\phi := \\{f_n|f_n : [0,1] \\to \\mathbb{R}, x \\mapsto f_n(x) := x^n \\mbox{ für } n \\in \\mathbb{N}\\}\n\\end{equation}\\] Dann definiert \\[\\begin{equation}\nF : \\mathbb{N} \\to \\phi, n\\mapsto F(n)\n\\end{equation}\\] eine Funktionenfolge. Für die Funktionswerte der Folgenglieder von \\(F\\) gilt \\[\\begin{equation}\nf_1(x) := x^1, f_2(x) := x^2,  f_3(x) := x^3, ...\n\\end{equation}\\]\nWir betrachten die Menge \\(\\phi\\) der univariaten reellwertigen Funktionen der Form \\[\\begin{equation}\n\\phi := \\{f_n|f_n : [-a,a] \\to \\mathbb{R}, x \\mapsto f_n(x) := \\sum_{k=0}^n \\frac{x^k}{k!} \\mbox{ für } n \\in \\mathbb{N}\\}\n\\end{equation}\\] Dann definiert \\[\\begin{equation}\nF : \\mathbb{N} \\to \\phi, n\\mapsto F(n)\n\\end{equation}\\] eine Funktionenfolge. Für die Funktionswerte der Folgenglieder von \\(F\\) gilt \\[\\begin{equation}\nf_1(x) := \\sum_{k=0}^1 \\frac{x^k}{k!}, f_2(x) := \\sum_{k=0}^2 \\frac{x^k}{k!},  f_3(x) := \\sum_{k=0}^3 \\frac{x^k}{k!}, ...\n\\end{equation}\\]",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Folgen, Grenzwerte, Stetigkeit</span>"
    ]
  },
  {
    "objectID": "106-Folgen-Grenzwerte-Stetigkeit.html#sec-grenzwerte",
    "href": "106-Folgen-Grenzwerte-Stetigkeit.html#sec-grenzwerte",
    "title": "6  Folgen, Grenzwerte, Stetigkeit",
    "section": "6.2 Grenzwerte",
    "text": "6.2 Grenzwerte\nWenn man die Folgenglieder einer Folge betrachtet, kann man sich fragen, welche Werte eine Folge wohl annimmt, wenn der Folgenindex \\(n\\) sehr groß wird, also gegen unendlich strebt. Wenn in diesem Fall die Folgenglieder sehr ähnliche Werte annehmen (und nicht etwa auch unendlich groß werden), so ist man auf den Begriff des Grenzwerts für reelle Folgen bzw. der Grenzfunktion für Funktionenfolgen geführt.\n\nDefinition 6.3 (Grenzwert einer Folge) \\(x \\in \\mathbb{R}\\) heißt Grenzwert einer reellen Folge \\((x_n)_{n=1}^\\infty\\), wenn es zu jedem \\(\\epsilon&gt;0\\) ein \\(m \\in \\mathbb{N}\\) gibt, so dass \\[\\begin{equation}\n|x_n - x| &lt; \\epsilon \\mbox{ für alle } n \\ge m.\n\\end{equation}\\] Eine Folge, die einen Grenzwert besitzt, wird genannt, eine Folge die keinen Grenzwert besitzt, wird genannt. Dafür, dass \\(x \\in \\mathbb{R}\\) Grenzwert der Folge \\((x_n)_{n=1}^\\infty\\) ist, schreibt man auch \\[\\begin{equation}\n\\lim_{n \\to \\infty} x_n = x\n\\mbox{ oder }\nx_n \\to x \\mbox{ für } n \\to \\infty\n\\mbox{ oder }\nx_n\\xrightarrow[]{n \\to \\infty} x.\n\\end{equation}\\]\n\nDer Grenzwert einer Folge kann also, aber muss nicht existieren. So hat zum Beispiel die Folge \\[\\begin{equation}\nf : \\mathbb{N} \\to \\mathbb{R}, n \\mapsto f(n) := n\n\\end{equation}\\] keinen Grenzwert, da hier sowohl \\(n\\) als auch \\(f(n)\\) unendlich groß werden. Die oben betrachteten Beispiele für reelle Folgen dagegen haben Grenzwert. Dies ist Inhalt folgender Beispiele\nBeispiele\n\nFür die verallgemeinerten harmonischen Folgen gilt mit \\(p,q \\in \\mathbb{N}\\) \\[\n\\lim_{n\\to \\infty} \\left(\\frac{1}{n}\\right)^{\\frac{p}{q}} = 0.\n\\tag{6.1}\\]\nFür die geometrischen Folgen gilt mit \\(q \\in ]-1,1[\\) \\[\n\\lim_{n\\to \\infty} q^n = 0.\n\\tag{6.2}\\]\n\nMan nennt die harmonischen und geometrischen Folgen entsprechend auch Nullfolgen. Für Beweise von Gleichung 6.1 und Gleichung 6.2 verweisen wir auf die weiterführende Literatur. Tatsächlich sind diese Beweise nicht trivial und rühren an die Grundannahmen über das Wesen der reellen Zahlen. Wir visualisieren die ersten zehn Folgenglieder sowie die Grenzwerte der harmonischen Folge fr \\(p := q := 1\\) und der geometrischen Folge für \\(q := 1/2\\) in Abbildung 6.1.\n\n\n\n\n\n\nAbbildung 6.1: Beispiele für Grenzwerte reeller Folgen.\n\n\n\nFür Funktionenfolgen ist eine Möglichkeit der Erweiterung der Begriffe der Konvergenz und des Grenzwertes folgende.\n\nDefinition 6.4 (Punktweise Konvergenz und Grenzfunktion einer Funktionenfolge) \\(F = (f_n)_{n\\in \\mathbb{N}}\\) sei eine Funktionenfolge von univariaten reellwertigen Funktionen mit Definitionsbereich \\(D\\). \\(F\\) heißt , wenn die reelle Folge \\(\\left(f_n(x)\\right)_{n\\in \\mathbb{N}}\\) für jedes \\(x \\in D\\) eine konvergente Folge ist, also einen Grenzwert besitzt. Die Funktion, die jedem \\(x \\in D\\) diesen Grenzwert von \\(\\left(f_n(x)\\right)_{n\\in \\mathbb{N}}\\) zuordnet, heißt dann die und hat die Form \\[\\begin{equation}\nf : D \\to \\mathbb{R}, x \\mapsto f(x) := \\lim_{n\\to \\infty}f_n(x).\n\\end{equation}\\]\n\nMan beachte, dass die Grenzwerte von konvergenten reellen Folgen reelle Zahlen sind, die Grenzfunktionen von punktweise konvergenten Funktionenfolgen dagegen sind Funktionen. Neben der punktweisen Konvergenz von Funktionenfolgen gibt es noch den mächtigeren Begriff der gleichmäßigen Konvergenz von Funktionenfolgen, für den wir aber auf die weiterführende Literatur verweisen. Als Beispiel betrachten wir die Grenzfunktionen der oben diskutierten Funktionenfolgen, wobei wir für Beweise ebenfalls auf die weiterführende Literatur verweisen.\nBeispiele\n\nWir betrachten die Funktionenfolge \\[\\begin{equation}\nF : \\mathbb{N} \\to \\phi, n\\mapsto F(n)\n\\end{equation}\\] mit \\[\\begin{equation}\n\\phi := \\{f_n|f_n : [0,1] \\to \\mathbb{R}, x \\mapsto f_n(x) := x^n \\mbox{ für } n \\in \\mathbb{N}\\}\n\\end{equation}\\] Dann ist \\(F\\) punktweise konvergent mit Grenzfunktion \\[\\begin{equation}\nf : [0,1] \\to \\mathbb{R}, x \\mapsto f(x)\n:=\n\\begin{cases}\n0, & \\mbox{ für } x \\in [0,1[ \\\\\n1, & \\mbox{ für } x = 1       \\\\\n\\end{cases}\n\\end{equation}\\] da \\(f_n(x) := x^n\\) für \\(x \\in [0,1[\\) eine geometrische Folge und damit eine Nullfolge ist und \\(f_n(x) := x^n\\) für \\(x = 1\\) eine konstante Folge ist, für die alle Folgenglieder den Abstand \\(0\\) von \\(1\\) haben. Die Funktionenfolge \\(F\\) konvergiert also gegen eine Funktion, die auf dem gesamten Intervall \\([0,1]\\) gleich Null ist, außer im Punkt \\(1\\). Diese Funktion hat offenbar einen Sprung.\nWir betrachten die Funktionenfolge \\[\\begin{equation}\nF : \\mathbb{N} \\to \\phi, n\\mapsto F(n)\n\\end{equation}\\] mit \\[\\begin{equation}\n\\phi := \\{f_n|f_n : [-a,a] \\to \\mathbb{R}, x \\mapsto f_n(x) := \\sum_{k=0}^n \\frac{x^k}{k!} \\mbox{ für } n \\in \\mathbb{N}\\}\n\\end{equation}\\] Dann ist \\(F\\) punktweise konvergent mit Grenzfunktion \\[\\begin{equation}\nf : [-a,a] \\to \\mathbb{R}, x \\mapsto f(x)\n:= \\sum_{k=0}^\\infty \\frac{x^k}{k!}\n=: \\exp(x)\n\\end{equation}\\] Die Funktionenfolge \\(F\\) konvergiert also gegen die Exponentialfunktion auf \\([-a,a]\\). Umgekehrt betrachtet ist die Exponentialfunktion gerade durch \\[\\begin{equation}\n\\exp(x) :=  \\sum_{k=0}^\\infty \\frac{x^k}{k!}\n\\end{equation}\\] definiert.\n\n\n\n\n\n\n\nAbbildung 6.2: Beispiele für Grenzwerte von Funktionenfolgen",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Folgen, Grenzwerte, Stetigkeit</span>"
    ]
  },
  {
    "objectID": "106-Folgen-Grenzwerte-Stetigkeit.html#stetigkeit",
    "href": "106-Folgen-Grenzwerte-Stetigkeit.html#stetigkeit",
    "title": "6  Folgen, Grenzwerte, Stetigkeit",
    "section": "6.3 Stetigkeit",
    "text": "6.3 Stetigkeit\nIn diesem Abschnitt versuchen wir uns dem Begriff der Stetigkeit einer Funktion zu nähern. Intuitiv ist eine Funktion stetig, wenn sie keine Sprünge hat oder äquivalent, wenn kleine Änderungen in ihren Argumenten stets nur zu kleinen Änderungen in ihren Funktionswerten (und damit eben keinen Sprüngen) führen. Zur Definition der Stetigkeit benötigen wir zunächst den Begriff des Grenzwertes einer Funktion.\n\nDefinition 6.5 (Grenzwert einer Funktion)  \nFür \\(D\\subseteq\\mathbb{R}\\) und \\(Z\\subseteq\\mathbb{R}\\) sei \\(f : D \\to Z, x \\mapsto f(x)\\) eine Funktion und es seien \\(a,b \\in \\mathbb{R}\\). \\(b\\) heißt , wenn\nWenn \\(b\\) Grenzwert der Funktion \\(f\\) für \\(x\\) gegen \\(a\\) ist, so schreibt man auch \\(\\lim_{x \\to a} f(x) = b\\).\n\nIn Abbildung 6.3 visualisieren wir den Grenzwert der Exponential funktion in \\(a = 1\\) durch Darstellung von Folgenglieder \\(x_n \\to 1\\) und den entsprechenden Folgengliedern \\(f(x_n)\\). Offenbar gilt \\(\\lim_{x\\to 1}\\exp(x) = e\\).\n\n\n\n\n\n\nAbbildung 6.3: Beispiele für einen Grenzwert einer Funktion\n\n\n\nWir können nun den Begriff der Stetigkeit einer Funktion definieren.\n\nDefinition 6.6 (Stetigkeit einer Funktion) Eine Funktion \\(f : D \\to Z\\) mit \\(D \\subseteq \\mathbb{R}, Z \\subseteq \\mathbb{R}\\) heißt , wenn \\[\\begin{equation}\n\\lim_{x\\to a} f(x) = f(a).\n\\end{equation}\\] Ist \\(f\\) in jedem \\(x \\in D\\) stetig, so heißt .\n\nMan beachte, dass für eine in \\(a\\) stetige Funktion folgt, dass \\[\\begin{equation}\n\\lim_{x \\to a} f(x) = f\\left(\\lim_{x\\to a} x\\right)\n\\end{equation}\\] Bei stetigen Funktion können also Grenzwertbildung und Auswertung der Funktion vertauscht werden.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Folgen, Grenzwerte, Stetigkeit</span>"
    ]
  },
  {
    "objectID": "107-Integralrechnung.html",
    "href": "107-Integralrechnung.html",
    "title": "7  Integralrechnung",
    "section": "",
    "text": "7.1 Unbestimmte Integrale\nWir beginnen mit der Definition des unbestimmen Integrals und dem Begriff der Stammfunktion.\nObige Definition besagt, dass die Ableitung der Stammfunktion einer Funktion \\(f\\) eben \\(f\\) ist. Das unbestimmte Integral einer Funktion \\(f\\) ist darüber hinaus die Menge aller durch Addition verschiedener Konstanten \\(c \\in \\mathbb{R}\\) gegebenen Stammfunktionen von \\(f\\). Eine solche Konstante \\(c \\in \\mathbb{R}\\) heißt auch Integrationskonstante; es gilt natürlich \\(\\frac{d}{dx}c = 0\\). Das Symbol \\(\\int f(x) \\,dx\\) ist als \\(F + c\\) definiert. \\(f(x)\\) wird in diesem Ausdruck Integrand genannt.\n\\(\\int\\) und \\(\\,dx\\) haben keine eigentliche Bedeutung, sondern sind reine Symbole.\nFür die in vorherigen Abschnitten eingeführten elementaren Funktionen ergeben sich die in Tabelle aufgelisteten Stammfunktionen. Man überzeugt sich davon durch Ableiten der jeweiligen Stammfunktion mithilfe der Rechenregeln der Differentialrechnung. Die uneigentlichen Integrale dieser elementaren Funktionen ergeben sich dann direkt aus diesen Stammfunktionen durch Addition einer Integrationskonstanten.\nStammfunktionen elementarer Funktionen\n\n\n\n\n\n\n\nName\nDefinition\nStammfunktion\n\n\n\n\nPolynomfunktion\n\\(f(x) := \\sum_{i=0}^n a_ix^i\\)\n\\(F(x) = \\sum_{i=0}^n \\frac{a_i}{i+1}x^{i+1}\\)\n\n\nKonstante Funktion\n\\(f(x) := a\\)\n\\(F(x) = ax\\)\n\n\nIdentitätsfunktion\n\\(f(x) := x\\)\n\\(F(x) = \\frac{1}{2}x^2\\)\n\n\nLinear-affine Funktion\n\\(f(x) := ax + b\\)\n\\(F(x) = \\frac{1}{2}ax^2 + bx\\)\n\n\nQuadratfunktion\n\\(f(x) := x^2\\)\n\\(F(x) = \\frac{1}{3}x^3\\)\n\n\nExponentialfunktion\n\\(f(x) := \\exp(x)\\)\n\\(F(x) = \\exp(x)\\)\n\n\nLogarithmusfunktion\n\\(f(x) := \\ln(x)\\)\n\\(F(x) = x \\ln x - x\\)\nDie in nachfolgendem Theorem zusammengestellten Rechenregeln sind oft hilfreich, um Stammfunktionen von Funktionen zu bestimmen, die sich aus Funktionen mit bekannten Stammfunktionen zusammensetzen.\nUnbestimmte Integrale nehmen in der Lösung von Differentialgleichungen einen zentralen Platz ein. Naheliegender ist aber zunächst die Anwendung unbestimmter Integrale im Kontext der Auswertung bestimmter Integrale, wie im nächsten Abschnitt eingeführt.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Integralrechnung</span>"
    ]
  },
  {
    "objectID": "107-Integralrechnung.html#sec-unbestimmte-integrale",
    "href": "107-Integralrechnung.html#sec-unbestimmte-integrale",
    "title": "7  Integralrechnung",
    "section": "",
    "text": "Definition 7.1 (Unbestimmtes Integral und Stammfunktion) Für ein Intervall \\(I \\subseteq \\mathbb{R}\\) sei \\(f : I \\to \\mathbb{R}\\) eine univariate reellwertige Funktion. Dann heißt eine differenzierbare Funktion \\(F : I \\to \\mathbb{R}\\) mit der Eigenschaft \\[\\begin{equation}\nF' = f\n\\end{equation}\\] Stammfunktion von \\(f\\). Ist \\(F\\) eine Stammfunktion von \\(f\\), dann heißt \\[\\begin{equation}\n\\int f(x) \\,dx := F + c \\mbox{ mit } c \\in \\mathbb{R}\n\\end{equation}\\] unbestimmtes Integral der Funktion \\(f\\). Das unbestimmte Integral einer Funktion bezeichnet damit die Menge aller Stammfunktionen einer Funktion.\n\n\n\n\n\n\nTheorem 7.1 (Rechenregeln für Stammfunktionen) \\(f\\) und \\(g\\) seien univariate reellwertige Funktion, die Stammfunktionen besitzen, und \\(g\\) sei invertierbar. Dann gelten folgende Rechenregeln für die Bestimmung von Stammfunktionen\n\nSummenregel \\[\\begin{equation}\n\\int a f(x) + bg(x)\\,dx  = a\\int f(x)\\,dx + b\\int g(x)\\,dx \\mbox{ für } a,b \\in \\mathbb{R}\n\\end{equation}\\]\nPartielle Integration \\[\\begin{equation}\n\\int f'(x)g(x)\\,dx = f(x)g(x) - \\int f(x)g'(x)\\,dx\n\\end{equation}\\]\nSubstitionsregel \\[\\begin{equation}\n\\int f(g(x))g'(x)\\,dx = \\int f(t)\\,dt \\mbox{ mit } t  = g(x)\n\\end{equation}\\]\n\n\n\nBeweis. Für einen Beweis der Summenregel verweisen wir auf die weiterführende Literatur. Die Rechenregel der partiellen Integration ergibt sich durch Integration der Produktregel der Differentiation. Wir erinnern uns, dass gilt \\[\\begin{equation}\n(f(x)g(x))' = f'(x)g(x) + f(x)g'(x).\n\\end{equation}\\] Integration beider Seiten der Gleichung und Berücksichtigung der Summenregel für Stammfunktionen ergibt dann \\[\\begin{align}\n\\begin{split}\n\\smallint (f(x)g(x))' \\,dx & = \\smallint f'(x)g(x) + f(x)g'(x) \\,dx     \\\\\n\\Leftrightarrow\nf(x)g(x) & = \\smallint f'(x)g(x)\\,dx + \\smallint f(x)g'(x) \\,dx         \\\\\n\\Leftrightarrow\n\\smallint f'(x)g(x)\\,dx & = f(x)g(x) - \\smallint f(x)g'(x) \\,dx.\n\\end{split}\n\\end{align}\\] Die Substitutionsregel ergibt sich für \\(F' = f\\) durch Anwendung der Kettenregel der Differentiation auf die verkettete Funktion \\(F(g)\\). Speziell gilt zunächst \\[\\begin{align}\n\\begin{split}\n(F(g(x)))' = F'(g(x))g'(x) = f(g(x))g'(x).\n\\end{split}\n\\end{align}\\] Integration beider Seiten der Gleichung \\[\\begin{equation}\n(F(g(x))) ' = f(g(x))g'(x)\n\\end{equation}\\] ergibt dann \\[\\begin{align}\n\\begin{split}\n\\smallint (F(g(x)))' \\,dx  & = \\smallint f(g(x))g'(x) \\,dx              \\\\\n\\Leftrightarrow\nF(g(x)) + c & = \\smallint f(g(x))g'(x) \\,dx                          \\\\\n\\Leftrightarrow\n\\smallint f(g(x))g'(x) \\,dx & = \\smallint f(t)\\,dt  \\mbox{ mit } t := g(x).\n\\end{split}\n\\end{align}\\] Dabei ist die rechte Seite der letzten obigen Gleichung zu verstehen als \\(F(g(x)) + c\\), also als Stammfunktion von \\(f\\) evaluiert an der Stelle \\(t := g(x)\\). Das \\(dt\\) ist nicht durch \\(dg(x)\\) zu ersetzen, sondern rein notationeller Natur.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Integralrechnung</span>"
    ]
  },
  {
    "objectID": "107-Integralrechnung.html#sec-bestimmte-integrale",
    "href": "107-Integralrechnung.html#sec-bestimmte-integrale",
    "title": "7  Integralrechnung",
    "section": "7.2 Bestimmte Integrale",
    "text": "7.2 Bestimmte Integrale\nAnschaulich entspricht ein bestimmtes Integral der vorzeichenbehafteten und auf ein Intervall \\([a,b]\\) beschränkten Fläche zwischen dem Graphen einer Funktion \\(f\\) und der \\(x\\)-Achse (vgl. Abbildung 7.1). Vorzeichenbehaftet heißt dabei, dass Flächen zwischen der \\(x\\)-Achse und positiven Werten von \\(f\\) positiv zur Fläche beitragen, Flächen zwischen der \\(x\\) und negativen Werten von \\(f\\) dagegen negativ. So ergeben sich zum Beispiel der Wert des in Abbildung 7.1 A gezeigten bestimmten Integral zu 0.68, der Wert des in Abbildung Abbildung 7.1 B gezeigten bestimmten Integrals zu 0.95 (die eingezeichnete Fläche ist offensichtlich größer als in Abbildung 7.1 A) und der Wert des in Abbildung 7.1 C gezeigten bestimmten Integrals zu 0 (die eingezeichneten positiven und negativen Flächen gleichen sich genau aus). Letzteres Beispiel legt auch die Interpretation des Integrals als Durchschnittswert einer Funktion \\(f\\) über einem Intervall \\([a,b]\\) nahe.\n\n\n\n\n\n\nAbbildung 7.1: Beispiele bestimmter Integrale\n\n\n\nUm den Begriff des bestimmten Integrals im Sinne des Riemannschen Integrals einführen zu können, müssen wir zunächst etwas Vorarbeit leisten. Wir beginnen damit, einen Begriff für die Aufteilung eines Intervalls in kleinere Abschnitte einzuführen.\n\nDefinition 7.2 (Zerlegung eines Intervalls und Feinheit) Es sei \\([a,b] \\subset \\mathbb{R}\\) ein Intervall und \\(x_0,x_1,x_2,...,x_n\n\\in [a,b]\\) eine Menge von Punkten mit \\[\\begin{equation}\na =: x_0 &lt; x_1 &lt;  x_2 \\cdots &lt; x_n := b\n\\end{equation}\\] und \\[\\begin{equation}\n\\Delta x_i := x_i - x_{i-1} \\mbox{ für } i = 1,...,n.\n\\end{equation}\\] Dann heißt die Menge \\[\\begin{equation}\nZ := \\{[x_0,x_1], [x_1,x_2], ..., [x_{n-1},x_n]\\}\n\\end{equation}\\] der durch \\(x_0,x_1,x_2,...,x_n\\) definierten Teilintervalle von \\([a,b]\\) eine Zerlegung von \\([a,b]\\). Weiterhin heißt \\[\\begin{equation}\nZ_{\\mbox{max}} := \\max_{i \\in n} \\Delta x_i,\n\\end{equation}\\] also die größte der Teilintervalllängen \\(\\Delta x_i\\), die Feinheit von \\(Z\\).\n\nAnschaulich ist \\(\\Delta x_i\\) die Breite der Rechtecke in Abbildung 7.2, wie wir in der Folge sehen werden. Mithilfe der Begriffe der Zerlegung eines Intervalls können wir nun den Begriff der Riemannschen Summen einführen.\n\nDefinition 7.3 (Riemannsche Summen) \\(f : [a,b] \\to \\mathbb{R}\\) sei eine beschränkte Funktion auf \\([a,b]\\), d.h. \\(|f(x)| &lt; c\\) für \\(0 &lt; c &lt; \\infty\\) und alle \\(x \\in [a,b]\\), \\(Z\\) sei eine Zerlegung von \\([a,b]\\) mit Teilintervalllängen \\(\\Delta x_i\\) für \\(i = 1,...,n\\). Weiterhin sei \\(\\xi_{i}\\) für \\(i = 1,...,n\\) ein beliebiger Punkt im Teilintervall \\([x_{i-1}, x_{i}]\\) der Zerlegung \\(Z\\). Dann heißt \\[\\begin{equation}\nR(Z) := \\sum_{i=1}^n f(\\xi_i)\\Delta x_i\n\\end{equation}\\] Riemannsche Summe von \\(f\\) auf \\([a,b]\\) bezüglich der Zerlegung \\(Z\\).\n\nWählt man zum Beispiel in der Riemannschen Summe in jedem Teilintervall das Maximum von \\(f\\), so ergibt sich die sogenannte Riemannsche Obersumme, \\[\\begin{equation}\nR_o(Z) := \\sum_{i=1}^n \\left(\\max_{[x_{i-1}, x_{i}]} f(\\xi_i) \\right)\\Delta x_i.\n\\end{equation}\\] Wählt man dagegen in jedem Teilintervall dagegen das Minimum von \\(f\\), so ergibt sich dies sogenannte Riemannsche Untersumme. \\[\\begin{equation}\nR_u(Z) := \\sum_{i=1}^n \\left(\\min_{[x_{i-1}, x_{i}]} f(\\xi_i) \\right)\\Delta x_i.\n\\end{equation}\\] Abbildung 7.2 verdeutlicht die Definition dieser Riemannschen Summen: die dunkelgrauen Rechtecke haben jeweils die Fläche \\([x_{i-1}, x_{i}] \\cdot \\min_{[x_{i-1}, x_{i}]} f(\\xi)\\) und bilden damit die Summenterme in der Riemannschen Untersumme \\[\\begin{equation}\nR_u(Z) := \\sum_{i=1}^4 \\left(\\min_{[x_{i-1}, x_{i}]} f(\\xi_i) \\right) \\cdot \\Delta x_i.\n\\end{equation}\\] Die vertikale Kombination aus dunkelgrauen und hellgrauen Rechtecken hat jeweils die Fläche \\([x_{i-1}, x_{i}] \\cdot \\max_{[x_{i-1}, x_{i}]} f(\\xi)\\) und bilden damit die Summenterme in der Riemannschen Obersumme \\[\\begin{equation}\nR_o(Z) := \\sum_{i=1}^4 \\left(\\max_{[x_{i-1}, x_{i}]} f(\\xi_i) \\right) \\cdot \\Delta x_i.\n\\end{equation}\\] Stellt man sich nun vor, dass man \\(\\Delta x_i\\) für alle \\(i = 1,...,n\\) gegen Null gehen lässt, verkleinert man die Feinheit der Zerlegung \\(Z\\) also immer weiter, so werden sich die Werte von \\(\\min_{[x_{i-1}, x_{i}]} f(\\xi_i)\\) und \\(\\max_{[x_{i-1}, x_{i}]} f(\\xi_i)\\) und damit auch die Werte von \\(R_u(Z)\\) und \\(R_o(Z)\\) immer weiter annähern. Diesen Grenzprozess macht man sich in der Definition des Riemannschen Integrals zunutze.\n\n\n\n\n\n\nAbbildung 7.2: Riemannsche Summen\n\n\n\n\nDefinition 7.4 (Bestimmtes Riemannsches Integral) \\(f : [a,b] \\to \\mathbb{R}\\) sei eine beschränkte reellwertige Funktion auf \\([a,b]\\). Weiterhin sei für \\(Z_k\\) mit \\(k = 1,2,3...\\) eine Folge von Zerlegungen von \\([a,b]\\) mit zugehörigen Feinheit \\(Z_{\\mbox{max},k}\\). Wenn für jede Folge von Zerlegungen \\(Z_1, Z_2,...\\) mit \\(|Z_{\\mbox{max},k}| \\to 0\\) für \\(k \\to \\infty\\) und für beliebig gewählte Punkte \\(\\xi_{ki}\\) mit \\(i = 1,...,n\\) im Teilintervall \\([x_{k,i-1}, x_{k,i}]\\) der Zerlegung \\(Z_k\\) gilt, dass die Folge der zugehörigen Riemannschen Summen \\(R(Z_1), R(Z_2), ...\\) gegen den gleichen Grenzwert strebt, dann heißt \\(f\\) auf \\([a,b]\\) integrierbar. Der entsprechende Grenzwert der Folge von Riemannschen Summen wird bestimmtes Riemannsches Integral genannt und mit \\[\\begin{equation}\n\\int_a^b f(x)\\,dx := \\lim_{k \\to \\infty} R(Z_k)\n\\mbox{ für } |Z_{\\mbox{max},k}| \\to 0\n\\end{equation}\\] bezeichnet. Die Werte \\(a\\) und \\(b\\) bezeichnet man in diesem Kontext als untere und obere Integrationsgrenzen, respektive, \\(f(x)\\) als Integrand und \\(x\\) als Integrationsvariable.\n\nDie Riemannsche Integrierbarkeit einer Funktion und der Wert eines bestimmten Riemannschen Integrals sind also im Sinne einer Grenzwertbildung definiert. Die Theorie der Riemannschen Integrale lässt sich allerding um die Hauptsätze der Differential- und Integralrechnung erweitern, so dass zur konkreten Berechnung eines bestimmten Integrals die Bildung von Zerlegungen und die Bestimmung eines Grenzwertes nur selten nötig ist. Der Einfachheit halber verzichten wir in der Folge auf die Bezeichungen Riemannsche und sprechen einfach von bestimmten Integralen.\nEin erster Schritt zur Vereinfachung der Berechnung von bestimmten Integralen ist das Feststellen folgender Rechenregeln, für deren Beweis wir auf die weiterführende Literatur verweisen.\n\nTheorem 7.2 (Rechenregeln für bestimmte Integrale) Es seien \\(f\\) und \\(g\\) integrierbare Funktionen auf \\([a,b]\\). Dann gelten folgende Rechenregeln.\n\nLinearität. Für \\(c_1,c_2\\in \\mathbb{R}\\) gilt \\[\\begin{equation}\n\\int_a^b (c_1 f(x) + c_2g(x))\\,dx = c_1 \\int_a^b f(x)\\,dx + c_2 \\int_a^b f(x)\\,dx.\n\\end{equation}\\]\nAdditivität. Für \\(a &lt; c &lt; b\\) gilt \\[\\begin{equation}\n\\int_a^b f(x)\\,dx = \\int_a^c f(x)\\,dx + \\int_c^b f(x)\\,dx.\n\\end{equation}\\]\nVorzeichenwechsel bei Umkehrung der Integralgrenzen \\[\\begin{equation}\n\\int_a^b f(x)\\,dx = - \\int_b^a f(x)\\,dx.\n\\end{equation}\\]\nUnabhängigkeit von der Wahl der Integrationsvariable \\[\\begin{equation}\n\\int_a^b f(x)\\,dx = \\int_a^b f(y)\\,dy.\n\\end{equation}\\]\nUnabhängigkeit des Integrals von Art des Intervalls. Es gilt \\[\\begin{equation}\n\\int_{a}^{b} f(x)\\,dx\n= \\int_{]a,b[}f(x)\\,dx\n= \\int_{[a,b[}f(x)\\,dx\n= \\int_{]a,b]}f(x)\\,dx\n= \\int_{[a,b]}f(x)\\,dx.\n\\end{equation}\\] wobei \\(\\int_I\\) das bestimmte Integral von \\(f\\) auf dem Intervall \\(I \\subseteq \\mathbb{R}\\) bezeichnet.\n\n\nEine graphische Darstellung der Rechenregel der Additivität findet sich in Abbildung 7.3. Die Summe der durch die bestimmten Integrale gegebenen Flächen \\(\\int_a^c f(x)\\,dx\\) und \\(\\int_c^b f(x)\\,dx\\) mit \\(a &lt; c &lt; b\\) ergibt sich dabei zur Fläche von \\(\\int_a^b f(x)\\,dx\\).\n\n\n\n\n\n\nAbbildung 7.3: Additivität bestimmter Integrale\n\n\n\nDie in der Nachfolge vermerkten Hauptsätze der Differential- und Integralrechnung schließlich, ermöglichen es, bestimmte Integrale einer Funktion \\(f\\) direkt mithilfe der Stammfunktion \\(F\\) von \\(f\\) zu berechnen.\n\nTheorem 7.3 (Erster Hauptsatz der Differential- und Integralrechnung) Ist \\(f :  I  \\to \\mathbb{R}\\) eine auf dem Intervall \\(I\n\\subset \\mathbb{R}\\) stetige Funktion, dann ist die Funktion \\[\\begin{equation}\nF :  I  \\to \\mathbb{R}, x \\mapsto F(x) := \\int_a^x f(t)\\,dt \\mbox{ mit } x,\na \\in I\n\\end{equation}\\] eine Stammfunktion von \\(f\\).\n\n\nBeweis. Wir betrachten den Differenzquotienten \\[\\begin{equation}\n\\frac{1}{h}(F(x+h) - F(x))\n\\end{equation}\\] Mit der Definition \\(F(x) := \\smallint_a^x f(t)\\,dt\\) und der Additivität des bestimmten Integrals gilt dann \\[\\begin{equation}\n\\frac{1}{h}(F(x+h) - F(x))\n= \\frac{1}{h}\\left(\\int_a^{x + h} f(t)\\,dt - \\int_a^{x} f(t)\\,dt\\right)\n= \\frac{1}{h} \\int_x^{x + h}f(t)\\,dt\n\\end{equation}\\] Mit dem Mittelwertsatz der Integralrechnung gibt es also ein \\(\\xi \\in ]x,x+h[\\), so dass \\[\\begin{equation}\n\\frac{1}{h}(F(x+h) - F(x)) = f(\\xi)\n\\end{equation}\\] Grenzwertbildung ergibt dann \\[\\begin{equation}\n\\lim_{h \\to 0}\\frac{1}{h}(F(x+h) - F(x)) = \\lim_{h \\to 0} f(\\xi) \\mbox{ für }\n\\xi \\in ]x, x + h[\n\\Leftrightarrow\nF'(x) = f(x).\n\\end{equation}\\]\n\nFür den Beweis des Ersten Hauptsatzes der Differential- und Integralrechnung benötigen wir offenbar den Mittelwertsatz der Integralrechnung, welchen wir hier ohne Beweis wiedergeben und in Abbildung 7.4 veranschaulichen.\n\nTheorem 7.4 (Mittelwertsatz der Integralrechnung) Für eine stetige Funktion \\(f : [a,b] \\to \\mathbb{R}\\) existiert ein \\(\\xi \\in ]a,b[\\) mit \\[\\begin{equation}\n\\int_a^b f(x)\\,dx = f(\\xi)(b-a)\n\\end{equation}\\]\n\nDer Mittelwertsatz der Integralrechnung garantiert die Existenz eines \\(\\xi \\in [a,b]\\), so dass das bestimmte Integral \\(\\int_a^b f(x)\\,dx\\) gleich dem Produkt aus der “Rechteckhöhe” \\(f(\\xi)\\) und und der “Rechteckbreite” \\((b-a)\\) ist. In Abbildung 7.4 liegt dieses \\(\\xi\\) genau mittig zwischen \\(a\\) und \\(b\\). Dass die sich so ergebene grau eingefärbte Rechteckfläche gleich \\(\\int_a^b f(x)\\,dx\\) ist, ergibt sich aus der visuell zumindest nachvollziebaren Tatsache, dass die Flächen zwischen \\(f(x)\\) und \\(f(\\xi)\\) im Intervall \\([a,\\xi]\\) und zwischen \\(f(\\xi)\\) und \\(f(x)\\) im Intervall \\([\\xi,b]\\) den gleichen Betrag haben, erstere aber mit einem negativen Vorzeichen behaftet ist. Der Mittelwertsatz der Integralrechnung garantiert im Allgemeinen aber nur die Existenz eines \\(\\xi \\in [a,b]\\) mit der diskutierten Eigenschaft, gibt aber keine Formel zu Bestimmung von \\(\\xi\\) an.\n\n\n\n\n\n\nAbbildung 7.4: Zum Mittelwertsatz der Integralrechnung\n\n\n\nDer Zweite Hauptsatz der Differential- und Integralrechnung schließlich besagt, wie man mithilfe der Stammfunktion ein bestimmtes Integral berechnet.\n\nTheorem 7.5 (Zweiter Hauptsatz der Differential- und Integralrechnung) Ist \\(F\\) eine Stammfunktion einer stetigen Funktion \\(f : I \\to \\mathbb{R}\\) auf einem Intervall \\(I\\), so gilt für \\(a,b \\in I\\) mit \\(a \\le b\\) \\[\\begin{equation}\n\\int_a^b f(x)\\,dx = F(b) - F(a) =: F(x)\\vert_a^b\n\\end{equation}\\]\n\n\nBeweis. Mit den Rechenregeln für bestimmte Integrale und dem ersten Hauptsatz der Differential- und Integralrechnung ergibt sich \\[\\begin{equation}\nF(b) - F(a) = \\int_\\alpha^b f(t)\\,dt - \\int_\\alpha^a f(t)\\,dt = \\int_a^b f(x)\\,dx\n\\end{equation}\\]\n\nWir wollen den Zweiten Haupsatz der Differential- und Integralrechnung in drei Beispielen anwenden (vgl. Abbildung 7.5).\n\n\n\n\n\n\nAbbildung 7.5: Beispiele zum Zweiten Hauptsatz der Differential- und Integralrechnung\n\n\n\nBeispiel (1)\nWir betrachten die Identitätsfunktion \\[\\begin{equation}\nf : \\mathbb{R} \\to \\mathbb{R}, x \\mapsto f(x) := x\n\\end{equation}\\] und wollen das bestimmte Integral dieser Funktion auf dem Intervall \\([0,1]\\), also \\[\\begin{equation}\n\\int_0^1 f(x)\\,dx = \\int_0^1 x \\,dx\n\\end{equation}\\] berechnen. Dazu erinnern wir uns, dass eine Stammfunktion von \\(f\\) durch \\[\\begin{equation}\nF : \\mathbb{R} \\to \\mathbb{R}, x \\mapsto F(x) := \\frac{1}{2}x^2\n\\end{equation}\\] gegeben ist, weil \\[\\begin{equation}\nF'(x) = \\frac{d}{dx}\\left(\\frac{1}{2}x^2 \\right) = 2 \\cdot \\frac{1}{2} x^{2-1} = x.\n\\end{equation}\\] Einsetzen in den Zweiten Hauptsatz der Differential- und Integralrechnung ergibt dann sofort \\[\\begin{equation}\n\\int_0^1 x \\,dx =\\frac{1}{2}1^2  - \\frac{1}{2}0^2 = \\frac{1}{2}.\n\\end{equation}\\] Dieses Ergebnis ist mit der Intuition, die sich anhand der grauen Fläche in Abbildung 7.5 A, ergibt kongruent.\nBeispiel (2)\nAls nächstes betrachten wird die Quadratfunktion \\[\\begin{equation}\nf : \\mathbb{R} \\to \\mathbb{R}, x \\mapsto f(x) := x^2\n\\end{equation}\\] und wollen das bestimmte Integral auch dieser Funktion auf dem Intervall \\([0,1]\\), also \\[\\begin{equation}\n\\int_0^1 f(x)\\,dx = \\int_0^1 x^2 \\,dx\n\\end{equation}\\] berechnen. Dazu erinnern wir uns, dass eine Stammfunktion von \\(f\\) durch \\[\\begin{equation}\nF : \\mathbb{R} \\to \\mathbb{R}, x \\mapsto F(x) := \\frac{1}{3}x^3\n\\end{equation}\\] gegeben ist, weil \\[\\begin{equation}\nF'(x) = \\frac{d}{dx}\\left(\\frac{1}{3}x^3 \\right) = 3 \\cdot \\frac{1}{3} x^{3-1} = x^2.\n\\end{equation}\\] Einsetzen in den Zweiten Hauptsatz der Differential- und Integralrechnung ergibt dann sofort \\[\\begin{equation}\n\\int_0^1 x^2 \\,dx =\\frac{1}{3}1^3  - \\frac{1}{3}0^3 = \\frac{1}{3}.\n\\end{equation}\\] Dieses Ergebnis ist mit der Intuition, die sich aus dem Vergleich der grauen Flächen in Abbildung 7.5 A und Abbildung 7.5 B ergibt, kongruent.\nBeispiel (3)\nSchließlich betrachten wir die lineare Funktion \\[\\begin{equation}\nf : \\mathbb{R} \\to \\mathbb{R}, x \\mapsto f(x) := -x + 1\n\\end{equation}\\] und wollen das bestimmte Integral auch dieser Funktion auf dem Intervall \\([0,2]\\), also \\[\\begin{equation}\n\\int_0^2 f(x)\\,dx = \\int_0^2 -x + 1 \\,dx\n\\end{equation}\\] berechnen. Dazu erinnern wir uns, dass eine Stammfunktion der linearen Funktion mit \\(a = -1\\) und \\(b = 1\\) (vgl. Tablle ) durch \\[\\begin{equation}\nF : \\mathbb{R} \\to \\mathbb{R}, x \\mapsto F(x) := -\\frac{1}{2}x^2 + x\n\\end{equation}\\] gegeben ist, weil \\[\\begin{equation}\nF'(x) = \\frac{d}{dx}\\left(-\\frac{1}{2}x^2 + x \\right) = - 2 \\cdot \\frac{1}{2} x^{2-1} + 1 \\cdot x^{1-1} = -x + 1.\n\\end{equation}\\] Einsetzen in den Zweiten Hauptsatz der Differential- und Integralrechnung ergibt dann sofort \\[\\begin{equation}\n\\int_0^2 -x + 1 \\,dx\n= \\left(-\\frac{1}{2}2^2 + 2 \\right) - \\left(-\\frac{1}{2}0^2 + 0 \\right).\n= -2 + 2 - 0\n= 0.\n\\end{equation}\\] Dieses Ergebnis ist mit der Intuition kongruent, dass sich die “positive” und die “negative” graue Fläche in Abbildung 7.5 C ausgleichen, kongruent.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Integralrechnung</span>"
    ]
  },
  {
    "objectID": "107-Integralrechnung.html#sec-uneigentliche-integrale",
    "href": "107-Integralrechnung.html#sec-uneigentliche-integrale",
    "title": "7  Integralrechnung",
    "section": "7.3 Uneigentliche Integrale",
    "text": "7.3 Uneigentliche Integrale\nUneigentliche Integrale sind bestimmte Integrale bei denen mindestens eine Integrationsgrenze keine reelle Zahl ist, sondern \\(-\\infty\\) oder \\(\\infty\\). Wir beleuchten die Natur uneigentlicher Integrale mit folgender Definition und einem Beispiel.\n\nDefinition 7.5 (Uneigentliche Integrale) \\(f : \\mathbb{R} \\to \\mathbb{R}\\) sei eine univariate reellwertige Funktion. Mit den Definitionen \\[\\begin{equation}\n\\int_{-\\infty}^b f(x)\\,dx := \\lim_{a \\to -\\infty} \\int_a^b f(x)\\,dx\n\\mbox{ und }\n\\int_a^\\infty f(x)\\,dx := \\lim_{b \\to \\infty} \\int_a^b f(x)\\,dx\n\\end{equation}\\] und der Additivität von Integralen \\[\\begin{equation}\n\\int_{-\\infty}^\\infty f(x)\\,dx = \\int_{-\\infty}^b f(x)\\,dx + \\int_b^{\\infty}f(x)\\,dx\n\\end{equation}\\] wird der Begriff des bestimmten Integrals auf die unbeschränkten Integrationsintervalle \\(]-\\infty,b]\\), \\([a,\\infty[\\) und \\(]-\\infty,\\infty[\\) erweitert. Integrale mit unbeschränkten Integrationsintervallen heißen uneigentliche Integrale. Wenn die entsprechenden Grenzwerte existieren, sagt man, dass die uneigentlichen Integrale konvergieren.\n\nAls Beispiel betrachten wir das uneigentliche Integral der Funktion \\[\\begin{equation}\nf : \\mathbb{R} \\to \\mathbb{R}, x \\mapsto f(x) \\frac{1}{x^2}\n\\end{equation}\\] auf dem Intervall \\([1, \\infty[\\), also \\[\\begin{equation}\n\\int_1^{\\infty} \\frac{1}{x^2}\\,dx.\n\\end{equation}\\] Nach den Festlegungen in der Definition uneigentlicher Integrale gilt \\[\\begin{equation}\n\\int_1^{\\infty} \\frac{1}{x^2}\\,dx = \\lim_{b \\to \\infty} \\int_1^b \\frac{1}{x^2}\\,dx.\n\\end{equation}\\] Mit der Stammfunktion \\(F(x) = -x^{-1}\\) von \\(f(x) = x^{-2}\\) ergibt sich für das bestimmte Integral in obiger Gleichung \\[\\begin{equation}\n\\int_1^b \\frac{1}{x^2}\\,dx\n= F(b) - F(1)\n= -\\frac{1}{b} - \\left(-\\frac{1}{1}\\right)\n= -\\frac{1}{b} + 1.\n\\end{equation}\\] Es ergibt sich also \\[\\begin{equation}\n\\int_1^{\\infty} \\frac{1}{x^2}\\,dx\n= \\lim_{b \\to \\infty} \\int_1^b \\frac{1}{x^2}\\,dx\n= \\lim_{b \\to \\infty}\\left(-\\frac{1}{b} + 1\\right)\n= - \\lim_{b \\to \\infty}\\frac{1}{b} + \\lim_{b \\to \\infty} 1\n= 0 + 1\n= 1.\n\\end{equation}\\]",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Integralrechnung</span>"
    ]
  },
  {
    "objectID": "107-Integralrechnung.html#sec-mehrdimensionale-integrale",
    "href": "107-Integralrechnung.html#sec-mehrdimensionale-integrale",
    "title": "7  Integralrechnung",
    "section": "7.4 Mehrdimensionale Integrale",
    "text": "7.4 Mehrdimensionale Integrale\nBisher haben wir nur Integrale univariater reellwertiger Funktionen betrachtet. Der Integralbegriff lässt sich auch auf multivariate reellwertige Funktionen erweitern. Allerdings ist dann der Integrationsbereich der Funktion nicht notwendigerweise so einfach zu beschreiben wie ein Intervall; insbesondere sind zum Beispiel schon im zweidimensionalen arbiträr geformte zweidimensionale Integrationsbereiche möglich. Wir wollen hier nun den einfachsten Fall eines Hyperrechtecks betrachten. In diesem Fall können wir mehrdimensionale bestimmte Integrale wie folgt definieren.\n\nDefinition 7.6 (Mehrdimensionale Integrale) \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\) sei eine multivariate reellwertige Funktion. Dann heißen Integrale der Form \\[\\begin{equation}\n\\int\\limits_{[a_1,b_1]\\times \\cdots \\times [a_n,b_n]} f(x)\\,dx\n= \\int_{a_1}^{b_1} \\cdots \\int_{a_n}^ {b_n} f(x_1,...,x_n)\\,dx_1...\\,dx_n\n\\end{equation}\\] mehrdimensionale bestimmte Integrale auf Hyperrechtecken. Weiterhin heißen Integrale der Form \\[\\begin{equation}\n\\int_{\\mathbb{R}^n} f(x)\\,dx\n= \\int_{-\\infty}^{\\infty}  \\cdots \\int_{-\\infty}^{\\infty}\nf(x_1,...,x_n)\\,dx_1...\\,dx_n\n\\end{equation}\\] mehrdimensionale uneigentliche Integrale.\n\nWie schon erwähnt kann man multivariate reellwertige Funktion nicht nur auf Hyperrechtecken, sondern im Prinzip auf beliebigen Hyperflächen integrieren. Dies kann sich jedoch oft schwierig gestalten.\nAls Beispiel betrachten wir das zweidimensionale bestimmte Integral der Funktion \\[\\begin{equation}\nf : \\mathbb{R}^2 \\to \\mathbb{R}, (x_1,x_2) \\mapsto f(x_1,x_2) := x_1^2 + 4x_2\n\\end{equation}\\] auf dem Rechteck \\([0,1] \\times [0,1]\\). Der Satz von Fubini der Theorie mehrdimensionaler Integrale besagt, dass man mehrdimensionale Integrale in beliebiger Koordinatenfolge auswerten kann. Es gilt also zum Beispiel, dass \\[\\begin{equation}\n\\int_{a_1}^{b_1} \\left(\\int_{a_2}^{b_2} f(x_1,x_2)\\,dx_2\\right) \\,dx_1\n= \\int_{a_2}^{b_2} \\left(\\int_{a_1}^{b_1} f(x_1,x_2)\\,dx_1 \\right) \\,dx_2.\n\\end{equation}\\] In diesem Sinne betrachten wir für das Beispiel \\[\\begin{equation}\n\\int_0^1 \\int_0^1 x_1^2 + 4x_2 \\,dx_1\\,dx_2\n= \\int_0^1 \\left(\\int_0^1 x_1^2 + 4x_2 \\,dx_1\\right)\\,dx_2\n\\end{equation}\\] also zunächst das innere Integral. \\(x_2\\) nimmt dabei die Rolle einer Konstanten ein. Eine Stammfunktion von \\(g(x_1) := x_1^2  +4 x_2\\) ist \\(G(x_1)\n= \\frac{1}{3}x_1^3 + 4x_2x_1\\), wie man sich durch Ableiten von \\(G\\) überzeugt. Es ergibt sich also für das innere Integral \\[\\begin{align}\n\\begin{split}\n\\int_0^1 x_1^2 + 4x_2 \\,dx_1\n& = G(1) - G(0) \\\\\n& = \\frac{1}{3}\\cdot1^3 + 4x_2\\cdot 1 - \\frac{1}{3}\\cdot 0^3 - 4x_2\\cdot 0 \\\\\n& = \\frac{1}{3} + 4x_2.\n\\end{split}\n\\end{align}\\] Betrachten des äußeren Integrals ergibt dann mit der Stammfunktion \\[\\begin{equation}\nH(x_2) = \\frac{1}{3}x_2 + 2x_2^2\n\\end{equation}\\] von \\[\\begin{equation}\nh(x_2) := \\frac{1}{3} + 4x_2,\n\\end{equation}\\] dass \\[\\begin{align}\n\\begin{split}\n\\int_0^1 \\int_0^1 x_1^2 + 4x_2 \\,dx_1\\,dx_2\n& = \\int_0^1 \\frac{1}{3} + 4x_2 \\,dx_2                                          \\\\\n& = H(1) - H(0)                                                                 \\\\\n& = \\frac{1}{3}\\cdot 1 + 4\\cdot 1^2 - \\frac{1}{3}\\cdot 0 + 4\\cdot 0^2           \\\\\n& = \\frac{13}{3}.\n\\end{split}\n\\end{align}\\]",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Integralrechnung</span>"
    ]
  },
  {
    "objectID": "107-Integralrechnung.html#sec-selbstkontrollfragen-integralrechnung",
    "href": "107-Integralrechnung.html#sec-selbstkontrollfragen-integralrechnung",
    "title": "7  Integralrechnung",
    "section": "7.5 Selbstkontrollfragen",
    "text": "7.5 Selbstkontrollfragen\n\nGeben Sie die Definition des Begriffs der Stammfunktion wieder.\nGeben Sie die Definition des Begriffs des unbestimmten Integrals wieder.\nErläutern Sie die intuitive Bedeutung des Begriff des Riemannschen Integrals.\nGeben Sie den ersten Hauptsatz der Differential- und Integralrechnung wieder.\nGeben Sie den zweiten Hauptsatz der Differential- und Integralrechnung wieder.\nErläutern Sie den Begriff des uneigentlichen Integrals.\nErläutern Sie den Begriff des mehrdimensionalen Integrals.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Integralrechnung</span>"
    ]
  },
  {
    "objectID": "108-Vektoren.html",
    "href": "108-Vektoren.html",
    "title": "8  Vektoren",
    "section": "",
    "text": "8.1 Reeller Vektorraum\nWir beginnen mit der allgemeinen Definition eines Vektorraums, die grundlegende Regeln zum Rechnen mit Vektoren festlegt.\nEs fällt auf, dass Definition 8.1 zwar festlegt, wie mit Vektoren gerechnet werden soll, jedoch keine Aussage darüber macht, was ein Vektor, über ein ein Element einer Menge hinaus, eigentlich ist. Dies ist der Tatsache geschuldet, dass es verschiedenste mathematische Objekte gibt, für die Vektorraumstrukturen definiert werden können. Beispiele dafür sind die Menge der reellen \\(m\\)-Tupel, die Menge der Matrizen, die Menge der Polynome, die Menge der Lösungen eines linearen Gleichungssystems, die Menge der reellen Folgen, die Menge der stetigen Funktionen u.v.a.m.\nWir sind hier zunächst nur am Vektorraum der Menge reellen \\(m\\)-Tupel interessiert. Wir erinnern dazu daran, dass wir die reellen \\(m\\)-Tupel mit \\[\\begin{equation}\n\\mathbb{R}^m := \\left\\lbrace \\begin{pmatrix} x_1 \\\\ \\vdots \\\\ x_m \\end{pmatrix} | x_i \\in \\mathbb{R} \\mbox{ für alle } 1 \\le i \\le m \\right\\rbrace\n\\end{equation}\\] bezeichnen und \\(\\mathbb{R}^m\\) als “\\(\\mathbb{R}\\) hoch m” aussprechen. Die Elemente \\(x \\in \\mathbb{R}^m\\) nennen wir reelle Vektoren oder auch einfach Vektoren. Wir wollen nun der Definition eines Vektorraums die Menge \\(\\mathbb{R}^m\\) zugrunde legen. Dazu definieren wir zunächst die Vektoraddition für Elemente von \\(\\mathbb{R}^m\\) und die Skalarmultiplikation für Elemente von \\(\\mathbb{R}\\) und \\(\\mathbb{R}^m\\)\nEs ergibt sich dann folgendes Resultat.\nFür einen Beweis, auf den wir hier verzichten wollen, muss man die Bedingungen (1) bis (8) aus Definition 8.1 für die hier betrachtete Menge und die hier festgelegten Formen der Vektoraddition und der Skalarmultiplikation nachweisen. Diese ergeben sich aber leicht aus den Rechenregeln von Addition und Multiplikation in \\(\\mathbb{R}\\) und der Tatsache, dass Vektoraddition und Skalarmultiplikation für Elemente von \\(\\mathbb{R}^m\\) in Definition 8.2 komponentenweise definiert wurden. Wir definieren damit den Begriff des reellen Vektorraums.\nAuf Grundlage von Definition 8.3 wollen wir uns nun das Rechnen mit reellen Vektoren anhand einiger Beispiele verdeutlichen.\nBeispiele\n(1) Für\n\\[\nx:=\n\\begin{pmatrix}\n1 \\\\ 2 \\\\ 3 \\\\ 4\n\\end{pmatrix}\n\\in \\mathbb{R}^4\n\\mbox{ und }\ny:=\n\\begin{pmatrix}\n2 \\\\ 1 \\\\ 0 \\\\ 1\n\\end{pmatrix}\n\\in \\mathbb{R}^4\n\\] gilt \\[\nx + y\n=\n\\begin{pmatrix}\n1 \\\\ 2 \\\\ 3 \\\\ 4\n\\end{pmatrix}\n+\n\\begin{pmatrix}\n2 \\\\ 1 \\\\ 0 \\\\ 1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 + 2 \\\\ 2 + 1 \\\\ 3 + 0\\\\ 4 + 1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n3 \\\\ 3\\\\ 3 \\\\ 5\n\\end{pmatrix}\n\\in \\mathbb{R}^4.\n\\] In R implementiert dieses Beispiel wie folgt\nx = matrix(c(1,2,3,4), nrow = 4)      # Vektordefinition\ny = matrix(c(2,1,0,1), nrow = 4)      # Vektordefinition\nx + y                                 # Vektoraddition\n\n     [,1]\n[1,]    3\n[2,]    3\n[3,]    3\n[4,]    5\n(2) Für \\[\nx:=\n\\begin{pmatrix}\n2 \\\\ 3\n\\end{pmatrix}\n\\in \\mathbb{R}^2\n\\mbox{ und }\ny:=\n\\begin{pmatrix}\n1 \\\\ 3 \\\n\\end{pmatrix}\n\\in \\mathbb{R}^2\n\\] gilt \\[\nx - y\n=\n\\begin{pmatrix}\n2 \\\\ 3\n\\end{pmatrix}\n-\n\\begin{pmatrix}\n1 \\\\ 3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n2 - 1 \\\\ 3 - 3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 \\\\ 0\n\\end{pmatrix}\n\\in \\mathbb{R}^2.\n\\] In R implementiert man dieses Beispiel wie folgt\nx = matrix(c(2,3), nrow = 2)         # Vektordefinition\ny = matrix(c(1,3), nrow = 2)         # Vektordefinition\nx - y                                # Vektorsubtraktion\n\n     [,1]\n[1,]    1\n[2,]    0\n(3) Für \\[\nx:=\n\\begin{pmatrix}\n2 \\\\ 1 \\\\ 3\n\\end{pmatrix}\n\\in \\mathbb{R}^3\n\\mbox{ und }\na := 3 \\in \\mathbb{R}\n\\] gilt \\[\nax\n=\n3\n\\begin{pmatrix}\n2 \\\\ 1 \\\\ 3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n3 \\cdot 2 \\\\ 3 \\cdot 1 \\\\ 3 \\cdot 3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n6 \\\\ 3 \\\\ 9\n\\end{pmatrix}\n\\in \\mathbb{R}^3.\n\\] In R implementiert man dieses Beispiel wie folgt\nx = matrix(c(2,1,3), nrow = 3)       # Vektordefinition\na = 3                                # Skalardefinition\na*x                                  # Skalarmultiplikation\n\n     [,1]\n[1,]    6\n[2,]    3\n[3,]    9\nFür \\(m \\in \\{1,2,3\\}\\) kann man sich reelle Vektoren und das Rechnen mit ihnen visuell veranschaulichen. Für \\(m &gt; 3\\), wenn also zum Beispiel für eine Person mehr als drei quantitative Merkmale zu ihrem Gesundheitszustand vorliegen, was in der Anwendung regelmäßig der Fall ist, ist dies nicht möglich. Trotzdem mag die visuelle Intuition für \\(m \\le 3\\) einen Einstieg in das Verständnis von Vektorräumen erleichtern. Wir fokussieren hier auf den Fall \\(m := 2\\). In diesem Fall liegen die betrachteten reellen Vektoren in der zweidimensionalen Ebene und werden üblicherweise als Punkte oder Pfeile visualisiert (Abbildung 8.1).\nAbbildung 8.2 visualisiert die Vektoraddition \\[\\begin{equation}\n\\begin{pmatrix}\n1 \\\\ 2\n\\end{pmatrix}\n+\n\\begin{pmatrix}\n3 \\\\ 1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n4 \\\\ 3\n\\end{pmatrix}.\n\\end{equation}\\] Der Summenvektor entspricht dabei der Diagonale des von den beiden Summanden aufgespannten Parallelogramms.\nAbbildung 8.3 visualisiert die Vektorsubtraktion \\[\\begin{equation}\n\\begin{pmatrix}\n1 \\\\ 2\n\\end{pmatrix}\n-\n\\begin{pmatrix}\n3 \\\\ 1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 \\\\ 2\n\\end{pmatrix}\n+\n\\begin{pmatrix}\n-3 \\\\ -1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n-2 \\\\ \\,\\, 1\n\\end{pmatrix}\n\\end{equation}\\] Der resultierende Vektor entspricht dabei der Diagonale des von dem ersten Vektors und dem entgegensetzten Vektor des zweiten Vektors aufgespannten Parallelogramms.\nAbbildung 8.4 schließlich visualisiert die Skalarmultiplikation \\[\\begin{equation}\n3\n\\begin{pmatrix}\n1 \\\\ 1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n3 \\\\ 3\n\\end{pmatrix}\n\\end{equation}\\] Die Multiplikation eines Vektors mit einem Skalar ändert dabei immer nur seine Länge, nicht jedoch seine Richtung.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Vektoren</span>"
    ]
  },
  {
    "objectID": "108-Vektoren.html#sec-reeller-vektorraum",
    "href": "108-Vektoren.html#sec-reeller-vektorraum",
    "title": "8  Vektoren",
    "section": "",
    "text": "Definition 8.1 (Vektorraum) Es seien \\(V\\) eine nichtleere Menge und \\(S\\) eine Menge von Skalaren. Weiterhin sei eine Abbildung \\[\\begin{equation}\n+ : V \\times V \\to V, (v_1,v_2) \\mapsto +(v_1, v_2) =: v_1 + v_2,\n\\end{equation}\\] genannt Vektoraddition, definiert. Schließlich sei eine Abbildung \\[\\begin{equation}\n\\cdot : S \\times V \\to V, (s,v) \\mapsto \\cdot(s,v) =: sv,\n\\end{equation}\\] genannt Skalarmultiplikation definiert. Dann wird das Tupel \\((V,S,+,\\cdot)\\) genau dann Vektorraum genannt, wenn für beliebige Elemente \\(v,w,u\\in V\\) und \\(a,b \\in S\\) folgende Bedingungen gelten:\n(1) Kommutativität der Vektoraddition. \\[\nv + w = w + v.\n\\] (2) Assoziativität der Vektoraddition. \\[\n(v + w) + u = v + (w + u)\n\\] (3) Existenz eines neutralen Elements der Vektoraddition. \\[\n\\mbox{Es gibt einen Vektor } 0 \\in V \\mbox{ mit } v + 0 = 0 + v = v.\n\\] (4) Existenz inverser Elemente der Vektoraddition \\[\n\\mbox{Für alle Vektoren } v \\in V \\mbox{ gibt es einen Vektor } -v \\in V \\mbox{ mit } v + (-v) = 0.\n\\] (5) Existenz eines neutralen Elements der Skalarmultiplikation. \\[\n\\mbox{Es gibt einen Skalar } 1 \\in S \\mbox{ mit } 1 \\cdot v = v.\n\\] (6) Assoziativität der Skalarmultiplikation. \\[\na \\cdot (b \\cdot c) = (a \\cdot b)\\cdot c.\n\\] (7) Distributivität hinsichtlich der Vektoraddition. \\[\na\\cdot (v + w) = a\\cdot v + a \\cdot w.\n\\] (8) Distributivität hinsichtlich der Skalaraddition. \\[\n(a + b)\\cdot v = a\\cdot v + b\\cdot v.\n\\]\n\n\n\n\nDefinition 8.2 (Vektoraddition und Skalarmultiplikation in \\(\\mathbb{R}^m\\)) Für alle \\(x,y \\in \\mathbb{R}^m\\) und \\(a \\in \\mathbb{R}\\) sei die Vektoraddition durch \\[\\begin{equation}\n+ : \\mathbb{R}^m \\times \\mathbb{R}^m \\to \\mathbb{R}^m, (x,y) \\mapsto x + y =\n\\begin{pmatrix}\nx_1 \\\\ \\vdots \\\\ x_m\n\\end{pmatrix}\n+\n\\begin{pmatrix}\ny_1 \\\\ \\vdots \\\\ y_m\n\\end{pmatrix}\n:=\n\\begin{pmatrix}\nx_1 + y_1 \\\\ \\vdots \\\\ x_m + y_m\n\\end{pmatrix}\n\\end{equation}\\] und die Skalarmultiplikation durch \\[\\begin{equation}\n\\cdot : \\mathbb{R} \\times \\mathbb{R}^m \\to \\mathbb{R}^m, (a,x) \\mapsto\nax =\na\n\\begin{pmatrix}\nx_1  \\\\ \\vdots \\\\ x_m\n\\end{pmatrix}\n:=\n\\begin{pmatrix}\nax_1  \\\\ \\vdots \\\\a x_m\n\\end{pmatrix}\n\\end{equation}\\] definiert.\n\n\n\nTheorem 8.1 (Reeller Vektorraum) \\((\\mathbb{R}^m,+,\\cdot)\\) mit den Rechenregeln der Addition und Multiplikation in \\(\\mathbb{R}\\) einen Vektorraum.\n\n\n\nDefinition 8.3 (Reeller Vektorraum) Für \\(\\mathbb{R}^m\\) seien \\(+\\) und \\(\\cdot\\) die in Definition 8.2 definierte Vektoraddition und Skalarmultiplikation. Dann nennen wir auf Grundlage von Theorem 8.1 den Vektorraum \\((\\mathbb{R}^m,+,\\cdot)\\) den reellen Vektorraum\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbbildung 8.1: Visualisierung von Vektoren in \\(\\mathbb{R}^2\\)\n\n\n\n\n\n\n\n\n\n\nAbbildung 8.2: Vektoraddition in \\(\\mathbb{R}^2\\)\n\n\n\n\n\n\n\n\n\n\nAbbildung 8.3: Vektorsubtraktion in \\(\\mathbb{R}^2\\)\n\n\n\n\n\n\n\n\n\n\nAbbildung 8.4: Skalarmultiplikation in \\(\\mathbb{R}^2\\)",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Vektoren</span>"
    ]
  },
  {
    "objectID": "108-Vektoren.html#sec-euklidischer-vektorraum",
    "href": "108-Vektoren.html#sec-euklidischer-vektorraum",
    "title": "8  Vektoren",
    "section": "8.2 Euklidischer Vektorraum",
    "text": "8.2 Euklidischer Vektorraum\nDer reelle Vektorraum kann durch Definition des Skalarprodukts im Sinne eines Euklidischen Vektorraums mit räumlich-geometrischer Intuition versehen werden. Diese ermöglicht es insbesondere, Begriffe wie die Länge eines Vektors, den Abstand zwischen zwei Vektoren, und nicht zuletzt den Winkel zwischen zwei Vektoren zu definieren und zu berechnen. Wir führen zunächst das Skalarprodukt ein.\n\nDefinition 8.4 (Skalarprodukt auf \\(\\mathbb{R}^m\\)) Das Skalarprodukt auf \\(\\mathbb{R}^m\\) ist definiert als die Abbildung \\[\\begin{equation}\n\\langle \\rangle : \\mathbb{R}^m \\times \\mathbb{R}^m \\to \\mathbb{R},\n(x,y) \\mapsto \\langle (x,y) \\rangle := \\langle x,y \\rangle := \\sum_{i=1}^m x_i y_i.\n\\end{equation}\\]\n\nDas Skalarprodukt heißt Skalarprodukt, weil es einen Skalar ergibt, nicht etwa, weil mit Skalaren multipliziert wird. Das Skalarprodukt steht in enger Beziehung zum Matrixprodukt, wie wir an späterer Stelle sehen werden. Wir betrachten zunächst ein Beispiel und seine Implementation in R.\nBeispiel\nEs seien \\[\\begin{equation}\nx :=\n\\begin{pmatrix}\n1 \\\\ 2 \\\\ 3\n\\end{pmatrix}\n\\mbox{ und }\ny :=\n\\begin{pmatrix}\n2 \\\\ 0 \\\\ 1\n\\end{pmatrix}\n\\end{equation}\\] Dann ergibt sich \\[\\begin{equation}\n\\langle x,y \\rangle\n= x_1y_1 + x_2y_2 + x_3y_3\n= 1 \\cdot 2 + 2 \\cdot 0 + 3 \\cdot 1\n= 2 + 0 + 3\n= 5.\n\\end{equation}\\]\nIn R gibt es verschiedene Möglichkeiten, ein Skalarprodukt auszuwerten. Wir führen zwei von ihnen für das gebebene Beispiel untenstehend auf.\n\n# Vektordefinitionen\nx = matrix(c(1,2,3), nrow = 3)\ny = matrix(c(2,0,1), nrow = 3)\n\n# Skalarprodukt mithilfe von R's komponentenweiser Multiplikation und sum() Funktion\nsum(x*y)\n\n[1] 5\n\n# Skalarprodukt mithilfe von R's Matrixtransposition und -multiplikation\nt(x) %*% y\n\n     [,1]\n[1,]    5\n\n\nMithilfe des Skalarprodukts kann der Begriff des reellen Vektorraums zum Begriff des reellen kanonischen Euklidischen Vektorraums erweiter werden.\n\nDefinition 8.5 (Euklidischer Vektorraum) Das Tupel \\(\\left((\\mathbb{R}^m, +, \\cdot), \\langle \\rangle \\right)\\) aus dem reellen Vektorraum \\((\\mathbb{R}^m, +, \\cdot)\\) und dem Skalarprodukt \\(\\langle \\rangle\\) auf \\(\\mathbb{R}^m\\) heißt reeller kanonischer Euklidischer Vektorraum.\n\nGenerell heißt jedes Tupel aus einem Vektorraum und einem Skalarprodukt “Euklidischer Vektorraum”. Informell sprechen wir aber oft auch einfach von \\(\\mathbb{R}^m\\) als “Euklidischer Vektorraum” und insbesondere bei \\(\\left((\\mathbb{R}^m, +, \\cdot), \\langle \\rangle \\right)\\) vom “Euklidischen Vektorraum”. Ein Euklidischer Vektorraum ist ein Vektorraum mit geometrischer Struktur, die durch das Skalarprodukt induziert wird. Insbesondere bekommen im Euklidischen Vektorraum nun die geometrischen Begriffe von Länge, Abstand und Winkel eine Bedeutung. Wir definieren sie wie folgt.\n\nDefinition 8.6 \\(\\left((\\mathbb{R}^m, +, \\cdot), \\langle \\rangle \\right)\\) sei der Euklidische Vektorraum.\n(1) Die Länge eines Vektors \\(x \\in \\mathbb{R}^m\\) ist definiert als \\[\\begin{equation}\n\\Vert x \\Vert := \\sqrt{\\langle x, x \\rangle}.\n\\end{equation}\\] (2) Der Abstand zweier Vektoren \\(x,y \\in \\mathbb{R}^m\\) ist definiert als \\[\\begin{equation}\nd(x,y) := \\Vert x - y \\Vert.\n\\end{equation}\\] (3) Der Winkel \\(\\alpha\\) zwischen zwei Vektoren \\(x,y \\in \\mathbb{R}^m\\) mit \\(x,y \\neq 0\\) ist definiert durch \\[\\begin{equation}\n0 \\le \\alpha \\le \\pi \\mbox{ und } \\cos \\alpha\n:= \\frac{\\langle x, y \\rangle}{\\Vert x \\Vert \\Vert y \\Vert}\n\\end{equation}\\]\n\nDie Länge \\(\\Vert x \\Vert\\) eines Vektors \\(x \\in \\mathbb{R}^m\\) heißt auch Euklidische Norm von \\(x\\) oder \\(\\ell_2\\)-Norm von \\(x\\) oder einfach Norm von \\(x\\). Sie wird häufig auch mit \\(\\Vert x \\Vert_2\\) bezeichnet. Wir betrachten drei Beispiele für die Bestimmung der Länge eines Vektors und ihre entsprechende R Implementation. Wir veranschaulichen diese Beispiele in Abbildung 8.5.\nBeispiel (1)\n\\[\\begin{equation}\n\\left\\lVert \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} \\right\\rVert\n= \\sqrt{\\left\\langle \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} \\right\\rangle}\n= \\sqrt{2^2 + 0^2}\n= \\sqrt{4}\n= 2.00\n\\end{equation}\\]\n\nnorm(matrix(c(2,0),nrow = 2), type = \"2\")             # Vektorlänge = l_2 Norm\n\n[1] 2\n\n\nBeispiel (2)\n\\[\\begin{equation}\n\\left\\lVert \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} \\right\\rVert\n= \\sqrt{\\left\\langle \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}, \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} \\right\\rangle}\n= \\sqrt{2^2 + 2^2}\n= \\sqrt{8}\n\\approx 2.83\n\\end{equation}\\]\n\nnorm(matrix(c(2,2),nrow = 2), type = \"2\")             # Vektorlänge = l_2 Norm\n\n[1] 2.828427\n\n\nBeispiel (3)\n\\[\\begin{equation}\n\\left\\lVert \\begin{pmatrix} 2 \\\\ 4 \\end{pmatrix} \\right\\rVert\n= \\sqrt{\\left\\langle \\begin{pmatrix} 2 \\\\ 4 \\end{pmatrix}, \\begin{pmatrix} 2 \\\\ 4 \\end{pmatrix} \\right\\rangle}\n= \\sqrt{2^2 + 4^2}\n= \\sqrt{20}\n\\approx 4.47\n\\end{equation}\\]\n\nnorm(matrix(c(2,4),nrow = 2), type = \"2\")             # Vektorlänge = l_2 Norm\n\n[1] 4.472136\n\n\n\n\n\n\n\n\nAbbildung 8.5: Vektorlänge in \\(\\mathbb{R}^2\\)\n\n\n\nFür den Abstand \\(d(x,y)\\) zweier Vektoren \\(x,y\\in\\mathbb{R}^m\\) halten wir ohne Beweis fest, dass er zum einen nicht-negativ und symmetrisch ist, also dass \\[\\begin{equation}\nd(x,y) \\ge 0, d(x,x) = 0 \\mbox{ und } d(x,y) = d(y,x)\n\\end{equation}\\] gelten. Zudem erfüllt \\(d(x,y)\\) die sogenannte Dreiecksungleichung, die besagt, dass die direkte Wegstrecke zwischen zwei Punkten im Raum immer kürzer ist als eine indirekte Wegstrecke über einen dritten Punkt, \\[\\begin{equation}\nd(x,y) \\le d(x,z) + d(z,y).\n\\end{equation}\\] Damit erfüllt \\(d(x,y)\\) wichtige Aspekte der räumlichen Anschauung. Wir geben zwei Beispiele für die Bestimmung von Abständen von Vektoren in \\(\\mathbb{R}^2\\), die wir in Abbildung 8.6 visualisieren.\nBeispiel (1)\n\\[\\begin{equation}\nd\\left(\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}\\right)\n= \\left\\lVert \\begin{pmatrix}  1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} \\right\\rVert\n= \\left\\lVert \\begin{pmatrix} -1 \\\\ -1 \\end{pmatrix}  \\right\\rVert\n= \\sqrt{(-1)^2 + (-1)^2}\n= \\sqrt{2}\n\\approx 1.41\n\\end{equation}\\]\n\nnorm(matrix(c(1,1),nrow = 2) - matrix(c(2,2),nrow = 2), type = \"2\")\n\n[1] 1.414214\n\n\nBeispiel (2)\n\\[\\begin{equation}\nd\\left(\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\begin{pmatrix} 4 \\\\ 1 \\end{pmatrix}\\right)\n= \\left\\lVert \\begin{pmatrix}  1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 4 \\\\ 1 \\end{pmatrix} \\right\\rVert\n= \\left\\lVert \\begin{pmatrix} -3 \\\\ 0 \\end{pmatrix} \\right\\rVert\n= \\sqrt{(-3)^2 + 0^2}\n= \\sqrt{9}\n= 3\n\\end{equation}\\]\n\nnorm(matrix(c(1,1),nrow = 2) - matrix(c(1,4),nrow = 2), type = \"2\")\n\n[1] 3\n\n\n\n\n\n\n\n\nAbbildung 8.6: Vektorabstände in \\(\\mathbb{R}^2\\)\n\n\n\nSchließlich halten wir fest, dass für die Berechnung des Winkels zwischen zwei Vektoren anhand obiger Definition gilt, dass die Kosinusfunktion \\(\\cos\\) auf \\([0,\\pi]\\) bijektiv, also invertierbar mit der Umkehrfunktion \\(acos\\), der Arkuskosinusfunktion, ist. Auch für den Begriff des Winkels wollen wir zwei Beispiele betrachten. Man beachte dabei insbesondere, dass die Definition 8.6 den Winkel in Radians angibt. Für eine Angabe in Grad ist eine entsprechende Umrechnung erforderlich.\nBeispiel (1)\n\\[\\begin{equation}\n\\mbox{acos}\n\\left(\\frac{\\left\\langle \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 3 \\\\ 3 \\end{pmatrix} \\right\\rangle}\n           {\\left\\lVert  \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix} \\right\\rVert \\left\\lVert \\begin{pmatrix} 3 \\\\ 3 \\end{pmatrix} \\right\\rVert}\n\\right)\n= \\mbox{acos}\n\\left(\\frac{3\\cdot 3 + 3 \\cdot 0}\n           {\\sqrt{3^2 + 0^2} \\cdot \\sqrt{3^2 + 3^2}}\n\\right)\n= \\mbox{acos}\n\\left(\\frac{9}\n           {3 \\cdot \\sqrt{18}}\n\\right)\n= \\frac{\\pi}{4}\n\\approx 0.785\n\\end{equation}\\] \nDie Umrechnung in Grad ergibt dann \\[\\begin{equation}\n0.785 \\cdot \\frac{180°}{\\pi} = 45°\n\\end{equation}\\] In R implementiert man dies wie folgt.\n\nx = matrix(c(3,0), nrow = 2)                                 # Vektor 1\ny = matrix(c(3,3), nrow = 2)                                 # Vektor 2\nw = acos(sum(x*y)/(sqrt(sum(x*x))*sqrt(sum(y*y)))) * 180/pi  # Winkel in Grad\nprint(w)\n\n[1] 45\n\n\nBeispiel (2)\n\\[\\begin{equation}\n\\alpha\n= \\mbox{acos}\n\\left(\\frac{\\left\\langle \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 3 \\end{pmatrix} \\right\\rangle}\n           {\\left\\lVert  \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix} \\right\\rVert \\left\\lVert \\begin{pmatrix} 0 \\\\ 3 \\end{pmatrix} \\right\\rVert}\n\\right)\n= \\mbox{acos}\n\\left(\\frac{3\\cdot 0 + 0 \\cdot 3}\n           {\\sqrt{3^2 + 0^2} \\cdot \\sqrt{0^2 + 3^2}}\n\\right)\n= \\mbox{acos}\n\\left(\\frac{0}\n           {3 \\cdot 3}\n\\right)\n= \\frac{\\pi}{2}\n\\approx 1.57\n\\end{equation}\\] Die Umrechnung in Grad ergibt dann \\[\\begin{equation}\n\\frac{\\pi}{2} \\cdot \\frac{180°}{\\pi} = 90°\n\\end{equation}\\] Die entsprechende R Implementation lautet wie folgt.\n\nx = matrix(c(3,0), nrow = 2)                                    # Vektor 1\ny = matrix(c(0,3), nrow = 2)                                    # Vektor 2\nw = acos(sum(x*y)/(sqrt(sum(x*x))*sqrt(sum(y*y)))) * 180/pi     # Winkel in Grad\nprint(w)\n\n[1] 90\n\n\n\n\n\n\n\n\nAbbildung 8.7: Winkel in \\(\\mathbb{R}^2\\)\n\n\n\nDie Tatsache, dass zwei Vektoren einen rechten Winkel bilden können, also gewissermaßen maximal nicht-parallel sein können, ist ein wichtiges geometrisches Prinzip und wird deshalb mit folgender Definition speziell ausgezeichnet.\n\nDefinition 8.7 (Orthogonalität und Orthonormalität von Vektoren) \\(\\left((\\mathbb{R}^m, +, \\cdot), \\langle \\rangle \\right)\\) sei der Euklidische Vektorraum.\n(1) Zwei Vektoren \\(x,y \\in \\mathbb{R}^m\\) heißen orthogonal, wenn gilt, dass \\[\\begin{equation}\n\\langle x, y \\rangle = 0\n\\end{equation}\\] (2) Zwei Vektoren \\(x,y \\in \\mathbb{R}^m\\) heißen orthonormal, wenn gilt, dass \\[\\begin{equation}\n\\langle x, y \\rangle = 0 \\mbox{ und } \\Vert x \\Vert = \\Vert y \\Vert = 1.\n\\end{equation}\\]\n\nFür orthogonale und orthonormale Vektoren gilt also insbesondere auch \\[\\begin{equation}\n\\cos \\alpha\n= \\frac{\\langle x, y \\rangle}{\\Vert x \\Vert \\Vert y \\Vert}\n= \\frac{0}{\\Vert x \\Vert \\Vert y \\Vert}\n= 0,\n\\end{equation}\\] also \\[\\begin{equation}\n\\alpha = \\frac{\\pi}{2} = 90°.\n\\end{equation}\\]",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Vektoren</span>"
    ]
  },
  {
    "objectID": "108-Vektoren.html#sec-lineare-unabhaengigkeit",
    "href": "108-Vektoren.html#sec-lineare-unabhaengigkeit",
    "title": "8  Vektoren",
    "section": "8.3 Lineare Unabhängigkeit",
    "text": "8.3 Lineare Unabhängigkeit\nIn diesem Abschnitt führen wir den Begriff der linearen Unabhängigkeit von Vektoren ein. Wir definieren dazu zunächst den Begriff der Linearkombination von Vektoren.\n\nDefinition 8.8 (Linearkombination) \\(\\{v_1, v_2, ..., v_k\\}\\) sei eine Menge von \\(k\\) Vektoren eines Vektorraums \\(V\\) und \\(a_1, a_2,...,a_k\\) seien Skalare. Dann ist die Linearkombination der Vektoren in \\(\\{v_1, v_2, ..., v_k\\}\\) mit den Koeffizienten \\(a_1, a_2,...,a_k\\) definiert als der Vektor \\[\\begin{equation}\nw := \\sum_{i=1}^k a_i v_i \\in V.\n\\end{equation}\\]\n\nBeispiel\nEs seien \\[\\begin{equation}\nv_1 := \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix},\nv_2 := \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix},\nv_3 := \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n\\mbox{ und }\na_1 := 2, a_2 := 3, a_3 := 0.\n\\end{equation}\\] Dann ergibt sich die Linearkombination von \\(v_1,v_2,v_3\\) mit den Koeffizienten \\(a_1,a_2,a_3\\) zu \\[\\begin{align}\n\\begin{split}\nw\n& = a_1v_1 + a_2v_2 + a_3v_3                        \\\\\n& =  2 \\cdot \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}\n     + 3 \\cdot \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n     + 0 \\cdot \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}   \\\\\n& =   \\begin{pmatrix} 4 \\\\ 2 \\end{pmatrix}\n     + \\begin{pmatrix} 3 \\\\ 3 \\end{pmatrix}\n     + \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}          \\\\\n& =   \\begin{pmatrix} 7 \\\\ 5 \\end{pmatrix}.\n\\end{split}\n\\end{align}\\]\nBasierend auf dem Begriff der Linearkombination kann man nun den Begriff der Linearen Unabhängigkeit von Vektoren definieren.\n\nDefinition 8.9 (Lineare Unabhängigkeit) \\(V\\) sei ein Vektorraum. Eine Menge \\(W := \\{w_1, w_2, ...,w_k\\}\\) von Vektoren in \\(V\\) heißt linear unabhängig, wenn die einzige Repräsentation des Nullelements \\(0 \\in V\\) durch eine Linearkombination der \\(w \\in W\\) die sogenannte triviale Repräsentation \\[\\begin{equation}\n0 = a_1 w_1 + a_2 w_2 + \\cdots + a_k w_k \\mbox{ mit } a_1 = a_2 =  \\cdots = a_k = 0\n\\end{equation}\\] ist. Wenn die Menge \\(W\\) nicht linear unabhängig ist, dann heißt sie linear abhängig.\n\nUm zu prüfen, ob eine gegeben Menge von Vektoren linear abhängig oder unabhängig ist muss man prinzipiell für jede mögliche Linearkombination der gegebenen Vektoren, ob sie Null ist. Theorem 8.2 und Theorem 8.3 zeigen, wie dies für zwei bzw. endliche viele Vektoren auch mit weniger Aufwand gelingen kann.\n\nTheorem 8.2 (Lineare Abhängigkeit von zwei Vektoren) \\(V\\) sei ein Vektorraum. Zwei Vektoren \\(v_1, v_2 \\in V\\) sind linear abhängig, wenn einer der Vektoren ein skalares Vielfaches des anderen Vektors ist.\n\n\nBeweis. \\(v_1\\) sei ein skalares Vielfaches von \\(v_2\\), also \\[\\begin{equation}\nv_1 = \\lambda v_2 \\mbox{ mit } \\lambda \\neq 0.\n\\end{equation}\\] Dann gilt \\[\\begin{equation}\nv_1 - \\lambda v_2 = 0.\n\\end{equation}\\] Dies aber entspricht der Linearkombination \\[\\begin{equation}\na_1v_1 + a_2v_2 = 0\n\\end{equation}\\] mit \\(a_1 = 1 \\neq 0\\) und \\(a_2 = -\\lambda \\neq 0\\). Es gibt also eine Linearkombination des Nullelementes, die nicht die triviale Repräsentation ist, und damit sind \\(v_1\\) und \\(v_2\\) nicht linear unabhängig.\n\n\nTheorem 8.3 (Lineare Abhängigkeit einer Menge von Vektoren) \\(V\\) sei ein Vektorraum und \\(w_1,...,w_k \\in V\\) sei eine Menge von Vektoren in \\(V\\). Wenn einer der Vektoren \\(w_i\\) mit \\(i = 1,...,k\\) eine Linearkombination der anderen Vektoren ist, dann ist die Menge der Vektoren linear abhängig.\n\n\nBeweis. Die Vektoren \\(w_1,...,w_k\\) sind genau dann linear abhängig, wenn gilt, dass \\(\\sum_{i=1}^k a_i w_i = 0\\) mit mindestens einem \\(a_i \\neq 0\\) . Es sei also zum Beispiel \\(a_j \\neq 0\\). Dann gilt \\[\\begin{equation}\n0 = \\sum_{i=1}^k a_i w_i = \\sum_{i=1, i \\neq j}^k a_i w_i + a_jw_j\n\\end{equation}\\] Also folgt \\[\\begin{equation}\na_jw_j  = - \\sum_{i=1, i \\neq j}^k a_i w_i\n\\end{equation}\\] und damit \\[\\begin{equation}\nw_j  = - a_j^{-1}\\sum_{i=1, i \\neq j}^k a_i w_i = - \\sum_{i=1, i \\neq j}^k (a_j^{-1}a_i) w_i\n\\end{equation}\\] Also ist \\(w_j\\) eine Linearkombination der \\(w_i\\) für \\(i = 1,...,k\\) mit \\(i \\neq j\\).",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Vektoren</span>"
    ]
  },
  {
    "objectID": "108-Vektoren.html#sec-vektorraumbasen",
    "href": "108-Vektoren.html#sec-vektorraumbasen",
    "title": "8  Vektoren",
    "section": "8.4 Vektorraumbasen",
    "text": "8.4 Vektorraumbasen\nIn diesem Abschnitt wollen wir den Begriff der Vektorraumbasis einführen. Eine Basis eines Vektorraums ist eine Untermenge von Vektoren des Vektorraums, die zur Darstellung aller Vektoren des Vektorraums genutzt werden kann. Im Sinne der linearen Kombination von Vektoren enthält also eine Vektorraumbasis alle nötige Information zur Konstruktion des entsprechenden Vektorraums. Allerdings ist eine Vektorraumbasis in der Regel nicht eindeutig und die viele Vektorräume haben in der Tat unendlich viele Basen. Die folgenden Definition sagt zunächst aus, wie aus einer beschränkten Anzahl von Vektoren mithilfe von Linearkombinationen unendlich viele Vektoren gebildet werden können.\n\nDefinition 8.10 (Lineare Hülle und Aufspannen) \\(V\\) sei ein Vektorraum und es sei \\(W := \\{w_1,...,w_k\\} \\subset V\\). Dann ist die lineare Hülle von \\(W\\) definiert als die Menge aller Linearkombinationen der Elemente von \\(W\\), \\[\\begin{equation}\n\\mbox{Span}(W) := \\left\\lbrace \\sum_{i=1}^k a_iw_i \\vert a_1,...,a_k \\mbox{ sind skalare Koeffizienten } \\right\\rbrace\n\\end{equation}\\] Man sagt, dass eine Menge von Vektoren \\(W \\subseteq V\\) einen Vektorraum \\(V\\) aufspannt, wenn jedes \\(v \\in V\\) als eine Linearkombination von Vektoren in \\(W\\) geschrieben werden kann.\n\nWir definieren nun den Begriff der Basis eines Vektorraums.\n\nDefinition 8.11 (Basis) \\(V\\) sei ein Vektorraum und es sei \\(B \\subseteq V\\). \\(B\\) heißt eine Basis von \\(V\\), wenn\n\ndie Vektoren in \\(B\\) linear unabhängig sind und\ndie Vektoren in \\(B\\) den Vektorraum \\(V\\) aufspannen.\n\n\nBasen von Vektorräumen haben folgende wichtige Eigenschaften.\n\nTheorem 8.4 (Eigenschaften von Basen)  \n\nAlle Basen eines Vektorraums beinhalten die gleiche Anzahl von Vektoren.\nJede Menge von \\(m\\) linear unabhängigen Vektoren ist Basis eines \\(m\\)-dimensionalen Vektorraums.\n\n\nFür einen Beweis dieses sehr tiefen Theorems verweisen wir auf die weiterführende Literatur. Die mit obigem Theorem benannte eindeutige Anzahl der Vektoren einer Basis eines Vektorraums heißt die Dimension des Vektorraums. Da es in der Regel unendliche viele Mengen von m linear unabhängigen Vektoren in einem Vektorraum gibt haben Vektorräume in der Regel unendlich viele Basen.\nBetrachtet man nun einen einzelnen Vektor in einem Vektorraum, so kann man sich fragen, wie man diesen mithilfe einer Vektorraumbasis darstellen kann. Dies führt auf folgende Begriffsbildungen.\n\nDefinition 8.12 (Basisdarstellung und Koordinaten) \\(B := \\{b_1,...,b_m\\}\\) sei eine Basis eines \\(m\\)-dimensionalen Vektorraumes \\(V\\) und es sei \\(v \\in V\\). Dann heißt die Linearkombination \\[\\begin{equation}\nv = \\sum_{i = 1}^m c_i b_i\n\\end{equation}\\] die Darstellung von \\(v\\) bezüglich der Basis \\(B\\) und die Koeffizienten \\(c_1,...,c_m\\) heißen die Koordinaten von \\(v\\) bezüglich der Basis \\(B\\).\n\nBei fester Basis sind auch die Koordinaten eines Vektors bezüglich dieser Basis fest und eindeutig. Dies ist die Aussage folgenden Theorems.\n\nTheorem 8.5 (Eindeutigkeit der Basisdarstellung) Die Basisdarstellung eines \\(v \\in V\\) bezüglich einer Basis \\(B\\) ist eindeutig.\n\n\nBeweis. Ohne Beschränkung der Allgemeinheit nehmen wir an, dass der Vektorraum von Dimension \\(m\\) ist. Nehmen wir an, dass zwei Darstellungen von \\(v\\) bezüglich der Basis \\(B\\) existieren, also dass \\[\\begin{align}\n\\begin{split}\nv & = a_1 b_1 + \\cdots + a_m b_m \\\\\nv & = c_1 b_1 + \\cdots + c_m b_m\n\\end{split}\n\\end{align}\\] Subtraktion der unteren von dern oberen Gleichung ergibt \\[\\begin{equation}\n0 = (a_1 - c_1) b_1 + \\cdots + (a_m - c_m) b_m\n\\end{equation}\\] Weil die \\(b_1,...,b_m\\) linear unabhängig sind, gilt aber, dass \\((a_i - c_i) = 0\\) für alle \\(i = 1,...,m\\) und somit sind die beiden Darstellungen von \\(v\\) bezüglich der Basis \\(B\\) identisch.\n\nZum Abschluss dieses Abschnitts wollen wir eine spezielle Basis des reellen Vektorraums betrachten.\n\nDefinition 8.13 (Orthonormalbasis von \\(\\mathbb{R}^m\\)) Eine Menge von \\(m\\) Vektoren \\(v_1,...,v_m \\in \\mathbb{R}^m\\) heißt Orthonormalbasis von \\(\\mathbb{R}^m\\), wenn \\(v_1,...,v_m\\) jeweils die Länge 1 haben und wechselseitig orthogonal sind, also wenn \\[\\begin{equation}\n\\langle v_i, v_j \\rangle =\n\\begin{cases}\n1   & \\mbox{ für } i = j    \\\\\n0   & \\mbox{ für } i \\neq j\n\\end{cases}.\n\\end{equation}\\]\n\nWir wollen zunächst ein Beispiel für eine Orthonormalbasis betrachten.\nBeispiel (1)\nEs ist \\[\\begin{equation}\nB_1 :=\n\\left\\lbrace\n\\begin{pmatrix}\n1 \\\\ 0\n\\end{pmatrix},\n\\begin{pmatrix}\n0 \\\\ 1\n\\end{pmatrix}\n\\right\\rbrace\n\\end{equation}\\] eine Orthonormalbasis von \\(\\mathbb{R}^2\\), denn \\(B_1\\) besteht aus zwei Vektoren und es gelten \\[\\begin{equation}\n\\left\\langle\n\\begin{pmatrix}\n1 \\\\ 0\n\\end{pmatrix},\n\\begin{pmatrix}\n1 \\\\ 0\n\\end{pmatrix}\n\\right\\rangle\n= 1 \\cdot 1 + 0 \\cdot 0\n= 1 + 0\n= 1\n\\end{equation}\\] sowie \\[\\begin{equation}\n\\left \\langle\n\\begin{pmatrix}\n0 \\\\ 1\n\\end{pmatrix},\n\\begin{pmatrix}\n0 \\\\1\n\\end{pmatrix}\n\\right \\rangle\n= 0 \\cdot 0 + 1 \\cdot 1\n= 0 + 1\n= 1\n\\end{equation}\\] und \\[\\begin{equation}\n\\left \\langle\n\\begin{pmatrix}\n1 \\\\ 0\n\\end{pmatrix},\n\\begin{pmatrix}\n0 \\\\ 1\n\\end{pmatrix}\n\\right \\rangle\n=    1 \\cdot 0 +  0  \\cdot 1\n= 0 + 0\n= 0\n\\end{equation}\\]\nFür allgemeine reelle Vektorräume werden Basen der Form von \\(B_1\\) mit dem Begriff der kanonischen Basis speziell ausgezeichnet.\n\nDefinition 8.14 (Kanonische Basis und kanonische Einheitsvektoren) Die Orthonormalbasis \\[\\begin{equation}\nB :=\n\\left\\lbrace\ne_1,...,e_m\n\\vert\ne_{i_j} = 1 \\mbox{ für } i =  j \\mbox{ und } e_{i_j} =  0 \\mbox{ für } i \\neq j\n\\right\\rbrace\n\\subset \\mathbb{R}^m\n\\end{equation}\\] heißt die kanonische Basis von \\(\\mathbb{R}^m\\) und die \\(e_{i_j}\\) heißen kanonische Einheitsvektoren.\n\n\\(B_1\\) aus Beispiel (1) ist also die kanonische Basis von \\(\\mathbb{R}^2\\).\nDie kanonische Basis von \\(\\mathbb{R}^3\\) ist \\[\\begin{equation}\nB :=\n\\left\\lbrace\n\\begin{pmatrix}\n1 \\\\ 0 \\\\ 0\n\\end{pmatrix},\n\\begin{pmatrix}\n0 \\\\ 1 \\\\ 0\n\\end{pmatrix},\n\\begin{pmatrix}\n0 \\\\ 0 \\\\ 1\n\\end{pmatrix}\n\\right\\rbrace.\n\\end{equation}\\]\nAllerdings gibt es auch nicht kanonische Orthonormalbasen. Dazu betrachten wir ein weiteres Beispiel\nBeispiel (2)\nEs ist auch \\[\\begin{equation}\nB_2 :=\n\\left\\lbrace\n\\begin{pmatrix}\n\\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}}\n\\end{pmatrix},\n\\begin{pmatrix} - \\frac{1}{\\sqrt{2}} \\\\ \\quad \\frac{1}{\\sqrt{2}}\n\\end{pmatrix}\n\\right\\rbrace\n\\end{equation}\\] eine Orthonormalbasis von \\(\\mathbb{R}^2\\), denn \\(B_2\\) besteht aus zwei Vektoren und es gelten \\[\\begin{equation}\n\\left \\langle\n\\begin{pmatrix}\n\\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}}\n\\end{pmatrix},\n\\begin{pmatrix}\n\\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}}\n\\end{pmatrix}\n\\right \\rangle\n= \\frac{1}{\\sqrt{2}} \\cdot \\frac{1}{\\sqrt{2}} + \\frac{1}{\\sqrt{2}} \\cdot \\frac{1}{\\sqrt{2}}\n= \\frac{1}{2} + \\frac{1}{2}\n= 1,\n\\end{equation}\\] sowie \\[\\begin{equation}\n\\left \\langle\n\\begin{pmatrix} - \\frac{1}{\\sqrt{2}} \\\\ \\quad \\frac{1}{\\sqrt{2}}\n\\end{pmatrix},\n\\begin{pmatrix} - \\frac{1}{\\sqrt{2}} \\\\ \\quad \\frac{1}{\\sqrt{2}}\n\\end{pmatrix}\n\\right \\rangle\n= \\left(- \\frac{1}{\\sqrt{2}} \\right)\\cdot \\left(- \\frac{1}{\\sqrt{2}} \\right) + \\frac{1}{\\sqrt{2}} \\cdot \\frac{1}{\\sqrt{2}}\n= \\frac{1}{2} + \\frac{1}{2}\n= 1\n\\end{equation}\\] und \\[\\begin{equation}\n\\left \\langle\n\\begin{pmatrix} - \\frac{1}{\\sqrt{2}} \\\\ \\quad \\frac{1}{\\sqrt{2}}\n\\end{pmatrix},\n\\begin{pmatrix}\n\\frac{1}{\\sqrt{2}} \\\\  \\frac{1}{\\sqrt{2}}\n\\end{pmatrix}\n\\right \\rangle\n= - \\frac{1}{\\sqrt{2}} \\cdot \\frac{1}{\\sqrt{2}} + \\frac{1}{\\sqrt{2}}   \\cdot \\frac{1}{\\sqrt{2}}\n= - \\frac{1}{2} + \\frac{1}{2}\n= 0\n\\end{equation}\\]\nWir visualisieren die beiden Orthonormalbasen \\(B_1\\) und \\(B_2\\) von \\(\\mathbb{R}^2\\) in Abbildung 8.8.\n\n\n\n\n\n\nAbbildung 8.8: Zwei Basen von \\(\\mathbb{R}^2\\)",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Vektoren</span>"
    ]
  },
  {
    "objectID": "108-Vektoren.html#sec-selbstkontrollfragen-vektoren",
    "href": "108-Vektoren.html#sec-selbstkontrollfragen-vektoren",
    "title": "8  Vektoren",
    "section": "8.5 Selbstkontrollfragen",
    "text": "8.5 Selbstkontrollfragen\n\nGeben Sie die Definition eines Vektorraums wieder.\nGeben Sie die Definition des reellen Vektorraums wieder.\nEs seien \\[\\begin{equation}\nx := \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix},\ny := \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n\\mbox{ und }\na := 2.\n\\end{equation}\\] Berechnen Sie \\[\\begin{equation}\nv = a(x+y) \\mbox{ und } w = \\frac{1}{a}(y-x)\n\\end{equation}\\]\nGeben Sie die Definition des Skalarproduktes auf \\(\\mathbb{R}^m\\) wieder.\nFür \\[\nx := \\begin{pmatrix} 2 \\\\ 1 \\\\ 3 \\end{pmatrix},\ny := \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix},\nz := \\begin{pmatrix} 3 \\\\ 1 \\\\ 0 \\end{pmatrix}\n\\tag{8.1}\\] berechnen Sie \\[\\begin{equation}\n\\langle x,y \\rangle, \\langle x, z \\rangle, \\langle y,z \\rangle\n\\end{equation}\\]\nGeben Sie die Definition des Euklidischen Vektorraums wieder.\nGeben Sie die Definition der Länge eines Vektors im Euklidischen Vektorraum wieder,\nBerechnen Sie die Längen der Vektoren \\(x,y,z\\) aus Gleichung 8.1.\nGeben Sie Definition des Abstands zweier Vektoren im Euklidischen Vektorraum wieder.\nBerechnen Sie \\(d(x,y), d(x,z)\\) und \\(d(y,z)\\) für \\(x,y,z\\) aus Gleichung 8.1.\nGeben Sie die Definition des Winkels zwischen zwei Vektoren im Euklidischen Vektorraum wieder.\nBerechnen Sie die Winkel zwischen den Vektoren \\(x\\) und \\(y\\), \\(x\\) und \\(z\\), sowie \\(y\\) und \\(z\\) aus Gleichung 8.1.\nGeben Sie die Definitionen der Orthogonalität und Orthonormalität von Vektoren wieder.\nGeben Sie die Definition der Linearkombination von Vektoren wieder.\nGeben Sie die Definition der linearen Unabhängigkeit von Vektoren wieder.\nWoran kann man erkennen, ob zwei reelle Vektoren linear abhängig sind oder nicht?\nGeben Sie die Definition der linearen Hülle einer Menge von Vektoren wieder.\nGeben Sie die Definition der Basis eines Vektorraums wieder.\nGeben Sie das Theorem zu den Eigenschaften von Vektorraumbasen wieder.\nGeben Sie die Definition der Basisdarstellung eines Vektors wieder.\nGeben Sie die Definition eienr Orthonormalbasis von \\(\\mathbb{R}^m\\) wieder.\nGeben Sie die Definition der kanonischen Basis von \\(\\mathbb{R}^m\\) wieder.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Vektoren</span>"
    ]
  },
  {
    "objectID": "109-Matrizen.html",
    "href": "109-Matrizen.html",
    "title": "9  Matrizen",
    "section": "",
    "text": "9.1 Definition\nWir beginnen mit der Definition einer Matrix.\nMatrizen bestehen aus Zeilen (rows) und Spalten (columns). Die Matrixeinträge \\(a_{ij}\\) werden mit einem Zeilenindex \\(i\\) und einem Spaltenindex \\(j\\) indiziert. Zum Beispiel gilt für \\[\\begin{equation}\nA:=\\begin{pmatrix}\n2 & 7 & 5 & 2 \\\\\n8 & 2 & 5 & 6 \\\\\n6 & 4 & 0 & 9 \\\\\n9 & 2 & 1 & 2\n\\end{pmatrix},\n\\end{equation}\\] dass \\(a_{32} = 4\\). Die Größe oder Dimension einer Matrix ergibt sich aus der Anzahl ihrer Zeilen \\(n \\in \\mathbb{N}\\) und Spalten \\(m \\in \\mathbb{N}\\). Matrizen mit \\(n = m\\) heißen quadratische Matrizen.\nIn der Folge benötigen wir nur Matrizen mit reellen Einträgen, also \\(a_{ij} \\in \\mathbb{R}\\) für alle \\(i = 1,...,n\\) und \\(j = 1,...,m\\). Wir nennen die Matrizen mit reellen Einträge reelle Matrizen und bezeichnen die Menge der reellen Matrizen mit \\(n\\) Zeilen und \\(m\\) Spalten mit \\(\\mathbb{R}^{n \\times m}\\). An dem Ausdruck \\[\\begin{equation}\nA \\in \\mathbb{R}^{n\\times m}\n\\end{equation}\\] können wir also ablesen, dass \\(A\\) eine reelle Matrix mit \\(n\\) Zeilen und \\(m\\) Spalten ist. Wir identifizieren dabei die Menge \\(\\mathbb{R}^{1 \\times 1}\\) mit der Menge \\(\\mathbb{R}\\), die Menge \\(\\mathbb{R}^{n \\times 1}\\) mit der Menge \\(\\mathbb{R}^n\\). Reelle Matrizen mit einer Spalte und \\(n\\) Zeilen entsprechen also \\(n\\)-dimensionalen reellen Vektoren und reelle Matrizen mit einer Spalte und einer Zeile entsprechen reellen Zahlen.\nDefinition von Matrizen in R\nIn R werden Matrizen definiert, indem R Vektoren mithilfe der matrix() Funktion in die Repräsentation einer mathematischen Matrix transformiert werden. Die Einträge eines R Vektors werden dabei anhand der spezifizierten Zeilenanzahl nrow anhand ihrer Gesamtanzahl auf die Matrix verteilt. Wollen wir beispielsweise die Matrix \\[\\begin{equation}\nA :=\n\\begin{pmatrix}\n2 & 3 & 0 \\\\\n1 & 6 & 5\n\\end{pmatrix}\n\\end{equation}\\] in R definieren, so ergibt sich\n# Spaltenweise Definition von A (R default)\nA = matrix(c(2,1,3,6,0,5), nrow = 2)\nprint(A)\n\n     [,1] [,2] [,3]\n[1,]    2    3    0\n[2,]    1    6    5\nR folgt hier per default einer sogenannten column-major-order, das heißt, die Elemente des R Vektors c(2,1,3,6,0,5) werden der Reihe nach von oben nach unten in die Spalten der Matrix von links nach rechts überführt. Einen etwas klareren Zusammenhang zwischen dem visuellen Layout des R Codes und der resultierenden Matrix erhält man, indem man den R Vektor mithilfe von Zeilenumbrüchen anhand des intendierten Matrixlayouts formatiert und dann die column-major-order mithilfe des Arguments byrow = TRUE zu einer row-major-order umstellt. Es wird dann zunächst die erste Zeile der Matrix von links nach rechts mit den Elementen des R Vektors gefüllt wird und dann die zweite Zeile usw. bis alle Elemente des Vektors auf die Matrix verteilt sind.\n# Reihenweise Definition von A (R default)\nA = matrix(c(2,3,0,\n             1,6,5),\n             nrow = 2,\n             byrow = TRUE)\nprint(A)\n\n     [,1] [,2] [,3]\n[1,]    2    3    0\n[2,]    1    6    5\n# Zeilenweise Definition von B\nB = matrix(c(4,1,0,\n            -4,2,0), \n            nrow = 2, \n            byrow = TRUE)\nprint(B)\n\n     [,1] [,2] [,3]\n[1,]    4    1    0\n[2,]   -4    2    0",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Matrizen</span>"
    ]
  },
  {
    "objectID": "109-Matrizen.html#sec-matrix-definition",
    "href": "109-Matrizen.html#sec-matrix-definition",
    "title": "9  Matrizen",
    "section": "",
    "text": "Definition 9.1 Eine Matrix ist eine rechteckige Anordnung von Zahlen, die wie folgt bezeichnet wird \\[\\begin{equation}\nA := \\begin{pmatrix}\na_{11} & a_{12} & \\cdots & a_{1m} \\\\\na_{21} & a_{22} & \\cdots & a_{2m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & \\cdots & a_{nm}\n\\end{pmatrix}\n:= {(a_{ij})}_{1\\le i\\le n,\\, 1\\le j\\le m}.\n\\end{equation}\\]",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Matrizen</span>"
    ]
  },
  {
    "objectID": "109-Matrizen.html#sec-grundlegende-matrixoperationen",
    "href": "109-Matrizen.html#sec-grundlegende-matrixoperationen",
    "title": "9  Matrizen",
    "section": "9.2 Grundlegende Matrixoperationen",
    "text": "9.2 Grundlegende Matrixoperationen\nMan kann mit Matrizen rechnen. Dabei sind folgende Matrixoperationen grundlegend:\n\nDie Addition von Matrizen gleicher Größe, genannt Matrixaddition,\ndie Subtraktion von Matrizen gleicher Größe, genannt Matrixsubtraktion,\ndie Multiplikation einer Matrix mit einem Skalar, genannt Skalarmultiplikation,\ndas Vertauschen der Zeilen- und Spalten einer Matrix, genannt Matrixtransposition.\n\nWir führen diese Operationen in der Folge in Operatorform, also als Funktionen ein. Dies dient insbesondere dazu, bei jeder Operation mit Hilfe ihrer Definitionsmenge zu betonen, von welcher Art die Objekte der jeweiligen Operation sind und mithilfe ihrer Bildmenge zu betonen, von welcher Art das Resultat der jeweiligen Operation ist.\n\n9.2.1 Matrixaddition\n\nDefinition 9.2 Es seien \\(A,B\\in \\mathbb{R}^{n\\times m}\\). Dann ist die Addition von \\(A\\) und \\(B\\) definiert als die Abbildung \\[\\begin{equation}\n+ : \\mathbb{R}^{n\\times m} \\times \\mathbb{R}^{n\\times m} \\to \\mathbb{R}^{n \\times m}, \\,\n(A,B) \\mapsto +(A,B) := A + B\n\\end{equation}\\] mit \\[\\begin{align}\n\\begin{split}\nA + B\n& =\n\\begin{pmatrix}\na_{11} & a_{12} & \\cdots & a_{1m} \\\\\na_{21} & a_{22} & \\cdots & a_{2m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & \\cdots & a_{nm}\n\\end{pmatrix}\n+\n\\begin{pmatrix}\nb_{11} & b_{12} & \\cdots & b_{1m} \\\\\nb_{21} & b_{22} & \\cdots & b_{2m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nb_{n1} & b_{n2} & \\cdots & b_{nm}\n\\end{pmatrix}\n\\\\\n&\n:=\n\\begin{pmatrix}\na_{11} + b_{11} & a_{12} + b_{12} & \\cdots & a_{1m} + b_{1m} \\\\\na_{21} + b_{21} & a_{22} + b_{22} & \\cdots & a_{2m} + b_{2m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} + b_{n1} & a_{n2} + b_{n2} & \\cdots & a_{nm} + b_{nm}\n\\end{pmatrix}.\n\\end{split}\n\\end{align}\\]\n\nDie Definition der Matrixaddition legt insbesondere fest, dass nur Matrizen gleicher Größe addiert werden können und dass die Operation der Matrixaddition elementweise definiert ist.\nBeispiel\nEs seien \\(A,B\\in \\mathbb{R}^{2\\times 3}\\) definiert als \\[\\begin{equation}\nA:=\\begin{pmatrix}\n2 & -3 & 0\\\\\n1 &  6 & 5\\\\\n\\end{pmatrix}\n\\mbox{ und }\nB := \\begin{pmatrix}\n4 & 1 & 0\\\\\n-4 & 2 & 0\\\\\n\\end{pmatrix}.\n\\end{equation}\\] Da \\(A\\) und \\(B\\) gleich groß sind, können wir sie addieren \\[\\begin{align}\n\\begin{split}\nC\n= A+B\n& =\n\\begin{pmatrix}\n2 & -3 & 0\\\\\n1 &  6 & 5\\\\\n\\end{pmatrix}\n+\n\\begin{pmatrix}\n4 & 1 & 0\\\\\n-4 & 2 & 0\\\\\n\\end{pmatrix}\\\\\n& =\n\\begin{pmatrix}\n2 + 4 & -3 + 1 & 0 + 0\\\\\n1 - 4 &  6 + 2 & 5 + 0\\\\\n\\end{pmatrix}\\\\\n& =\n\\begin{pmatrix}\n6 & -2 & 0\\\\\n-3 &  8 & 5 \\\\\n\\end{pmatrix}.\n\\end{split}\n\\end{align}\\]\nIn R führt man obige Rechnung wie folgt aus.\n\n# Definition\nA = matrix(c(2, -3, 0,\n             1,  6, 5),\n             nrow  = 2,\n             byrow = TRUE)\nB = matrix(c( 4, 1, 0,\n             -4, 2, 0),\n              nrow = 2,\n             byrow = TRUE)\n\n# Addition\nC = A + B\nprint(C)\n\n     [,1] [,2] [,3]\n[1,]    6   -2    0\n[2,]   -3    8    5\n\n\n\n\n9.2.2 Matrixsubtraktion\nDie Subtraktion von Matrizen gleicher Größe ist analog zur Addition definiert.\n\nDefinition 9.3 (Matrixsubtraktion) Es seien \\(A,B\\in \\mathbb{R}^{n\\times m}\\). Dann ist die Subtraktion von \\(A\\) und \\(B\\) definiert als die Abbildung \\[\\begin{equation}\n- : \\mathbb{R}^{n\\times m} \\times \\mathbb{R}^{n\\times m} \\to \\mathbb{R}^{n\\times m}, \\,\n(A,B) \\mapsto -(A,B) := A - B\n\\end{equation}\\] mit \\[\\begin{align}\n\\begin{split}\nA - B\n& =\n\\begin{pmatrix}\na_{11} & a_{12} & \\cdots & a_{1m} \\\\\na_{21} & a_{22} & \\cdots & a_{2m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & \\cdots & a_{nm}\n\\end{pmatrix}\n-\n\\begin{pmatrix}\nb_{11} & b_{12} & \\cdots & b_{1m} \\\\\nb_{21} & b_{22} & \\cdots & b_{2m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nb_{n1} & b_{n2} & \\cdots & b_{nm}\n\\end{pmatrix}\n\\\\\n&\n:=\n\\begin{pmatrix}\na_{11} - b_{11} & a_{12} - b_{12} & \\cdots & a_{1m} - b_{1m} \\\\\na_{21} - b_{21} & a_{22} - b_{22} & \\cdots & a_{2m} - b_{2m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} - b_{n1} & a_{n2} - b_{n2} & \\cdots & a_{nm} - b_{nm}\n\\end{pmatrix}.\n\\end{split}\n\\end{align}\\]\n\nWie bei der Matrixaddition legt die Definition der Matrixsubtraktion fest, dass nur Matrizen gleicher Größe voneinander subtrahiert werden können und dass die Subktration zweier gleich großer Matrizen elementweise definiert ist.\nBeispiel\nWir können die im Beispiel zur Matrixaddition definierten Matrizen \\(A\\) und \\(B\\) auch voneinander subtrahieren, \\[\\begin{align}\n\\begin{split}\nD\n= A-B\n& =\n\\begin{pmatrix}\n2 & -3 & 0\\\\\n1 &  6 & 5\\\\\n\\end{pmatrix}\n-\n\\begin{pmatrix}\n4 & 1 & 0\\\\\n-4 & 2 & 0\\\\\n\\end{pmatrix}\\\\\n& =\n\\begin{pmatrix}\n2 - 4 & -3 - 1 & 0 - 0\\\\\n1 + 4 &  6 - 2 & 5 - 0\\\\\n\\end{pmatrix}\\\\\n& =\n\\begin{pmatrix}\n-2 & -4 & 0\\\\\n5 &  4 & 5 \\\\\n\\end{pmatrix}.\n\\end{split}\n\\end{align}\\]\nIn R führt man diese Rechnung wie folgt aus.\n\n# Subtraktion\nD = A - B\nprint(D)\n\n     [,1] [,2] [,3]\n[1,]   -2   -4    0\n[2,]    5    4    5\n\n\n\n\n9.2.3 Skalarmultiplikation\nDie Skalarmultiplikation einer Matrix bezeichnet die Multiplikation eines Skalars mit einer Matrix.\n\nDefinition 9.4 (Skalarmultiplikation) Es sei \\(c \\in \\mathbb{R}\\) ein Skalar und \\(A \\in \\mathbb{R}^{n\\times m}\\). Dann ist die Skalarmultiplikation von \\(c\\) und \\(A\\) definiert als die Abbildung \\[\\begin{equation}\n\\cdot : \\mathbb{R} \\times \\mathbb{R}^{n\\times m} \\to \\mathbb{R}^{n\\times m}, \\,\n(c,A) \\mapsto \\cdot (c,A) := cA\n\\end{equation}\\] mit \\[\\begin{align}\n\\begin{split}\ncA\n=\nc\n\\begin{pmatrix}\na_{11} & a_{12} & \\cdots & a_{1m} \\\\\na_{21} & a_{22} & \\cdots & a_{2m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & \\cdots & a_{nm}\n\\end{pmatrix}\n:=\n\\begin{pmatrix}\nca_{11} & ca_{12} & \\cdots & ca_{1m}  \\\\\nca_{21} & ca_{22} & \\cdots & ca_{2m}  \\\\\n\\vdots  & \\vdots  & \\ddots & \\vdots    \\\\\nca_{n1} & ca_{n2} & \\cdots & ca_{nm}\n\\end{pmatrix}.\n\\end{split}\n\\end{align}\\]\n\nDie Skalarmultiplikation ist mit dieser Definition also elementweise definiert.\nBeispiel\nEs seien \\(c:=-3\\) und \\(A\\in \\mathbb{R}^{4\\times 3}\\) definiert als\n\\[\\begin{equation}\nA := \\begin{pmatrix}\n3 & 1 & 1\\\\\n5 & 2 & 5\\\\\n2 & 7 & 1\\\\\n3 & 4 & 2\n\\end{pmatrix}.\n\\end{equation}\\] Dann ergibt sich \\[\\begin{align}\n\\begin{split}\nB :=\ncA\n= -3\\begin{pmatrix}\n3 & 1 & 1\\\\\n5 & 2 & 5\\\\\n2 & 7 & 1\\\\\n3 & 4 & 2\n\\end{pmatrix}\n= \\begin{pmatrix}\n-3\\cdot3 & -3\\cdot1 & -3\\cdot1\\\\\n-3\\cdot5 & -3\\cdot2 & -3\\cdot5\\\\\n-3\\cdot2 & -3\\cdot7 & -3\\cdot1\\\\\n-3\\cdot3 & -3\\cdot4 & -3\\cdot2\n\\end{pmatrix}\n= \\begin{pmatrix}\n-9  &  -3 & -3  \\\\\n-15 &  -6 & -15 \\\\\n-6  & -21 & -3  \\\\\n-9  & -12 & -6\n\\end{pmatrix}.\n\\end{split}\n\\end{align}\\]\nIn R führt man diese Skalarmultiplikation aus wie folgt. \n\n# Definitionen\nA = matrix(c(3,1,1,\n             5,2,5,\n             2,7,1,\n             3,4,2),\n           nrow = 4,\n           byrow = TRUE)\nc = -3\n\n# Skalarmultiplikation\nB = c*A\nprint(B)\n\n     [,1] [,2] [,3]\n[1,]   -9   -3   -3\n[2,]  -15   -6  -15\n[3,]   -6  -21   -3\n[4,]   -9  -12   -6\n\n\nMithilfe der Definition von Matrixaddition und Skalarmultiplikation ist es möglich, einen Vektorraum zu definieren, dessen Elemente die reellen Matrizen sind. Insbesondere legt diese Definition auch die Rechenregeln beim Umgang mit Matrixaddition und Skalarmultiplikation fest.\n\nTheorem 9.1 (Vektorraum der reellwertigen Matrizen) Das Tripel \\((\\mathbb{R}^{n \\times m}, +, \\cdot)\\) mit der oben definierten Matrixaddition und Skalarmultiplikation ist ein Vektorraum. Insbesondere gelten damit für \\(A,B,C\\in \\mathbb{R}^{n \\times m}\\) und \\(r,s,t\\in \\mathbb{R}\\) folgende Rechenregeln:\n\nKommutativität der Addition: \\(A + B = B + A\\).\nAssoziativität der Addition: \\((A + B) + C = A + (B + C)\\).\nExistenz eines neutralen Elements der Addition: \\(\\exists\\, 0 \\in \\mathbb{R}^{n \\times m}\\) mit \\(A + 0 = 0 + A = A\\).\nExistenz inverser Elemente der Addition: \\(\\forall A\\,\\exists -A \\in \\mathbb{R}^{n \\times m}\\) mit \\(A + (-A) = 0\\).\nExistenz eines neutralen Elements der Skalarmultiplikation: \\(\\exists\\, 1 \\in \\mathbb{R}\\) mit \\(1 \\cdot A = A\\).\nAssoziativität der Skalarmultiplikation: \\(r \\cdot (s \\cdot t) = (r \\cdot s)\\cdot t\\).\nDistributivität hinsichtlich der Matrixaddition: \\(r\\cdot (A + B) = r\\cdot A + r\\cdot B\\).\nDistributivität hinsichtlich der Skalaraddition: \\((r + s)\\cdot A = r\\cdot A + s\\cdot A\\).\n\n\nWir verzichten auf einen Beweis, der sich mit einigem Notationsaufwand direkt aus dem elementweisen Charakter von Matrixaddition und Skalarmultiplikation sowie den aus dem Umgang mit den reellen Zahlen bekannten Rechenregeln ergibt. Das im Theorem erwähnte neutrale Element der Addition wird Nullmatrix genannt, wir werden dazu später eine allgemeine Notation einführen. Die inversen Elemente der Addition sind durch \\[\\begin{equation}\n-A := (-a_{ij})_{1\\le i \\le n, 1 \\le j \\le m}\n\\end{equation}\\] gegeben und erlauben es, die Matrixsubtraktion als Spezialfall der Matrixaddition zu betrachten.\n\n\n9.2.4 Matrixtransposition\nEine weitere häufig auftretende grundlegende Matrixoperation ist das Vertauschen der Zeilen- und Spaltenanordnung einer Matrix, genannt Matrixtransposition.\n\nDefinition 9.5 (Matrixtransposition) Es sei \\(A \\in \\mathbb{R}^{n\\times m}\\). Dann ist die Transposition von \\(A\\) definiert als die Abbildung \\[\\begin{equation}\n\\cdot^{T} : \\mathbb{R}^{n\\times m} \\to \\mathbb{R}^{m \\times n}, \\,\nA \\mapsto \\cdot^{T}(A) := A^T\n\\end{equation}\\] mit \\[\\begin{align}\n\\begin{split}\nA^T\n=\n\\begin{pmatrix}\na_{11} & a_{12} & \\cdots & a_{1m} \\\\\na_{21} & a_{22} & \\cdots & a_{2m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & \\cdots & a_{nm}\n\\end{pmatrix}^T\n:=\n\\begin{pmatrix}\na_{11} & a_{21} & \\cdots & a_{n1} \\\\\na_{12} & a_{22} & \\cdots & a_{n2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{1m} & a_{2m} & \\cdots & a_{nm}\n\\end{pmatrix}.\n\\end{split}\n\\end{align}\\]\n\nFür \\(A \\in \\mathbb{R}^{n \\times m}\\) gilt damit also immer \\(A^T \\in \\mathbb{R}^{m \\times n}\\). Weiterhin gelten folgende Rechenregeln der Matrixtransposition, wie man sich an Beispielen klar macht:\n\nFür \\(A \\in \\mathbb{R}^{1 \\times 1}\\) gilt \\[\\begin{equation}\nA^T = A.\n\\end{equation}\\]\nEs gilt \\[\\begin{equation}\n\\left(A^T\\right)^T = A.\n\\end{equation}\\]\nEs gilt \\[\\begin{equation}\n\\left(a_{ii}\\right)_{1 \\le i \\le \\mbox{min}(n,m)} = \\left(a_{ii}\\right)^T_{1 \\le i \\le \\mbox{min}(n,m)}.\n\\end{equation}\\]\n\nLetztere Eigenschaft der Transposition besagt, dass die Elemente auf der Hauptdiagonalen einer Matrix bei Transposition unberührt bleiben.\nBeispiel\nEs sei \\(A \\in \\mathbb{R}^{2 \\times 3}\\) definiert durch \\[\\begin{equation}\nA:=\\begin{pmatrix}\n2 & 3 & 0 \\\\\n1 & 6 & 5 \\\\\n\\end{pmatrix},\n\\end{equation}\\] Dann gilt \\(A^T \\in \\mathbb{R}^{3 \\times 2}\\) und speziell \\[\\begin{equation}\nA^{T} :=\n\\begin{pmatrix}\n2  & 1 \\\\\n3  & 6 \\\\\n0  & 5 \\\\\n\\end{pmatrix}.\n\\end{equation}\\] Weiterhin gilt offenbar \\(\\min(m,n) = 2\\) und folglich \\[\\begin{equation}\n(a_{11}) = \\left(a_{11}\\right)^T\n\\mbox{ und }\n(a_{22}) = \\left(a_{22}\\right)^T.\n\\end{equation}\\] In R führt man die Transposition einer Matrix wie folgt durch.\n\n# Definition\nA = matrix(c(2,3,0,\n             1,6,5),\n           nrow = 2,\n           byrow = TRUE)\nprint(A)\n\n     [,1] [,2] [,3]\n[1,]    2    3    0\n[2,]    1    6    5\n\n# Transposition\nAT = t(A)\nprint(AT)\n\n     [,1] [,2]\n[1,]    2    1\n[2,]    3    6\n[3,]    0    5\n\n\nSchließlich gelten in der Verbindung mit der Matrixaddition, Matrixsubtraktion und der Skalarmultiplikation folgende Rechenregeln, wie man sich an Beispielen klar macht:\n\nFür \\(A,B\\in \\mathbb{R}^{n \\times m}\\) gilt \\[\\begin{equation}\n(A+B)^T = A^T + B^T.\n\\end{equation}\\]\nFür \\(A,B\\in \\mathbb{R}^{n \\times m}\\) gilt \\[\\begin{equation}\n(A-B)^T = A^T - B^T.\n\\end{equation}\\]\nFür \\(c\\in \\mathbb{R}\\) und \\(A \\in \\mathbb{R}^{n \\times m}\\) gilt \\[\\begin{equation}\n(cA)^T = cA^T.\n\\end{equation}\\]",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Matrizen</span>"
    ]
  },
  {
    "objectID": "109-Matrizen.html#sec-matrixmultiplikation",
    "href": "109-Matrizen.html#sec-matrixmultiplikation",
    "title": "9  Matrizen",
    "section": "9.3 Matrixmultiplikation",
    "text": "9.3 Matrixmultiplikation\nDie Matrixmultiplikation ist die zentrale Operation beim Rechnen mit Matrizen. Sie ist definiert wie folgt.\n\nDefinition 9.6 (Matrixmultiplikation) Es seien \\(A\\in \\mathbb{R}^{n \\times m}\\) und \\(B \\in \\mathbb{R}^{m \\times k}\\). Dann ist die Matrixmultiplikation von \\(A\\) und \\(B\\) definiert als die Abbildung \\[\\begin{equation}\n\\cdot : \\mathbb{R}^{n\\times m} \\times \\mathbb{R}^{m\\times k} \\to \\mathbb{R}^{n \\times k}, \\,\n(A,B) \\mapsto \\cdot(A,B) := AB\n\\end{equation}\\] mit \\[\\begin{align}\n\\begin{split}\nAB\n& =\n\\begin{pmatrix}\na_{11} & a_{12} & \\cdots & a_{1m} \\\\\na_{21} & a_{22} & \\cdots & a_{2m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & \\cdots & a_{nm}\n\\end{pmatrix}\n\\begin{pmatrix}\nb_{11} & b_{12} & \\cdots & b_{1k} \\\\\nb_{21} & b_{22} & \\cdots & b_{2k} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nb_{m1} & b_{m2} & \\cdots & b_{mk}\n\\end{pmatrix}\n\\\\\n&\n:=\n\\begin{pmatrix}\n\\sum_{i=1}^m a_{1i}b_{i1} & \\sum_{i=1}^m a_{1i}b_{i2} & \\cdots & \\sum_{i=1}^m a_{1i}b_{ik}  \\\\\n\\sum_{i=1}^m a_{2i}b_{i1} & \\sum_{i=1}^m a_{2i}b_{i2} & \\cdots & \\sum_{i=1}^m a_{2i}b_{ik}  \\\\\n\\vdots                    & \\vdots                    & \\ddots & \\vdots                     \\\\\n\\sum_{i=1}^m a_{ni}b_{i1} & \\sum_{i=1}^m a_{ni}b_{i2} & \\cdots & \\sum_{i=1}^m a_{ni}b_{ik}\n\\end{pmatrix}\n\\\\\n&\n= \\left(\\sum_{i=1}^m a_{ji}b_{il} \\right)_{1 \\le j \\le n, 1 \\le l \\le k}\n\\end{split}\n\\end{align}\\]\n\nDas Matrixprodukt \\(AB\\) ist also nur dann definiert, wenn \\(A\\) genau so viele Spalten hat wie \\(B\\) Zeilen hat. Informell gilt für die beteiligten Matrixgrößen dabei die Merkregel \\[\\begin{equation}\n(n \\times m)(m \\times k) = (n \\times k).\n\\end{equation}\\] Der Eintrag \\((AB)_{ij}\\) in \\(AB\\) entspricht der Summe der multiplizierten \\(i\\)ten Zeile von \\(A\\) und \\(j\\)ten Spalte von \\(B\\). Zum Berechnen von \\((AB)_{ij}\\) geht man für \\(i = 1,...,n\\) und \\(j = 1,...,k\\) also in Gedanken wie folgt vor:\n\nMan legt die Tranposition der \\(i\\)ten Zeile von \\(A\\) über die \\(j\\)te Spalte von \\(B\\).\nWeil \\(A\\) genau \\(m\\) Spalten hat und \\(B\\) genau \\(m\\) Zeilen hat, gibt es dann zu jedem Element der Zeile aus \\(A\\) ein korrespondierendes Element in der Spalte von \\(B\\).\nMan multipliziert die korrespondierenden Elemente miteinander.\nDie Summe dieser Produkte ist dann der Eintrag mit Index \\(ij\\) in \\(AB\\).\n\nBeispiel\n\\(A\\in \\mathbb{R}^{2\\times 3}\\) und \\(B\\in \\mathbb{R}^{3\\times 2}\\) seien definiert als \\[\\begin{equation}\nA := \\begin{pmatrix}\n2 & -3 &  0   \\\\\n1 &  6 &  5\n\\end{pmatrix}\n\\mbox{ und }\nB := \\begin{pmatrix}\n4 & 2  \\\\\n-1 & 0  \\\\\n1 & 3\n\\end{pmatrix}.\n\\end{equation}\\] Wir wollen \\(C := AB\\) und \\(D := BA\\) berechnen. Mit \\(n = 2, m = 3\\) und \\(k = 2\\) wissen wir schon, dass \\(C \\in \\mathbb{R}^{2 \\times 2}\\) und \\(D \\in \\mathbb{R}^{3 \\times 3}\\), weil \\[\\begin{equation}\n(2 \\times 3)(3 \\times 2) = (2 \\times 2)\n\\end{equation}\\] und \\[\\begin{equation}\n(3 \\times 2)(2 \\times 3) = (3 \\times 3).\n\\end{equation}\\] Es gilt hier also sicher \\(AB \\neq BA\\). Für \\(C\\) ergibt sich dann \\[\\begin{align}\n\\begin{split}\nC\n& = AB\n\\\\\n& = \\begin{pmatrix}\n2 & -3 & 0 \\\\\n1 &  6 & 5 \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n4  & 2 \\\\\n-1 & 0 \\\\\n1  & 3\n\\end{pmatrix}\n\\\\\n& =\n\\begin{pmatrix}\n2\\cdot 4 + (-3)\\cdot (-1) + 0\\cdot 1 & 2\\cdot 2 + (-3)\\cdot 0 + 0\\cdot 3 \\\\\n1\\cdot 4 +    6\\cdot (-1) + 5\\cdot 1 & 1\\cdot 2 +  6\\cdot 0 + 5\\cdot 3 \\\\\n\\end{pmatrix}\n\\\\\n& =\n\\begin{pmatrix}\n8 + 3 + 0 & 4 + 0 + 0 \\\\\n4 - 6 + 5 & 2 + 0 + 15 \\\\\n\\end{pmatrix}\n\\\\\n& =\n\\begin{pmatrix}\n11 & 4 \\\\\n3 & 17 \\\\\n\\end{pmatrix}.\n\\end{split}\n\\end{align}\\]\nIn R nutzt man für die Matrixmultiplikation den %*% Operator.\n\n# Definitionen\nA = matrix(c(2,-3,0,\n             1, 6,5),\n           nrow  = 2,\n           byrow = TRUE)\nB = matrix(c( 4,2,\n             -1,0,\n              1,3),\n           nrow  = 3,\n           byrow = TRUE)\n\n# Matrixmultiplikation\nC = A %*% B\nprint(C)\n\n     [,1] [,2]\n[1,]   11    4\n[2,]    3   17\n\n\nFür \\(D\\) ergibt sich weiterhin \\[\\begin{align}\n\\begin{split}\nD\n& = BA\n\\\\\n& =\n\\begin{pmatrix}\n4  & 2 \\\\\n-1 & 0 \\\\\n1  & 3\n\\end{pmatrix}\n\\begin{pmatrix}\n2 & -3 & 0 \\\\\n1 &  6 & 5 \\\\\n\\end{pmatrix}\n\\\\\n& =\n\\begin{pmatrix}\n  4    \\cdot   2  + 2 \\cdot 1\n& 4    \\cdot (-3) + 2 \\cdot 6\n& 4    \\cdot   0  + 2 \\cdot 5\n\\\\\n  (-1) \\cdot  2  + 0 \\cdot 1\n& (-1) \\cdot(-3) + 0 \\cdot 6\n& (-1) \\cdot  0  + 0 \\cdot 5\n\\\\\n  1    \\cdot  2  + 3 \\cdot 1\n& 1    \\cdot(-3) + 3 \\cdot 6\n& 1    \\cdot  0  + 3 \\cdot 5\n\\end{pmatrix}\n\\\\\n& =\n\\begin{pmatrix}\n    8 + 2\n& -12 + 12\n&   0 + 5\n\\\\\n   -2 + 0\n&   3 + 0\n&   0 + 0\n\\\\\n    2 + 3\n&  -3 + 18\n&   0 + 15\n\\end{pmatrix}\n\\\\\n& =\n\\begin{pmatrix}\n  10\n&  0\n& 10\n\\\\\n  -2\n&  3\n&  0\n\\\\\n   5\n& 15\n& 15\n\\\\\n\\end{pmatrix}\n\\end{split}\n\\end{align}\\]\nIn R überprüft man diese Rechnung wie folgt.\n\n# Definitionen\nA = matrix(c(2,-3,0,\n             1, 6,5),\n           nrow  = 2,\n           byrow = TRUE)\nB = matrix(c( 4,2,\n             -1,0,\n              1,3),\n           nrow  = 3,\n           byrow = TRUE)\n\n# Matrixmultiplikation\nD = B %*% A\nprint(D)\n\n     [,1] [,2] [,3]\n[1,]   10    0   10\n[2,]   -2    3    0\n[3,]    5   15   15\n\n\nIst allerdings eine Matrixmultiplikation aufgrund nicht-adäquater Matrizengrößen nicht definiert, so lässt sich diese auch nicht numerisch auswerten.\n\n# Beispiel für eine undefinierte Matrixmultipliation\nE = t(A) %*% B      # (3 x 2)(3 x 2)\n\nError in t(A) %*% B: non-conformable arguments\n\n\nFolgendes Theorem, das wir nicht beweisen wollen, stellt den Bezug zwischen dem Skalarprodukt zweier Vektoren und der Multiplikation zweier Matrizen her. Dieser ergibt sich im Wesentlichen durch die Identifikation von \\(\\mathbb{R}^{n}\\) und \\(\\mathbb{R}^{n \\times 1}\\) und der Tatsache, dass nach Definition der Eintrag \\((AB)_{ij}\\) im Produkt von \\(A \\in \\mathbb{R}^{n \\times m}\\) und \\(B \\in \\mathbb{R}^{m \\times k}\\) dem Vektorskalarprodukt der \\(i\\)ten Spalte von \\(A^T\\) und der \\(j\\)ten Spalte von \\(B\\) entspricht.\n\nTheorem 9.2 (Matrixmultiplikation und Vektorskalarprodukt) Es seien \\(x,y \\in \\mathbb{R}^n\\). Dann gilt \\[\\begin{equation}\n\\langle x,y \\rangle = x^Ty.\n\\end{equation}\\] Weiterhin seien für \\(A \\in \\mathbb{R}^{n\\times m}\\) für \\(i = 1,...,n\\) \\[\\begin{equation}\n\\bar{a}_i := (a_{ji})_{1 \\le j \\le m} \\in \\mathbb{R}^m\n\\end{equation}\\] die Spalten von \\(A^T\\) und für \\(B \\in \\mathbb{R}^{m \\times k}\\) für \\(i = 1,...,k\\) \\[\\begin{equation}\n\\bar{b}_j := (b_{ij})_{1 \\le j \\le m} \\in \\mathbb{R}^m\n\\end{equation}\\] die Spalten von \\(B\\), also \\[\\begin{equation}\nA^T =\n\\begin{pmatrix}\n\\bar{a}_1 & \\bar{a}_2 & \\cdots & \\bar{a}_n\n\\end{pmatrix}\n\\in \\mathbb{R}^{m \\times n}\n\\mbox{ und }\nB =\n\\begin{pmatrix}\n\\bar{b}_1 & \\bar{b}_2 & \\cdots & \\bar{b}_k\n\\end{pmatrix}\n\\in \\mathbb{R}^{m \\times k}.\n\\end{equation}\\] Dann gilt \\[\\begin{equation}\nAB = \\left(\\langle \\bar{a}_i,\\bar{b}_j \\rangle \\right)_{1 \\le i \\le n, 1 \\le j \\le k}.\n\\end{equation}\\]\n\n\n9.3.1 Rechenregeln der Matrixmultiplikation\nIm Folgenden stellen wir einige grundlegende Rechenregeln der Matrixmultiplikation, insbesondere auch in Kombination mit anderen Matrixoperationen zusammen.\nFür Beweise der folgenden zwei Theoreme zur Assoziativität und Distributivität, die sich im Wesentlichen mit den entsprechenden Rechenregeln für Summen und Produkte der reellen Zahlen ergeben, verweisen wir auf die weiterführende Literatur.\n\nTheorem 9.3 (Assoziativität) Es seien \\(A \\in \\mathbb{R}^{n \\times m}\\), \\(B \\in \\mathbb{R}^{m \\times k}\\), \\(C \\in \\mathbb{R}^{k \\times p}\\) und \\(c \\in \\mathbb{R}\\). Dann gelten\n\nDie Multiplikation von Matrizen ist assoziativ, es gilt \\[\\begin{equation}\nA(BC) = (AB)C.\n\\end{equation}\\]\nDie Kombination von Matrizenmultiplikation und Skalarmultiplikation ist assoziativ, \\[\\begin{equation}\nc(AB) = (cA)B = A(cB).\n\\end{equation}\\]\n\n\nDie Assoziativität von Matrizenmultiplikation und Skalarmultiplikation erkennt man leicht bei Betrachtung des \\(j,l\\)ten Elements von \\(c(AB), (cA)B\\) und \\(A(cB)\\) anhand von \\[\\begin{equation}\nc\\left(\\sum_{i = 1}^m a_{ji}b_{il}\\right)\n= \\sum_{i = 1}^m \\left(c a_{ji}\\right) b_{il}  \n= \\sum_{i = 1}^m a_{ji}\\left(c b_{il}\\right).\n\\end{equation}\\]\n\nTheorem 9.4 (Distributivität) Es seien \\(A \\in \\mathbb{R}^{n \\times m}\\), \\(B \\in \\mathbb{R}^{n \\times m}\\), \\(C \\in \\mathbb{R}^{m \\times p}\\). Dann gelten \\[\\begin{equation}\n(A + B)C = AC + BC\n\\end{equation}\\] und \\[\\begin{equation}\nC^T(A + B) = C^TA + C^TB\n\\end{equation}\\]\n\nIm Gegensatz zur Kommutativität der Multiplikation reeller Zahlen ist die Matrixmultiplikation im Allgemeinen nicht kommutativ.\n\nTheorem 9.5 (Nichtkommutativität) Es seien \\(A \\in \\mathbb{R}^{n \\times m}\\) und \\(B \\in \\mathbb{R}^{m \\times p}\\). Dann gilt im Allgemeinen \\[\\begin{equation}\nAB \\neq BA.\n\\end{equation}\\]\n\n\nBeweis. Im Fall \\(p \\neq n\\) ist \\(BA\\) nicht definiert, wir betrachten also nur den Fall \\(p = n\\). Wir zeigen durch Angabe eines Gegenbeispiels mit \\(A,B\\in \\mathbb{R}^{2 \\times n}\\), dass im Allgemeinen \\(AB = BA\\) nicht gilt. Es seien \\[\\begin{equation}\nA := \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}\n\\mbox{ und }\nB := \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix}.\n\\end{equation}\\] Dann gilt \\[\\begin{equation}\nAB\n=\n\\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}\n\\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix}\n=\n\\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}\n\\neq\n\\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix}\n=\n\\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix}\n\\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}\n=\nBA.\n\\end{equation}\\]\n\n\nTheorem 9.6 (Kombination von Matrixmultiplikation und Transposition) Es seien \\(A \\in \\mathbb{R}^{m \\times n}\\) und \\(B \\in \\mathbb{R}^{n \\times k}\\). Dann gilt \\[\\begin{equation}\n(AB)^T = B^TA^T.\n\\end{equation}\\]\n\n\nBeweis. Ein Beweis ergibt sich wie folgt \\[\\begin{align}\n\\begin{split}\n(AB)^T\n& = \\left(\\left(\\sum_{i=1}^m a_{ji}b_{il} \\right)_{1 \\le j \\le n, 1 \\le l \\le k}\\right)^T \\\\\n& = \\left(\\sum_{i=1}^m a_{ij}b_{li} \\right)_{1 \\le i \\le k, 1 \\le j \\le n}  \\\\\n& = \\left(\\sum_{i=1}^m b_{li}a_{ij} \\right)_{1 \\le j \\le k, 1 \\le l \\le n}  \\\\\n& = B^TA^T.\n\\end{split}\n\\end{align}\\]",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Matrizen</span>"
    ]
  },
  {
    "objectID": "109-Matrizen.html#sec-matrixinversion",
    "href": "109-Matrizen.html#sec-matrixinversion",
    "title": "9  Matrizen",
    "section": "9.4 Matrixinversion",
    "text": "9.4 Matrixinversion\nUm den Begriff der inversen Matrix zu motivieren, betrachten wir zunächst das Problem des Lösens eines linearen Gleichungssystems. Dazu seien \\(A\\in \\mathbb{R}^{n \\times n},\\, x \\in \\mathbb{R}^n\\) und \\(b \\in \\mathbb{R}^n\\) und es gelte \\[\\begin{equation}\nAx = b.\n\\end{equation}\\] \\(A\\) und \\(b\\) seien als bekannt vorausgesetzt, \\(x\\) sei unbekannt. Konkret seien beispielsweise \\[\\begin{equation}\nA := \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix} \\mbox{ und }b := \\begin{pmatrix}  5 \\\\ 11 \\end{pmatrix}.\n\\end{equation}\\]\nDann liegt folgendes lineares Gleichungssystem mit zwei Gleichungen und zwei Unbekannten vor: \\[\\begin{equation}\nAx = b\n\\Leftrightarrow\n\\begin{pmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\\nx_2\n\\end{pmatrix}\n= \\begin{pmatrix}\n5 \\\\\n11\n\\end{pmatrix}\n\\Leftrightarrow\n\\begin{matrix}\n1x_1 + 2x_2 & = 5 \\\\\n3x_1 + 4x_2 & = 11\n\\end{matrix}.\n\\end{equation}\\]\nZiel des Lösens von linearen Gleichungssystemen ist bekanntlich, herauszufinden, für welche \\(x\\) das Gleichungssystem erfüllt ist. Um in diesem Kontext den Begriff der inversen Matrix von \\(A\\) einzuführen, vereinfachen wir die Situation weiter. Wir nehmen an, dass \\(A = a\\) eine \\(1 \\times 1\\) Matrix, also ein Skalar, sei und ebenso \\(x\\) und \\(b\\), dass wir also für \\(a,x,b \\in \\mathbb{R}\\) die Gleichung \\[\\begin{equation}\nax = b\n\\end{equation}\\] haben. Um diese Gleichung nach \\(x\\) aufzulösen würde man natürlich beide Seiten der Gleichung mit dem multiplikativem Inversen von \\(a\\) multiplizieren, wobei das multiplikative Inverse von \\(a\\) den Wert bezeichnet, der mit \\(a\\) multipliziert \\(1\\) ergibt. Dieser ist bekanntlich durch \\[\\begin{equation}\na^{-1} = \\frac{1}{a}\n\\end{equation}\\] gegeben. Dann würde gelten \\[\\begin{equation}\nax = b \\Leftrightarrow a^{-1}ax = a^{-1}b \\Leftrightarrow 1 \\cdot x = a^{-1}b \\Leftrightarrow x = \\frac{b}{a}.\n\\end{equation}\\] Ganz konkret etwa \\[\\begin{equation}\n2x = 6 \\Leftrightarrow 2^{-1} 2x = 2^{-1}6 \\Leftrightarrow \\frac{1}{2}2x = \\frac{1}{2}6 \\Leftrightarrow x = 3.\n\\end{equation}\\] Analog zu dem Fall, dass die Matrizen in \\(Ax = b\\) allesamt Skalare sind, möchte man im Fall eines linearen Gleichungssystems beide Seiten der Gleichung mit dem multiplikativen Inversen \\(A^{-1}\\) von \\(A\\) multiplizieren können, sodass eine Gleichung der Form \\[\\begin{equation}\nA^{-1}A = \"1\".\n\\end{equation}\\] resultiert. Dann hätte man nämlich \\[\\begin{equation}\nAx = b \\Leftrightarrow A^{-1}Ax = A^{-1}b \\Leftrightarrow x = A^{-1}b.\n\\end{equation}\\] Diese intuitive Idee des multiplikativen Inversen einer Matrix \\(A\\) wird im Folgenden unter dem Begriff der inversen Matrix formalisiert. Dazu benötigen wir zunächst den Begriff der Einheitsmatrix.\n\nDefinition 9.7 (Einheitsmatrix) Die Matrix \\[\\begin{equation}\nI_n\n:= (a_{ij})_{1\\le i \\le n, 1 \\le j \\le n}  \\in \\mathbb{R}^{n \\times n}\n:=\n\\begin{pmatrix}\n1      & 0      & \\cdots & 0       \\\\\n0      & 1      & \\cdots & 0       \\\\\n\\vdots & \\vdots & \\ddots & \\vdots  \\\\\n0      & 0      & \\cdots & 1       \\\\\n\\end{pmatrix}\n\\end{equation}\\] mit \\(a_{ij} = 1\\) für \\(i = j\\) und \\(a_{ij} = 0\\) für \\(i \\neq j\\) heißt \\(n\\)-dimensionale Einheitsmatrix.\n\nIn R wird \\(I_n\\) mit dem Befehl diag(n) erzeugt. Die Einheitsmatrix ist für die Matrixmultiplikation das Analog zur 1 bei der Multiplikation reeller Zahlen. Das ist die Aussage folgenden Theorems.\n\nTheorem 9.7 (Neutrales Element der Matrixmultiplikation) \\(I_n\\) ist das neutrale Element der Matrixmultiplikation, das heißt es gilt für \\(A \\in \\mathbb{R}^{n \\times m}\\), dass \\[\\begin{equation}\nI_nA = A \\mbox{ und } AI_m = A.\n\\end{equation}\\]\n\n\nBeweis. Es sei \\(B = (b_{ij}) = I_nA \\in \\mathbb{R}^{n\\times m}\\). Dann gilt für alle \\(1 \\le i \\le n\\) und alle \\(1 \\le j \\le n\\) \\[\\begin{equation}\nd_{ij}\n= 0 \\cdot a_{1j}\n+ 0 \\cdot a_{2j}\n+ \\cdots\n+ 0 \\cdot a_{i-1,j}\n+ 1 \\cdot a_{ij}\n+ \\cdots\n+ 0 \\cdot a_{i+1,j}\n+ 0 \\cdot a_{nj}\n= a_{ij}.\n\\end{equation}\\] Analog zeigt man dies für \\(AI_m\\).\n\nMit dem Begriff der Einheitsmatrix können wir jetzt die Begriffe der inversen Matrix und der invertierbaren Matrix definieren:\n\nDefinition 9.8 (Invertierbare Matrix und inverse Matrix) Eine quadratische Matrix \\(A \\in \\mathbb{R}^{n \\times n}\\) heißt invertierbar, wenn es eine quadratische Matrix \\(A^{-1} \\in \\mathbb{R}^{n \\times n}\\) gibt, so dass \\[\\begin{equation}\nA^{-1}A = AA^{-1} = I_n\n\\end{equation}\\] ist. Die Matrix \\(A^{-1}\\) heißt die inverse Matrix von \\(A\\).\n\nMan beachte, dass sich die Begriffe der inversen Matrix und der Invertierbarkeit nur auf quadratische Matrizen beziehen. Insbesondere können quadratische Matrizen invertierbar sein, müssen es aber nicht sein (lineare Gleichungssysteme können also Lösungen haben, müssen es aber nicht). Nicht invertierbare Matrizen nennt man auch singuläre Matrizen, invertierbare Matrizen manchmal auch nicht-singuläre Matrizen. Schließlich beachte man, dass Definition 9.8 lediglich aussagt, was eine inverse Matrix ist, aber nicht wie man sie berechnet.\nBeispiel für eine invertierbare Matrix\nDie Matrix \\[\\begin{equation}\nA := \\begin{pmatrix} 2.0 & 1.0 \\\\ 3.0 & 4.0 \\end{pmatrix}\n\\end{equation}\\] ist invertierbar und ihre inverse Matrix ist gegeben durch \\[\\begin{equation}\nA^{-1} = \\begin{pmatrix} 0.8 & -0.2 \\\\  -0.6 & 0.4 \\end{pmatrix},\n\\end{equation}\\] denn \\[\\begin{equation}\n\\begin{pmatrix} 2.0 &  1.0 \\\\   3.0 & 4.0 \\end{pmatrix}\n\\begin{pmatrix} 0.8 & -0.2 \\\\  -0.6 & 0.4 \\end{pmatrix}\n=\n\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}\n=\n\\begin{pmatrix} 0.8 & -0.2  \\\\  -0.6 & 0.4 \\end{pmatrix}\n\\begin{pmatrix} 2.0 &  1.0  \\\\   3.0 & 4.0 \\end{pmatrix},\n\\end{equation}\\] wovon man sich durch Nachrechnen überzeugt.\nBeispiel für eine nicht-invertierbare Matrix\nDie Matrix \\[\\begin{equation}\nB := \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}\n\\end{equation}\\] ist nicht invertierbar, denn wäre \\(B\\) invertierbar, dann gäbe es \\[\\begin{equation}\n\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}\n\\end{equation}\\] mit \\[\\begin{equation}\n\\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}\n\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}\n=\n\\begin{pmatrix} a & b \\\\ 0 & 0 \\end{pmatrix}\n=\n\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}.\n\\end{equation}\\] Das würde aber bedeuten, dass \\(0 = 1\\) in \\(\\mathbb{R}\\) und das ist ein Widerspruch. Also kann \\(B\\) nicht invertierbar sein.\nZum Berechnen inverser Matrizen\n\\(2 \\times 2\\) bis etwa \\(5 \\times 5\\) Matrizen kann man prinzipiell per Hand invertieren, dazu stellt die Lineare Algebra verschiedene Verfahren bereit. Wir wollen hier auf eine Einführung in die Matrizeninvertierung per Hand verzichten, da in der Anwendung Matrizen standardmäßig numerisch invertiert werden. Die numerische Matrixinversion ist dann auch ein großes Feld der Forschung zur Numerischen Mathematik, die eine Vielzahl von Algorithmen zu diesem Zweck bereitstellt. In R werden Matrizen per default mit der Funktion solve(), in Anlehnung an das Lösen linearer Gleichungssysteme, invertiert. Für das obige Beispiel einer invertierbaren Matrix ergibt sich dabei folgender R Code.\n\n# Definition\nA = matrix(c(2,1,\n             3,4),\n           nrow  = 2,\n           byrow = TRUE)\n\n# Berechnen von A^{-1}\nprint(solve(A))\n\n     [,1] [,2]\n[1,]  0.8 -0.2\n[2,] -0.6  0.4\n\n# Überprüfen der Eigenschaften einer inversen Matrix\nprint(solve(A) %*% A)\n\n              [,1] [,2]\n[1,]  1.000000e+00    0\n[2,] -1.110223e-16    1\n\n# Bei der umgekehrten Berechnung ergebn sich kleine Rundungsfehler\nprint(A %*% solve(A))\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n\n\nNicht-invertierbare Matrizen sind dabei natürlich auch numerisch nicht-invertierbar, wie folgende Fehlermeldung in R bezüglich obigen Beispiels einer nicht-invertierbaren Matrix demonstriert.\n\nB = matrix(c(1,0,\n             0,0),\n           nrow  = 2,\n           byrow = 2)\nsolve(B)\n\nError in solve.default(B): Lapack routine dgesv: system is exactly singular: U[2,2] = 0",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Matrizen</span>"
    ]
  },
  {
    "objectID": "109-Matrizen.html#sec-determinanten",
    "href": "109-Matrizen.html#sec-determinanten",
    "title": "9  Matrizen",
    "section": "9.5 Determinanten",
    "text": "9.5 Determinanten\nDie Determinante ist eine vielseitig einsetzbare Maßzahl einer quadratischen Matrix. Für das Verständnis der Eigenanalyse und der Matrixzerlegung ist der Begriff der Determinante im Kontext des charakteristischen Polynoms grundlegend.\nAllgemein ist eine Determinante eine nichtlineare Abbildung der Form \\[\\begin{equation}\n\\lvert \\cdot \\rvert: \\mathbb{R}^{n \\times n} \\to \\mathbb{R}, A \\mapsto \\lvert A \\rvert,\n\\end{equation}\\] das heißt, eine Determinante ordnet einer quadratischen Matrix \\(A\\) die reelle Zahl \\(\\lvert A \\rvert\\) zu. Die Zahl \\(\\lvert A \\rvert\\) wird dabei rekursiv anhand folgender Definition bestimmt.\n\nDefinition 9.9 (Determinante) Für \\(A = (a_{ij})_{1 \\le i,j \\le n} \\in \\mathbb{R}^{n \\times n}\\) mit \\(n&gt;1\\) sei \\(A_{ij} \\in \\mathbb{R}^{n-1 \\times n-1}\\) die Matrix, die aus \\(A\\) durch Entfernen der \\(i\\)ten Zeile und der \\(j\\)ten Spalte entsteht. Dann heißt die Zahl \\[\\begin{align}\n\\lvert A \\rvert & := a_{11} \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad   \\mbox{ für } n = 1\\\\\n\\lvert A \\rvert & := \\sum_{j = 1}^n a_{1j}(-1)^{1+j} \\det\\left(A_{1j}\\right)               \\mbox{ für } n &gt; 1\n\\end{align}\\] die Determinante von \\(A\\).\n\nDie Definition führt die Bestimmung der Determinante einer quadratischen Matrix also sukzessive durch Streichen von Zeilen und Spalten auf die Determinante einer \\(1 \\times 1\\) Matrix zurück, die durch ihr einziges Element gegeben ist. Für \\[\\begin{equation}\nA :=\n\\begin{pmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9 \\\\\n\\end{pmatrix}\n\\in \\mathbb{R}^{3 \\times 3}\n\\end{equation}\\] ergeben sich dabei zum Beispiel folgende Matrizen der Form \\(A_{ij} \\in \\mathbb{R}^{3-1 \\times 3-1}\\): \\[\\begin{equation}\nA_{11}\n=\n\\begin{pmatrix}\n5 & 6 \\\\\n8 & 9 \\\\\n\\end{pmatrix},\nA_{12}\n=\n\\begin{pmatrix}\n4 & 6 \\\\\n7 & 9 \\\\\n\\end{pmatrix},\nA_{21}\n=\n\\begin{pmatrix}\n2 & 3 \\\\\n8 & 9 \\\\\n\\end{pmatrix},\nA_{22}\n=\n\\begin{pmatrix}\n1 & 3 \\\\\n7 & 9 \\\\\n\\end{pmatrix}.\n\\end{equation}\\]\nFür die Berechnung der Determinanten von zwei- und dreidimensionalen quadratischen Matrizen gibt es direkte, nicht-rekursive Rechenregeln, die in folgendem Theorem festgehalten sind.\n\nTheorem 9.8 (Determinanten von zwei- und dreidimensionalen Matrizen) \\(\\quad\\)\nEs sei \\(A = (a_{ij})_{1 \\le i,j \\le 2} \\in \\mathbb{R}^{2 \\times 2}\\). Dann gilt \\[\\begin{equation}\n\\lvert A \\rvert = a_{11}a_{22} - a_{12}a_{21}.\n\\end{equation}\\] Es sei \\(A = (a_{ij})_{1 \\le i,j \\le 3} \\in \\mathbb{R}^{3 \\times 3}\\). Dann gilt \\[\\begin{equation}\n\\lvert A \\rvert= a_{11}a_{22}a_{33} + a_{12}a_{23}a_{31} + a_{13}a_{21}a_{32} - a_{12}a_{21}a_{33} - a_{11}a_{23}a_{32} - a_{13}a_{22}a_{31}.\n\\end{equation}\\]\n\n\nBeweis. Für \\(A \\in \\mathbb{R}^{2 \\times 2}\\) gilt nach Definition \\[\\begin{align}\n\\begin{split}\n\\lvert A \\rvert\n& = \\sum_{j = 1}^n a_{1j}(-1)^{1+j} |A_{1j}| \\\\\n& = a_{11}(-1)^{1 + 1}|A_{11}| + a_{12}(-1)^{1 + 2}|A_{12}| \\\\\n& = a_{11}|(a_{22})| - a_{12}|(a_{21})| \\\\\n& = a_{11}a_{22} - a_{12}a_{21}. \\\\\n\\end{split}\n\\end{align}\\] Für \\(A \\in \\mathbb{R}^{3 \\times 3}\\) gilt nach Definition und mit der Formel für Determinanten von \\(2 \\times 2\\) Matrizen \\[\\begin{align}\n\\begin{split}\n\\lvert A \\rvert\n& = \\sum_{j = 1}^n a_{1j}(-1)^{1+j} |(A_{1j}| \\\\\n& =   a_{11}(-1)^{1+1} |A_{1j}| + a_{12}(-1)^{1+2} |A_{12}| +  a_{13}(-1)^{1+3}|A_{13}| \\\\\n& =   a_{11}|A_{11}| - a_{12}|A_{12}| + a_{13}|A_{13}| \\\\\n& =   a_{11}\\left\\vert\\begin{pmatrix} a_{22} & a_{23} \\\\ a_{32} & a_{33}\\end{pmatrix}\\right\\vert\n    - a_{12}\\left\\vert\\begin{pmatrix} a_{21} & a_{23} \\\\ a_{31} & a_{33}\\end{pmatrix}\\right\\vert\n    + a_{13}\\left\\vert\\begin{pmatrix} a_{21} & a_{22} \\\\ a_{31} & a_{32}\\end{pmatrix}\\right\\vert \\\\\n& =   a_{11}(a_{22}a_{33} - a_{23}a_{32})\n    - a_{12}(a_{21}a_{33} - a_{23}a_{31})\n    + a_{13}(a_{21}a_{32} - a_{22}a_{31}) \\\\\n& =   a_{11}a_{22}a_{33} - a_{11}a_{23}a_{32}\n    - a_{12}a_{21}a_{33} + a_{12}a_{23}a_{31}\n    + a_{13}a_{21}a_{32} - a_{13}a_{22}a_{31} \\\\\n& =   a_{11}a_{22}a_{33} + a_{12}a_{23}a_{31} + a_{13}a_{21}a_{32}\n    - a_{12}a_{21}a_{33} - a_{11}a_{23}a_{32} - a_{13}a_{22}a_{31}.\n\\end{split}\n\\end{align}\\]\n\nFür die Bestimmung der Determinanten von \\(2 \\times 2\\) und \\(3 \\times 3\\) Matrizen gilt somit die sogennante Sarrusche Merkregel: \\[\\begin{equation*}\n\\mbox{``Summe der Produkte auf den Diagonalen minus Summe der Produkte auf den Gegendiagonalen.''}\n\\end{equation*}\\] Dabei bezieht sich die Merkregeln bei \\(3 \\times 3\\) Matrizen auf das Schema \\[\\begin{equation}\n\\begin{pmatrix}\na_{11} & a_{12} & a_{13} & \\vert & a_{11} & a_{12} \\\\\na_{21} & a_{22} & a_{23} & \\vert & a_{21} & a_{22} \\\\\na_{31} & a_{32} & a_{33} & \\vert & a_{31} & a_{32}\n\\end{pmatrix}.\n\\end{equation}\\]\nBeispiele für Determinanten von \\(2 \\times 2\\) und \\(3 \\times 3\\) Matrizen\nEs seien \\[\\begin{equation}\nA :=\n\\begin{pmatrix}\n2 & 1 \\\\\n3 & 4\n\\end{pmatrix},\nB :=\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 0\n\\end{pmatrix}\n\\mbox{ und }\nC :=\n\\begin{pmatrix}\n2 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 3\n\\end{pmatrix}\n\\end{equation}\\] Dann ergeben sich \\[\\begin{equation}\n\\lvert A \\rvert\n= 2 \\cdot 4 - 1 \\cdot 3 = 8 - 3 = 5\n\\end{equation}\\] und \\[\\begin{equation}\n\\lvert B \\rvert\n= 1 \\cdot 0 - 0 \\cdot 0 = 0 - 0 = 0\n\\end{equation}\\] und \\[\\begin{equation}\n\\lvert C \\rvert\n= 2 \\cdot 1 \\cdot 3  + 0 \\cdot 0 \\cdot 0 + 0 \\cdot 0 \\cdot 0 - 0 \\cdot 0 \\cdot 3 - 0 \\cdot 0 \\cdot 0  - 0 \\cdot 1 \\cdot 0\n= 2 \\cdot 1 \\cdot 3\n= 6.\n\\end{equation}\\]\nIn R rechnet man dies mithilfe der det() Funktion wie folgt nach.\n\n# Matrixdefinition und Determinantenberechnung\nA = matrix(c(2,1,\n             3,4),\n           nrow = 2,\n           byrow = TRUE)\ndet(A)\n\n[1] 5\n\n# Matrixdefinition und Determinantenberechnung\nB = matrix(c(1,0,\n             0,0),\n           nrow = 2,\n           byrow = TRUE)\ndet(B)\n\n[1] 0\n\n# Matrixdefinition und Determinantenberechnung\nC = matrix(c(2,0,0,\n             0,1,0,\n             0,0,3),\n           nrow = 3,\n           byrow = TRUE)\ndet(C)\n\n[1] 6\n\n\nFür Determinanten bestehen zahlreiche Rechenregeln im Zusammenspiel mit Matrixmultiplikation und Matrixinversion. Ohne Beweis stellen wir diese in folgendem Theorem zusammen.\n\nTheorem 9.9 (Rechenregeln für Determinanten) \\(\\quad\\)\n(Determinantenmultiplikationssatz). Für \\(A,B \\in \\mathbb{R}^{n \\times n}\\) gilt \\[\\begin{equation}\n|AB| = \\lvert A \\rvert\\lvert B \\rvert.\n\\end{equation}\\] (Transposition). Für \\(A \\in \\mathbb{R}^{n \\times n}\\) gilt \\[\\begin{equation}\n\\lvert A \\rvert = \\left\\vert A^T \\right\\vert.\n\\end{equation}\\] (Inversion). Für eine invertierbare Matrix \\(A \\in \\mathbb{R}^{n \\times n}\\) gilt \\[\\begin{equation}\n\\left\\vert A^{-1}\\right\\vert = \\frac{1}{\\lvert A \\rvert}.\n\\end{equation}\\] (Dreiecksmatrizen). Für Matrizen \\(A = (a_{ij})_{1 \\le i,j\\le n} \\in \\mathbb{R}^{n \\times n}\\) mit \\(a_{ij} = 0\\) für \\(i &gt; j\\) oder \\(a_{ij} = 0\\) \\(j &gt; i\\) gilt \\[\\begin{equation}\n\\lvert A \\rvert = \\prod_{i=1}^n a_{ii}.\n\\end{equation}\\]\n\nFolgendes sehr tiefgehendes Theorem, welches wir nicht vollständig beweisen wollen, gibt eine Möglichkeit an, anhand der Determinante einer quadratischen Matrix zu bestimmen, ob sie invertierbar ist.\n\nTheorem 9.10 \\(A \\in \\mathbb{R}^{n \\times n}\\) ist dann und nur dann invertierbar, wenn gilt, dass \\(\\lvert A \\rvert \\neq 0\\). Es gilt also \\[\\begin{equation}\nA \\mbox{ ist invertierbar} \\Leftrightarrow \\lvert A \\rvert \\neq 0\n\\mbox{ und }\nA \\mbox{ ist nicht invertierbar} \\Leftrightarrow \\lvert A \\rvert = 0.\n\\end{equation}\\]\n\n\nBeweis. Wir deuten einen Beweis lediglich an und zeigen, dass aus der Invertierbarkeit von \\(A\\) folgt, dass \\(\\lvert A \\rvert\\) nicht gleich Null sein kann. Nehmen wir also an, dass \\(A\\) invertierbar ist. Dann gibt es eine Matrix \\(B\\) mit \\(AB = I_n\\) und mit dem Determinantenmultiplikationssatz folgt \\[\\begin{equation}\n\\lvert AB \\rvert = \\lvert A \\rvert\\lvert B \\rvert= |I_n| = 1.\n\\end{equation}\\] Also kann \\(\\lvert A \\rvert = 0\\) nicht gelten, denn sonst wäre \\(0 = 1\\).\n\nVisuelle Intuition\nDer abstrakte Begriff der Determinante einer quadratischen Matrix kann mithilfe des Vektorraumbegriffs etwas veranschaulicht werden. Dazu seien \\(a_1,...,a_n \\in \\mathbb{R}^n\\) die Spalten von \\(A \\in \\mathbb{R}^{n \\times n}\\). Dann gilt (wie wir nicht beweisen wollen), dass \\(\\lvert A \\rvert\\) dem signierten Volumen des von \\(a_1,...,a_n\\in \\mathbb{R}^n\\) aufgespannten Parallelotops entspricht. Um dies visuell zu veranschaulichen betrachten wir die Matrizen \\[\\begin{equation}\nA_1 =\n\\begin{pmatrix}\n3 & 1 \\\\\n1 & 2\n\\end{pmatrix},\nA_2 =\n\\begin{pmatrix}\n2 & 0 \\\\\n0 & 2\n\\end{pmatrix},\nA_3 =\n\\begin{pmatrix}\n2 & 2 \\\\\n2 & 2\n\\end{pmatrix}\n\\end{equation}\\] mit den jeweiligen Determinanten \\[\\begin{equation}\n\\lvert A_1 \\rvert = 3\\cdot 2 - 1 \\cdot 1 = 5, \\quad\n\\lvert A_2 \\rvert= 2\\cdot 2 - 0 \\cdot 0 = 4, \\quad\n\\lvert A_3 \\rvert = 2\\cdot 2 - 2 \\cdot 2 = 0.\n\\end{equation}\\]\nAbbildung 9.1 visualisiert die entsprechende Intuition.\n\n\n\n\n\n\nAbbildung 9.1: Determinanten als Parallelotopvolumina.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Matrizen</span>"
    ]
  },
  {
    "objectID": "109-Matrizen.html#sec-spezielle-matrizen",
    "href": "109-Matrizen.html#sec-spezielle-matrizen",
    "title": "9  Matrizen",
    "section": "9.6 Spezielle Matrizen",
    "text": "9.6 Spezielle Matrizen\nIn dieser Sektion stellen wir einige häufig auftretende Typen von Matrizen und ihre Eigenschaften zusammen. Zum Beweis der allermeisten Eigenschaften verweisen wir dabei auf die weiterführende Literatur.\n\n9.6.1 Einheitsmatrizen\nDie Einheitsmatrix und die Einheitsvektoren haben wir bereits kennengelernt. Wir fassen sie hier noch einmal in einer gemeinsamen Definition zusammen.\n\nDefinition 9.10 (Einheitsmatrix und Einheitsvektoren) Wir bezeichnen die Einheitsmatrix mit \\[\\begin{equation}\nI_{n} := (i_{jk})_{1 \\le j \\le n, 1 \\le k \\le n} \\in \\mathbb{R}^{n \\times n} \\mbox{ mit } i_{jk} = 1 \\mbox{ für } j = k \\mbox{ und } i_{jk} = 0 \\mbox{ für } j \\neq k.\n\\end{equation}\\] Wir bezeichnen die Einheitsvektoren \\(e_i, i = 1,...,n\\) mit \\[\\begin{equation}\ne_{i} := (e_{{i}_j})_{1 \\le j \\le n} \\in \\mathbb{R}^{n} \\mbox{ mit } e_{{i}_j} = 1 \\mbox{ für } i = j \\mbox{ und } e_{{i}_j} = 0 \\mbox{ für } i \\neq j.\n\\end{equation}\\]\n\nDie Einheitsmatrix \\(I_n\\) besteht nur aus Nullen und Diagonalelementen gleich Eins, die Einheitsvektoren bestehen nur aus Nullen und einer Eins in der jeweils indizierten Komponente. Es gilt \\[\\begin{equation}\nI_n = \\begin{pmatrix} e_1 & \\cdots & e_n \\end{pmatrix}\n\\in \\mathbb{R}^{n \\times n}\n\\end{equation}\\] Für \\(n = 3\\) gilt also zum Beispiel \\[\\begin{equation}\nI_3 =\n\\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}\n\\mbox{ und }\ne_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix},\ne_2 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix},\ne_3 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}.\n\\end{equation}\\] Weiterhin gelten für die Einheitsvektoren bekanntlich für \\(1 \\le i,j \\le n\\) \\[\\begin{equation}\ne^T_ie_j = 0 \\mbox{ für } i \\neq j,  e^T_ie_i = 1 \\mbox{ und } e^T_iv = v^Te_i = v_i \\mbox{ für } v \\in \\mathbb{R}^n.\n\\end{equation}\\]\n\n\n9.6.2 Einsmatrizen und Nullmatrizen\n\nDefinition 9.11 (Nullmatrizen, Nullvektoren, Einsmatrizen, Einsvektoren) Wir bezeichnen Nullmatrizen und Nullvektoren mit \\[\\begin{equation}\n0_{nm} := (0)_{1 \\le i \\le m, 1 \\le j \\le n} \\in \\mathbb{R}^{n \\times m}\n\\mbox{ und }\n0_{n} := (0)_{1 \\le i \\le n} \\in \\mathbb{R}^{n}.\n\\end{equation}\\] Wir bezeichnen Einsmatrizen und Einsvektoren mit \\[\\begin{equation}\n1_{nm} := (1)_{1 \\le i \\le n, 1 \\le j \\le m} \\in \\mathbb{R}^{n \\times m}\n\\mbox{ und }\n1_n := (1)_{1 \\le i \\le n} \\in \\mathbb{R}^n.\n\\end{equation}\\]\n\n\\(0_{nm}\\) und \\(0_{n}\\) bestehen also nur aus Nullen und \\(1_{nm}\\) und \\(1_{n}\\) bestehen nur aus Einsen. Es gilt also beispielsweise \\[\\begin{equation}\n0_{32} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\\\ 0 & 0 \\end{pmatrix},\n0_{3}  = \\begin{pmatrix} 0  \\\\ 0  \\\\ 0  \\end{pmatrix},\n1_{32} = \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\\\ 1 & 1 \\end{pmatrix} \\mbox{ und }\n1_{3}  = \\begin{pmatrix} 1  \\\\ 1  \\\\ 1  \\end{pmatrix}.\n\\end{equation}\\] Weiterhin gelten zum Beispiel \\[\\begin{equation}\n0_n0_n^T = 0_{nn} \\mbox{ und } 1_n1_n^T = 1_{nn},\n\\end{equation}\\] wovon man sich durch Nachrechnen überzeugt.\n\n\n9.6.3 Diagonalmatrizen\n\nDefinition 9.12 (Diagonalmatrix) Eine Matrix \\(D \\in \\mathbb{R}^{n \\times m}\\) heißt Diagonalmatrix, wenn \\(d_{ij} = 0\\) für \\(1 \\le i \\le n, 1 \\le j \\le m\\) mit \\(i \\neq j\\).\n\nEine quadratische Diagonalmatrix \\(D\\in \\mathbb{R}^{n \\times n}\\) mit den Diagonalelementen \\(d_1,...,d_n \\in \\mathbb{R}\\) schreibt man auch als \\[\\begin{equation}\nD = \\mbox{diag}(d_1,...,d_n).\n\\end{equation}\\] Zum Beispiel gelten \\[\\begin{equation}\nD\n:= \\mbox{diag}(1,2,3)\n= \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 2 & 0 \\\\\n0 & 0 & 3\n\\end{pmatrix}\n\\end{equation}\\] und für \\(\\sigma^2 \\in \\mathbb{R}\\) \\[\\begin{equation}\n\\Sigma\n= \\mbox{diag}(\\sigma^2,\\sigma^2,\\sigma^2)\n= \\begin{pmatrix}\n\\sigma^2 & 0 & 0 \\\\\n0 & \\sigma^2 & 0 \\\\\n0 & 0 & \\sigma^2\n\\end{pmatrix}\n= \\sigma^2I_3.\n\\end{equation}\\]\nIn folgendem Theorem stellen wir einige wichtige Eigenschaften von quadratischen Diagonalmatrizen zusammen.\n\nTheorem 9.11 (Eigenschaften quadratischer Diagonalmatrizen) \\(\\quad\\)\n(Determinante.) \\(D := \\mbox{diag}(d_1,...,d_n) \\in \\mathbb{R}^{n \\times n}\\) sei eine quadratische Diagonalmatrix. Dann gilt \\[\\begin{equation}\n|D| = \\prod_{i=1}^n d_i.\n\\end{equation}\\]\n\n\n\n9.6.4 Symmetrische Matrizen\nSymmetrische Matrizen sind quadratische Matrizen, die bei Transposition unverändert bleiben:\n\nDefinition 9.13 Eine Matrix \\(S \\in \\mathbb{R}^{n \\times n}\\) heißt symmetrisch, wenn \\(S^T = S\\).\n\nEin Beispiel für eine symmetrische Matrix ist \\[\\begin{equation}\nS :=\n\\begin{pmatrix}\n1 & 2 & 3 \\\\\n2 & 1 & 2 \\\\\n3 & 2 & 1\n\\end{pmatrix}.\n\\end{equation}\\]\nIn folgendem Theorem stellen wir einige wichtige Eigenschaften symmetrischer Matrizen zusammen.\n\nTheorem 9.12 (Eigenschaften symmetrischer Matrizen) \\(\\quad\\)\n(Summation.) \\(S_1 \\in \\mathbb{R}^{n \\times n}\\) und \\(S_2 \\in \\mathbb{R}^{n \\times n}\\) seien symmetrische Matrizen. Dann gilt \\[\\begin{equation}\nS_1 + S_2 = (S_1 + S_2)^T.\n\\end{equation}\\] (Inverse.) \\(S\\) sei eine invertierbare symmetrische Matrix und \\(S^{-1}\\) ihre Inverse. Dann ist auch \\(S^{-1}\\) eine symmetrische Matrix, das heißt es gilt \\[\\begin{equation}\n\\left(S^{-1}\\right)^T = S^{-1}.\n\\end{equation}\\]\n\n\n\n9.6.5 Orthogonale Matrizen\n\nDefinition 9.14 Eine Matrix \\(Q \\in \\mathbb{R}^{n \\times n}\\) heißt orthogonal, wenn \\(Q^TQ = I_n\\).\n\nDie Spalten einer orthogonalen Matrix sind also paarweise orthogonal, es gilt für \\[\\begin{equation}\nQ = \\begin{pmatrix} q_1 & \\cdots & q_n \\end{pmatrix} \\mbox{ mit } q_i \\in \\mathbb{R}^n \\mbox{ für } 1 \\le i \\le n,\n\\end{equation}\\] dass \\[\\begin{equation}\nq_i^Tq_j = 0 \\mbox{ für } i \\neq j \\mbox{ und }  q_i^Tq_j = 1 \\mbox{ für } i = j \\mbox{ mit } 1 \\le i,j \\le n.\n\\end{equation}\\]\n\nTheorem 9.13 (Eigenschaften orthogonaler Matrizen) \\(Q \\in \\mathbb{R}^{n \\times n}\\) sei eine orthogonale Matrix. Dann gelten folgende Eigenschaften von \\(Q\\). \\(\\quad\\)\n(Inverse.) Die Inverse von \\(Q\\) ist \\(Q^T\\), es gilt\n\\[\\begin{equation}\nQ^{-1} = Q^T.\n\\end{equation}\\] (Transposition) Die Zeilen von \\(Q\\) sind orthonormal, es gilt \\[\\begin{equation}\nQQ^T = I_n\n\\end{equation}\\]\n\n\nBeweis. (Inverse) Unter der Annahme, dass \\(Q^{-1}\\) existiert, gilt \\[\\begin{equation}\nQ^TQ = I_n \\Leftrightarrow Q^TQQ^{-1} = I_nQ^{-1} \\Leftrightarrow  Q^{-1} = Q^T.\n\\end{equation}\\] (Transposition) Es gilt \\[\\begin{equation}\nQ^TQ = I_n \\Leftrightarrow QQ^TQ = QI_n \\Leftrightarrow  QQ^TQQ^T = QQ^T \\Leftrightarrow  QQ^T = I_n.\n\\end{equation}\\]\n\n\n\n9.6.6 Positiv-definite Matrizen\nPositiv-definite Matrizen sind für die probabilistiche Modellbildung unter Verwendung multivariater Normalverteilungen zentral.\n\nDefinition 9.15 Eine quadratische Matrix \\(C \\in \\mathbb{R}^{n \\times n}\\) heißt positiv-definit (\\(\\mbox{p.d.}\\)), wenn\n\n\\(C\\) eine symmetrische Matrix ist und\nfür alle \\(x \\in \\mathbb{R}^n, x \\neq 0_n\\) gilt, dass \\(x^TCx &gt; 0\\) ist.\n\n\nIn folgendem Theorem stellen wir einige wichtige Eigenschaften positiv-definiter Matrizen zusammen.\n\nTheorem 9.14 (Eigenschaften positiv-definiter Matrizen) \\(\\quad\\)\n(Inverse.) \\(C \\in \\mathbb{R}^{n \\times n}\\) sei eine positiv-definite Matrix. Dann gilt, dass \\(C^{-1}\\) existiert und ebenfalls positiv-definit ist.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Matrizen</span>"
    ]
  },
  {
    "objectID": "109-Matrizen.html#literaturhinweise",
    "href": "109-Matrizen.html#literaturhinweise",
    "title": "9  Matrizen",
    "section": "9.7 Literaturhinweise",
    "text": "9.7 Literaturhinweise\nSearle (1982) gibt eine umfassende Einführung in die Matrixtheorie vor dem Hintergrund der probabilistischen Datenanalyse, Strang (2009) gibt ein umfassende Einführung in die Matrixtheorie im Kontext der linearen Algebra. In ihrer modernen Inkarnation tauchen Matrizen als algebraische Objekte wohl zunächst in den Arbeiten von Arthur Caley (1821-1895) auf, siehe zum Beispiel Caley (1858).",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Matrizen</span>"
    ]
  },
  {
    "objectID": "109-Matrizen.html#selbstkontrollfragen",
    "href": "109-Matrizen.html#selbstkontrollfragen",
    "title": "9  Matrizen",
    "section": "9.8 Selbstkontrollfragen",
    "text": "9.8 Selbstkontrollfragen\n\nGeben Sie die Definition einer Matrix wieder.\nNennen Sie sechs Matrixoperationen.\nGeben Sie die Definitionen der Matrixaddition und der Matrixsubtraktion wieder.\nGeben Sie die Definition der Skalarmultiplikation für Matrizen wieder.\nGeben Sie die Definition der Matrixtransposition wieder.\nEs seien \\[\\begin{equation}\nA :=\n\\begin{pmatrix}\n1 & 2 \\\\\n2 & 1\n\\end{pmatrix},\nB :=\n\\begin{pmatrix}\n3 & 0 \\\\\n1 & 2\n\\end{pmatrix}\n\\mbox{ und }\nc := 2.\n\\end{equation}\\] Berechnen Sie \\[\\begin{equation}\nD := c\\left(A - B^T\\right)\n\\mbox{ und }\nE := \\left(cA\\right)^T + B.\n\\end{equation}\\]\nGeben Sie die Definition der Matrixmultiplikation wieder.\nEs seien \\(A \\in \\mathbb{R}^{3 \\times 2}, B \\in \\mathbb{R}^{2\\times 4}\\) und \\(C \\in \\mathbb{R}^{3 \\times 4}\\). Prüfen Sie, ob folgende Matrixprodukte definiert sind, und wenn ja, geben Sie die Größe der resultierenden Matrix an: \\[\\begin{equation}\nABC, ABC^T, A^TCB^T, BAC.\n\\end{equation}\\]\nEs seien \\[\\begin{equation}\nA :=\n\\begin{pmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n3 & 2 & 0\n\\end{pmatrix}\nB :=\n\\begin{pmatrix}\n1 & 2 & 2 \\\\\n1 & 3 & 1 \\\\\n2 & 0 & 0\n\\end{pmatrix}\n\\mbox{ und }\nC :=\n\\begin{pmatrix}\n1 \\\\ 3 \\\\ 2\n\\end{pmatrix}.\n\\end{equation}\\] Berechnen Sie die Matrixprodukte \\[\\begin{equation}\nAB,\nB^TA^T,\n\\left(B^TA^T\\right)^T,\nAC.\n\\end{equation}\\]\nDefinieren Sie die Begriff der inversen Matrix und der Invertierbarkeit einer Matrix.\nGeben Sie die Formel für die Determinante von \\(A := (A_{ij})_{1 \\le i,j \\le 2} \\in \\mathbb{R}^2\\) wieder.\nGeben Sie die Formel für die Determinante von \\(A := (A_{ij})_{1 \\le i,j \\le 3} \\in \\mathbb{R}^3\\) wieder.\nBerechnen Sie die Determinanten von \\[\\begin{equation}\nA := \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}\nB := \\begin{pmatrix} 3 & 2 & 1 \\\\ 2 & 3 & 2 \\\\ 1 & 2 & 3 \\end{pmatrix} \\mbox{ und }\nC := \\mbox{diag}(1,2,3).\n\\end{equation}\\]\nGeben Sie die Definitionen von Einheitsmatrix und Einheitsvektoren wieder.\nGeben Sie die Definitionen von Nullmatrizen und Einsmatrizen wieder.\nGeben Sie die Definition einer symmetrischen Matrix wieder.\nGeben Sie die Definition einer Diagonalmatrix wieder.\nGeben Sie die Definition einer positiv-definiten Matrix wieder.\n\n\n\n\n\nCaley, A. (1858). A Memoir on the Theory of Matrices. Philosophical Transactions of the Royal Society of London, 148, 17–37. https://doi.org/10.1098/rstl.1858.0002\n\n\nSearle, S. (1982). Matrix Algebra Useful for Statistics. Wiley-Interscience.\n\n\nStrang, G. (2009). Introduction to Linear Algebra. Cambridge University Press.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Matrizen</span>"
    ]
  },
  {
    "objectID": "110-Eigenanalyse.html",
    "href": "110-Eigenanalyse.html",
    "title": "10  Eigenanalyse",
    "section": "",
    "text": "10.1 Eigenvektoren und Eigenwerte\nUnter der Eigenanalyse einer quadratischen Matrix versteht man das bestimmen ihrer Eigenvektoren und Eigenwerte. Diese sind für eine quadratische Matrix wie folgt definiert.\nNach Definition hat also jeder Eigenvektor einen zugehörigen Eigenwert, allerdings können die Eigenwerte verschiedener Eigenvektoren durchaus identisch sein. Intuitiv bedeutet die Definition von Eigenvektor und Eigenwert, dass ein Eigenvektor einer Matrix durch Multiplikation mit eben dieser Matrix in seiner Länge, nicht aber in seiner Richtung, verändert wird. Der zugehörige Eigenwert des Eigenvektors entspricht dem Faktor der Längenänderung. Allerdings ist die Zuordnung von Eigenvektoren und Eigenwerten nicht eindeutig, wie folgendes Theorem zeigt.\nUm nun die Uneindeutigkeit in der Definition des zu einem Eigenwert zugeordneten Eigenvektors aufzulösen, nutzen wir die Konvention, nur diejenigen Vektoren also Eigenvektoren zu einem Eigenwert \\(\\lambda\\) zu betrachten, die die Länge 1 haben, für die also gilt, dass \\[\\begin{equation}\n\\Vert v \\Vert = 1.\n\\end{equation}\\] Sollten wir also einen Eigenvektor \\(v\\) zu einem Eigenwert \\(\\lambda\\) einer Matrix \\(A\\) finden, der nicht von der Länge 1 ist, so können wir ihn immer mit \\(\\Vert v \\Vert^{-1}\\) multiplizieren. Der resultierende Vektor \\(v' = v/\\Vert v \\Vert\\) hat dann die Länge 1 und ist nach Theorem 10.1 ebenso ein Eigenvektor von \\(A\\) zum Eigenwert \\(\\lambda\\). Bevor wir uns der Bestimmung von Eigenwerten und Eigenvektoren widmen, wollen wir die Konzepte von Eigenwert und Eigenvektor für den Fall einer \\(2 \\times 2\\) Matrix an einem Beispiel veranschaulichen",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Eigenanalyse</span>"
    ]
  },
  {
    "objectID": "110-Eigenanalyse.html#eigenvektoren-und-eigenwerte",
    "href": "110-Eigenanalyse.html#eigenvektoren-und-eigenwerte",
    "title": "10  Eigenanalyse",
    "section": "",
    "text": "Definition 10.1 (Eigenvektor und Eigenwert) \\(A \\in \\mathbb{R}^{m \\times m}\\) sei eine quadratische Matrix. Dann heißt jeder vom Nullvektor \\(0_m\\) verschiedene Vektor \\(v \\in \\mathbb{R}^m\\), für den mit einem Skalar \\(\\lambda \\in \\mathbb{R}\\) gilt, dass \\[\\begin{equation}\nAv = \\lambda v\n\\end{equation}\\] ist, ein Eigenvektor von \\(A\\) und \\(\\lambda\\) heißt dann ein Eigenwert von \\(A\\).\n\n\n\nTheorem 10.1 (Multiplikativität von Eigenvektoren) \\(A \\in \\mathbb{R}^{m \\times m}\\) sei eine quadratische Matrix. Wenn \\(v \\in \\mathbb{R}^m\\) Eigenvektor von \\(A\\) mit Eigenwert \\(\\lambda \\in \\mathbb{R}\\) ist, dann ist für \\(c \\in \\mathbb{R}\\) auch \\(cv \\in \\mathbb{R}^m\\) Eigenvektor von \\(A\\) und zwar wiederum mit Eigenwert \\(\\lambda \\in \\mathbb{R}\\).\n\n\nBeweis. Es gilt \\[\\begin{equation}\nAv = \\lambda v   \\Leftrightarrow\ncAv = c\\lambda v \\Leftrightarrow\nA(cv) = \\lambda(cv).\n\\end{equation}\\] Also ist \\(cv\\) ein Eigenvektor von \\(A\\) mit Eigenwert \\(\\lambda\\).\n\n\n\nBeispiel\nEs sei \\[\\begin{equation}\nA :=\n\\begin{pmatrix}\n2 & 1 \\\\\n1 & 2\n\\end{pmatrix}\n\\end{equation}\\] Dann ist der Vektor der Länge 1 \\[\\begin{equation}\nv :=\n\\frac{1}{\\sqrt{2}}\n\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n\\end{equation}\\] ein Eigenvektor von \\(A\\) zum Eigenwert \\(\\lambda = 3\\), da gilt, dass \\[\\begin{align}\n\\begin{split}\nAv\n& =\n\\begin{pmatrix}\n2 & 1 \\\\\n1 & 2\n\\end{pmatrix}\n\\left(\n\\frac{1}{\\sqrt{2}}\n\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n\\right)\n\\\\\n& =\n\\frac{1}{\\sqrt{2}}\n\\begin{pmatrix}\n2 & 1 \\\\\n1 & 2\n\\end{pmatrix}\n\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n\\\\\n& =\n\\frac{1}{\\sqrt{2}}\n\\begin{pmatrix} 3 \\\\ 3 \\end{pmatrix}\n\\\\\n& =\n\\frac{1}{\\sqrt{2}}\n3\n\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n\\\\\n& =\n3\n\\left(\n\\frac{1}{\\sqrt{2}}\n\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n\\right)\n\\\\\n& =\n\\lambda v.\n\\end{split}\n\\end{align}\\] Inspektion von Abbildung 10.1 zeigt dementsprechend, dass für die hier definierte Matrix \\(A\\) die Vektoren \\(v\\) und \\(Av\\) in die gleiche Richtung zeigen, dass aber \\(Av\\) um den Faktor \\(\\lambda\\) länger ist als \\(v\\).\nDer Vektor \\[\\begin{equation}\nw := \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n\\end{equation}\\] dagegen hat zwar die Länge 1, ist aber im Gegensatz zu \\(v\\) kein Eigenvektor von \\(A\\), da es im Falle von \\[\\begin{equation}\nAw =\n\\begin{pmatrix}\n2 & 1 \\\\\n1 & 2\n\\end{pmatrix}\n\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n=\n\\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}\n\\end{equation}\\] keinen Skalar \\(\\lambda\\) geben kann, der mit Null, dem zweiten Eintrag von \\(w\\), multipliziert einen Wert ungleich Null ergeben kann. Inspektion von Abbildung 10.1 zeigt dementsprechend, dass der aus der Multiplikation von \\(w\\) mit \\(A\\) resultierende Vektor in eine andere Richtung zeigt als \\(w\\).\n\n\n\n\n\n\nAbbildung 10.1: Eigenvektor einer \\(2 \\times 2\\) Matrix. Für die Matrix \\[\\begin{equation}\n\\begin{pmatrix}\n2 & 1 \\\\\n1 & 2\n\\end{pmatrix}\n\\end{equation}\\] ist \\(v\\) ein Eigenvektor, \\(w\\) jedoch nicht\n\n\n\n\n\nBestimmung von Eigenwerten und Eigenvektoren\nFolgendes Theorem besagt, wie die Eigenwerte und Eigenvektoren einer quadratischen Matrix berechnet werden können.\n\nTheorem 10.2 (Bestimmung von Eigenwerten und Eigenvektoren) \\(A \\in \\mathbb{R}^{m \\times m}\\) sei eine quadratische Matrix. Dann ergeben sich die Eigenwerte von \\(A\\) als die Nullstellen des \\[\\begin{equation}\n\\chi_A(\\lambda) := |A - \\lambda I_m|\n\\end{equation}\\] von \\(A\\). Weiterhin seien \\(\\lambda_i^*, i = 1,2,...\\) die auf diese Weise bestimmten Eigenwerte von \\(A\\). Die entsprechenden Eigenvektoren \\(v_i, i = 1,2,...\\) von \\(A\\) können dann durch Lösen der linearen Gleichungssysteme \\[\\begin{equation}\n(A - \\lambda_i^* I_m)v_i = 0_m \\mbox{ für } i = 1,2,...\n\\end{equation}\\] bestimmt werden.\n\n\nBeweis. (1) Bestimmen von Eigenwerten\nWir halten zunächst fest, dass mit der Definition von Eigenvektoren und Eigenwerten gilt, dass \\[\\begin{equation}\nAv = \\lambda v\n\\Leftrightarrow Av - \\lambda v = 0_m\n\\Leftrightarrow (A - \\lambda I_m)v = 0_m.\n\\end{equation}\\] Für den Eigenwert \\(\\lambda\\) wird der Eigenvektor \\(v\\) also durch Multiplikation mit \\((A - \\lambda I_m)\\) auf den Nullvektor \\(0_m\\) abgebildet. Weil aber per Definition \\(v \\neq 0_m\\) gilt, ist die Matrix \\((A - \\lambda I_m)\\) somit nicht invertierbar: sowohl der Nullvektor als auch \\(v\\) werden durch \\(A\\) auf \\(0_m\\) abgebildet, die Abbildung \\[\\begin{equation}\nf : \\mathbb{R}^m \\to \\mathbb{R}^m, x \\mapsto (A - \\lambda I_m)x\n\\end{equation}\\] ist also nicht bijektiv, und \\((A - \\lambda I_m)^{-1}\\) kann nicht existieren. Die Tatsache, dass \\((A - \\lambda I_m)\\) nicht invertierbar ist, ist aber äquivalent dazu, dass die Determinante von \\((A -\\lambda I_m)\\) gleich Null ist. Also ist \\[\\begin{equation}\n\\chi_A(\\lambda) = |A - \\lambda I_m| = 0\n\\end{equation}\\] eine notwendige und hinreichende Bedingung dafür, dass \\(\\lambda\\) ein Eigenwert von \\(A\\) ist.\n(2) Bestimmen von Eigenvektoren\nEs sei \\(\\lambda_i^*\\) ein Eigenwert von \\(A\\). Dann gilt mit den obigen Überlegungen, dass Auflösen von \\[\\begin{equation}\n(A - \\lambda_i^* I_m)v_i^* = 0_m\n\\end{equation}\\] nach \\(v_i^*\\) einen Eigenvektor zum Eigenwert \\(\\lambda^*\\) ergibt.\n\nAllgemein müssen zur Bestimmung von Eigenwerten und Eigenvektoren also Polynomnullstellen bestimmt und lineare Gleichungssysteme gelöst werden. Dies kann für kleine Matrizen mit \\(m \\le 4\\) durchaus manuell geschehen. Die in der Anwendung auftretetenden Matrizen sind jedoch meist weitaus größer, so dass zur Eigenananalyse numerische Verfahren der Nullstellenbestimmung und des Lösens linearer Gleichungssysteme eingesetzt werden, die zum Beispiel in Funktionen wie R’s eigen(), SciPy’s linalg.eig() oder Julia’s eigvals() und eigvecs() genutzt werden. Für Details zu diesen Verfahren verweisen wir auf die weiterführende Literatur, zum Beispiel Burden et al. (2016) und Richter & Wick (2017). Wir wollen Theorem 10.2 hier lediglich anhand eines Beispiels illustrieren.\n\n\nBeispiel\nDazu sei wiederum \\[\\begin{equation}\nA :=\n\\begin{pmatrix*}[r]\n2 & 1 \\\\\n1 & 2\\end{pmatrix*}\n\\end{equation}\\] Wir wollen zunächst die Eigenwerte von \\(A\\) berechnen. Nach Theorem 10.2 sind dies die Nullstellen des charakteristischen Polynoms von \\(A\\). Wir berechnen also zunächst das charakteristische Polynom von \\(A\\) durch \\[\\begin{equation}\n\\chi_A(\\lambda)\n=\n\\left\\vert\n\\begin{pmatrix*}[r]\n2 & 1 \\\\\n1 & 2\\end{pmatrix*}\n-\n\\begin{pmatrix*}[r]\n\\lambda & 0 \\\\\n0       & \\lambda\n\\end{pmatrix*}\n\\right\\vert\n=\n\\left\\vert\n\\begin{pmatrix*}[r]\n2 - \\lambda & 1 \\\\\n1 & 2 - \\lambda\n\\end{pmatrix*}\n\\right\\vert\n= (2 - \\lambda)^2 - 1.\n\\end{equation}\\] Mithilfe der pq-Formel zur Lösung quadratischer Gleichungen findet man dann \\[\\begin{equation}\n(2 - \\lambda^*_{1/2})^2 - 1 = 0 \\Leftrightarrow \\lambda_1^* = 3 \\mbox{ oder } \\lambda_2^* = 1.\n\\end{equation}\\] Die Eigenwerte von \\(A\\) sind also \\(\\lambda_1 = 3\\) und \\(\\lambda_2 = 1\\). Die zugehörigen Eigenvektoren ergeben sich dann für \\(i = 1,2\\) durch Lösen des linearen Gleichungssystems \\[\\begin{equation}\n(A - \\lambda_i I_2)v_i = 0_2.\n\\end{equation}\\] Speziell ergibt sich hier, dass für \\(\\lambda_1 = 3\\) aus \\[\\begin{equation}\n(A - 3I_2)v_1 = 0_2\n\\Leftrightarrow\n\\begin{pmatrix*}[r]\n-1 & 1 \\\\\n1 & -1\n\\end{pmatrix*}\n\\begin{pmatrix*}[r]\nv_{1_1} \\\\\nv_{1_2}\n\\end{pmatrix*}\n=\n\\begin{pmatrix*}[r]\n0 \\\\\n0\n\\end{pmatrix*}\n\\end{equation}\\] folgt, dass \\[\\begin{equation}\nv_1 =\n\\frac{1}{\\sqrt{2}}\n\\begin{pmatrix*}[r]\n1 \\\\\n1\n\\end{pmatrix*}\n\\end{equation}\\] ein Eigenvektor zum Eigenwert \\(\\lambda_1\\) ist und dass für \\(\\lambda_2 = 1\\) aus \\[\\begin{equation}\n(A - 1I_2)v_2 = 0_2\n\\Leftrightarrow\n\\begin{pmatrix*}[r]\n1 & 1 \\\\\n1 & 1\n\\end{pmatrix*}\n\\begin{pmatrix*}[r]\nv_{2_1} \\\\\nv_{2_2}\n\\end{pmatrix*}\n=\n\\begin{pmatrix*}[r]\n0 \\\\\n0\n\\end{pmatrix*}\n\\end{equation}\\] folgt, dass \\[\\begin{equation}\nv_2 =\n\\frac{1}{\\sqrt{2}}\n\\begin{pmatrix*}[r]\n-1 \\\\\n1\n\\end{pmatrix*}\n\\end{equation}\\] ein Eigenvektor zum Eigenwert \\(\\lambda_2 = 1\\) ist. Weiterhin gelten hier offenbar \\[\\begin{equation}\nv_1^Tv_2 = 0 \\mbox{ und } \\Vert v_1 \\Vert = \\Vert v_2 \\Vert = 1.\n\\end{equation}\\]\nFolgender R Code demonstriert die Bestimmung der Eigenwerte und Eigenvektoren der hier betrachteten Matrix mithilfe der eigen() Funktion.\n\n# Matrixdefinition\nA = matrix(c(2,1,\n             1,2),\n           nrow  = 2,\n           byrow = TRUE)\n\n# Eigenanalyse\neigen(A)\n\neigen() decomposition\n$values\n[1] 3 1\n\n$vectors\n          [,1]       [,2]\n[1,] 0.7071068 -0.7071068\n[2,] 0.7071068  0.7071068\n\n\nZum Abschluss dieses Abschnittes betrachten wir zwei technische Theoreme, die Aussagen zum Zusammenhang spezieller Matrixprodukte und ihrer Eigenwerte und Eigenvektoren machen. Wir benötigen dieses Theoreme im Kontext der Kanonischen Korrelationsanalyse (Kapitel 39).\n\nTheorem 10.3 (Eigenwerte und Eigenvektoren von Matrixprodukten) Für \\(A \\in \\mathbb{R}^{n \\times m}\\) und \\(B \\in \\mathbb{R}^{m \\times n}\\) sind die Eigenwerte von \\(AB \\in \\mathbb{R}^{n \\times n}\\) und \\(BA \\in \\mathbb{R}^{m \\times m}\\) gleich. Weiterhin gilt, dass für einen Eigenvektor \\(v\\) zu einem von Null verschiedenen Eigenwert \\(\\lambda\\) von \\(AB\\) \\(w := Bv\\) ein Eigenvektor von \\(BA\\) zum Eigenwert \\(\\lambda\\) ist.\n\nFür einen Beweis verweisen wir auf Mardia et al. (1979), S. 468. Wir demonstrieren die Aussage dieses Theorems anhand untenstehenden R Codes.\n\nA   = matrix(1:6, nrow = 2,  byrow = T)           # Matrix A \\in \\mathbb{R}^{2 x 3}\nB   = matrix(1:6, ncol = 2,  byrow = T)           # Matrix B \\in \\mathbb{R}^{3 x 2}\nEAB = eigen(A %*% B)                              # Eigenanalyse von AB \\in \\mathbb{R}^{2 \\times 2}\nEBA = eigen(B %*% A)                              # Eigenanalyse von BA \\in \\mathbb{R}^{3 \\times 3}\nw   = B %*% EAB$vectors[,1]                       # Eigenvektor von BA\ncat(\"Eigenwerte von AB :\"  , EAB$values[1:2],\n    \"\\nEigenwerte von BA :\", EBA$values[1:2],\n    \"\\nBAw mit w = Bv    :\", B %*% A %*% w,\n    \"\\nlw mit w = Bv     :\", EBA$values[1] * w)\n\nEigenwerte von AB : 85.57934 0.4206623 \nEigenwerte von BA : 85.57934 0.4206623 \nBAw mit w = Bv    : -191.1333 -416.7586 -642.3839 \nlw mit w = Bv     : -191.1333 -416.7586 -642.3839\n\n\n\nTheorem 10.4 Für \\(A \\in \\mathbb{R}^{n \\times m}, B \\in \\mathbb{R}^{p \\times n}, a \\in \\mathbb{R}^m\\) und \\(b \\in \\mathbb{R}^p\\) gilt, dass der einzige von Null verschiedene Eigenwert von \\(Aab^TB \\in \\mathbb{R}^{n \\times n}\\) gleich \\(b^T BAa\\) mit zugehörigem Eigenvektor \\(Aa\\) ist.\n\nFür einen Beweis verweisen wir auf Mardia et al. (1979), S. 468. Wir demonstrieren die Aussage dieses Theorems anhand untenstehenden R Codes.\n\nA      = matrix(1:6, nrow = 2,  byrow = T)     # Matrix A \\in \\mathbb{R}^{2 x 3}\nB      = matrix(1:8, ncol = 2,  byrow = T)     # Matrix B \\in \\mathbb{R}^{4 x 2}\na      = matrix(1:3, nrow = 3,  byrow = T)     # Vektor a \\in \\mathbb{R}^{3 x 1}\nb      = matrix(1:4, nrow = 4,  byrow = T)     # Vektor b \\in \\mathbb{R}^{4 x 1}\nEAabTB = eigen(A %*% a %*% t(b) %*% B)         # Eigenanalyse von Aab^TB \\in \\mathbb{R}^{4 x 4}\ncat(\"Eigenwerte von AabTB :\", EAabTB$values,\n    \"\\nbTBAa                :\", t(b) %*% B %*% A %*% a,\n    \"\\nAa                   :\", A %*% a,\n    \"\\n(AabTB)Aa            :\",(A %*% a %*% t(b) %*% B) %*% A %*% a,            # Mv\n    \"\\n(bTBAa)Aa            :\",as.vector((t(b) %*% B %*% A %*% a)) * (A %*% a)) # = \\lambda v\n\nEigenwerte von AabTB : 2620 0 \nbTBAa                : 2620 \nAa                   : 14 32 \n(AabTB)Aa            : 36680 83840 \n(bTBAa)Aa            : 36680 83840",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Eigenanalyse</span>"
    ]
  },
  {
    "objectID": "110-Eigenanalyse.html#sec-orthonormalzerlegung",
    "href": "110-Eigenanalyse.html#sec-orthonormalzerlegung",
    "title": "10  Eigenanalyse",
    "section": "10.2 Orthonormalzerlegung",
    "text": "10.2 Orthonormalzerlegung\nMit dem Begriff der Zerlegung einer Matrix wird das Aufspalten einer gegebenen Matrix in das Matrixprodukt mehrerer Matrizen bezeichnet. Verschiedenste Matrixzerlegungen spielen in vielen mathematischen Anwendungen eine wichtige Rolle, für einen Überblick siehe beispielsweise Golub & Van Loan (2013). In diesem Abschnitt führen wir mit der Orthonormalzerlegung einer symmetrischen Matrix eine spezielle Matrixzerlegung ein, die direkt auf der Eigenanalyse aufbaut. Wir halten zunächst folgendes grundlegendes Theorem zu den Eigenwerten und Eigenvektoren symmetrischer Matrizen fest.\n\nTheorem 10.5 (Eigenwerte und Eigenvektoren symmetrischer Matrizen)  \n\\(S \\in \\mathbb{R}^{m \\times m}\\) sei eine symmetrische Matrix. Dann gelten\n\n\nBeweis. Wir setzen die Tatsache, dass eine symmetrische Matrix \\(m\\) reelle Eigenwerte hat, als gegeben voraus und zeigen lediglich, dass die Eigenvektoren zu je zwei verschiedenen Eigenwerten einer symmetrischen Matrix orthogonal sind. Ohne Beschränkung der Allgemeinheit seien also \\(\\lambda_i, \\lambda_j \\in \\mathbb{R}\\) mit \\(1 \\le i,j \\le m\\) und \\(\\lambda_i \\neq \\lambda_j\\) zwei verschiedenen Eigenwerte von \\(S\\) mit zugehörigen Eigenvektoren \\(q_i\\) und \\(q_j\\), respektive. Dann ergibt sich wie unten gezeigt, dass \\[\\begin{equation}\n\\lambda_i q_i^Tq_j = \\lambda_j q_i^Tq_j.\n\\end{equation}\\] Mit \\(q_i \\neq 0_m, q_j \\neq 0_m\\) und \\(\\lambda_i \\neq \\lambda_j\\) folgt damit \\(q_i^Tq_j = 0\\), weil weil es keine andere Zahl \\(c\\) als die Null gibt, für die bei \\(a,b\\in \\mathbb{R}\\) und \\(a \\neq b\\) gilt, dass \\[\\begin{equation}\nac = bc.\n\\end{equation}\\] Um abschließend \\[\\begin{equation}\n\\lambda_i q_i^Tq_j = \\lambda_j q_i^Tq_j.\n\\end{equation}\\] zu zeigen, halten wir zunächst fest, dass \\[\\begin{equation}\n                 Sq_i       = \\lambda_i q_i      \n\\Leftrightarrow (Sq_i)^T    = (\\lambda_i q_i)^T  \n\\Leftrightarrow q_i^TS^T    = q_i^T \\lambda_i^T  \n\\Leftrightarrow q_i^T S     = q_i^T \\lambda_i   \n\\Leftrightarrow q_i^T Sq_j  = \\lambda_i q_i^Tq_j\n\\end{equation}\\] und \\[\\begin{equation}\n                 Sq_j           = \\lambda_j q_j      \n\\Leftrightarrow q_j^T S         = q_j^T \\lambda_j   \n\\Leftrightarrow q_j^T Sq_i      = \\lambda_j q_j^Tq_i\n\\Leftrightarrow (q_j^T S q_i)^T = (\\lambda_j q_j^Tq_i)^T\n\\Leftrightarrow q_i^T S q_j     =  \\lambda_j q_i^Tq_j\n\\end{equation}\\] gelten. Sowohl \\(\\lambda_i q_i^Tq_j\\) als auch \\(\\lambda_j q_i^Tq_j\\) sind also mit \\(q_i^T Sq_j\\) und damit auch miteinander identisch.\n\nOffenbar haben wir nur Aussage (2) von Theorem 10.5 bewiesen. Ein vollständiger Beweis des Theorems findet sich zum Beispiel bei Strang (2009). Wir merken außerdem an, dass, weil wir nach Konvention Eigenvektoren der Länge 1 betrachten, die in Theorem 10.5 angesprochenen orthogonalen Eigenvektoren insbesondere auch orthonormal sind. Mithilfe von Theorem 10.5 können wir nun die Orthonormalzerlegung einer symmetrischen Matrix formulieren und ihre Existenz beweisen.\n\nTheorem 10.6 (Orthonormalzerlegung einer symmetrischen Matrix) \\(S \\in \\mathbb{R}^{m \\times m}\\) sei eine symmetrische Matrix mit \\(m\\) verschiedenen Eigenwerten. Dann kannn \\(S\\) geschrieben werden als \\[\\begin{equation}\nS = Q \\Lambda Q^T,\n\\end{equation}\\] wobei \\(Q \\in \\mathbb{R}^{m \\times m}\\) eine orthogonale Matrix ist und \\(\\Lambda \\in \\mathbb{R}^{m\\times m}\\) eine Diagonalmatrix ist.\n\n\nBeweis. Es seien \\(\\lambda_1 &gt; \\lambda_2 &gt; ... &gt; \\lambda_m\\) die der Größe nach geordneten Eigenwerte von \\(S\\) und \\(q_1,...,q_m\\) die zugehörigen orthonormalen Eigenvektoren. Mit \\[\\begin{equation}\nQ :=\n\\begin{pmatrix*}[r]\nq_1 & q_2 & \\cdots & q_m\n\\end{pmatrix*}\n\\in \\mathbb{R}^{m \\times m}\n\\mbox{ und }\n\\Lambda :=\n\\mbox{diag}\\begin{pmatrix*}[r]\n\\lambda_1,\\lambda_2,...,\\lambda_m\n\\end{pmatrix*}\n\\in \\mathbb{R}^{m \\times m},\n\\end{equation}\\] folgt dann mit den Definitionen von Eigenwerten und Eigenvektoren zunächst, dass \\[\\begin{equation}\nSq_i = \\lambda_i q_i \\mbox{ für } i = 1,...,m\n\\Leftrightarrow\nSQ = Q\\Lambda.\n\\end{equation}\\] Rechtseitige Multiplikation mit \\(Q^T\\) ergibt dann mit \\(QQ^T = I_m\\), dass \\[\\begin{equation}\nSQQ^T = Q \\Lambda Q^T\n\\Leftrightarrow SI_m = Q \\Lambda Q^T\n\\Leftrightarrow S    = Q \\Lambda Q^T.\n\\end{equation}\\]\n\nMan nennt das Aufspalten von \\(S\\) in das Matrixprodukt \\(Q\\Lambda Q^T\\) aufgrund der Diagonalität von \\(\\Lambda\\) auch eine Diagonalisierung von \\(S\\). Wie im Beweis gezeigt, wählt man zur Darstellung von \\(S\\) in Diagonaldarstellung für die Diagonalelemente von \\(\\Lambda\\) die der Größe nach geordneten Eigenwerte von \\(S\\) und für die Spalten von \\(Q\\) die jeweils zugehörigen Eigenvektoren von \\(S\\). Wir verdeutlichen dies an einem Beispiel.\nBeispiel\nFür die symmetrische Matrix \\[\\begin{equation}\nA = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}\n\\end{equation}\\] mit den oben bestimmten Eigenwerten \\(\\lambda_1 = 3\\) und \\(\\lambda_2 = 1\\) sowie den zugehörigen orthonormalen Eigenvektoren \\[\\begin{equation}\nv_1 = \\frac{1}{\\sqrt{2}}\n\\begin{pmatrix*}[r]\n1 \\\\\n1\n\\end{pmatrix*},\nv_2 = \\frac{1}{\\sqrt{2}}\n\\begin{pmatrix*}[r]\n-1 \\\\\n1\n\\end{pmatrix*}\n\\end{equation}\\] seien \\[\\begin{equation}\nQ := \\begin{pmatrix*}[r]\nv_1 & v_2\n\\end{pmatrix*}\n\\mbox{ und }\n\\Lambda = \\mbox{diag}(\\lambda_1,\\lambda_2).\n\\end{equation}\\] Dann ergibt sich offenbar \\[\\begin{align*}\nQ\\Lambda Q^T\n& =\n\\begin{pmatrix*}[r]\nv_1 & v_2\n\\end{pmatrix*}\n\\mbox{diag}(\\lambda_1,\\lambda_2)\n\\begin{pmatrix*}[r]\nv_1 & v_2\n\\end{pmatrix*}^T \\\\\n& =\n\\left(\n\\frac{1}{\\sqrt{2}}\n\\begin{pmatrix*}[r]\n1 & -1\\\\\n1 &  1\n\\end{pmatrix*}\n\\begin{pmatrix*}[r]\n3 & 0 \\\\\n0 & 1\n\\end{pmatrix*}\n\\right)\n\\left(\n\\frac{1}{\\sqrt{2}}\n\\begin{pmatrix*}[r]\n1 &  1 \\\\\n-1 &  1\n\\end{pmatrix*}\n\\right)\n\\\\\n& =\n\\left(\n\\frac{1}{\\sqrt{2}}\n\\begin{pmatrix*}[r]\n3 & -1 \\\\\n3 &  1\n\\end{pmatrix*}\n\\right)\n\\left(\n\\frac{1}{\\sqrt{2}}\n\\begin{pmatrix*}[r]\n1 &  1 \\\\\n-1 &  1\n\\end{pmatrix*}\n\\right)\n\\\\\n& =\n\\frac{1}{2}\n\\begin{pmatrix*}[r]\n4 & 2 \\\\\n2 & 4\n\\end{pmatrix*} \\\\\n& =\n\\begin{pmatrix*}[r]\n2 & 1 \\\\\n1 & 2\n\\end{pmatrix*} \\\\\n& = A\n\\end{align*}\\] und wir haben Theorem 10.6 für dieses Beispiel verifiziert.\n\nSymmetrische Quadratwurzel einer Matrix\nDie Definition der Orthonormalzerlegung einer symmetrischen Matrix erlaubt es, den Begriff der symmetrischen Quadratwurzel einer Matrix einzuführen.\n\nDefinition 10.2 (Symmetrische Quadratwurzel einer Matrix) \\(S \\in \\mathbb{R}^{m \\times m}\\) sei eine invertierbare symmetrische Matrix mit positiven Eigenwerten. Dann sind für \\(r \\in \\mathbb{N}^0\\) und \\(s \\in \\mathbb{N}\\) die rationalen Potenzen von \\(S\\) mit der orthonormalen Matrix \\(Q \\in \\mathbb{R}^{m \\times m}\\) der Eigenvektoren von \\(S\\) und der Diagonalmatrix \\(\\Lambda = \\mbox{diag}(\\lambda_i) \\in \\mathbb{R}^{m \\times m}\\) der zugehörigen Eigenwerte \\(\\lambda_1,...,\\lambda_m\\) von \\(S\\) definiert als \\[\\begin{equation}\nS^{r/s} = Q \\Lambda^{r/s} Q^T \\mbox{ mit } \\Lambda^{r/s} = \\mbox{diag}\\left(\\lambda_i^{r/s}\\right).\n\\end{equation}\\] Der Spezialfall \\(r:= 1, s := 2\\) wird als symmetrische Quadratwurzel von \\(S\\) bezeichnet und hat die Form \\[\\begin{equation}\nS^{1/2} = Q\\Lambda^{1/2}Q^T \\mbox{ mit } \\Lambda^{1/2} = \\mbox{diag}\\left(\\lambda_i^{1/2}\\right).\n\\end{equation}\\]\n\nWir halten fest, dass mit Definition 10.2 offenbar gilt, dass \\[\\begin{equation}\n\\left(S^{1/2} \\right)^2\n= Q\\Lambda^{1/2}Q^TQ\\Lambda^{1/2}Q^T\n= Q\\Lambda^{1/2}\\Lambda^{1/2}Q^T\n= Q\\Lambda Q^T\n= S.\n\\end{equation}\\] Weiterhin gilt, dass \\[\\begin{equation}\n\\left(S^{-1/2} \\right)^2\n= Q\\Lambda^{-1/2}Q^TQ\\Lambda^{-1/2}Q^T\n= Q\\Lambda^{-1/2}\\Lambda^{-1/2}Q^T\n= Q\\Lambda^{-1}Q^T\n= S^{-1}.\n\\end{equation}\\] Schließlich gilt, dass \\[\\begin{align}\n\\begin{split}\nS^{-1/2}SS^{-1/2}\n& =  Q\\Lambda^{-1/2}Q^T Q\\Lambda Q^T Q\\Lambda^{-1/2}Q^T   \\\\\n& =  Q\\Lambda^{-1/2}\\Lambda \\Lambda^{-1/2}Q^T             \\\\\n& =  Q\\Lambda \\Lambda^{-1}Q^T                             \\\\\n& =  I_m                                          \n\\end{split}\n\\end{align}\\]",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Eigenanalyse</span>"
    ]
  },
  {
    "objectID": "110-Eigenanalyse.html#sec-singulärwertzerlegung",
    "href": "110-Eigenanalyse.html#sec-singulärwertzerlegung",
    "title": "10  Eigenanalyse",
    "section": "10.3 Singulärwertzerlegung",
    "text": "10.3 Singulärwertzerlegung\nEine vielseitig einsetzbare Matrixzerlegung einer beliebigen Matrix ist die Singulärwertzerlegung. Wir sind an dieser Stelle lediglich an dem Zusammenhang von Singulärwertzerlegung und Eigenanalyse interessiert und verweisen für eine ausführliche Diskussion der Singulärwertzerlegung auf die weiterführende Literatur, beispielsweise Strang (2009). Der Begriff der Singulärwertzerlegung ist wie folgt definiert.\n\nDefinition 10.3 (Singulärwertzerlegung) \\(Y \\in \\mathbb{R}^{m \\times n}\\) sei eine Matrix. Dann heißt die Zerlegung \\[\\begin{equation}\nY = USV^T,\n\\end{equation}\\] wobei \\(U \\in \\mathbb{R}^{m \\times m}\\) eine orthogonale Matrix ist, \\(S \\in \\mathbb{R}^{m \\times n}\\) eine Diagonalmatrix ist und \\(V \\in \\mathbb{R}^{n \\times n}\\) eine orthogonale Matrix ist, Singulärwertzerlegung von \\(Y\\). Die Diagonalelemente von \\(S\\) heißen die Singulärwerte von \\(Y\\).\n\nSingulärwertzerlegungen werden auf Englisch singular value decompositions genannt und entsprechend mit SVD abgekürzt. Wir verzichten auf eine Diskussion der Berechnung einer Singulärwertzerlegung und weisen lediglich daraufhin, dass Singulärwertzerlegungen zum Beispiel in R mit der Funktion svd(), in SciyPy mit scipy.linalg.svd() und in Julia mit svd() berechnet werden können. Folgendes Theorem beschreibt den Zusammenhang zwischen Singulärwertzerlegung und Eigenanalyse und wird an vielen Stellen eingesetzt.\n\nTheorem 10.7 (Singulärwertzerlegung und Eigenanalyse)  \n\\(Y \\in \\mathbb{R}^{m \\times n}\\) sei eine Matrix und \\[\\begin{equation}\nY = USV^T\n\\end{equation}\\] sei ihre Singulärwertzerlegung. Dann gilt:\n\n\nBeweis. Wir halten zunächst fest, dass mit \\[\\begin{equation}\n\\left(YY^T\\right)^T = YY^T \\mbox{ und } \\left(Y^TY\\right)^T = Y^TY,\n\\end{equation}\\] \\(YY^T\\) und \\(Y^TY\\) symmetrische Matrizen sind und somit Orthornomalzerlegungen haben. Wir halten weiterhin fest, dass mit \\(V^TV = I_n\\), \\(U^TU = I_m\\) und \\(S^T = S\\) gilt, dass \\[\\begin{equation}\nYY^T\n= USV^T \\left(USV^T\\right)^T\n= USV^TVS^TU^T\n= USSU^T\n=: U\\Lambda U^T\n\\end{equation}\\] und \\[\\begin{equation}\nY^TY\n= \\left(USV^T\\right)^T USV^T\n= VS^TU^T US^T V^T\n=: V\\Lambda V^T\n\\end{equation}\\] ist, wobei wir \\(\\Lambda := SS\\) definiert haben. Weil das Produkt von Diagonalmatrizen wieder eine Diagonalmatrix ist, ist \\(\\Lambda\\) eine Diagonalmatrix und per Definition sind \\(U\\) und \\(V\\) orthogonale Matrizen. Wir haben also \\(YY^T\\) und \\(Y^TY\\) in Form der Orthonormalzerlegungen \\[\\begin{equation}\nYY^T = U \\Lambda U^T  \\mbox{ und } Y^TY = V \\Lambda V^T\n\\end{equation}\\] geschrieben, wobei für die Diagonalelemente von \\(\\Lambda\\) gilt, dass sie die quadrierten Werte der Diagonalemente von \\(S\\) sind.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Eigenanalyse</span>"
    ]
  },
  {
    "objectID": "110-Eigenanalyse.html#literaturhinweise",
    "href": "110-Eigenanalyse.html#literaturhinweise",
    "title": "10  Eigenanalyse",
    "section": "10.4 Literaturhinweise",
    "text": "10.4 Literaturhinweise\nDie in diesem Kapitel behandelten Konzepte werden ausführlich zum Beispiel in Searle (1982) und Strang (2009) behandelt. Die Verwendung des Präfix Eigen- für die beschriebenen Vektoren und Skalare in bezug zu einer Matrix beginnt offenbar Hilbert (1904) im Kontext der Analyse von Integralgleichungen und hat sich auch im Englischen durchgesetzt.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Eigenanalyse</span>"
    ]
  },
  {
    "objectID": "110-Eigenanalyse.html#selbstkontrollfragen",
    "href": "110-Eigenanalyse.html#selbstkontrollfragen",
    "title": "10  Eigenanalyse",
    "section": "10.5 Selbstkontrollfragen",
    "text": "10.5 Selbstkontrollfragen\n\nGeben Sie die Definition eines Eigenvektors einer quadratischen Matrix wieder.\nGeben Sie die Definition eines Eigenwerts einer quadratischen Matrix wieder.\nGeben Sie das Theorem zur Bestimmung von Eigenwerten und Eigenvektoren wieder.\nGeben Sie das Theorem zu den Eigenwerten und Eigenvektoren symmetrischer Matrizen wieder.\nGeben Sie das Theorem zur Orthonormalzerlegung einer symmetrischen Matrix wieder.\nGeben Sie die Definition der symmetrischen Quadratwurzel einer Matrix wieder.\nGeben Sie die Definition einer Singulärwertzerlegung wieder.\nGeben Sie das Theorem zum Zusammenhang von Singulärwertzerlegung und Eigenanalyse wieder.\n\n\n\n\n\nBurden, R. L., Faires, J. D., & Burden, A. M. (2016). Numerical Analysis (Tenth edition). Cengage Learning.\n\n\nGolub, G. H., & Van Loan, C. F. (2013). Matrix Computations (Fourth edition). The Johns Hopkins University Press.\n\n\nHilbert, D. (1904). Grundzüge Einer Allgemeinen Theorie Der Linearen Integralgleichungen. Nachrichten von der Gesellschaft der Wissenschaften zu Göttingen, Mathematisch-Physikalische Klasse. https://doi.org/10.1007/978-3-322-84410-1_1\n\n\nMardia, K. V., Kent, J. T., & Bibby, J. M. (1979). Multivariate Analysis. Academic Press.\n\n\nRichter, T., & Wick, T. (2017). Einführung in die Numerische Mathematik: Begriffe, Konzepte und zahlreiche Anwendungsbeispiele. Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-662-54178-4\n\n\nSearle, S. (1982). Matrix Algebra Useful for Statistics. Wiley-Interscience.\n\n\nStrang, G. (2009). Introduction to Linear Algebra. Cambridge University Press.",
    "crumbs": [
      "Mathematische Grundlagen",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Eigenanalyse</span>"
    ]
  },
  {
    "objectID": "200-Wahrscheinlichkeitstheorie.html",
    "href": "200-Wahrscheinlichkeitstheorie.html",
    "title": "Wahrscheinlichkeitstheorie",
    "section": "",
    "text": "Vorbemerkungen\nDie Wahrscheinlichkeitstheorie ist ein mathematisches Modell zur Beschreibung von und zum quantitativen Schlussfolgern über Zufallsvorgänge der Wirklichkeit (Abbildung 1). Unter Zufallsvorgängen verstehen wir dabei alle Phänomene, die von uns nicht mit absoluter Sicherheit vorhergesagt werden können, deren Ergebnis also mit Unsicherheit behaftet ist. Offensichtliche und vertraute Beispiele für Zufallsvorgänge sind das Werfen eines Würfels oder einer Münze. Allerdings ist der Begriff des Zufallsvorgangs und damit der Anwendungsbereich der Wahrscheinlichkeitstheorie als sehr viel weiter gefasst zu verstehen. Nicht mit vollständiger Sicherheit vorhersagbar und damit mit Unsicherheit behaftet sind zum Beispiel auch der Ausgang einer Wahl, das morgige Wetter, der Messwert einer EEG-Elektrode zu einem bestimmten Zeitpunkt nach Applikation eines Reizes, oder der Effekt einer Psychotherapieintervention auf den Gesundheitszustand einer Patient:in. Beginnt man darüber nachzudenken, welche Phänomene der Wirklichkeit mit Unsicherheit behaftet sind, so fällt es schwer, nichttriviale Phänomene anzugeben, hinsichtlich deren Ergebnis man vollständige Sicherheit besitzt.\n\n\n\n\n\n\nAbbildung 1: Wahrscheinlichkeitstheorie als Modell von Zufallsvorgängen. Ausgangspunkt der Wahrscheinlichkeitstheorie ist die Absicht, über einen Zufallsvorgang, also ein mit Unsicherheit behaftetes Phänomen der Wirklichkeit, logisch-quantitative Schlüsse zu ziehen. Die Repräsentation zentraler Aspekte des Zufallsvorgang mithilfe wahrscheinlichkeitstheoretischer Begrifflichkeiten bezeichnet man als Modellierung. Das wahrscheinlichkeitstheoretische Modell selbst garantiert dann im Sinne der Wahrscheinlichkeitsrechnung die Korrektheit logisch-quantitativer Schlussfolgerungen, welche zur Vorhersage von Aspekten des Zufallsvorgangs genutzt werden können.\n\n\n\nAls mathematisches Modell von Zufallsvorgängen erlaubt die Wahrscheinlichkeitstheorie insbesondere das vernunftbasierte, quantitative Schlussfolgern über Zufallsvorgänge. Dies schlägt sich primär in der sogenannten Wahrscheinlichkeitsrechnung nieder. Quantitative Schlussfolgerungen der Wahrscheinlichkeitsrechnung haben beispielsweise folgende Form: Wenn ich annehme, dass das Ereignis \\(A\\) mit Wahrscheinlichkeit \\(x\\) und Ereignis \\(B\\) mit Wahrscheinlichkeit \\(y\\) eintritt, dann ergibt sich für die Wahrscheinlichkeit von Ereignis \\(C\\) eine Wahrscheinlichkeit von \\(z\\). Dabei ist der Schluss auf die Wahrscheinlichkeit von \\(C\\) logisch-mathematisch abgesichert, in dem Sinne wie zum Beispiel logisch-mathematisch abgesichert ist, dass \\(1+1=2\\) ist. Ob die Annahmen hinsichtlich der Wahrscheinlichkeiten von \\(A\\) und \\(B\\) aber den Gegebenheiten des Zufallsvorgangs in der Wirklichkeit entsprechen, darüber macht die Wahrscheinlichkeitstheorie keine Aussagen.\nDie Wahrscheinlichkeitstheorie selbst bedient sich dabei der mathematischen Theorie der Mengen und Funktionen. Spätestens seit Kolmogoroff (1933) herrscht dabei ein axiomatischer Zugang vor: Man fragt in der Wahrscheinlichkeitstheorie selbst nicht, was denn eine Wahrscheinlichkeit sei oder inwieweit die Vorhersagen der Wahrscheinlichkeitstheorie mit der Wirklichkeit übereinstimmen, sondern versucht ein in sich schlüssiges formal-mathematisches System von unbegründeten, aber intuitiv plausiblen, Grundannahmen und ihren Folgerungen zu entwickeln. Ausgangspunkt dieser Entwicklung ist das Wahrscheinlichkeitsraummodell eines Zufallsvorgangs, das wir in 11  Wahrscheinlichkeitsräume einführen werden. In der Tat gibt es neben dem formal-mathematischen System der Wahrscheinlichkeitstheorie bis heute mathematisch-philosophische Diskussionen darüber, was genau denn unter dem Begriff der “Wahrscheinlichkeit eines Ereignisses” zu verstehen ist (vgl. Hájek (2019)). Dabei sind grob gesagt zwei etwas gegensätzliche Interpretationen vorherrschend, die sogenannte Frequentistische Interpretation und die sogenannte Bayesianische Interpretation.\nNach der Frequentistischen Interpretation ist die Wahrscheinlichkeit eines Ereignisses die idealisierte relative Häufigkeit, mit der ein Ereignis unter den gleichen äußeren Bedingungen einzutreten pflegt. Zum Beispiel ist die Frequentistische Interpretation der Aussage “Mit einer Wahrscheinlichkeit von 1/6 zeigt der Würfel im nächsten Wurf eine 2” die folgende: “Wenn man einen Würfel unendlich oft werfen würde und dabei die relative Häufigkeit des Ereignisses, dass der Würfel eine 2 zeigt, bestimmen würde, dann wäre diese relative Häufigkeit gleich 1/6”. Man beachte bei dieser Interpretation, dass man de-facto die Wahrscheinlichkeit eines Ereignisses nicht empirisch bestimmen kann, da man einen Würfel nicht unendlich oft werfen kann. Natürlich kann man die Wahrscheinlichkeit in dieser Interpretation aber empirisch schätzen. Schätzvorgänge selbst wiederrum sind allerdings kein Teil der Wahrscheinlichkeitstheorie, sondern der Frequentistischen oder Bayesianischen Inferenz.\nNach der Bayesianischen Interpretation ist die Wahrscheinlichkeit eines Ereignisses der Grad der Sicherheit, den eine Beobachter:in aufgrund ihrer subjektiven Einschätzung der Lage dem Eintreten des Ereignisses \\(A\\) zumisst. Zum Beispiel ist die Bayesianische Interpretation der Aussage “Mit einer Wahrscheinlichkeit von 1/6 zeigt der Würfel im nächsten Wurf eine Zwei” dann etwa die folgende: “Basierend auf meiner eigenen und der tradierten Erfahrung mit dem Werfen eines Würfels bin ich mir zu 16.6% sicher, dass der Würfel beim nächsten Wurf eine Zwei zeigt.”\nIn Modellen von tatsächlich zumindest unter ähnlichen Umständen wiederholbaren Zufallsvorgängen wie dem Werfen eines Würfels ist der Unterschied zwischen Frequentistischer und Bayesianischer Interpretation oft eher subtil. Es gibt aber wie oben angedeutet viele Zufallsvorgänge, die mit Wahrscheinlichkeiten beschrieben werden können, bei denen aufgrund ihrer Einmaligkeit eine Frequentistische Interpretation nicht angemessen ist. Zum Beispiel machen Aussagen der Form “Die Wahrscheinlichkeit dafür, dass die weltweiten Hitzerekorde im Jahr 2023 nicht auf den Klimawandel zurückzuführen sind, ist kleiner als 0.01” (vgl. Philip et al. (2020)) nur unter der Bayesianischen Interpretation Sinn, da es sich bei den Wetteraufzeichnungen des Jahres 2023 um ein einmaliges, nicht wiederholbares Ereignis handelt.\nObwohl also die Interpretation des Begriffes der Wahrscheinlichkeit durchaus nicht eindeutig ist, unterscheiden sich die formalen Definitionen und Rechenregeln für Wahrscheinlichkeiten nicht. Sowohl die Frequentistische als auch die Bayesianische Inferenz, auf die wir an späterer Stelle eingehen, haben mit der Wahrscheinlichkeitstheorie also ein identisches mathematisches Bezugssystem und gemeinsames Fundament.\n\n\n\n\nHájek, A. (2019). Interpretations of Probability. In E. N. Zalta (Hrsg.), The Stanford Encyclopedia of Philosophy (Fall 2019). Metaphysics Research Lab, Stanford University.\n\n\nKolmogoroff, A. (1933). Grundbegriffe der Wahrscheinlichkeitsrechnung. Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-642-49888-6\n\n\nPhilip, S., Kew, S., Van Oldenborgh, G. J., Otto, F., Vautard, R., Van Der Wiel, K., King, A., Lott, F., Arrighi, J., Singh, R., & Van Aalst, M. (2020). A Protocol for Probabilistic Extreme Event Attribution Analyses. Advances in Statistical Climatology, Meteorology and Oceanography, 6(2), 177–203. https://doi.org/10.5194/ascmo-6-177-2020",
    "crumbs": [
      "Wahrscheinlichkeitstheorie"
    ]
  },
  {
    "objectID": "201-Wahrscheinlichkeitsräume.html",
    "href": "201-Wahrscheinlichkeitsräume.html",
    "title": "11  Wahrscheinlichkeitsräume",
    "section": "",
    "text": "11.1 Definition und erste Eigenschaften\nWir beginnen mit der Definition des Wahrscheinlichkeitsraummodells nach Kolmogoroff (1933), das wir dann nachfolgend in seinen Einzelteilen aus Frequentistischer Perspektive erläutern wollen.",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Wahrscheinlichkeitsräume</span>"
    ]
  },
  {
    "objectID": "201-Wahrscheinlichkeitsräume.html#sec-definition-und-erste-eigenschaften",
    "href": "201-Wahrscheinlichkeitsräume.html#sec-definition-und-erste-eigenschaften",
    "title": "11  Wahrscheinlichkeitsräume",
    "section": "",
    "text": "Definition 11.1 (Wahrscheinlichkeitsraum) Ein Wahrscheinlichkeitsraum ist ein Triple \\((\\Omega, \\mathcal{A}, \\mathbb{P})\\), wobei\n\n\\(\\Omega\\) eine beliebige nichtleere Menge von Ergebnissen \\(\\omega\\) ist und Ergebnismenge heißt,\n\\(\\mathcal{A}\\) eine Menge von Teilmengen von \\(\\Omega\\) mit den Eigenschaften\n\n\\(\\Omega \\in \\mathcal{A}\\),\nfür alle \\(A\\in \\mathcal{A}\\) gilt, dass auch \\(A^c := \\Omega \\setminus A \\in \\mathcal{A}\\) ist,\naus \\(A_1,A_2,... \\in \\mathcal{A}\\) folgt, dass auch \\(\\cup_{i=1}^\\infty A_i \\in \\mathcal{A}\\) ist,ist, \\(\\sigma\\)-Algebra auf \\(\\Omega\\) genannt wird und Ereignissystem heißt,\n\n\\(\\mathbb{P}\\) eine Abbildung der Form \\(\\mathbb{P}:\\mathcal{A} \\to [0,1]\\) mit den Eigenschaften\n\n\\(\\mathbb{P}(A) \\ge 0\\) für alle \\(A \\in \\mathcal{A}\\) (Nicht-Negativität),\n\\(\\mathbb{P}(\\Omega) = 1\\) (Normiertheit) und\n\\(\\mathbb{P}(\\cup_{i=1}^\\infty A_i) = \\sum\\limits_{i=1}^\\infty \\mathbb{P}(A_i)\\) für paarweise disjunkte \\(A_i \\in \\mathcal{A}\\) (\\(\\sigma\\)-Additivität) ist und Wahrscheinlichkeitsmaß heißt.\n\n\n\n\nErgebnismenge und Mechanik\nWir beginnen mit Erläuterungen zum Begriff der Ergebnismenge \\(\\Omega\\) und der impliziten Mechanik des Wahrscheinlichkeitsraummodells. Um den Einstieg zu erleichtern betrachten wir im Folgenden zunächst vor allem endliche Wahrscheinlichkeitsräume, bei denen die Kardinalität von \\(\\Omega\\) nicht unendlich groß ist. Es sei also \\(|\\Omega|&lt;\\infty\\), \\(\\Omega\\) habe also nur endlich viele Elemente. Zum Modellieren des Werfen eines Würfels könnte man zum Beispiel \\(\\Omega := \\{1,2,3,4,5,6\\}\\) definieren.\nHinter der formalen Definition des Wahrscheinlichkeitsraummodells stehen folgende Frequentistisch-geprägten Annahmen über seine Mechanik als Modell eines Zufallsvorgangs. Wir stellen uns zunächst sequentielle Durchgänge eines Zufallsvorgangs vor, also zum Beispiel das wiederholte Werfen eines Würfels. Nach Annahme des Wahrscheinlichkeitsraummodells wird in jedem dieser Durchgänge genau ein \\(\\omega\\) aus \\(\\Omega\\) realisiert, also als tatsächlich vorliegend ausgewählt. Wirft man zum Beispiel einen Würfel und fällt eine Zwei, so sagt man, dass eine Zwei realisiert wurde. Die Wahrscheinlichkeit, mit der ein \\(\\omega\\) aus \\(\\Omega\\) in einem Durchgang realisiert wird, wird durch den Wert \\(\\mathbb{P}(\\{\\omega\\}) \\in [0,1]\\) beschrieben. Ist zum Beispiel \\(\\mathbb{P}(\\{\\omega\\}) = 1\\), so wird dieses \\(\\omega\\) in jedem Durchgang des Zufallsvorgangs realisiert; ist \\(\\mathbb{P}(\\{\\omega\\}) = 0\\), so wird dieses \\(\\omega\\) in keinem Durchgang des Zufallsvorgangs realisiert; und ist \\(\\mathbb{P}(\\{\\omega\\}) = 1/2\\), so wird \\(\\omega\\) in etwa der Hälfte der Durchgänge des Zufallsvorgangs realisiert. Beim Modell des Werfens eines fairen Würfels nimmt man üblicherweise \\(\\mathbb{P}(\\{\\omega\\}) = 1/6\\) für alle \\(\\omega \\in \\Omega\\) an. Hier könnte zum Beispiel im ersten Durchgang eine Vier realisiert werden, im zweiten Durchgang eine Eins, im dritten Durchgang eine Fünf, dann vielleicht wieder eine Vier und so weiter.\n\n\nEreignisse und Ereignissystem\nDen Begriff des Ereignisses \\(A \\in \\mathcal{A}\\) stellt man sich am besten als konzeptionelle Zusammenfassung ein oder mehrerer Ergebnisse vor. Beim Werfen eines Würfels sind mögliche Ereignisse zum Beispiel “Es fällt eine gerade Augenzahl”, das heißt \\(\\omega \\in \\{2,4,6\\}\\); “Es fällt eine Augenzahl größer als Zwei”, das heißt \\(\\omega \\in \\{3,4,5,6\\}\\); oder etwa “Es fällt eine Eins oder eine Fünf”, das heißt \\(\\omega \\in \\{1,5\\}\\). Man beachte, dass zum Beispiel das Ereignis “Es fällt eine gerade Augenzahl” vor dem Hintergrund der Mechanik des Wahrscheinlichkeitsraums genau dann eintritt, wenn in einem Durchgang des Zufallsvorgangs das realisierte \\(\\omega\\) ein Element der Menge \\(\\{2,4,6\\}\\) ist, wenn also zum Beispiel eine Vier fällt. Man mag das Eintreten des Ereignisses “Es fällt eine Augenzahl größer als Zwei” also auch lesen als “In einem Durchgang des Zufallsvorgangs wird ein Element von \\(\\{3,4,5,6\\}\\) realisiert”, d.h. konkret fällt entweder eine Drei, eine Vier, eine Fünf oder eine Sechs. Natürlich sind auch die Ergebnisse \\(\\omega \\in \\Omega\\) selbt mögliche Ereignisse, so dass zum Beispiel folgende Interpretationen gelten: Das Ereignis “Es fällt eine Eins” entspricht der Realisation \\(\\omega \\in \\{1\\}\\) und das Ereignis “Es fällt eine Sechs” entspricht der Realisation \\(\\omega \\in \\{6\\}\\). Betrachtet man in diesem Zusammenhang ein Ergebnis \\(\\omega \\in \\Omega\\) als Ereignis, so nennt man es Elementarereignis und schreibt es als einelementige Menge \\(\\{\\omega\\}\\).\nInsgesamt entspricht dieses Vorgehen zur Beschreibung zufälliger Ereignisse dem inhärenten Ziel der Definition des Wahrscheinlichkeitsraums. Kolmogoroff (1933) schreibt dazu “Wir haben die eigentlichen Objekte unserer weiteren Betrachtungen - die zufälligen Ereignisse - als Mengen definiert.” Dies hat den Vorteil, dass Mengen mathematische Objekte sind, mit denen mathematisch gearbeitet werden kann und damit ein Aspekt der Wirklichkeit, ein “zufälliges Ereignis”, in den Modellbereich der Mathematik übersetzt wurde.\nAlleiniger Sinn des Ereignissystems \\(\\mathcal{A}\\) ist es nun, alle Ereignisse, die sich basierend auf einer gegebenen Ergebnismenge bei Auswahl eines \\(\\omega \\in \\Omega\\) ergeben können, mathematisch zu repräsentieren. Es soll also keine Ereignisse in der Wirklichkeit geben, die nicht im Vorhinein im Wahrscheinlichkeitsraummodells des Zufallsvorgangs mitgedacht wurden. Wäre dies der Fall, so wäre das Modell defizitär, da es für bestimmte, in der Wirklichkeit eintretende Ergeignisse keine Wahrscheinlichkeiten angeben könnte. Das Ereignissystem \\(\\mathcal{A}\\) soll also die vollständige Menge aller möglichen Ereignisse bei vorgegebenem \\(\\Omega\\) sein. Die Forderung, dass \\(\\mathcal{A}\\) zu diesem Zweck die sogenannten \\(\\sigma\\)-Algebra Kriterien erfüllen soll, begründet sich dabei intuitiv wie folgt.\n\nEs soll zunächst einmal sichergestellt sein, dass \\(\\omega \\in \\Omega\\) für ein beliebiges \\(\\omega\\), dass also irgendein Ergebnis realisiert wird, eines der möglichen Ereignisse ist. Dies entspricht der Eigenschaft \\(\\Omega \\in \\mathcal{A}\\).\nZu jedem Ereignis soll es weiterhin auch möglich sein, dass dieses Ereignis gerade nicht eintritt. Dies entspricht der Eigenschaft, dass aus \\(A \\in \\mathcal{A}\\) folgen soll, dass \\(A^c := \\Omega \\setminus A\\) auch in \\(\\mathcal{A}\\) ist. Dies impliziert insbesondere auch, dass \\(\\emptyset = \\Omega \\setminus \\Omega \\in \\mathcal{A}\\). Ein Ereignis ist also, dass kein Elementarereignis eintritt, allerdings passiert dies nur mit Wahrscheinlichkeit Null, \\(\\mathbb{P}(\\emptyset) = 0\\). Es tritt also sicher immer zumindest ein Elementarereignis ein.\nSchließlich soll die Kombination von Ereignissen auch immer ein Ereignis sein. Bei der Modellierung des Werfen eines Würfels soll also zum Beispiel neben den Ereignissen “Es fällt eine gerade Zahl” und “Es fällt eine Zahl größer Zwei” auch das Ereignis “Es fällt eine gerade Zahl und/oder diese Zahl ist größer als Zwei” ein Ereignis sein. Dies entspricht, in allgemeinster Form, dass aus \\(A_1,A_2,... \\in \\mathcal{A}\\) folgen soll, dass auch \\(\\cup_{i=1}^\\infty A_i \\in \\mathcal{A}\\) ist.\n\nWenn auch die Begriffe des Ereignissystems und der \\(\\sigma\\)-Algebra etwas abstrakt anmuten mögen, so stellt ihre Definition in der praktischen Modellierung von Zufallsvorgängen meist keine großen Herausforderung dar, da sowohl für endliche Ergebnismengen als auch für unendliche (abzählbare und überabzählbare) Ergebnismengen passende Ereignissysteme schon lange bekannt sind. So erfüllt bei Ergebnismengen mit endlicher Kardinalität die Potenzmenge der Ergebnismenge immer die Anforderungen eines Ereignissystems und kann immer zur Modellformulierung eines Zufallsvorgangs mit endlicher Ergebnismenge genutzt werden. Dies ist die Aussage folgenden Theorems.\n\nTheorem 11.1 (Ereignissystem bei endlicher Ergebnismenge) \\(\\Omega := \\{\\omega_1,\\omega_2,...,\\omega_n\\}\\) mit \\(n \\in \\mathbb{N}\\) sei eine endliche Menge. Dann ist die Potenzmenge \\(\\mathcal{P}(\\Omega)\\) von \\(\\Omega\\) eine \\(\\sigma\\)-Algebra auf \\(\\Omega\\) und damit ein geeignetes Ereignisssytem im Wahrscheinlichkeitsraummodell.\n\n\nBeweis. Die Potenzmenge von \\(\\Omega\\) ist die Menge aller Teilmengen von \\(\\Omega\\). Wir überprüfen die \\(\\sigma\\)-Algebra Eigenschaften. Zunächst gilt, dass \\(\\Omega\\) selbst eine der Teilmengen von \\(\\Omega\\) ist, also ist die erste \\(\\sigma\\)-Algebra Eigenschaft erfüllt. Sei nun \\(A\\) eine Teilmenge von \\(\\Omega\\). Dann ist auch \\(A^c = \\Omega \\setminus\nA\\) eine Teilmenge von \\(\\Omega\\) und somit ist auch die zweite \\(\\sigma\\)-Algebra Eigenschaft erfüllt. Schließlich betrachten wir die Vereinigung von \\(n\\) Teilmengen \\(A_1, A_2, ...,A_n \\subseteq \\Omega\\). Dann ist \\(\\cup_{i=1}^n A_i\\) die Menge der \\(\\omega \\in \\Omega\\) für die gilt, dass \\(\\omega \\in A_1\\) und/oder \\(\\omega \\in A_2\\) … und/oder \\(\\omega \\in A_n\\). Da für alle diese \\(\\omega\\) gilt, dass \\(\\omega \\in \\Omega\\) ist also auch \\(\\cup_{i=1}^n A_i\\) eine Teilmenge von \\(\\Omega\\) und damit auch die dritte \\(\\sigma\\)-Algebra Eigenschaft erfüllt. Die Potenzmenge erfüllt also die geforderten Eigenschaften an ein Ereignissystem.\n\nBei überabzählbaren Ergebnismengen wie den reellen Zahlen \\(\\mathbb{R}\\) oder dem \\(n\\)-dimensionalen reellen Raum \\(\\mathbb{R}^n\\) ist die Konstruktion eines geeigneten Ereignissystems komplexer, so dass wir in dieser Hinsicht für formale Entwicklungen auf die weiterführende Literatur verweisen wollen (z.B. Meintrup & Schäffler (2005), Schmidt (2009)). Mit der auf Borel (1898) zurückgehenden sogenannten Borelschen \\(\\sigma\\)-Algebra ist jedoch ein Mengensystem bekannt, das den Anforderungen einer \\(\\sigma\\)-Algebra auf überabzähbaren Ergebnismengen genügt. Wir bezeichnen die Borelsche \\(\\sigma\\)-Algebra auf \\(\\mathbb{R}^n\\) mit \\(\\mathcal{B}(\\mathbb{R})\\) und die Borelsche \\(\\sigma\\)-Algebra auf \\(\\mathbb{R}^n\\) mit \\(\\mathcal{B}(\\mathbb{R}^n)\\). Als Menge von Teilmengen von \\(\\mathbb{R}\\) bzw. \\(\\mathbb{R}^n\\) enthalten \\(\\mathcal{B}(\\mathbb{R})\\) bzw. \\(\\mathcal{B}(\\mathbb{R}^n)\\) alle Mengen, an denen man hinsichtlich ihrer durch \\(\\mathbb{P}\\) zugeordneten Wahrscheinlichkeit interessiert sein mag. Intuitiv mag man sich die Borelschen \\(\\sigma\\)-Algebren \\(\\mathcal{B}(\\mathbb{R})\\) und \\(\\mathcal{B}(\\mathbb{R}^n)\\) also als die Potenzmengen von \\(\\mathbb{R}\\) bzw. \\(\\mathbb{R}^n\\) denken, auch wenn dies formal falsch ist. Tatsächlich enthält die Borelsche \\(\\sigma\\)-Algebra nur Teilmengen, die durch abzählbare Mengenoperationen generiert werden, nicht aber durch überabzählbare.\nInsgesamt ergibt sich also folgendes Vorgehen zur Auswahl von Ereignissystemen in Abhängigkeit von der Ergebnismenge \\(\\Omega\\). Ist \\(\\Omega\\) endlich, so wählt man als Ereignissystem \\(\\mathcal{A}\\) die Potenzmenge \\(\\mathcal{P}(\\Omega)\\) von \\(\\Omega\\). Ist \\(\\Omega\\) gegeben durch \\(\\mathbb{R}\\), so wählt man als Ereignissystem \\(\\mathcal{A}\\) die Borelsche \\(\\sigma\\)-Algebra \\(\\mathcal{B}(\\mathbb{R})\\). Ist \\(\\Omega\\) schließlich gegeben durch \\(\\mathbb{R}^n\\), so wählt man für \\(\\mathcal{A}\\) die Borelsche \\(\\sigma\\)-Algebra \\(\\mathcal{B}(\\mathbb{R}^n)\\). In Spezialfällen und für sehr spezielle Ergebnismengen \\(\\Omega\\) mag man von diesem Vorgehen abweichen wollen, allgemein decken die drei betrachteten Fälle jedoch die meisten Anwendungen ab.\n\n\nWahrscheinlichkeitsmaß \\(\\mathbb{P}\\)\nMit \\(\\Omega\\) und \\(\\mathcal{A}\\), die in Tupleform \\((\\Omega,\\mathcal{A})\\) auch als Messraum bezeichnet werden, haben wir bisher die strukturelle Basis eines Wahrscheinlichkeitsraummodells genauer betrachtet. Viele Wahrscheinlichkeitsräume, zum Beispiel für Zufallsvorgänge die reellen Zahlen betreffend, sind hinsichtlich ihres Messraums identisch. Das Wahrscheinlichkeitsmaß \\(\\mathbb{P}\\) nun repräsentiert die probabilistischen Charakteristika eines Wahrscheinlichkeitsraummodells und formt damit die funktionelle Basis eines Wahrscheinlichkeitsraummmodells. Wir werden im Folgenden, insbesondere nach Einführung der Begriffe der Zufallsvariablen und Zufallsvektoren, sehr viele verschiedene Wahrscheinlichkeitsmaße kennenlernen. An dieser Stelle wollen wir zunächst nur allgemeine Eigenschaften von Wahrscheinlichkeitsmaßen betrachten.\nMit der Definition \\[\\begin{equation}\n\\mathbb{P}: \\mathcal{A} \\to [0,1], A \\mapsto \\mathbb{P}(A)\n\\end{equation}\\] gilt zunächst einmal, dass ein Wahrscheinlichkeitsmaß auf einer Menge von Mengen definiert ist und den Elementen dieser Menge, also den Mengen \\(A \\in \\mathcal{A}\\), Wahrscheinlichkeiten, also Werte im Intervall \\([0,1]\\), zuordnet. Natürlich gilt mit \\(\\{\\omega\\} \\in \\mathcal{A}\\) für alle \\(\\omega \\in \\Omega\\), dass \\(\\mathbb{P}\\) auch den Elementareignissen Wahrscheinlichkeiten zuordnet, aber eben nicht nur. Wir betonen auch, dass nach Definition die Wahrscheinlichkeit \\(\\mathbb{P}(A)\\) eines Ereignisses \\(A \\in \\mathcal{A}\\) eine Zahl im Intervall \\([0,1]\\) ist und nicht etwa eine Prozentzahl oder ein Verhältnis. Wir wollen nachfolgend die definierenden Eigenschaften der Nicht-Negativität, der Normiertheit und der \\(\\sigma\\)-Additivität von \\(\\mathbb{P}\\) näher beleuchten.\nDie Nicht-Negativität \\(\\mathbb{P}(A) \\ge 0\\) für alle \\(A \\in \\mathcal{A}\\) ist natürlich in der Definition \\([0,1]\\) der Zielmenge von \\(\\mathbb{P}\\) implizit. Tatsächlich ist die Abbildungsform von \\(\\mathbb{P}\\) eine von uns vorgenommene Ergänzung der Formulierung von Kolmogoroff (1933), die der Klarheit dienen soll. Formal folgt die Form der Zielmenge von \\(\\mathbb{P}\\) eigentlich aus den definierenden Eigenschaften der Nicht-Negativität, Normiertheit, und \\(\\sigma\\)-Additivität von \\(\\mathbb{P}\\).\nDie Normiertheit \\(\\mathbb{P}(\\Omega) = 1\\) entspricht der Tatsache, dass in jedem Durchgang eines Zufallsvorgangs sicher gilt, dass ein realisiertes \\(\\omega\\) ein Element von \\(\\Omega\\) ist. In jedem Durchgang eines Zufallsvorgangs tritt also ein Elementarereignis ein und, je nach Beschaffenheit des Messraums, noch viele weitere. Beim Modell des Werfen eines Würfels gilt also, dass das in einem Durchgang des Zufallsvorgangs realisierte Ergebnis/Elementarereignis mit Wahrscheinlichkeit \\(1\\) ein Element von \\(\\Omega := \\{1,2,3,4,5,6\\}\\) ist. Ist das realisierte Ergebnis zum Beispiel eine Eins, so treten neben dem Ereignis “Es fällt eine Eins” auch noch die Ereignisse “Eine ungerade Zahl fällt”, “Eine Zahl kleiner als Drei fällt”, “Eine ungerade Zahl kleiner als Drei fällt” und viele weitere ein.\nDie \\(\\sigma\\)-Additivität des Wahrscheinlichkeitsmaßes \\(\\mathbb{P}\\) schließlich bildet das Fundament der Wahrscheinlichkeitsrechnung, also die Grundlage für das Rechnen mit Wahrscheinlichkeiten. Die \\(\\sigma\\)-Additivität von \\(\\mathbb{P}\\) erlaubt es nämlich, aus bereits bekannten Ereigniswahrscheinlichkeiten die Wahrscheinlichkeiten anderer Ereignisse zu berechnen. Man kann damit basierend auf einer Definition von \\(\\Omega, \\mathcal{A}\\) und \\(\\mathbb{P}\\) also Wahrscheinlichkeiten für alle möglichen Ereignisse eines Wahrscheinlichkeitsraummodells berechnen. Ob diese Wahrscheinlichkeiten nun aber tatsächlich etwas mit den realen Ereignissen bezüglich eines Zufallsvorgangs der Wirklichkeit zu tun haben, kommt darauf an, ob die Modellierung einigermaßen gelungen ist oder nicht. Dabei werden berechnete Wahrscheinlichkeiten aber zumindest rational, also nach den Regeln der Vernunft, d.h. der Logik und der Mathematik, bestimmt. Insgesamt erlaubt das Wahrscheinlichkeitsmodell damit das schlussfolgernde Nachdenken über mit Unsicherheit behaftete Phänomene.\nWir wollen abschließend das auf der \\(\\sigma\\)-Addivitität von \\(\\mathbb{P}\\) beruhende Rechnen mit Wahrscheinlichkeiten noch an zwei grundlegenden Beispielen verdeutlichen.\nDas erste Beispiel besagt, dass die Wahrscheinlichkeit dafür, dass das in einem Durchgang eines Zufallsvorgangs realisierte \\(\\omega\\) kein Element der Ergebnismenge ist, gleich Null ist.\n\nTheorem 11.2 (Wahrscheinlichkeit des unmöglichen Ereignisses) \\((\\Omega, \\mathcal{A}, \\mathbb{P})\\) sei ein Wahrscheinlichkeitsraum. Dann gilt \\[\\begin{equation}\n\\mathbb{P}(\\emptyset) = 0.\n\\end{equation}\\]\n\n\nBeweis. Für \\(i = 1,2,...\\) sei \\(A_i := \\emptyset\\). Dann ist \\(A_1,A_2,...\\) eine Folge disjunkter Ereignisse, weil gilt, dass \\(\\emptyset \\cap \\emptyset = \\emptyset\\) und es ist \\(\\cup_{i=1}^\\infty A_i = \\emptyset\\). Mit der \\(\\sigma\\)-Additivität von \\(\\mathbb{P}\\) folgt dann, dass \\[\\begin{equation}\n\\mathbb{P}(\\emptyset)\n= \\mathbb{P}\\left(\\cup_{i=1}^\\infty A_i\\right)\n= \\sum_{i=1}^\\infty \\mathbb{P}\\left(A_i\\right)\n= \\sum_{i=1}^\\infty \\mathbb{P}\\left(\\emptyset\\right).\n\\end{equation}\\] Das unendliche Aufaddieren der Zahl \\(\\mathbb{P}(\\emptyset) \\in [0,1]\\) soll also wieder \\(\\mathbb{P}(\\emptyset)\\) ergeben. Dies ist aber nur möglich, wenn \\(\\mathbb{P}(\\emptyset) = 0\\).\n\nMan beachte, dass hier intuitiv natürlich eine mögliche Unzulänglichkeit des Wahrscheinlichkeitsraums als Modell für Zufallsvorgänge in der Wirklichkeit auftritt: Fällt beim Würfelspiel der Würfel zum Beispiel unerreichbar unter das Sofa, so ist ein Elementarereignis \\(\\omega \\notin \\Omega\\) eingetreten, obwohl seine modellierte Wahrscheinlichkeit gleich Null ist.\nAls zweites Beispiel wollen wir zeigen, dass die \\(\\sigma\\)-Additivät, die in der Definition des Wahrscheinlichkeitsraums (nur) für die Vereinigung unendlich vieler disjunkter Ereignisse definiert ist, die \\(\\sigma\\)-Additivät endlich vieler disjunkter Ereignisse, wie sie in in der Anwendung oft vorkommen, impliziert.\n\nTheorem 11.3 (\\(\\sigma\\)-Additivität bei endlichen Folgen disjunkter Ereignisse) \\((\\Omega, \\mathcal{A}, \\mathbb{P})\\) sei ein Wahrscheinlichkeitsraum und \\(A_1,...,A_n\\) sei eine endliche Folge paarweise disjunkter Ereignisse. Dann gilt \\[\\begin{equation}\n\\mathbb{P}(\\cup_{i=1}^n A_i) = \\sum_{i=1}^n \\mathbb{P}(A_i).\n\\end{equation}\\]\n\n\nBeweis. Wir betrachten eine unendliche Folge von paarweise disjunkten Ereignissen \\(A_1, A_2, ...\\) wobei für ein \\(n\\in \\mathbb{N}\\) gelten soll, dass \\(A_i := \\emptyset\\) für \\(i&gt;n\\). Dann gilt mit der \\(\\sigma\\)-Additivität von \\(\\mathbb{P}\\) zunächst, dass \\[\\begin{equation}\n\\mathbb{P}\\left(\\cup_{i=1}^n A_i\\right)\n= \\mathbb{P}\\left(\\cup_{i=1}^\\infty A_i\\right)\n= \\sum_{i=1}^\\infty \\mathbb{P}\\left(A_i\\right)\n= \\sum_{i=1}^n \\mathbb{P}\\left(A_i\\right) + \\sum_{i=n+1}^\\infty \\mathbb{P}\\left(A_i\\right).\n\\end{equation}\\] Mit \\(\\mathbb{P}\\left(A_i\\right) = \\mathbb{P}(\\emptyset) = 0\\) für \\(i = n+1, n+2,...\\) folgt dann direkt \\[\\begin{equation}\n\\mathbb{P}\\left(\\cup_{i=1}^n A_i\\right)\n= \\sum_{i=1}^n \\mathbb{P}\\left(A_i\\right) + 0\n= \\sum_{i=1}^n \\mathbb{P}\\left(A_i\\right).\n\\end{equation}\\]",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Wahrscheinlichkeitsräume</span>"
    ]
  },
  {
    "objectID": "201-Wahrscheinlichkeitsräume.html#sec-wahrscheinlichkeitsfunktionen",
    "href": "201-Wahrscheinlichkeitsräume.html#sec-wahrscheinlichkeitsfunktionen",
    "title": "11  Wahrscheinlichkeitsräume",
    "section": "11.2 Wahrscheinlichkeitsfunktionen",
    "text": "11.2 Wahrscheinlichkeitsfunktionen\nIn diesem Abschnitt wollen wir mit den Wahrscheinlichkeitsfunktionen eine erste Möglichkeit kennenlernen, für Wahrscheinlichkeitsräume mit endlicher Ergebnismenge Wahrscheinlichkeitsmaße festzulegen. In Kapitel 11.3 nutzen wir dieses Hilfsmittel intensiv, um erste Beispiele für die Modellierung von Zufallsvorgängen mithilfe von Wahrscheinlichkeitsräumen geben zu können. Wir definieren den Begriff der Wahrscheinlichkeitsfunktion wie folgt.\n\nDefinition 11.2 (Wahrscheinlichkeitsfunktion) \\(\\Omega\\) sei eine endliche Menge. Dann heißt eine Funktion \\(\\pi:\\Omega \\to [0,1]\\) Wahrscheinlichkeitsfunktion, wenn gilt, dass \\[\\begin{equation}\n\\sum_{\\omega \\in \\Omega} \\pi(\\omega) = 1.\n\\end{equation}\\] Sei weiterhin \\(\\mathbb{P}\\) ein Wahrscheinlichkeitsmaß. Dann heißt die durch \\[\\begin{equation}\n\\pi : \\Omega \\to [0,1], \\omega \\mapsto \\pi(\\omega) := \\mathbb{P}(\\{\\omega\\})\n\\end{equation}\\] definierte Funktion Wahrscheinlichkeitsfunktion von \\(\\mathbb{P}\\) auf \\(\\Omega\\).\n\nWir merken an, dass weil \\(\\mathbb{P}\\) per Definition \\(\\sigma\\)-additiv ist, insbesondere auch gilt, dass \\[\\begin{equation}\n\\mathbb{P}(\\Omega)\n= \\mathbb{P}(\\cup_{\\omega \\in \\Omega}\\{\\omega\\})\n= \\sum_{\\omega \\in \\Omega}\\mathbb{P}(\\{\\omega\\})\n= \\sum_{\\omega \\in \\Omega}\\pi(\\omega)\n= 1.\n\\end{equation}\\] Zur Konstruktion von Wahrscheinlichkeitsmaßen durch Wahrscheinlichkeitsfunktionen stellt folgendes Theorem die formale Basis bereit. Es besagt insbesondere, dass bei endlichem \\(\\Omega\\) die Wahrscheinlichkeiten aller möglichen Ereignisse aus den Wahrscheinlichkeiten der Elementarereignisse \\(\\pi(\\omega)\\) berechnet werden können.\n\nTheorem 11.4 \\((\\Omega, \\mathcal{A}, \\mathbb{P})\\) sei ein Wahrscheinlichkeitsraum mit endlicher Ergebnismenge und \\(\\pi: \\Omega \\to [0,1]\\) sei eine Wahrscheinlichkeitsfunktion. Dann existiert ein Wahrscheinlichkeitsmaß \\(\\mathbb{P}\\) auf \\(\\Omega\\) mit \\(\\pi\\) als Wahrscheinlichkeitsfunktion von \\(\\mathbb{P}\\). Dieses Wahrscheinlichkeitsmaß ist definiert als \\[\\begin{equation}\n\\mathbb{P} : \\mathcal{A} \\to [0,1], A \\mapsto \\mathbb{P}(A) := \\sum_{\\omega \\in A} \\pi(\\omega).\n\\end{equation}\\]\n\n\nBeweis. Wir überprüfen zunächst die Wahrscheinlichkeitsmaßeigenschaften von \\(\\mathbb{P}\\). Weil \\(\\pi(\\omega) \\in [0,1]\\) für alle \\(\\omega \\in \\Omega\\), gilt auch immer \\(\\sum_{\\omega \\in A} \\pi(\\omega) \\ge 0\\) und damit die Nicht-Negativität von \\(\\mathbb{P}\\). Ferner folgt wie oben gesehen mit der Normiertheit von \\(\\pi\\) direkt die Normiertheit von \\(\\mathbb{P}\\). Seien nun \\(A_1, A_2,... \\in \\mathcal{A}\\). Dann gilt \\[\\begin{equation}\n\\mathbb{P}\\left(\\cup_{i=1}^\\infty A_i \\right)\n= \\sum_{\\omega \\in \\cup_{i=1}^\\infty A_i} \\pi(\\omega)\n= \\sum_{i = 1}^\\infty \\sum_{\\omega \\in A_i} \\pi(\\omega)\n= \\sum_{i = 1}^\\infty \\mathbb{P}(A_i).\n\\end{equation}\\] und damit die \\(\\sigma\\)-Addivität von \\(\\mathbb{P}\\).\n\nDefiniert man also für eine gegebene Ergebnismenge \\(\\Omega\\) eine Funktion \\(\\pi : \\Omega \\to [0,1]\\) und stellt sicher, dass sich die Funktionswerte \\(\\pi(\\omega)\\) über alle \\(\\omega \\in \\Omega\\) hinweg zu 1 summieren und interpretiert den einzelnen Funktionswert \\(\\pi(\\omega)\\) dann als die Wahrscheinlichkeit \\(\\mathbb{P}(\\{\\omega\\})\\) des Elementarereignisses \\(\\{\\omega\\} \\in \\mathcal{A}\\), so hat man ein Wahrscheinlichkeitsmaß konstruiert.",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Wahrscheinlichkeitsräume</span>"
    ]
  },
  {
    "objectID": "201-Wahrscheinlichkeitsräume.html#sec-beispiele-bei-endlichem-ergebnisraum",
    "href": "201-Wahrscheinlichkeitsräume.html#sec-beispiele-bei-endlichem-ergebnisraum",
    "title": "11  Wahrscheinlichkeitsräume",
    "section": "11.3 Beispiele bei endlichem Ergebnisraum",
    "text": "11.3 Beispiele bei endlichem Ergebnisraum\nAus dem bis hierin Gesagtem lässt sich nun zusammenfassend folgendes Vorgehen zur Modellierung eines Zufallsvorganges mithilfe eines Wahrscheinlichkeitsraums \\((\\Omega, \\mathcal{A}, \\mathbb{P})\\) festhalten:\n\nIn einem ersten Schritt überlegt man sich eine sinnvolle Definition der Ergebnismenge \\(\\Omega\\), also der Ergebnisse bzw. Elementarereignisse, die in jedem Durchgang des Zufallsvorgangs realisiert werden sollen.\nIn einem zweiten Schritt wählt man dann ein geignetes Ereignissystem; im Falle einer endlichen Ergebnismenge bietet sich die Potenzmenge \\(\\mathcal{P}(\\Omega)\\) an, im Falle der überabzählbaren Ergebnismenge \\(\\Omega := \\mathbb{R}\\) bietet sich die Borelsche \\(\\sigma\\)-Algebra \\(\\mathcal{B}(\\mathbb{R})\\) an.\nSchließlich definiert man ein Wahrscheinlichkeitsmaß \\(\\mathbb{P}\\), das die Wahrscheinlichkeiten für das Auftreten aller möglichen Ereignisse repräsentiert. Im Falle einer endlichen Ergebnismenge gelingt dies inbesondere durch Definition der Wahrscheinlichkeiten der Elementarereignisse. In der Folge verdeutlichen wir dieses Vorgehen anhand von Beispielen. Im Falle der überabzählbaren Ergebnismenge \\(\\Omega := \\mathbb{R}\\) bietet sich die Definition von \\(\\mathbb{P}\\) mithilfe von Wahrscheinlichkeitsdichtefunktionen an, wie wir später sehen werden.\n\n\nWürfeln mit einem Würfel\nWir modellieren das Würfeln mit einem Würfel. Es ist sicherlich sinnvoll, die Ergebnismenge als \\(\\Omega := \\{1,2,3,4,5,6\\}\\) zu definieren. Allerdings wäre auch die Definition von \\(\\Omega := \\{\n\\cdot,\n\\cdot\\cdot,\n\\cdot\\cdot\\cdot,\n\\cdot\\cdot\\cdot\\cdot,\n\\cdot\\cdot\\cdot\\cdot\\cdot,\n\\cdot\\cdot\\cdot\\cdot\\cdot\\cdot\n\\}\\) in äquivalenter Weise möglich.\nDa es sich um eine endliche Ergebnismenge handelt, wählen wir als \\(\\sigma\\)-Algebra \\(\\mathcal{A}\\) die Potenzmenge \\(\\mathcal{P}(\\Omega)\\). \\(\\mathcal{A}\\) enthält dann automatisch alle möglichen Ereignisse. Die Kardinalität von \\(\\mathcal{A} := \\mathcal{P}(\\Omega)\\) ist \\(|\\mathcal{P}(\\Omega)| = 2^{|\\Omega|} = 2^6 = 64\\). In untenstehender Tabelle listen wir sechs dieser 64 Ereignisse in ihrer verbalen Beschreibung und als Teilmenge \\(A\\) von \\(\\Omega\\) auf.\n\n\n\nAusgewählte Ereignisse beim Modell des Werfen eines Würfels.\n\n\n\n\n\n\nBeschreibung\nMengenform\n\n\n\n\nEs fällt eine beliebige Augenzahl\n\\(\\omega \\in A = \\Omega\\)\n\n\nKeine Augenzahl fällt\n\\(\\omega \\in A = \\emptyset\\)\n\n\nEs fällt eine Augenzahl größer als 4\n\\(\\omega \\in A = \\{5,6\\}\\)\n\n\nEs fällt eine gerade Augenzahl\n\\(\\omega \\in A = \\{2,4,6\\}\\)\n\n\nEs fällt eine Sechs\n\\(\\omega \\in A = \\{6\\}\\)\n\n\nEine Eins, eine Drei oder eine Sechs fällt\n\\(\\omega \\in A = \\{1,3,6\\}\\)\n\n\n\n\n\nDamit ist die Definition des Messraum \\((\\Omega, \\mathcal{A})\\) in der Modellierung des Werfens eines Würfels abgeschlossen.\nWie in Kapitel 11.2 beschrieben kann das Wahrscheinlichkeitsmaß \\(\\mathbb{P}\\) durch Festlegung von \\(\\mathbb{P}(\\{\\omega\\})\\) für alle \\(\\omega \\in \\Omega\\) festgelegt werden. Für das Modell eines unverfälschten Würfels würde man \\[\\begin{equation}\n\\mathbb{P}(\\{\\omega\\}) := \\frac{1}{|\\Omega|} := 1/6 \\mbox{ für alle } \\omega \\in \\Omega\n\\end{equation}\\] wählen. Für ein Modell eines verfälschten Würfels, der das Werfen einer Sechs bevorzugt, könnte man zum Beispiel definieren, dass \\[\\begin{equation}\n\\mathbb{P}(\\{\\omega\\}) := \\frac{1}{8} \\mbox{ für } \\omega \\in \\{1,2,3,4,5\\}\n\\mbox{ und }\n\\mathbb{P}(\\{6\\}) := \\frac{3}{8}.\n\\end{equation}\\] Im Fall des unverfälschten Würfel ergibt sich beispielsweise die Wahrscheinlichkeit für das Ereignis “Es fällt eine gerade Augenzahl” mit der \\(\\sigma\\)-Additivät von \\(\\mathbb{P}\\) dann zu \\[\\begin{equation}\n\\mathbb{P}(\\{2,4,6\\})\n= \\mathbb{P}(\\{2\\} \\cup \\{4\\} \\cup \\{6\\} )\n= \\mathbb{P}(\\{2\\}) + \\mathbb{P}(\\{4\\}) + \\mathbb{P}(\\{6\\})\n= \\frac{1}{6} + \\frac{1}{6} +  \\frac{1}{6}\n= \\frac{3}{6}.\n\\end{equation}\\] Im Fall des obigen Modells eines verfälschten Würfels ergibt sich für das gleiche Ereignis die Wahrscheinlichkeit zu \\[\\begin{equation}\n\\mathbb{P}(\\{2,4,6\\})\n= \\mathbb{P}(\\{2\\} \\cup \\{4\\} \\cup \\{6\\} )\n= \\mathbb{P}(\\{2\\}) + \\mathbb{P}(\\{4\\}) + \\mathbb{P}(\\{6\\})\n= \\frac{1}{8} + \\frac{1}{8} +  \\frac{3}{8}\n= \\frac{5}{8}.\n\\end{equation}\\] Das betrachtete Ereignis hat im Modell des verfälschten Würfels eine höhere Wahrscheinlichkeit als im Modell des unverfälschten Würfels, was intuitiv sinnvoll ist, da die Sechs eine gerade Zahl ist.\n\n\nGleichzeitiges Würfeln mit einem blauem und einem roten Würfel\nWir wollen nun das gleichzeitige Werfen eines blauen und eines roten Würfels modellieren. Dazu ist es sinnvoll, die Ergebnismenge als \\[\\begin{equation}\n\\Omega := \\{(r,b)| r \\in \\{1,2,3,4,5,6\\}, b \\in \\{1,2,3,4,5,6\\}\\}\n\\end{equation}\\] mit Kardinalität \\(|\\Omega| = 36\\) zu definieren, wobei \\(r\\) die Augenzahl des blauen Würfels und \\(b\\) die Augenzahl des roten Würfels repräsentieren soll.\nWiederum bietet sich die Wahl der Potenzmenge von \\(\\Omega\\) als \\(\\sigma\\)-Algebra an, wir definieren also wieder \\(\\mathcal{A} := \\mathcal{P}(\\Omega)\\). Die Anzahl der in diesem Modell möglichen Ereignisse ergibt sich zu \\(|\\mathcal{A}| = 2^{|\\Omega|} = 2^{36} = 68.719.476.736\\). In untenstehender Tabelle listen wir sechs dieser Ereignisse in ihrer verbalen Beschreibung und als Teilmenge \\(A\\) von \\(\\Omega\\) auf.\n\n\n\nAusgewählte Ereignisse beim Modell des Werfens eines roten und eines blauen Würfels.\n\n\n\n\n\n\nBeschreibung\nMengenform\n\n\n\n\nAuf dem roten Würfel fällt eine Drei\n\\(\\omega \\in A = \\{(3,1),(3,2),(3,3),(3,4),(3,5),(3,6)\\}\\)\n\n\nAuf dem blauen Würfel fällt eine Drei\n\\(\\omega \\in A = \\{(1,3),(2,3),(3,3),(4,3),(5,3),(6,3)\\}\\)\n\n\nAuf beiden Würfeln fällt eine Drei\n\\(\\omega \\in A = \\{(3,3)\\}\\)\n\n\nEs fällt eine Pasch\n\\(\\omega \\in A = \\{(1,1),(2,2),(3,3),(4,4),(5,5),(6,6)\\}\\)\n\n\nDie Summe der gefallenen Zahlen ist Vier\n\\(\\omega \\in A = \\{(1,3),(3,1),(2,2)\\}\\)\n\n\n\n\n\nDie Definition des Messraum \\((\\Omega, \\mathcal{A})\\) ist damit abgeschlossen. Ein Wahrscheinlichkeitsmaß \\(\\mathbb{P}\\) kann wiederum durch Definition von \\(\\mathbb{P}(\\{\\omega\\})\\) für alle \\(\\omega \\in \\Omega\\) festgelegt werden. Für das Modell zweier unverfälschter Würfel würde man \\[\\begin{equation}\n\\mathbb{P}(\\{\\omega\\}) := \\frac{1}{|\\Omega|} = \\frac{1}{36} \\mbox{ für alle } \\omega \\in \\Omega\n\\end{equation}\\] wählen. Unter diesem Wahrscheinlichkeitsmaß ergibt sich dann beispielsweise die Wahrscheinlichkeit für das Ereignis “Die Summe der gefallenen Zahlen ist Vier” mit der \\(\\sigma\\)-Additivät von \\(\\mathbb{P}\\) zu \\[\\begin{align*}\n\\begin{split}\n\\mathbb{P}\\left(\\{(1,3),(3,1),(2,2)\\}\\right)\n& = \\mathbb{P}\\left(\\{(1,3)\\}\\cup \\{(3,1)\\} \\cup \\{(2,2)\\}\\right) \\\\\n& = \\mathbb{P}\\left(\\{(1,3)\\}\\right)\n  + \\mathbb{P}\\left(\\{(3,1)\\}\\right)\n  + \\mathbb{P}\\left(\\{(2,2)\\}\\right) \\\\\n& = 1/36 + 1/36 + 1/36                  \\\\\n& = 1/12.\n\\end{split}\n\\end{align*}\\]\n\n\nWerfen einer Münze\nWir modellieren das Werfen einer Münze, deren eine Seite Kopf (Heads) und deren andere Seite Zahl (Tails) zeigt. Es ist sinnvoll, die Ergebnismenge als \\(\\Omega := \\{H,T\\}\\) zu definieren, wobei \\(H\\) “Heads” und \\(T\\) “Tails” repräsentiert. Allerdings wäre auch jede andere binäre Definition von \\(\\Omega\\) möglich, z.B. \\(\\Omega := \\{0,1\\}, \\Omega := \\{-1,1\\}\\), oder \\(\\Omega := \\{1,2\\}\\).\nDie Potenzmenge \\(\\mathcal{A} := \\mathcal{P}(\\Omega)\\) enthält alle möglichen Ereignisse. In diesem Fall können wir das gesamte Mengensystem \\(\\mathcal{A}\\) leicht, wie in untenstehender Tabelle gezeigt, komplett auflisten.\n\n\n\nEreignissystem \\(\\mathcal{A}\\) beim Modell des Werfens einer Münze.\n\n\nBeschreibung\nMengenform\n\n\n\n\nWeder Kopf noch Zahl fällt\n\\(\\omega \\in A = \\emptyset\\)\n\n\nKopf fällt\n\\(\\omega \\in A = \\{H\\}\\)\n\n\nZahl fällt\n\\(\\omega \\in A = \\{T\\}\\)\n\n\nKopf oder Zahl fällt\n\\(\\omega \\in A = \\{H,T\\}\\)\n\n\n\n\n\nDie Definition des Messraums \\((\\Omega, \\mathcal{A})\\) ist damit abgeschlossen.\nEin Wahrscheinlichkeitsmaß \\(\\mathbb{P}\\) kann wiederum durch Definition von \\(\\mathbb{P}(\\{\\omega\\})\\) für alle \\(\\omega \\in \\Omega\\) festgelegt werden. Die Normiertheit von \\(\\Omega\\) bedingt hier insbesondere, dass \\[\\begin{equation}\n\\mathbb{P}(\\Omega) = 1\n\\Leftrightarrow\n\\mathbb{P}(\\{H\\}) + \\mathbb{P}(\\{T\\})  = 1\n\\Leftrightarrow\n\\mathbb{P}(\\{T\\}) = 1 - \\mathbb{P}(\\{H\\}).\n\\end{equation}\\] Bei Festlegung der Wahrscheinlichkeit des Elementarereignisses \\(\\{H\\}\\) wird also die Wahrscheinlichkeit des Elementarereignis \\(\\{T\\}\\) sofort mit festgelegt, andersherum gilt dies natürlich ebenso. Für das Modell einer fairen Münze würde man \\(\\mathbb{P}(\\{H\\}) = \\mathbb{P}(\\{T\\}) := 1/2\\) wählen. Die Wahrscheinlichkeiten aller möglichen Ereignisse ergeben sich in diesem Fall zu \\[\\begin{equation}\n\\mathbb{P}(\\emptyset) = 0,\n\\mathbb{P}(\\{H\\}) = 1/2,\n\\mathbb{P}(\\{T\\}) = 1/2 \\mbox{ und }\n\\mathbb{P}(\\{H,T\\}) = 1.\n\\end{equation}\\]\n\n\nZweifaches Werfen einer Münze\nWir modellieren das zweifache Werfen einer Münzen. Basierend auf dem Modell des einfachen Münzwurfs ist es sinnvoll, die Ergebnismenge als \\(\\Omega := \\{HH,HT,TH,TT\\}\\) zu definieren. Die Potenzmenge \\(\\mathcal{A} := \\mathcal{P}(\\Omega)\\) enthält wiederum alle \\(2^{|\\Omega|} = 2^4 = 16\\) möglichen Ereignisse. In untenstehender Tabelle listen wir vier davon.\n\n\n\nAusgewählte Ereignisse beim Modell des zweifachen Werfens einer Münze.\n\n\nBeschreibung\nMengenform\n\n\n\n\nKopf fällt im ersten Wurf\n\\(\\omega \\in A = \\{HH,HT\\}\\)\n\n\nKopf fällt im zweiten Wurf\n\\(\\omega \\in A = \\{HH,TH\\}\\)\n\n\nKopf fällt nicht\n\\(\\omega \\in A = \\{TT\\}\\)\n\n\nZahl fällt mindestens einmal\n\\(\\omega \\in A = \\{HT, TH, TT\\}\\)\n\n\n\n\n\nDie Definition des Messraum \\((\\Omega, \\mathcal{A})\\) ist damit abgeschlossen. Ein Wahrscheinlichkeitsmaß \\(\\mathbb{P}\\) kann wiederum durch Definition von \\(\\mathbb{P}(\\{\\omega\\})\\) für alle \\(\\omega \\in \\Omega\\) festgelegt werden. Für das Modell des zweifachen Werfens einer fairen Münze würde man \\[\\begin{equation}\n\\mathbb{P}(\\{HH\\}) = \\mathbb{P}(\\{HT\\}) = \\mathbb{P}(\\{TH\\})= \\mathbb{P}(\\{TT\\}) := \\frac{1}{4}\n\\end{equation}\\] definieren.",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Wahrscheinlichkeitsräume</span>"
    ]
  },
  {
    "objectID": "201-Wahrscheinlichkeitsräume.html#literaturhinweise",
    "href": "201-Wahrscheinlichkeitsräume.html#literaturhinweise",
    "title": "11  Wahrscheinlichkeitsräume",
    "section": "11.4 Literaturhinweise",
    "text": "11.4 Literaturhinweise\nDie Monographie “Grundbegriffe der Wahrscheinlichkeitsrechnung” von Andrey Kolmogorov (Kolmogoroff (1933)) symbolisiert die Grundlage der modernen Wahrscheinlichkeitsrechnung. Neben der hier diskutierten axiomatischen Einführung des Wahrscheinlichkeitsraummodells betrachtet Kolmogoroff (1933) noch viele weitere Aspekte der Wahrscheinlichkeitrechnung und bietet so einen gut lesbaren Einstieg in das gesamte Gebiet der Wahrscheinlichkeitstheorie. Natürlich ist der von Kolmogoroff (1933) formulierte Zugang nur ein vorläufiges Endprodukt der langen geschichtlichen Entwicklung der Wahrscheinlichkeitstheorie. Schließlich ist die Entwicklung der mathematischen Modellierung von Zufallvorgängen auch mit Kolmogoroff (1933) keinesfalls an einem Ende angelangt. Spätere Arbeiten im 20. Jahrhundert betrafen insbesondere die Interpretation des Wahrscheinlichkeitsbegriffs (vgl. De Finetti (1975)) oder führen verallgemeinerte quantitative Maße subjektiver Unsicherheit ein (vgl. Walley (1991)). Einen aktuellen Überblick zur Interpretation des Wahrscheinlichkeitsbegriffs und seiner formalen Grundlagen gibt Hájek (2019).",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Wahrscheinlichkeitsräume</span>"
    ]
  },
  {
    "objectID": "201-Wahrscheinlichkeitsräume.html#selbstkontrollfragen",
    "href": "201-Wahrscheinlichkeitsräume.html#selbstkontrollfragen",
    "title": "11  Wahrscheinlichkeitsräume",
    "section": "11.5 Selbstkontrollfragen",
    "text": "11.5 Selbstkontrollfragen\n\nGeben Sie die Definition des Begriffs der \\(\\sigma\\)-Algebra wieder.\nGeben Sie die Definition des Begriffs des Wahrscheinlichkeitsmaßes wieder.\nGeben Sie die Definition des Begriffs des Wahrscheinlichkeitsraums wieder.\nErläutern Sie den Begriff der Ergebnismenge \\(\\Omega\\).\nErläutern Sie die stillschweigende Mechanik des Wahrscheinlichkeitsraummodells.\nErläutern Sie den Begriff eines Ereignisses \\(A \\in \\mathcal{A}\\).\nErläutern Sie den Begriff des Ereignissystems \\(\\mathcal{A}\\).\nWelche \\(\\sigma\\)-Algebra wählt man sinnvoller Weise für einen Wahrscheinlichkeitsraum mit endlicher Ergebnismenge?\nErläutern Sie den Begriff des Wahrscheinlichkeitsmaßes \\(\\mathbb{P}\\).\nGeben Sie die Definition des Begriffs der Wahrscheinlichkeitsfunktion wieder.\nWarum ist der Begriff der Wahrscheinlichkeitsfunktion bei der Modellierung eines Zufallsvorgans durch einen Wahrscheinlichkeitsraums mit endlicher Ergebnismenge hilfreich?\nErläutern Sie die Modellierung des Werfens eines Würfels mithilfe eines Wahrscheinlichkeitsraums.\nErläutern Sie die Modellierung des gleichzeitigen Werfens eines roten und eines blauen Würfels mithilfe eines Wahrscheinlichkeitsraums. \n\n\n\n\n\nBorel, É. (1898). Leçons Sur La Théorie Des Fonctions. Paris: Gauthier Villars.\n\n\nDe Finetti, B. (1975). Theory of Probability. John Wiley & Sons.\n\n\nHájek, A. (2019). Interpretations of Probability. In E. N. Zalta (Hrsg.), The Stanford Encyclopedia of Philosophy (Fall 2019). Metaphysics Research Lab, Stanford University.\n\n\nKolmogoroff, A. (1933). Grundbegriffe der Wahrscheinlichkeitsrechnung. Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-642-49888-6\n\n\nMeintrup, D., & Schäffler, S. (2005). Stochastik: Theorie und Anwendungen. Springer.\n\n\nSchmidt, K. D. (2009). Maß und Wahrscheinlichkeit. Springer.\n\n\nWalley, P. (1991). Statistical Reasoning with Imprecise Probabilities (1st ed). Chapman and Hall.",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Wahrscheinlichkeitsräume</span>"
    ]
  },
  {
    "objectID": "202-Elementare-Wahrscheinlichkeiten.html",
    "href": "202-Elementare-Wahrscheinlichkeiten.html",
    "title": "12  Elementare Wahrscheinlichkeiten",
    "section": "",
    "text": "12.1 Gemeinsame Wahrscheinlichkeiten\nDer Begriff der gemeinsamen Wahrscheinlichkeit zweier Ereignisse ist wie folgt definiert.\nWie oben angemerkt entspricht \\(\\mathbb{P}(A \\cap B)\\) der Wahrscheinlichkeit dafür, dass die Ereignisse \\(A\\) und \\(B\\) “gleichzeitig” eintreten. Dies verdeutlicht man sich am besten vor dem Hintergrund der Mechanik des Wahrscheinlichkeitsraummodells. Danach ist \\(\\mathbb{P}(A \\cap B)\\) eben die Wahrscheinlichkeit, dass in einem Durchgang eines Zufallsvorgangs ein \\(\\omega\\) realisiert wird, für das sowohl \\(\\omega \\in A\\) als auch \\(\\omega \\in B\\) gelten.",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Elementare Wahrscheinlichkeiten</span>"
    ]
  },
  {
    "objectID": "202-Elementare-Wahrscheinlichkeiten.html#sec-gemeinsame-wahrscheinlichkeiten",
    "href": "202-Elementare-Wahrscheinlichkeiten.html#sec-gemeinsame-wahrscheinlichkeiten",
    "title": "12  Elementare Wahrscheinlichkeiten",
    "section": "",
    "text": "Definition 12.1 (Gemeinsame Wahrscheinlichkeit) \\((\\Omega, \\mathcal{A}, \\mathbb{P})\\) sei ein Wahrscheinlichkeitsraum und es seien \\(A,B \\in \\mathcal{A}\\). Dann heißt \\[\\begin{equation}\n\\mathbb{P}(A \\cap B)\n\\end{equation}\\] die gemeinsame Wahrscheinlichkeit von \\(A\\) und \\(B\\).\n\n\n\nBeispiel\nAls erstes Beispiel für eine gemeinsame Wahrscheinlichkeit zweier Ereignisse wollen wir wieder das Wahrscheinlichkeitsraummodells des Werfens eines fairen Würfels betrachten. Seien dazu \\(A\\) das Ereignis “Es fällt eine gerade Augenzahl”, also \\(A := \\{2,4,6\\}\\) und \\(B\\) das Ereignis “Es fällt eine Augenzahl größer als Drei”, also \\(B := \\{4,5,6\\}\\). Mengentheoretisch gilt dann \\[\\begin{equation}\nA \\cap B = \\{2,4,6\\} \\cap \\{4,5,6\\} = \\{4,6\\}.\n\\end{equation}\\] Die Interpretation von \\(A \\cap B = \\{4,6\\}\\) ist dabei gerade “Es fällt eine gerade Augenzahl und diese Augenzahl ist größer als Drei”. Bei Annahme der Fairness des Würfels, also für \\(\\mathbb{P}(\\{4\\}) = \\mathbb{P}(\\{6\\}) := 1/6\\) können wir mithilfe der \\(\\sigma\\)-Additivität von \\(\\mathbb{P}\\) die Wahrscheinlichkeit dieses Ereignisses leicht berechnen. Es ergibt sich \\[\\begin{align}\n\\begin{split}\n\\mathbb{P}(A \\cap B)\n& = \\mathbb{P}( \\{2,4,6\\} \\cap \\{4,5,6\\})       \\\\\n& = \\mathbb{P}( \\{4,6\\})                        \\\\\n& = \\mathbb{P}(\\{4\\}) + \\mathbb{P}(\\{6\\})       \\\\\n& = \\frac{1}{6} + \\frac{1}{6}                   \\\\\n& = \\frac{1}{3}.\n\\end{split}\n\\end{align}\\]\nBeim Nachdenken über gemeinsame Wahrscheinlichkeiten ist es natürlich wichtig, die gemeinsame Wahrscheinlichkeit \\(\\mathbb{P}(A \\cap B)\\) nicht mit der Wahrscheinlichkeit \\(\\mathbb{P}(A \\cup B)\\) des Ereignisses \\(A \\cup B\\) zur verwechseln. Es sei daran erinnert, dass die Vereinigung zweier Mengen \\(\\cup\\) dem inklusiven oder, also einem und/oder entspricht (vgl. Kapitel 1.3 und Kapitel 2.2). Das Ereignis \\(A \\cup B\\) entspricht also dem Ereignis, dass das Ereignis \\(A\\) und/oder das Ereignis \\(B\\) eintritt. Insbesondere ist \\(\\omega \\in A \\cup B\\) also auch schon dann erfüllt, wenn für das Ergebnis eines Durchgang eines Zufallsvorgangs nur \\(\\omega \\in A\\) oder nur \\(\\omega \\in B\\) gilt. Konkret ergibt sich etwa für die Ereignisse \\(A := \\{2,4,6\\}\\) und \\(B := \\{4,5,6\\}\\) aus obigem Würfelbeispiel \\[\\begin{equation}\nA \\cup B = \\{2,4,6\\} \\cup \\{4,5,6\\} = \\{2,4,5,6\\}\n\\end{equation}\\] mit der Interpretation “Es fällt eine gerade Augenzahl und/oder es fällt eine Augenzahl größer als Drei”. Für die entsprechende Wahrscheinlichkeit ergibt sich \\[\\begin{equation}\n\\mathbb{P}(\\{2,4,5,6\\}) = \\frac{2}{3},\n\\end{equation}\\] so dass in diesem Fall offenbar \\(\\mathbb{P}(A \\cap B) \\neq \\mathbb{P}(A \\cup B)\\) gilt.\nMithilfe folgendem Theorems wollen wir in diesem Abschnitt schließlich noch einige nützliche Eigenschaften zum Rechnen mit Wahrscheinlichkeiten festhalten, die sich direkt aus der Verbindung von Mengenverknüpfungen und der \\(\\sigma\\)-Additivität von Wahrscheinlichkeitsmaßen ergeben.\n\nTheorem 12.1 (Weitere Eigenschaften von Wahrscheinlichkeiten) \\((\\Omega, \\mathcal{A}, \\mathbb{P})\\) sei ein Wahrscheinlichkeitsraum und es seien \\(A,B \\in \\mathcal{A}\\) Ereignisse. Dann gelten\n\n\\(\\mathbb{P}(A^c) = 1 - \\mathbb{P}(A)\\).\n\\(A \\subset B \\Rightarrow \\mathbb{P}(A) \\le \\mathbb{P}(B)\\).\n\\(\\mathbb{P}(A \\cap B^c) = \\mathbb{P}(A) - \\mathbb{P}(A \\cap B)\\)\n\\(\\mathbb{P}(A \\cup B) = \\mathbb{P}(A) + \\mathbb{P}(B) - \\mathbb{P}(A \\cap B)\\).\n\\(A \\cap B = \\emptyset \\Rightarrow \\mathbb{P}(A \\cup B) = \\mathbb{P}(A) + \\mathbb{P}(B)\\).\n\n\n\nBeweis. Die zweite, dritte, und vierte Aussage dieses Theorems basieren auf elementaren mengentheoretischen Aussagen und der \\(\\sigma\\)-Addivität von \\(\\mathbb{P}\\). Wir wollen diese elementaren mengentheoretischen Aussagen hier nicht beweisen, sondern verweisen jeweils auf die Intuition, die durch die Venn-Diagramme in Abbildung 12.1 vermittelt wird.\n\n\n\n\n\n\nAbbildung 12.1: Venn-Diagramme zum Beweis von Theorem 12.1\n\n\n\nZu 1.: Wir halten zunächst fest, dass aus \\(A^c := \\Omega \\setminus A\\) folgt, dass \\(A^c \\cup A = \\Omega\\) und dass \\(A^c \\cap A = \\emptyset\\). Mit der Nomiertheit und der \\(\\sigma\\)-Additivität von \\(\\mathbb{P}\\) folgt dann \\[\\begin{equation}\n\\mathbb{P}(\\Omega) = 1\n\\Leftrightarrow\n\\mathbb{P}(A^c \\cup A) = 1\n\\Leftrightarrow\n\\mathbb{P}(A^c) + \\mathbb{P}(A)  = 1\n\\Leftrightarrow\n\\mathbb{P}(A^c) = 1 - \\mathbb{P}(A).\n\\end{equation}\\]\nZu 2.: Zunächst gilt (vgl. Abbildung A) \\[\\begin{equation}\nA \\subset B \\Rightarrow B = A \\cup (B \\cap A^c)\n\\mbox{ mit } A \\cap (B \\cap A^c) = \\emptyset.\n\\end{equation}\\] Mit der \\(\\sigma\\)-Additivät von \\(\\mathbb{P}\\) folgt dann aber \\[\\begin{equation}\n\\mathbb{P}(B) = \\mathbb{P}(A) + \\mathbb{P}(B \\cap A^c).\n\\end{equation}\\] Mit \\(\\mathbb{P}(B \\cap A^c) \\ge 0\\) folgt dann \\(\\mathbb{P}(A) \\le \\mathbb{P}(B)\\).\nZu 3.: Zunächst gilt (vgl. Abbildung B) \\[\\begin{equation}\n(A \\cap B) \\cap (A \\cap B^c)  = \\emptyset\n\\mbox{ und } A  = (A \\cap B) \\cup (A \\cap B^c).\n\\end{equation}\\] Mit der \\(\\sigma\\)-Addivität von \\(\\mathbb{P}\\) folgt dann \\[\\begin{align}\n\\begin{split}\n\\mathbb{P}(A) = \\mathbb{P}(A \\cap B) + \\mathbb{P}(A \\cap B^c)\n\\Leftrightarrow\n\\mathbb{P}(A \\cap B)\n= \\mathbb{P}(A) - \\mathbb{P}(A \\cap B^c)\n\\end{split}\n\\end{align}\\]\nZu 4.: Zunächst gilt (vgl. Abbildung C) \\[\\begin{equation}\nB \\cap (A \\cap B^c)  = \\emptyset\n\\mbox{ und } A \\cup B = B \\cup (A \\cap B^c).\n\\end{equation}\\] Mit der \\(\\sigma\\)-Addivität von \\(\\mathbb{P}\\) folgt dann \\[\\begin{equation}\n\\mathbb{P}(A \\cup B) = \\mathbb{P}(B) + \\mathbb{P}(A \\cap B^c).\n\\end{equation}\\] Mit 3. folgt dann \\[\\begin{equation}\n\\mathbb{P}(A \\cup B) = \\mathbb{P}(B) + \\mathbb{P}(A) - \\mathbb{P}(A \\cap B)\n\\end{equation}\\]\nZu 5.: Die Aussage folgt direkt aus 4. mit \\(\\mathbb{P}(A \\cap B) = \\emptyset\\) und \\(\\mathbb{P}(\\emptyset) = 0\\).",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Elementare Wahrscheinlichkeiten</span>"
    ]
  },
  {
    "objectID": "202-Elementare-Wahrscheinlichkeiten.html#sec-bedingte-wahrscheinlichkeiten",
    "href": "202-Elementare-Wahrscheinlichkeiten.html#sec-bedingte-wahrscheinlichkeiten",
    "title": "12  Elementare Wahrscheinlichkeiten",
    "section": "12.2 Bedingte Wahrscheinlichkeiten",
    "text": "12.2 Bedingte Wahrscheinlichkeiten\nWir wenden uns nun dem Begriff der bedingten Wahrscheinlichkeit zu.\n\nDefinition 12.2 (Bedingte Wahrscheinlichkeit) \\((\\Omega,\\mathcal{A}, \\mathbb{P})\\) sei ein Wahrscheinlichkeitsraum und \\(A, B\\in \\mathcal{A}\\) seien Ereignisse mit \\(\\mathbb{P}(B) &gt; 0\\). Die bedingte Wahrscheinlichkeit des Ereignisses \\(A\\) gegeben das Ereignis \\(B\\) ist definiert als \\[\\begin{equation}\n\\mathbb{P}(A|B) := \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)}.\n\\end{equation}\\] Weiterhin heißt das für ein festes \\(B \\in \\mathcal{A}\\) mit \\(\\mathbb{P}(B) &gt; 0\\) definierte Wahrscheinlichkeitsmaß \\[\\begin{equation}\n\\mathbb{P}(\\,\\,|B) : \\mathcal{A} \\to [0,1],\nA \\mapsto \\mathbb{P}(A|B) := \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)}\n\\end{equation}\\] die bedingte Wahrscheinlichkeit gegeben Ereignis \\(B\\).\n\nWir halten fest: Die bedingte Wahrscheinlichkeit \\(\\mathbb{P}(A|B)\\) eines Ereignisses \\(A\\) gegeben ein Ereignis \\(B\\) ist die mit \\(1/\\mathbb{P}(B)\\) skalierte gemeinsame Wahrscheinlichkeit \\(\\mathbb{P}(A \\cap B)\\) der Ereignisse \\(A\\) und \\(B\\). Legt man in der Formulierung eines probabilistischen Modells also die gemeinsame Wahrscheinlichkeit \\(\\mathbb{P}(A \\cap B)\\) sowie die Wahrscheinlichkeit \\(\\mathbb{P}(B) &gt; 0\\) des Ereignisses \\(B\\) fest, so legt man insbesondere auch die bedingte Wahrscheinlichkeit \\(\\mathbb{P}(A|B)\\) des Ereignisses \\(A\\) gegeben das Ereignis \\(B\\) fest.\nIm Unterschied zur gemeinsamen Wahrscheinlichkeit definiert \\(\\mathbb{P}(\\cdot \\vert B)\\) für ein fest gewähltes \\(B\\) ein Wahrscheinlichkeitsmaß für alle \\(A \\in \\mathcal{A}\\). Es gelten also insbesondere\n\n\\(\\mathbb{P}(A|B) \\ge 0\\) für alle \\(A \\in \\mathcal{A}\\),\n\\(\\mathbb{P}(\\Omega|B) =  1\\) und\n\\(\\mathbb{P}\\left(\\cup_{i=1}^\\infty A_i|B \\right) = \\sum_{i=1}^\\infty \\mathbb{P}(A_i|B)\\) für paarweise disjunkte \\(A_1,A_2,... \\in \\mathcal{A}\\).\n\nMan sollte sich in dieser Hinsicht intuitiv merken, dass die Regeln zum Rechnen mit Wahrscheinlichkeiten für die Ereignisse links des vertikalen Strichs gelten. Wir weisen ferner daraufhin, dass es keinen Grund gibt, die bedingten Wahrscheinlichkeiten \\[\\begin{equation}\n\\mathbb{P}(A|B) = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)}\n\\mbox{ und }\n\\mathbb{P}(B|A) = \\frac{\\mathbb{P}(B \\cap A)}{\\mathbb{P}(A)} = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(A)}\n\\end{equation}\\] zu verwechseln (vgl. Herzog & Ostwald (2013)). Insbesondere folgt aus \\(\\mathbb{P}(A) \\neq \\mathbb{P}(B)\\) immer direkt \\(\\mathbb{P}(A|B) \\neq \\mathbb{P}(B|A)\\). Schließlich sei angemerkt, dass eine Verallgemeinerung der bedingten Wahrscheinlichkeit in Definition 12.2 auf den Fall \\(\\mathbb{P}(B) = 0\\) möglich, aber technisch aufwändig ist. Wir verweisen dafür auf die weiterführende Literatur, z.B. Meintrup & Schäffler (2005) und Schmidt (2009).\n\nBeispiel\nAls erstes Beispiel für eine bedingte Wahrscheinlichkeit betrachten wir erneut das Modell \\((\\Omega, \\mathcal{A}, \\mathbb{P})\\) des fairen Würfels. Wir wollen die bedingte Wahrscheinlichkeit für das Ereignis “Es fällt eine gerade Augenzahl” gegeben das Ereignis “Es fällt eine Zahl größer als Drei” berechnen. Wir haben oben bereits gesehen, dass das Ereignis “Es fällt eine gerade Augenzahl” der Teilmenge \\(A := \\{2,4,6\\}\\) von \\(\\Omega\\) entspricht, und dass das Ereignis “Es fällt eine Zahl größer als Drei” der Teilmenge \\(B := \\{4,5,6\\}\\) von \\(\\Omega\\) entspricht. Weiterhin haben wir gesehen, dass unter der Annahme, dass der modellierte Würfel fair ist, gilt, dass \\[\\begin{align}\n\\begin{split}\n\\mathbb{P}(\\{2,4,6\\})\n= \\mathbb{P}(\\{2\\}) + \\mathbb{P}(\\{4\\}) + \\mathbb{P}(\\{6\\})\n= \\frac{1}{6} + \\frac{1}{6} + \\frac{1}{6}\n= \\frac{3}{6}   \n\\end{split}\n\\end{align}\\] und dass \\[\\begin{align}\n\\begin{split}\n\\mathbb{P}(\\{4,5,6\\})\n= \\mathbb{P}(\\{4\\}) + \\mathbb{P}(\\{5\\}) + \\mathbb{P}(\\{6\\})    \n= \\frac{1}{6} + \\frac{1}{6} + \\frac{1}{6}                                             \n= \\frac{3}{6}.        \n\\end{split}\n\\end{align}\\] Schließlich hatten wir auch gesehen, dass das Ereignis \\(A \\cap B\\), also das Ereignis “Es fällt eine gerade Augenzahl, die größer als Drei ist”, die Wahrscheinlichkeit \\[\\begin{align}\n\\begin{split}\n\\mathbb{P}(A \\cap B)\n= \\mathbb{P}(\\{2,4,6\\} \\cap \\{4,5,6\\})     \n= \\mathbb{P}(\\{4,6\\})                      \n= \\mathbb{P}(\\{4\\}) + \\mathbb{P}(\\{6\\})     \n= \\frac{1}{6} + \\frac{1}{6}                                 \n= \\frac{2}{6}\n\\end{split}\n\\end{align}\\] hat. Nach Definition der bedingten Wahrscheinlichkeit ergibt sich dann direkt \\[\\begin{align}\n\\begin{split}\n\\mathbb{P}(A|B)\n= \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)}                   \n= \\frac{\\mathbb{P}(\\{4,6\\})}{\\mathbb{P}(\\{4,5,6\\})}            \n= \\frac{2}{6}\\cdot\\frac{6}{3}                                       \n= \\frac{2}{3}.    \n\\end{split}\n\\end{align}\\]\nIn diesem Zusammenhang bietet sich eine Interpretation der bedingten Wahrscheinlichkeit \\(\\mathbb{P}(A|B)\\) als eine Abnahme subjektiver Unsicherheit bzw. als Zugewinn an subjektiver Information gegenüber der unbedingten Wahrscheinlichkeit \\(\\mathbb{P}(A)\\) an: Wenn man weiß, dass eine Augenzahl größer als Drei gefallen ist, dass also das Ereignis \\(\\omega \\in B\\) vorliegt ist, ist die Wahrscheinlichkeit, dass es sich bei \\(\\omega\\) um eine gerade Augenzahl handelt \\(2/3\\). Wenn man dagegen nicht weiß, dass das Ereignis \\(\\omega \\in B\\) vorliegt (und auch sonst keine Information über \\(\\omega\\) hat) ist die Wahrscheinlichkeit für das Fallen einer geraden Augenzahl nur \\(1/2\\). Bedingen auf dem Vorliegen eines Ereignisses entspricht also der Inkorporation von Information und damit der Abnahme von Unsicherheit in wahrscheinlichkeitstheoretische Modellen. Dies ist die Grundlage der Bayesianischen Statistik.\nZum Schluss dieses Abschnittes wollen wir noch drei technische Konsequenzen der Definition der bedingten Wahrscheinlichkeit betrachten, die wir als Theoreme formulieren.\nDas erste Theorem betrifft den Zusammenhang von gemeinsamen und bedingten Wahrscheinlichkeiten und reiteriert, wie gemeinsame Wahrscheinlichkeiten aus bedingten und totalen Wahrscheinlichkeiten berechnet werden können.\n\nTheorem 12.2 (Gemeinsame und bedingte Wahrscheinlichkeiten) Es seien \\((\\Omega,\\mathcal{A}, \\mathbb{P})\\) ein W-Raum und \\(A,B\\in \\mathcal{A}\\) mit \\(\\mathbb{P}(A) &gt; 0\\) und \\(P(B)&gt;0\\). Dann gilt \\[\\begin{equation}\n\\mathbb{P}(A \\cap B)\n= \\mathbb{P}(A|B)\\mathbb{P}(B)\n= \\mathbb{P}(B|A)\\mathbb{P}(A).\n\\end{equation}\\]\n\n\nBeweis. Mit der Definition der jeweiligen bedingten Wahrscheinlichkeit folgen direkt \\[\\begin{equation}\n\\mathbb{P}(A|B) = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)}\n\\Leftrightarrow\n\\mathbb{P}(A \\cap B) =\\mathbb{P}(A|B)\\mathbb{P}(B)\n\\end{equation}\\] und \\[\\begin{equation}\n\\mathbb{P}(B|A) = \\frac{\\mathbb{P}(B \\cap A)}{\\mathbb{P}(A)}\n\\Leftrightarrow\n\\mathbb{P}(A \\cap B) =\\mathbb{P}(B|A)\\mathbb{P}(A).\n\\end{equation}\\]\n\nEbenso wie das Festlegen von \\(\\mathbb{P}(A \\cap B)\\) und \\(\\mathbb{P}(A)\\) die bedingte Wahrscheinlichkeit \\(\\mathbb{P}(B|A)\\) festlegt, legt das Festlegen von \\(\\mathbb{P}(A)\\) und \\(\\mathbb{P}(B|A)\\) also die gemeinsame Wahrscheinlichkeit \\(\\mathbb{P}(A \\cap B)\\) fest.\nDas nachfolgende sogenannte Gesetz der totalen Wahrscheinlichkeit besagt, wie basierend auf gemeinsamen Wahrscheinlichkeiten unbedingte, sogenannte totale Wahrscheinlichkeiten berechnet werden können.\n\nTheorem 12.3 (Gesetz der totalen Wahrscheinlichkeit) \\((\\Omega,\\mathcal{A},\\mathbb{P})\\) sei ein Wahrscheinlichkeitsraum und \\(A_1,...,A_k\\) sei eine Partition von \\(\\Omega\\). Dann gilt für jedes \\(B \\in \\mathcal{A}\\), dass \\[\\begin{equation}\n\\mathbb{P}(B) = \\sum_{i=1}^k \\mathbb{P}(B \\cap A_i) = \\sum_{i=1}^k \\mathbb{P}(B|A_i)\\mathbb{P}(A_i).\n\\end{equation}\\]\n\n\nBeweis. Für \\(i = 1,...,k\\) sei \\(C_i := B \\cap A_i\\), so dass \\(\\cup_{i=1}^k C_i = B\\) und \\(C_i \\cap C_j = \\emptyset\\) für \\(1 \\le i,j \\le k,i \\neq j\\). Wir verdeutlichen diese Festlegungen in Abbildung 12.2 mithilfe eines Venn-Diagramms.\n\n\n\n\n\n\nAbbildung 12.2: Venn-Diagramm zum Beweis von Theorem 12.3.\n\n\n\nAlso gilt \\[\\begin{equation}\n\\mathbb{P}(B)\n= \\sum_{i=1}^k \\mathbb{P}(C_i)\n= \\sum_{i=1}^k \\mathbb{P}(B \\cap A_i)\n= \\sum_{i=1}^k \\mathbb{P}(B|A_i)\\mathbb{P}(A_i).\n\\end{equation}\\]\n\nIntuitiv entspricht \\(\\mathbb{P}(B)\\) also der gewichteten Summe der bedingten Wahrscheinlichkeiten \\(\\mathbb{P}(B|A_i)\\) wobei die Wichtungsfaktoren gerade die unbedingten Wahrscheinlichkeiten \\(\\mathbb{P}(A_i)\\) für \\(i = 1,..,k\\) sind.\nSchließlich betrachten wir mit dem Bayesschen Theorem eine Formel zur alternativen Berechnung von bedingten Wahrscheinlichkeiten.\n\nTheorem 12.4 (Bayessches Theorem) \\((\\Omega,\\mathcal{A},\\mathbb{P})\\) sei ein Wahrscheinlichkeitsraum und \\(A_1, ...,A_k\\) sei eine Partition von \\(\\Omega\\) mit \\(\\mathbb{P}(A_i) &gt; 0\\) für alle \\(i = 1,...,k\\). Wenn \\(\\mathbb{P}(B) &gt; 0\\) gilt, dann gilt für jedes \\(i = 1,...,k\\), dass \\[\\begin{equation}\n\\mathbb{P}(A_i|B) = \\frac{\\mathbb{P}(B|A_i)\\mathbb{P}(A_i)}{\\sum_{i=1}^k \\mathbb{P}(B|A_i)\\mathbb{P}(A_i)}.\n\\end{equation}\\]\n\n\nBeweis. Mit der Definition der bedingten Wahrscheinlichkeit und dem Gesetz der totalen Wahrscheinlichkeit gilt \\[\\begin{equation}\n\\mathbb{P}(A_i|B)\n= \\frac{\\mathbb{P}(A_i \\cap B)}{\\mathbb{P}(B)}\n= \\frac{\\mathbb{P}(B|A_i)\\mathbb{P}(A_i)}{\\mathbb{P}(B)}\n= \\frac{\\mathbb{P}(B|A_i)\\mathbb{P}(A_i)}{\\sum_{i=1}^k \\mathbb{P}(B|A_i)\\mathbb{P}(A_i)}.\n\\end{equation}\\]\n\nMan beachte, dass das Theorem von Bayes unabhängig von der Frequentistischen oder Bayesianischen Interpretation von Wahrscheinlichkeiten ist und lediglich eine Aussage zum Rechnen mit bedingten Wahrscheinlichkeiten macht. Im Rahmen der Frequentistischen Inferenz wird das Theorem von Bayes allerdings recht selten benutzt. Im Rahmen der Bayesianischen Inferenz dagegen ist das Theorem von Bayes zentral. In diesem Kontext wird \\(\\mathbb{P}(A_i)\\) dann oft die und \\(\\mathbb{P}(A_i|B)\\) die genannt. Wie oben erläutert entspricht \\(\\mathbb{P}(A_i|B)\\) der Wahrscheinlichkeit von \\(A_i\\), wenn man um das Eintreten von \\(B\\) weiß.",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Elementare Wahrscheinlichkeiten</span>"
    ]
  },
  {
    "objectID": "202-Elementare-Wahrscheinlichkeiten.html#sec-unabhängige-ereignisse",
    "href": "202-Elementare-Wahrscheinlichkeiten.html#sec-unabhängige-ereignisse",
    "title": "12  Elementare Wahrscheinlichkeiten",
    "section": "12.3 Unabhängige Ereignisse",
    "text": "12.3 Unabhängige Ereignisse\nDie Unabhängigkeit von Ereignissen dient der Modellierung der Abwesenheit von gegenseitigen Einflüssen von Ereignissen. Ihre Definition besagt, dass sich die gemeinsame Wahrscheinlichkeit zweier Ereignisse aus dem Produkt der Wahrscheinlichkeiten der einzelnen Ereignisse ergeben soll. Man spricht in diesem Kontext auch von der Faktorisierung der gemeinsamen Wahrscheinlichkeit der Ereignisse. Der Sinn dieser Definition erschließt sich dann im Lichte des Begriffs der bedingten Wahrscheinlichkeit in Theorem 12.5. Wir betrachten zunächst die Definition.\n\nDefinition 12.3 (Unabhängige Ereignisse) Zwei Ereignisse \\(A \\in \\mathcal{A}\\) and \\(B \\in \\mathcal{A}\\) heißen unabhängig, wenn \\[\\begin{equation}\n\\mathbb{P}(A \\cap B) = \\mathbb{P}(A)\\mathbb{P}(B).\n\\end{equation}\\] Eine Menge von Ereignissen \\(\\{A_i|i \\in I\\}\\subset \\mathcal{A}\\) mit beliebiger Indexmenge \\(I\\) heißt unabhängig, wenn für jede endliche Untermenge \\(J \\subseteq I\\) gilt, dass \\[\\begin{equation}\n\\mathbb{P}\\left(\\cap_{j \\in J} A_j \\right) = \\prod_{j \\in J}\\mathbb{P}(A_j).\n\\end{equation}\\]\n\nMan beachte, dass die Unabhängigkeit bestimmter Ereignissen in der Definition eines wahrscheinlichkeitstheoretischen Modells vorausgesetzt werden kann oder auch aus der Definition eines wahrscheinlichkeitstheoretischen Modells folgen kann. Sind zwei Ereignisse nicht unabhängig, so sagt man auch, dass diese Ereignisse abhängig sind. Ohne Beweis merken wir an, dass die Bedingung der beliebigen Untermengen von \\(I\\) in Definition 12.3 die paarweise Unabhängigkeit der \\(A_i, i \\in I\\) sichert (vgl. DeGroot & Schervish (2012)). Schließlich weisen wir daraufhin, dass unabhängige Ereignisse nicht mit disjunkten Ereignissen, also Ereignissen \\(A\\) und \\(B\\) für die \\(A \\cap B = \\emptyset\\) gilt, verwechselt werden sollten. Insbesondere sind disjunkte Ereignisse mit von Null verschiedenenn Wahrscheinlichkeiten \\(\\mathbb{P}(A)&gt;0\\) und \\(\\mathbb{P}(B) &gt;0\\) nie unabhängig, da in diesem Fall \\(\\mathbb{P}(A)\\mathbb{P}(B) &gt; 0\\) und \\(\\mathbb{P}(A \\cap B) = \\mathbb{P}(\\emptyset) = 0\\) gelten und damit offenbar gilt, dass \\(\\mathbb{P}(A \\cap B) \\neq \\mathbb{P}(A)\\mathbb{P}(B)\\).\nDer Sinn der Faktorisierung der gemeinsamen Wahrscheinlichkeit erschließt sich nun anhand folgenden Theorems.\n\nTheorem 12.5 (Bedingte Wahrscheinlichkeit unter Unabhängigkeit) \\((\\Omega,\\mathcal{A}, \\mathbb{P})\\) sei ein Wahrscheinlichkeitsraum und \\(A,B\\in \\mathcal{A}\\) seien unabhängige Ereignisse mit \\(\\mathbb{P}(B) \\ge 0\\). Dann gilt \\[\\begin{equation}\n\\mathbb{P}(A|B) = \\mathbb{P}(A).\n\\end{equation}\\]\n\n\nBeweis. Unter den Annahmen des Theorems gilt \\[\\begin{equation}\n\\mathbb{P}(A|B)\n= \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)}\n= \\frac{\\mathbb{P}(A)\\mathbb{P}(B)}{\\mathbb{P}(B)}\n= \\mathbb{P}(A).\n\\end{equation}\\]\n\nBei gegebener Unabhängigkeit zweier Ereignisse \\(A\\) und \\(B\\) ist es für die Wahrscheinlichkeit des Ereignisses \\(A\\) also unerheblich, ob auch \\(B\\) eintritt oder nicht, die Wahrscheinlichkeit \\(\\mathbb{P}(A)\\) bleibt gleich. Damit wird die Unabhängigkeit von Ereignissen also gerade als Faktorisierung der gemeinsamen Wahrscheinlichkeit von \\(A\\) und \\(B\\) modelliert, damit \\(\\mathbb{P}(A|B) = \\mathbb{P}(A)\\) folgt. Aus Sicht der Modellierung subjektiver Unsicherheit durch Wahrscheinlichkeiten bedeutet die Unabhängigkeit zweier Ereignisse also, dass das Wissen um das Vorliegen eines der beiden Ereignisse die Wahrscheinlichkeit für das Vorliegen des anderen Ereignisses nicht ändert. Andersherum bedeutet die Abhängigkeit zweier Ereignisse, dass das Wissen um das Vorliegen eines der beiden Ereignisse die Wahrscheinlichkeit für das Vorliegen des anderen Ereignisses verändert, also entweder erhöht oder verringert.",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Elementare Wahrscheinlichkeiten</span>"
    ]
  },
  {
    "objectID": "202-Elementare-Wahrscheinlichkeiten.html#literaturhinweise",
    "href": "202-Elementare-Wahrscheinlichkeiten.html#literaturhinweise",
    "title": "12  Elementare Wahrscheinlichkeiten",
    "section": "12.4 Literaturhinweise",
    "text": "12.4 Literaturhinweise\nViele der in diesem Abschnitt eingeführten Begrifflichkeiten sind auf engste mit der geschichtlichen Genese der Wahrscheinlichkeitstheorie verwoben, so dass keine einzelnen Referenzen angegeben werden sollen. Einen Einstieg in die Geschichte der Wahrscheinlichkeitstheorie der letzten zwei Jahrhunderte bietet Hald (1990), einen Überblick über modernere Entwicklungen gibt Von Plato (1994). Das Theorem von Bayes wird allgemein auf Bayes (1763) zurückgeführt, auch wenn es nicht das eigentliche Hauptthema dieser Arbeit ist.",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Elementare Wahrscheinlichkeiten</span>"
    ]
  },
  {
    "objectID": "202-Elementare-Wahrscheinlichkeiten.html#selbstkontrollfragen",
    "href": "202-Elementare-Wahrscheinlichkeiten.html#selbstkontrollfragen",
    "title": "12  Elementare Wahrscheinlichkeiten",
    "section": "12.5 Selbstkontrollfragen",
    "text": "12.5 Selbstkontrollfragen\n\nGeben Sie die Definition der gemeinsamen Wahrscheinlichkeit zweier Ereignisse wieder.\nErläutern Sie die intuitive Bedeutung der gemeinsamen Wahrscheinlichkeit zweier Ereignisse.\nGeben Sie das Theorem zu weiteren Eigenschaften von Wahrscheinlichkeiten wieder.\nGeben Sie die Definition der bedingten Wahrscheinlichkeit eines Ereignisses wieder.\nGeben Sie die Definition der bedingten Wahrscheinlichkeit wieder.\nGeben Sie das Theorem zu gemeinsamen und bedingten Wahrscheinlichkeiten wieder.\nGeben Sie das Gesetz von der totalen Wahrscheinlichkeit wieder.\nGeben Sie das Theorem von Bayes wieder.\nGeben Sie den Beweis des Theorems von Bayes wieder.\nGeben Sie die Definition der Unabhängigkeit zweier Ereignisse wieder.\nGeben Sie das Theorem zur bedingten Wahrscheinlichkeit unter Unabhängigkeit wieder.\nGeben Sie den Beweis des Theorems zur bedingten Wahrscheinlichkeit unter Unabhängigkeit wieder.\nErläutern Sie das Theorem zur bedingten Wahrscheinlichkeit unter Unabhängigkeit.\n\n\n\n\n\nBayes, T. (1763). An essay towards solving a problem in the doctrine of chances. By the late Rev. Mr. Bayes, F. R. S. communicated by Mr. Price, in a letter to John Canton, A. M. F. R. S. Philosophical Transactions of the Royal Society of London, 53, 370–418. https://doi.org/10.1098/rstl.1763.0053\n\n\nDeGroot, M. H., & Schervish, M. J. (2012). Probability and Statistics (4th ed). Addison-Wesley.\n\n\nHald, A. (1990). A History of Probability and Statistics and Their Applications before 1750. Wiley.\n\n\nHerzog, S., & Ostwald, D. (2013). Sometimes Bayesian Statistics Are Better. Nature, 494(7435), 35–35. https://doi.org/10.1038/494035b\n\n\nMeintrup, D., & Schäffler, S. (2005). Stochastik: Theorie und Anwendungen. Springer.\n\n\nSchmidt, K. D. (2009). Maß und Wahrscheinlichkeit. Springer.\n\n\nVon Plato, J. (1994). Creating Modern Probability: Its Mathematics, Physics, and Philosophy in Historical Perspective. Cambridge University Press.",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Elementare Wahrscheinlichkeiten</span>"
    ]
  },
  {
    "objectID": "203-Zufallsvariablen.html",
    "href": "203-Zufallsvariablen.html",
    "title": "13  Zufallsvariablen",
    "section": "",
    "text": "13.1 Konstruktion, Definition und Intuition\nWir skizzieren zunächst die Konstruktion einer Zufallsvariable und ihrer Verteilung anhand von Abbildung 13.1. Dazu sei \\((\\Omega,\\mathcal{A},\\mathbb{P})\\) ein Wahrscheinlichkeitsraum und \\[\\begin{equation}\n\\xi : \\Omega \\to \\mathcal{X}, \\omega \\mapsto \\xi(\\omega)\n\\end{equation}\\] eine Abbildung. Weiterhin sei \\(\\mathcal{S}\\) eine \\(\\sigma\\)-Algebra auf der Zielmenge \\(\\mathcal{X}\\) dieser Abbildung. Für jedes \\(S \\in \\mathcal{S}\\) sei die Urbildmenge von \\(S\\) gegeben durch (vgl. Definition 4.2) \\[\\begin{equation}\n\\xi^{-1}(S) := \\{\\omega \\in \\Omega|\\xi(\\omega) \\in S\\}.\n\\end{equation}\\] Wenn nun \\(\\xi^{-1}(S) \\in \\mathcal{A}\\) für alle \\(S \\in \\mathcal{S}\\) gilt, dann nennt man die Abbildung \\(\\xi\\) . Nehmen wir also an \\(\\xi\\) sei messbar. Dann kann allen \\(S \\in \\mathcal{S}\\) die Wahrscheinlichkeit \\[\\begin{equation}\n\\mathbb{P}_\\xi : \\mathcal{S} \\to [0,1], S \\mapsto\n\\mathbb{P}_\\xi(S)\n:= \\mathbb{P}\\left(\\xi^{-1}(S)\\right)\n= \\mathbb{P}\\left(\\{\\omega \\in \\Omega|\\xi(\\omega) \\in S\\}\\right)\n\\end{equation}\\] zugeordnet werden. In diesem Kontext nennt man \\(\\xi\\) nun eine Zufallsvariable und \\(\\mathbb{P}_\\xi\\) heißt das Bildmaß oder die Verteilung von \\(\\xi\\). Insgesamt wurde damit ausgehend von dem Wahrscheinlichkeitsraum \\((\\Omega,\\mathcal{A},\\mathbb{P})\\) mithilfe der Zufallsvariable \\(\\xi\\) der Wahrscheinlichkeitsraum \\((\\mathcal{X},\\mathcal{S},\\mathbb{P}_\\xi)\\) konstruiert. Formal ist eine Zufallsvariable damit wie folgt definiert.\nNach Definition 13.1 sind Zufallsvariablen weder “zufällig” noch “Variablen”, sondern messbare Abbildungen. Fragt man nach der Bedeutung des Zufalls für die Werte \\(\\xi(\\omega)\\) von Zufallsvariablen, so vermittelt weiterhin die implizite Frequentistische Mechanik des Wahrscheinlichkeitsraums \\((\\Omega, \\mathcal{A}, \\mathbb{P})\\) eine entsprechende Intuition: In jedem Durchgang des modellierten Zufallsvorgangs wird dabei ein \\(\\omega\\) anhand von \\(\\mathbb{P}\\) realisiert und dann (deterministisch) auf \\(\\xi(\\omega)\\) abgebildet. Wir definieren dementsprechend die Begriff des Ergebnisraums und der Realisierung einer Zufallsvariable.",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "203-Zufallsvariablen.html#sec-konstruktion-definition-und-intuition",
    "href": "203-Zufallsvariablen.html#sec-konstruktion-definition-und-intuition",
    "title": "13  Zufallsvariablen",
    "section": "",
    "text": "Abbildung 13.1: Konstruktion von Zufallsvariable und Verteilung.\n\n\n\n\nDefinition 13.1 (Zufallsvariable) Es sei \\((\\Omega, \\mathcal{A}, \\mathbb{P})\\) ein Wahrscheinlichkeitsraum und \\((\\mathcal{X},\\mathcal{S})\\) ein . Dann ist eine definiert als eine Abbildung \\(\\xi:\\Omega \\to \\mathcal{X}\\) mit der \\[\\begin{equation}\n\\{\\omega \\in \\Omega|\\xi(\\omega) \\in S \\} \\in \\mathcal{A} \\mbox{ für alle } S \\in \\mathcal{S}.\n\\end{equation}\\]\n\n\n\nDefinition 13.2 (Realisierung einer Zufallsvariable) \\((\\Omega, \\mathcal{A}, \\mathbb{P})\\) sei ein Wahrscheinlichkeitsraum, \\((\\mathcal{X},\\mathcal{S})\\) sei ein Messraum und \\(\\xi : \\Omega \\to \\mathcal{X}\\) sei eine Zufallsvariable. Dann heißt \\(\\mathcal{X}\\) der Ergebnisraum der Zufallsvariable \\(\\xi\\) und ein \\(\\xi(\\omega) \\in \\mathcal{X}\\) heißt eine Realisierung der Zufallvariable \\(\\xi\\).\n\n\nBeispiel\nAufbauend auf dem in Kapitel 11.3 betrachteten Beispiel eines Wahrscheinlichkeitsraums zur Modellierung des geichzeitigen Würfelns mit einem blauem und einem roten Würfel wollen wir mit der Summe der Würfelaugenzahlen ein erstes Beispiel für eine Zufallsvariable und ihre Verteilung betrachten. Wir haben in Kapitel 11.3 gesehen, dass ein sinnvolles Wahrscheinlichkeitsraummodell für das geichzeitige Würfeln mit einem blauem und einem roten Würfel durch \\((\\Omega,\\mathcal{A}, \\mathbb{P})\\) mit\n\n\\(\\Omega := \\{(r,b)| r  \\in \\mathbb{N}_6, b \\in \\mathbb{N}_6\\}\\),\n\\(\\mathcal{A} := \\mathcal{P}(\\Omega)\\) und\n\\(\\mathbb{P} : \\mathcal{A} \\to [0,1]\\) mit \\(\\mathbb{P}(\\{(r,b)\\}) = 1/36\\) für alle \\((r,b) \\in \\Omega\\).\n\ngegeben ist. Die Auswertung der Summe der beiden Würfelaugenzahlen wird dann sinnvoller Weise durch die Abbildung \\[\\begin{equation}\n\\xi : \\Omega \\to \\mathcal{X}, (r,b) \\mapsto \\xi((r,b)) := r + b,\n\\end{equation}\\] beschrieben, wobei offenbar \\(\\mathcal{X} := \\{2,3,...,12\\}\\) gelten muss. Der Ergebnisraum der Zufallsvariable ist also wiederrum endlich und \\(\\mathcal{S} := \\mathcal{P}(\\mathcal{X})\\) ist eine sinnvolle \\(\\sigma\\)-Algebra auf \\(\\mathcal{X}\\). Mithilfe der \\(\\sigma\\)-Addivität von \\(\\mathbb{P}\\) können wir nun die Verteilung \\(\\mathbb{P}_\\xi\\) von \\(\\xi\\) für alle Elementarereignisse \\(\\{x\\} \\in \\mathcal{S}\\) berechnen und damit insbesondere auch die Messbarkeit von \\(\\xi\\) nachweisen, wie in untenstehender Tabelle gezeigt.\nDie Wahrscheinlichkeiten der Elementarereignisse in \\(\\mathcal{S}\\) wiederrum erlauben mithilfe des Begriffs der Wahrscheinlichkeitsmassefunktion (vgl. Kapitel 11.2) das Berechnen beliebiger Ereigniswahrscheinlichkeiten hinsichtlich der Würfelaugenzahlsumme. Insgesamt haben wir basierend auf \\((\\Omega, \\mathcal{A}, \\mathbb{P})\\) und \\(\\xi\\) also ein weiteres Wahrscheinlichkeitsraummodell \\((\\mathcal{X}, \\mathcal{S}, \\mathbb{P}_\\xi)\\) konstruiert.\nFolgender R Code demonstriert, wie mithilfe der computerbasierten Erzeugung zufälliger Ergebnisse die Konstruktion des hier betrachteten Beispiels für einen Durchgang eines Zufallsvorgangs simuliert werden kann.\n\n# Wahrscheinlichkeitsraummodellformulierung\nOmega    = list()                                 # Ergebnisrauminitialisierung\nidx      = 1                                      # Ergebnisindexinitialisierung\nfor(r in 1:6){                                    # Ergebnisse roter  Würfel\n    for(b in 1:6){                                # Ergebnisse blauer Würfel\n        Omega[[idx]] = c(r,b)                     # \\omega \\in \\Omega\n        idx          = idx + 1 }}                 # Ergebnisindexupdate\nK        = length(Omega)                          # Kardinalität von \\Omega\npi       = rep(1/K,1,K)                           # Wahrscheinlichkeitsfunktion \\pi\n\n# Durchgang des Zufallsvorgangs\nomega    = Omega[[which(rmultinom(1,1,pi) == 1)]] # Auswahl von \\omega anhand \\mathbb{P}({\\omega})\n\n# Realisierung der Zufallsvariable\nxi_omega = sum(omega)                             # \\xi(\\omega)\n\n\n\nomega     :  6 1 \nxi(omega) :  7\n\n\nIm Kontext des Vermessens zufällig ausgewählter experimenteller Einheiten dienen Zufallsvariablen oft als Modelle für Messvorgänge. Betrachten wie beispielsweise die Bestimmung des Wertes eines Intelligenztests an zufällig ausgewählten Proband:innen (Abbildung 13.2), so ergibt sich folgende Interpretation: Der Ergebnisraum des zugrundeliegenden Wahrscheinlichkeitsraums \\((\\Omega)\\) soll die Gesamtheit aller in Frage kommender Proband:innen darstellen und die Auswahl einer Proband:in aus diesem Raum die Auswahl eines Ergebnisses \\(\\omega\\), welche mit Wahrscheinlichkeit \\(\\mathbb{P}(\\{\\omega\\})\\) geschehen soll. Wird nun eine bestimmte Eigenschaft dieser Proband:in in idealisierter Weise gemessen, so handelt es sich dabei um eine deterministische Abbildung auf einen dieser Probandin zugeordneten Messwert \\(\\xi(\\omega)\\) im Raum der Messwerte \\(\\mathcal{X}\\). Die Messwerte selbst unterliegen dann einer Wahrscheinlichkeitsverteilung, die durch die zugrundeliegende Verteilung und die Art des Messvorgangs bestimmt wird.\n\n\n\n\n\n\nAbbildung 13.2: Zufallsvariable als Modell eines Messvorgangs.\n\n\n\n\n\nNotation\nDie Konventionen zur Notation der mit Zufallsvariablen assoziierten Wahrscheinlichkeiten und Verteilungen sind etwas gewöhnungsbedürftig, so dass wir sie in folgender Definition festhalten wollen.\n\nDefinition 13.3 (Notation für Zufallsvariablen) Es seien \\((\\Omega,\\mathcal{A},\\mathbb{P})\\) und \\((\\mathcal{X},\\mathcal{S},\\mathbb{P}_\\xi)\\) Wahrscheinlichkeitsräume und \\(\\xi : \\Omega \\to \\mathcal{X}\\) sei eine Zufallsvariable. Dann gelten mit \\(S \\in \\mathcal{S}\\) und \\(x \\in \\mathcal{X}\\) folgende Notationskonventionen für Ereignisse in \\(\\mathcal{A}\\): \\[\\begin{align*}\n\\begin{split}\n\\{\\xi \\in S\\} & := \\{\\omega \\in \\Omega|\\xi(\\omega) \\in S\\}  \\\\\n\\{\\xi  =  x\\} & := \\{\\omega \\in \\Omega|\\xi(\\omega)  =  x\\}  \\\\\n\\{\\xi \\le x\\} & := \\{\\omega \\in \\Omega|\\xi(\\omega) \\le x\\}  \\\\\n\\{\\xi  &lt;  x\\} & := \\{\\omega \\in \\Omega|\\xi(\\omega)  &lt;  x\\}  \\\\\n\\{\\xi \\ge x\\} & := \\{\\omega \\in \\Omega|\\xi(\\omega) \\ge x\\}  \\\\\n\\{\\xi  &gt;  x\\} & := \\{\\omega \\in \\Omega|\\xi(\\omega)  &gt;  x\\}  \\\\\n\\end{split}\n\\end{align*}\\] Aus diesen Konventionen ergeben sich folgende Konventionen für Wahrscheinlichkeiten von Verteilungen \\[\\begin{align*}\n\\begin{split}\n    \\mathbb{P}_\\xi\\left(\\xi \\in S\\right)\n& = \\mathbb{P}\\left(\\{\\xi \\in S\\} \\right)\n  = \\mathbb{P}\\left( \\{\\omega \\in \\Omega|\\xi(\\omega) \\in S\\}\\right) \\\\\n    \\mathbb{P}_\\xi\\left(\\xi = x \\right)\n& = \\mathbb{P}\\left(\\{\\xi = x\\} \\right)\n  = \\mathbb{P}\\left( \\{\\omega \\in \\Omega|\\xi(\\omega) = x\\}\\right)  \\\\\n    \\mathbb{P}_\\xi\\left(\\xi \\le x \\right)\n& = \\mathbb{P}\\left(\\{\\xi \\le x\\} \\right)\n  = \\mathbb{P}\\left( \\{\\omega \\in \\Omega|\\xi(\\omega) \\le x\\}\\right) \\\\\n    \\mathbb{P}_\\xi\\left(\\xi &lt; x \\right)\n& = \\mathbb{P}\\left(\\{\\xi &lt; x\\} \\right)\n  = \\mathbb{P}\\left( \\{\\omega \\in \\Omega|\\xi(\\omega) &lt; x\\}\\right)   \\\\\n    \\mathbb{P}_\\xi\\left(\\xi \\ge x \\right)\n& = \\mathbb{P}\\left(\\{\\xi \\ge x\\} \\right)\n  = \\mathbb{P}\\left(\\{\\omega \\in \\Omega|\\xi(\\omega) \\ge x\\}\\right) \\\\\n    \\mathbb{P}_\\xi\\left(\\xi &gt; x \\right)\n& = \\mathbb{P}\\left(\\{\\xi &lt; x\\} \\right)\n  = \\mathbb{P}\\left( \\{\\omega \\in \\Omega|\\xi(\\omega) &gt; x\\} \\right). \\\\\n\\end{split}\n\\end{align*}\\] Oft wird zudem auf das Subskript bei Verteilungssymbolen verzichtet und zum Beispiel \\(\\mathbb{P}_\\xi\\left(\\xi \\in S\\right)\\) nur als \\(\\mathbb{P}\\left(\\xi \\in S\\right)\\) geschrieben, solange sich aus dem Kontext keine Verwechselungsmöglichkeit der beiden Wahrscheinlichkeitsmaße ergeben kann.\n\nWir wollen diesen Abschnitt mit einem technischem Theorem zum Rechnen mit Zufallsvariablen abschließen.\n\nTheorem 13.1 (Arithmetik reeller Zufallsvariablen) \\((\\Omega, \\mathcal{A}, \\mathbb{P})\\) sei ein Wahrscheinlichkeitsraum, \\((\\mathbb{R}, \\mathcal{B}(\\mathbb{R}))\\) sei der reelle Messraum, \\(\\xi : \\Omega \\to \\mathbb{R}\\), \\(\\upsilon : \\Omega \\to  \\mathbb{R}\\) seien reellwertige Zufallsvariablen und \\(c \\in \\mathbb{R}\\) sei eine Konstante. Weiterhin seien \\[\\begin{align}\n\\begin{split}\n\\xi + c         & : \\Omega \\to \\mathbb{R}, \\omega \\mapsto (\\xi + c)(\\omega)\\,             := \\xi(\\omega) + c              \\mbox{ für } c \\in \\mathbb{R}   \\\\\nc\\xi            & : \\Omega \\to \\mathbb{R}, \\omega \\mapsto (c\\xi)(\\omega) \\quad\\,\\,\\,      := c\\xi(\\omega)                 \\mbox{ für } c \\in \\mathbb{R}   \\\\\n\\xi + \\upsilon  & : \\Omega \\to \\mathbb{R}, \\omega \\mapsto (\\xi + \\upsilon)(\\omega)        := \\xi(\\omega) + \\upsilon(\\omega)                               \\\\\n\\xi\\upsilon    & : \\Omega \\to \\mathbb{R}, \\omega \\mapsto (\\xi\\upsilon)(\\omega)\\quad\\,\\,\\, := \\xi(\\omega)\\upsilon(\\omega)                                  \\\\\n\\end{split}\n\\end{align}\\] die Addition einer Konstante zu einer reellwertigen Zufallsvariable, die Multiplikation einer reellwertigen Zufallsvariable mit einer Konstante, die Addition zweier reellwertiger Zufallsvariablen und die Multiplikation zweier reellwertigen Zufallsvariablen, respektive. Dann sind auch \\(\\xi + c\\), \\(c\\xi\\), \\(\\xi + \\upsilon\\) und \\(\\xi\\upsilon\\) reellwertige Zufallsvariablen.\n\nFür einen Beweis dieses Theorems verweisen wir auf die weiterführende Literatur, beispielsweise Hesse (2009). Intuitiv besagt Theorem 13.1, dass sowohl Addition einer zufälligen Größe zu einer konstanten Größe, als auch die Multiplikation einer zufälligen Größe mit einer Konstante, als auch die Addition zweier zufälliger Größen und schließlich auch die Multiplikation zweier zufälliger Größen immer wieder zufällige Größen mit ihren dann eigenen Verteilungen ergeben.",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "203-Zufallsvariablen.html#sec-wahrscheinlichkeitsmassefunktionen",
    "href": "203-Zufallsvariablen.html#sec-wahrscheinlichkeitsmassefunktionen",
    "title": "13  Zufallsvariablen",
    "section": "13.2 Wahrscheinlichkeitsmassefunktionen",
    "text": "13.2 Wahrscheinlichkeitsmassefunktionen\nIn diesem Abschnitt führen wir mit den Wahrscheinlichkeitsmassefunktionen (WMFen) ein Hilfsmittel ein, um Verteilungen von Zufallsvariablen mit diskretem (genauer endlichem oder abzählbarem) Ergebnisraum zu definieren. Wir illustrieren den Begriff anhand dreier wichtiger Beispiele, den Bernoulli- und Binomialzufallsvariablen sowie den diskret-gleichverteilten Zufallsvariablen. Wir definieren zunächst den Begriff der WMF wie folgt.\n\nDefinition 13.4 (Diskrete Zufallsvariable und Wahrscheinlichkeitsmassefunktion) Eine Zufallsvariable \\(\\xi\\) heißt , wenn ihr Ergebnisraum \\(\\mathcal{X}\\) endlich oder abzählbar ist und eine Funktion der Form \\[\\begin{equation}\np_\\xi: \\mathcal{X} \\to \\mathbb{R}_{\\ge 0}, x \\mapsto p_\\xi(x)\n\\end{equation}\\] existiert, für die gilt\n\n\\(\\sum_{x \\in \\mathcal{X}} p_\\xi(x) = 1\\) und\n\\(\\mathbb{P}_\\xi(\\xi = x) = p_\\xi(x)\\) für alle \\(x \\in \\mathcal{X}\\).\n\nEine entsprechende Funktion \\(p_\\xi\\) heißt von \\(\\xi\\).\n\nWir erinnern daran, dass eine Menge abzählbar heißt, wenn sie bijektiv auf \\(\\mathbb{N}\\) abgebildet werden kann. Im Deutschen nennt man WMFen auch Zähldichten. Im Englischen nennt man WMFen probability mass functions (PMFs), an diesem Begriff orientieren wir uns hier. Der notationellen Einfachheit halber verzichtet man wie bei den Bildmaßen auch bei WMFen meist auf das Subskript \\(\\xi\\), schreibt also einfach \\(p(x)\\) anstelle von \\(p_\\xi(x)\\), wenn aus dem Kontext klar ist, auf welche Zufallsvariable sich die WMF bezieht. Ohne Beweis halten wir fest, dass jede Funktion \\(p : \\mathcal{X} \\to \\mathbb{R}_{\\ge 0}\\), die die Normiertheiteigenschaft \\(\\sum_{x \\in \\mathcal{X}} p(x) = 1\\) besitzt als WMF einer Zufallsvariable interpretiert werden kann.\n\nBeispiele\nWir wollen mit den Bernoulli-Zufallsvariablen, den Binomial-Zufallsvariablen und den Diskrete-gleichverteilten Zufallsvariablen drei erste Beispiel für die Definition von Verteilungen mithilfe von WMFen betrachten.\n\nDefinition 13.5 (Bernoulli Zufallsvariable) Es sei \\(\\xi\\) eine Zufallsvariable mit Ergebnisraum \\(\\mathcal{X} = \\{0,1\\}\\) und WMF \\[\\begin{equation}\np : \\mathcal{X} \\to [0,1], x\\mapsto p(x) := \\mu^{x}(1 - \\mu)^{1-x} \\mbox{ mit } \\mu \\in [0,1].\n\\end{equation}\\] Dann sagen wir, dass \\(\\xi\\) einer unterliegt und nennen \\(\\xi\\) eine . Wir kürzen dies mit \\(\\xi \\sim \\mbox{Bern}(\\mu)\\) ab. Die WMF einer Bernoulli-Zufallsvariable bezeichnen wir mit \\[\\begin{equation}\n\\mbox{Bern}(x;\\mu) := \\mu^x (1 - \\mu)^{1 - x}.\n\\end{equation}\\]\n\nBernoulli-Zufallsvariablen können immer dann zur probabilistischen Modellierung genutzt werden, wenn das betrachtete Phänomen binär ist und die möglichen Werte der Zufallsvariable bijektiv auf \\(\\{0,1\\}\\) abgebildet werden können. Man beachte, dass die funktionale Form der Bernoulli-Zufallsvariable \\(\\mbox{Bern}(x;\\mu)\\) nur für \\(x \\in \\{0,1\\}\\) Sinn ergibt und nicht etwa für \\(x \\in \\{\\mbox{Heads}, \\mbox{Tails}\\}\\). Bildet man aber die möglichen Ergebnisse eines Münzwurfes auf \\(\\{0,1\\}\\) ab, also definiert etwa \\(0 := \\mbox{Heads}\\) und \\(1 := \\mbox{Tails}\\), so kann eine Bernoulli-Zufallsvariable durchaus als Modell eines Münzwurfs dienen.\nDer Parameter \\(\\mu \\in [0,1]\\) einer Bernoulli-Zufallsvariable ist die Wahrscheinlichkeit dafür, dass die Zufallsvariable \\(\\xi\\) den Wert 1 annimmt, dies erkennt man anhand von \\[\\begin{equation}\n\\mathbb{P}(\\xi = 1) = \\mu^1 (1 -\\mu)^{1-1} = \\mu.\n\\end{equation}\\]\nWir visualisieren die WMFen von Bernoulli-Zufallsvariablen für \\(\\mu := 0.1, \\mu :=  0.5\\) und \\(\\mu := 0.7\\) in Abbildung 13.3.\n\n\n\n\n\n\nAbbildung 13.3: WMFen von Bernoulli-Zufallsvariablen.\n\n\n\n\nDefinition 13.6 (Binomialzufallsvariable) Es sei \\(\\xi\\) eine Zufallsvariable mit Ergebnisraum \\(\\mathcal{X} := \\mathbb{N}_n^0\\) und WMF \\[\\begin{equation}\np : \\mathcal{X} \\to [0,1],\nx\\mapsto p(x) :=\n\\begin{pmatrix}\nn \\\\ x\n\\end{pmatrix}\n\\mu^{x}(1 - \\mu)^{n-x} \\mbox{ für } \\mu \\in [0,1].\n\\end{equation}\\] Dann sagen wir, dass \\(\\xi\\) einer unterliegt und nennen \\(\\xi\\) eine Binomial-Zufallsvariable. Wir kürzen dies mit \\(\\xi \\sim \\mbox{Bin}(\\mu,n)\\) ab. Die WMF einer Binomial-Zufallsvariable bezeichnen wir mit \\[\\begin{equation}\n\\mbox{Bin}(x;\\mu,n) :=\n\\begin{pmatrix}\nn \\\\ x\n\\end{pmatrix}\n\\mu^{x}(1 - \\mu)^{n-x}.\n\\end{equation}\\]\n\nOhne Beweis halten wir fest, dass eine Binomial-Zufallsvariable als Modell der Summe von \\(n\\) unabhängig und identisch verteilten Bernoulli-Zufallsvariablen genutzt werden kann. Insbesondere gilt also \\(\\mbox{Bin}(x;\\mu,1) = \\mbox{Bern}(x;\\mu)\\). Binomial-Zufallsvariablen haben die Eigenschaft, dass mit \\(n\\) einer ihrer Parameter nicht nur die funktionale Form ihrer WMF, sondern auch ihren Ergebnisraum \\(\\mathcal{X}\\) festlegt. Wir visualisieren die WMFen von Binomial-Zufallsvariablen für \\((\\mu,n) := (0.1,5),  (\\mu,n) := (0.5,10)\\) und \\((\\mu,n) := (0.7,15)\\) in Abbildung 13.4.\n\n\n\n\n\n\nAbbildung 13.4: WMFen von Binomial-Zufallsvariablen.\n\n\n\n\nDefinition 13.7 (Diskret-gleichverteilte Zufallsvariable) Es sei \\(\\xi\\) eine diskrete Zufallsvariable mit endlichem Ergebnisraum \\(\\mathcal{X}\\) und WMF \\[\\begin{equation}\np : \\mathcal{X} \\to \\mathbb{R}_{\\ge 0}, x\\mapsto p(x) := \\frac{1}{|\\mathcal{X}|}.\n\\end{equation}\\] Dann sagen wir, dass \\(\\xi\\) einer unterliegt und nennen \\(\\xi\\) eine . Wir kürzen dies mit \\(\\xi \\sim U(|\\mathcal{X}|)\\) ab. Die WMF einer diskret-gleichverteilten Zufallsvariable bezeichnen wir mit \\[\\begin{equation}\nU(x;|\\mathcal{X}|) := \\frac{1}{|\\mathcal{X}|}.\n\\end{equation}\\]\n\nDiskrete-gleichverteilte Zufallsvariable können offenbar immer dann zur probabilistischen Modellierung genutzt werden, wenn die möglichen diskreten Ergebnisse des modellierten Phänomenens die gleiche Wahrscheinlichkeit haben. Im Fall der diskret-gleichverteilten Zufallsvariablen braucht es zur Definition ihrer funktionalen Form nach Festlegung des Ergebnisraums keinen weiteren Parameter. Offenbar gilt für \\(\\mathcal{X} := \\{0,1\\}\\). \\[\\begin{equation}\nU(x;|\\mathcal{X}|) = \\mbox{Bern}(x;0.5) = \\mbox{Bin}(x;1,0.5)\n\\end{equation}\\] Wir visualisieren die WMFen von diskret-gleichverteilten Zufallsvariablen für \\(\\mathcal{X} := \\{\\{0,1\\}\\, \\mathcal{X} := \\{-3,-2,-1,0,1\\}\\}\\) und \\(\\mathcal{X} := \\{-4,-3,-2,-1,0,1,2,3,4\\}\\) in Abbildung 13.5. Man beachte, dass Definition 13.7 formal die Sequentialität der Elemente von \\(\\mathcal{X}\\) nicht erfordert, man kann also genauso eine diskret-gleichverteilte Zufallsvariable mit Ergebnisraum \\(\\mathcal{X} := \\{1,5,7\\}\\) oder auch nicht numerischen Ergebnisraum \\(\\mathcal{X} := \\{a,b,x,y\\}\\) definieren.\n\n\n\n\n\n\nAbbildung 13.5: WMFen von diskret-gleichverteilten Zufallsvariablen.",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "203-Zufallsvariablen.html#sec-wahrscheinlichkeitsdichtefunktionen",
    "href": "203-Zufallsvariablen.html#sec-wahrscheinlichkeitsdichtefunktionen",
    "title": "13  Zufallsvariablen",
    "section": "13.3 Wahrscheinlichkeitsdichtefunktionen",
    "text": "13.3 Wahrscheinlichkeitsdichtefunktionen\nIn diesem Abschnitt führen wir mit den Wahrscheinlichkeitsdichtefunktionen (WDFen) ein Hilfsmittel ein, um Verteilungen von Zufallsvariablen mit kontinuierlichem (genauer überabzählbarem) Ergebnisraum zu definieren. Wir illustrieren den Begriff zunächst an der grundlegendsten aller Zufallsvariablen, der normalverteilten Zufallsvariable. Mit der Gamma-Zufallsvariable, der Beta-Zufallsvariable und der diskret-gleichverteilten Zufallsvariable wollen wir dann noch drei Beispiele von Zufallsvariablen betrachten, die sowohl in der Modellformulierung der Frequentistischen als auch der Bayesianischen Inferenz an vielen Stellen eingesetzt werden. Wir definieren den Begriff der WDF wie folgt.\n\nDefinition 13.8 (Kontinuierliche Zufallsvariable und Wahrscheinlichkeitsdichtefunktion) Eine Zufallsvariable \\(\\xi\\) heißt , wenn \\(\\mathbb{R}\\) der Ergebnisraum von \\(\\xi\\) ist und eine Funktion \\[\\begin{equation}\np_\\xi : \\mathbb{R} \\to \\mathbb{R}_{\\ge 0}, x \\mapsto p_\\xi(x)\n\\end{equation}\\] existiert, für die gilt\n\n\\(\\int_{-\\infty}^{\\infty} p_\\xi(x)\\,dx = 1\\) und\n\\(\\mathbb{P}_\\xi(\\xi \\in [a,b]) = \\int_a^b p_\\xi(x)\\,dx\\) für alle \\(a,b\\in\\mathbb{R}\\) mit \\(a \\le b\\).\n\nEine entsprechende Funktion \\(p_\\xi\\) heißt Wahrscheinlichkeitsdichtefunktion (WDF) von \\(\\xi\\).\n\nIm Englischen nennt man WDFen probability density functions (PDFs). Der notationellen Einfachheit halber verzichtet man wie bei den Bildmaßen und den WMFen auch bei WDFen meist auf das Subskript \\(\\xi\\), schreibt also einfach \\(p(x)\\) anstelle von \\(p_\\xi(x)\\), wenn aus dem Kontext klar ist, auf welche Zufallsvariable sich die WDF bezieht. Ohne Beweis halten wir fest, dass jede Funktion \\(p : \\mathbb{R} \\to \\mathbb{R}_{\\ge 0}\\), für deren uneigentliches Integral \\(\\int_{-\\infty}^{\\infty} p_\\xi(x)\\,dx = 1\\) gilt, die also normiert ist, als WDF einer Zufallsvariable interpretiert werden kann.\nIm Umgang mit WDFen und in Abgrenzung zu WMFen sollte man sich die Dichteeigenschaft einer WDF immer bewusst machen: Die Werte einer WDF stellen keine Wahrscheinlichkeiten, sondern Wahrscheinlichkeitsdichten dar, Wahrscheinlichkeiten werden aus WDFen durch Integration berechnet. Wie im physikalischen Sinn ergibt sich die einem reellen Intervall zugeordnete Wahrscheinlichkeit(smasse) also erst durch “Multiplikation” mit dem enstprechenden “Intervallvolumen”. Man denke hierzu auch an die Approximation des bestimmten Integrals \\(\\int_a^b p_\\xi(x)\\,dx\\) durch einen Riemannschen Summenterm (vgl. Definition 7.3). Intuitiv gilt also \\[\\begin{equation}\n\\mbox{(Wahrscheinlichkeits)Masse} = \\mbox{(Wahrscheinlichkeits)Dichte} \\cdot \\mbox{(Mengen)Volumen,}\n\\end{equation}\\] wobei sich das Volumen im Sinne des Lebesgue-Maßes auf die Breite des Intervalls \\([a,b]\\) bezieht. Wie in der physikalischen Analogie ist die Wahrscheinlichkeitsmasse eines Intervalls ohne Volumen gleich Null,\n\\[\\begin{equation}\n\\mathbb{P}_\\xi(\\xi = a) = \\int_a^a p(x) \\,dx = 0.\n\\end{equation}\\] Ferner gilt, dass bei entsprechend kleinen Intervallen WDFen auch Werte größer als \\(1\\) annehmen können, auch wenn dies für die Wahrscheinlichkeit, die sich dann durch entsprechende Integration ergibt, nicht der Fall sein kann. Schließlich sei trotz dieser technischen Feinheiten folgende Intuition betont: Betrachtet man die graphische Darstellung der WDF einer Zufallsvariable und stellt sich eine Zerlegung von \\(\\mathbb{R}\\) in gleich große Intervalle vor, so besitzt die Zufallsvariable natürlich eine höhere Wahrscheinlichkeit dafür, Werte in einem Intervall mit assoziierter höherer Wahrscheinlichkeitsdichte anzunehmen als Werte in einem Intervall mit assoziierter relativ niedrigerer Wahrscheinlichkeitsdichte.\n\nNormalverteilte Zufallsvariablen\nMit der normalverteilten Zufallsvariable wollen wir als erstes Beispiel für die Definition einer kontinuierlichen Zufallsvariable mithile einer WDF nun die wichtigste Zufallsvariable der probabilistischen Modellbildung einführen.\n\nDefinition 13.9 (Normalverteilte Zufallsvariable) Es sei \\(\\xi\\) eine Zufallsvariable mit Ergebnisraum \\(\\mathbb{R}\\) und WDF \\[\\begin{equation}\np : \\mathbb{R} \\to \\mathbb{R}_{&gt;0}, x\\mapsto p(x)\n:= \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left(-\\frac{1}{2\\sigma^2}(x - \\mu)^2\\right).\n\\end{equation}\\] Dann sagen wir, dass \\(\\xi\\) einer Normalverteilung mit Parametern \\(\\mu \\in \\mathbb{R}\\) und \\(\\sigma^2 &gt; 0\\) unterliegt und nennen \\(\\xi\\) eine normalverteilte Zufallsvariable. Wir kürzen dies mit \\(\\xi \\sim N\\left(\\mu,\\sigma^2\\right)\\) ab. Die WDF einer normalverteilten Zufallsvariable bezeichnen wir mit \\[\\begin{equation}\nN\\left(x;\\mu,\\sigma^2\\right) := \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left(-\\frac{1}{2\\sigma^2}(x - \\mu)^2\\right).\n\\end{equation}\\] Eine normalverteilte Zufallsvariable mit \\(\\mu = 0\\) und \\(\\sigma^2 = 1\\) nennt man eine standardnormalverteilte Zufallsvariable\n\nWir visualisieren die WDFen von normalverteilten Zufallsvariablen für \\((\\mu,\\sigma^2) := (0,1), (\\mu,\\sigma^2) := (-2.5, 10)\\) und \\((\\mu,\\sigma^2) := (3, 0.5)\\) in Abbildung 13.6. Man macht sich an dieser Abbildung graphisch klar, dass die WDFen von normalverteilten Zufallsvariablen immer genau einen Werte höchster Wahrscheinlichkeitsdichte haben und zwar an der Stelle des Parameters \\(\\mu \\in \\mathbb{R}\\). Dies ergibt sich durch die Tatsache, dass das Argument der Exponentialfunktion in der funktionalen Form von \\(N(x;\\mu,\\sigma^2)\\) aufgrund des negativen Vorzeichens des Quadrates von \\(x-\\mu\\) und der Positivität von \\(\\sigma^2\\) immer nicht-positiv ist und die Exponentialfunktion auf den nicht-positiven reellen Zahlen ihr Maximum bei \\(x = \\mu\\), also \\(x-\\mu = 0\\) annimmt. Weiterhin macht man sich graphisch klar, dass der Parameter \\(\\sigma^2&gt;0\\) die Breite der WDF einer normalverteilten Zufallsvariable enkodiert.\n\n\n\n\n\n\nAbbildung 13.6: WDFen von normalverteilten Zufallsvariablen.\n\n\n\n\n\nWeitere Beispiele\nWir wollen mit den Gamma-Zufallsvariablen, den Beta-Zufallsvariablen und den gleichverteilten Zufallsvariablen drei weitere Beispiele für die Definition von Verteilungen mithilfe von WDFen betrachten.\n\nDefinition 13.10 (Gamma-Zufallsvariable) Es sei \\(\\xi\\) eine Zufallsvariable mit Ergebnisraum \\(\\mathcal{X} := \\mathbb{R}_{&gt;0}\\) und WDF \\[\\begin{equation}\np : \\mathbb{R}_{&gt;0} \\to \\mathbb{R}_{&gt;0},  x \\mapsto p(x) :=\n\\frac{1}{\\Gamma(\\alpha)\\beta^{\\alpha}}x^{\\alpha-1}\\exp\\left(-\\frac{x}{\\beta}\\right),\n\\end{equation}\\] wobei \\(\\Gamma\\) die Gammafunktion bezeichne. Dann sagen wir, dass \\(\\xi\\) einer Gammaverteilung mit Formparameter \\(\\alpha &gt;0\\) und Skalenparameter \\(\\beta &gt; 0\\) unterliegt und nennen \\(\\xi\\) eine gammaverteilte Zufallsvariable. Wir kürzen dies mit \\(\\xi \\sim G(\\alpha,\\beta)\\) ab. Die WDF einer gammaverteilen Zufallsvariable bezeichnen wir mit \\[\\begin{equation}\nG(x;\\alpha,\\beta) := \\frac{1}{\\Gamma(\\alpha)\\beta^{\\alpha}}x^{\\alpha-1}\\exp\\left(-\\frac{x}{\\beta}\\right).\n\\end{equation}\\]\n\nDie spezielle Gamma-Zufallsvariable mit WDF \\(G\\left(x;\\frac{n}{2},2\\right)\\) wird Chi-Quadrat (\\(\\chi^2\\)) Verteilung mit \\(n\\) Freiheitsgraden genannt. Wir visualisieren die WDFen von Gamma-Zufallsvariablen für \\((\\alpha,\\beta) := (1,1),(\\alpha,\\beta) := (2,2)\\) und \\((\\alpha,\\beta) := (5,1)\\) in Abbildung 13.7.\n\n\n\n\n\n\nAbbildung 13.7: WDFen von Gamma-Zufallsvariablen.\n\n\n\n\nDefinition 13.11 (Beta-Zufallvariable) Es sei \\(\\xi\\) eine Zufallsvariable mit Ergebnisraum \\(\\mathcal{X} := [0,1]\\) und WDF \\[\\begin{equation}\np : \\mathcal{X} \\to [0,1], x \\mapsto p(x)\n:= \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\nx^{\\alpha-1}(1-x)^{\\beta-1} \\mbox{ mit } \\alpha,\\beta \\in \\mathbb{R}_{&gt;0},\n\\end{equation}\\] wobei \\(\\Gamma\\) die Gammafunktion bezeichne. Dann sagen wir, dass \\(\\xi\\) einer mit Parametern \\(\\alpha &gt;0\\) und \\(\\beta&gt;0\\) unterliegt, und nennen \\(\\xi\\) eine Beta-verteilte Zufallsvariable. Wir kürzen dies mit \\(\\xi \\sim \\mbox{Beta}(\\alpha,\\beta)\\) ab. Die WDF einer Beta-verteilten Zufallsvariable bezeichnen wir mit \\[\\begin{equation}\n\\mbox{Beta}(x;\\alpha,\\beta)\n:= \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\nx^{\\alpha-1}(1-x)^{\\beta-1}.\n\\end{equation}\\]\n\nDadurch, dass der Ergebnisraum einer Beta-Zufallsvariable auf das Intervall \\(\\mathcal{X} := [0,1]\\) (für \\(\\alpha &lt; 1, \\beta &lt; 1\\) genauer \\(\\mathcal{X} := ]0,1[\\)) beschränkt ist, bietet sich eine Beta-Zufallsvariable unter anderem dafür an, Wahrscheinlichkeiten von Wahrscheinlichkeiten (also Werten zwischen 0 und 1) zu beschreiben. Wir visualisieren die WDFen von Beta-Zufallsvariablen für \\((\\alpha,\\beta) := (1,1), (\\alpha,\\beta) := (3,2)\\) und \\((\\alpha,\\beta) := (10,5)\\) in Abbildung 13.8.\n\n\n\n\n\n\nAbbildung 13.8: WDFen von Beta-Zufallsvariablen.\n\n\n\nMit den gleichverteilten Zufallsvariablen betrachten wir abschließend noch das Analogon zu diskret-gleichverteilten Zufallsvariablen für den Fall kontinuierlicher Zufallsvariablen.\n\nDefinition 13.12 (Gleichverteilte Zufallsvariable) Es sei \\(\\xi\\) eine kontinuierliche Zufallsvariable mit Ergebnisraum \\(\\mathbb{R}\\) und WDF \\[\\begin{equation}\np : \\mathbb{R} \\to \\mathbb{R}_{\\ge 0}, x\\mapsto p(x) :=\n\\begin{cases}\n\\frac{1}{b - a} & x \\in [a,b] \\\\\n0                     & x \\notin [a,b]\n\\end{cases}.\n\\end{equation}\\] Dann sagen wir, dass \\(\\xi\\) einer Gleichverteilung mit Parametern \\(a\\) und \\(b\\) unterliegt und nennen \\(\\xi\\) eine gleichverteilte Zufallsvariable. Wir kürzen dies mit \\(\\xi \\sim U(a,b)\\) ab. Die WDF einer gleichverteilten Zufallsvariable bezeichnen wir mit \\[\\begin{equation}\nU(x;a,b) := \\frac{1}{b - a}.\n\\end{equation}\\]\n\nWir visualisieren die WDFen von gleichverteilten Zufallsvariablen für \\((a,b) := (0,1), (a,b) := (-3,1)\\) und \\((a,b) := (-4,4)\\) in Abbildung 13.9.\n\n\n\n\n\n\nAbbildung 13.9: WDFen von gleichverteilten-Zufallsvariablen.",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "203-Zufallsvariablen.html#sec-kumulative-verteilungsfunktion",
    "href": "203-Zufallsvariablen.html#sec-kumulative-verteilungsfunktion",
    "title": "13  Zufallsvariablen",
    "section": "13.4 Kumulative Verteilungsfunktionen",
    "text": "13.4 Kumulative Verteilungsfunktionen\nIm letzten Abschnitt dieses Kapitels wollen wir mit den kumulativen Verteilungsfunktionen (KVFen) eine weitere Möglichkeit einführen, die Verteilungen von diskreten oder kontinuierlichen Zufallsvariablen festzulegen. Es ist allgemein allerdings eher üblich, dies mithilfe von WMFen oder WDFen zu tun. Trotzdem sind KVFen an vielen Stellen nützlich, da sie sowohl für diskrete als nauch kontinuierliche Zufallsvariablen einen direkten Zusammenhang zwischen Werten der Zufallsvariable und bestimmten Wahrscheinlichkeiten herstellen, der in der Anwendung ohne Summation oder Integration auskommt. Wir betrachten zunächst die allgemeine Definition von KVFen für sowohl diskrete als auch kontinuierliche Zufallsvariablen und wenden uns dann den KVFen von diskreten und den KVFen von kontiniuerlichen Zufallsvariablen im einzelnen zu. Für eine beliebige Zufallsvariable definieren wir den Begriff der KVF wie folgt.\n\nDefinition 13.13 (Kumulative Verteilungsfunktion) Die kumulative Verteilungsfunktion (KVF) einer Zufallsvariable \\(\\xi\\) ist definiert als \\[\\begin{equation}\nP_\\xi : \\mathbb{R} \\to [0,1], x \\mapsto P_\\xi(x) := \\mathbb{P}_\\xi(\\xi \\le x).\n\\end{equation}\\]\n\nMan beachte, dass \\(P_\\xi(x)\\) ist für jedes \\(x \\in \\mathbb{R}\\) definiert ist, auch wenn für ein gegebenes \\(x\\in \\mathbb{R}\\) gilt, dass \\(x \\notin \\mathcal{X}\\). Wie bereits für Wahrscheinlichkeitsverteilungen, WMFen und WDFen gesehen verzichtet man meist auf das Subskript \\(_\\xi\\), wenn aus dem Kontext klar ist, auf welche Zufallsvariable sich eine gegebene KVF bezieht. Mithilfe von KVFen können zum Beispiel Überschreitungswahrscheinlichkeiten und Intervallwahrscheinlichkeiten von Zufallsvariablen direkt ausgewertet werden. Dies ist der Inhalt folgender Theoreme.\n\nTheorem 13.2 (Überschreitungswahrscheinlichkeit) Es sei \\(\\xi\\) eine Zufallsvariable mit Ergebnisraum \\(\\mathcal{X}\\) und \\(P\\) ihre kumulative Verteilungsfunktion. Dann gilt für die Überschreitungswahrscheinlichkeit \\(\\mathbb{P}(\\xi &gt; x)\\), dass \\[\\begin{equation}\n\\mathbb{P}(\\xi &gt; x) = 1 - P(x) \\mbox{ für alle } x \\in \\mathcal{X}.\n\\end{equation}\\]\n\n\nBeweis. Die Ereignisse \\(\\{\\xi &gt; x\\}\\) und \\(\\{\\xi \\le x\\}\\) sind disjunkt und \\[\\begin{equation}\n\\Omega\n= \\{\\omega\\in \\Omega| \\xi(\\omega) &gt; x\\} \\cup \\{\\omega\\in \\Omega|\\xi(\\omega) \\le x\\}\n= \\{\\xi &gt; x\\} \\cup \\{\\xi \\le x\\}.\n\\end{equation}\\] Mit der \\(\\sigma\\)-Additivität von \\(\\mathbb{P}\\) folgt dann \\[\\begin{align}\n\\begin{split}\n\\mathbb{P}(\\Omega) & = 1                                                    \\\\\n\\Leftrightarrow\n\\mathbb{P}( \\{\\xi &gt; x\\} \\cup \\{\\xi \\le x\\}) & = 1                 \\\\\n\\Leftrightarrow\n\\mathbb{P}(\\{\\xi &gt; x\\}) + \\mathbb{P}(\\{\\xi \\le x\\}) & = 1   \\\\\n\\Leftrightarrow\n\\mathbb{P}(\\{\\xi &gt; x\\}) &  =  1 -  \\mathbb{P}(\\{\\xi \\le x\\}) \\\\\n\\Leftrightarrow\n\\mathbb{P}(\\{\\xi &gt; x\\}) &  =  1 -  P(x).\n\\end{split}\n\\end{align}\\]\n\n\nTheorem 13.3 (Intervallwahrscheinlichkeiten) Es sei \\(\\xi\\) eine Zufallsvariable mit Ereignisraum \\(\\mathcal{X}\\) und \\(P\\) ihre KVF. Dann gilt für die Intervallwahrscheinlichkeit \\(\\mathbb{P}(\\xi \\in \\,]x_1,x_2])\\), dass \\[\\begin{equation}\n\\mathbb{P}(\\xi \\in \\, ]x_1,x_2]) = P(x_2) - P(x_1)\n\\mbox{ für alle } x_1,x_2 \\in \\mathcal{X}\n\\mbox{ mit } x_1 &lt; x_2.\n\\end{equation}\\]\n\n\nBeweis. Wir betrachten die Ereignisse \\(\\{\\xi \\le x_1\\}\\),\\(\\{x_1 &lt; \\xi \\le x_2\\}\\) und \\(\\{\\xi \\le x_2\\}\\), wobei \\[\\begin{equation}\n\\{\\xi \\le x_1\\} \\cap \\{x_1 &lt; \\xi \\le x_2\\} = \\emptyset\n\\mbox{ und }\n\\{\\xi \\le x_1\\} \\cup \\{x_1 &lt; \\xi \\le x_2\\} = \\{\\xi \\le x_2\\}.\n\\end{equation}\\] gelten. Mit der \\(\\sigma\\)-Additivität von \\(\\mathbb{P}\\) gilt dann \\[\\begin{align}\n\\begin{split}\n\\mathbb{P}(\\{\\xi \\le x_1\\} \\cup \\{x_1 &lt; \\xi \\le x_2\\}) & = \\mathbb{P}(\\{\\xi \\le x_2\\})              \\\\\n\\Leftrightarrow\n\\mathbb{P}(\\{\\xi \\le x_1\\}) + \\mathbb{P}(\\{x_1 &lt; \\xi \\le x_2\\}) & = \\mathbb{P}(\\{\\xi \\le x_2\\})         \\\\\n\\Leftrightarrow\n\\mathbb{P}(\\{x_1 &lt; \\xi \\le x_2\\}) & = \\mathbb{P}(\\{\\xi \\le x_2\\}) - \\mathbb{P}(\\{\\xi \\le x_1\\})         \\\\\n\\Leftrightarrow\n\\mathbb{P}(\\{x_1 &lt; \\xi \\le x_2\\}) & = P(x_2) - P(x_1)                                           \\\\\n\\Leftrightarrow\n\\mathbb{P}(\\xi \\in \\,]x_1,x_2]) & = P(x_2) - P(x_1).\n\\end{split}\n\\end{align}\\]\n\nFolgendes Theorem gibt drei zentrale Eigenschaften von KVFen an. Dabei besagt die dritte Eigenschaft, dass eine KVF keine Sprünge hat, wenn man sich Grenzpunkten von rechts nähert. Tatsächlich sind die diskutierten Eigenschaften auch gerade die definierenden Eigenschaften von KVFen, das heißt, jede Funktion \\(P\\), die die Eigenschaften von Theorem 13.4 erfüllt, kann als eine KVF einer Zufallsvariable interpretiert werden. Für einen Beweis dieser Tatsache verweisen wir auf die weiterführende Literatur.\n\nTheorem 13.4 (Eigenschaften von kumulative Verteilungsfunktionen) Es sei \\(\\xi\\) eine Zufallsvariable und \\(P\\) ihre kumulative Verteilungsfunktion. Dann hat \\(P\\) die folgenden Eigenschaften\n\n\\(P\\) ist monoton steigend, i.e., wenn \\(x_1 &lt; x_2\\), dann gilt \\(P(x_1)\\le P(x_2)\\).\n\\(\\lim_{x \\to -\\infty} P(x) = 0\\) und \\(\\lim_{x \\to \\infty} P(x) = 1\\).\n\\(P\\) ist rechtsseitig stetig, d.h., \\(P(x) = P\\left(x^+\\right) = \\lim_{y \\to x, y &gt; x} P(y)\\) für alle \\(x \\in \\mathbb{R}\\)\n\n\n\nBeweis. Wir betrachten die Eigenschaften nacheinander.\n\nWir halten zunächst fest, dass für Ereignisse \\(A \\subset B\\) gilt, dass \\(\\mathbb{P}(A)\\le \\mathbb{P}(B)\\). Wir halten dann fest, dass für \\(x_1 &lt; x_2\\), \\[\\begin{equation}\n\\{\\xi \\le x_1\\} =\n\\{\\omega \\in \\Omega|\\xi(\\omega)\\le x_1\\} \\subset\n\\{\\omega \\in \\Omega|\\xi(\\omega)\\le x_2\\} =\n\\{\\xi \\le x_2\\}.\n\\end{equation}\\] Also gilt \\[\\begin{equation}\n\\mathbb{P}(\\{\\xi \\le x_1\\})\n\\le\n\\mathbb{P}\\{\\xi \\le x_2\\}\n\\Rightarrow P(x_1) \\le P(x_2).\n\\end{equation}\\]\nFür einen Beweis verweisen wir auf die weiterführende Literatur.\nWir definieren \\[\\begin{equation}\nP\\left(x^+\\right) = \\lim_{y \\to x, y &gt; x} P(y).\n\\end{equation}\\] Seien nun \\(y_1 &gt; y_2 &gt; \\cdots\\) so, dass \\(\\lim_{n \\to \\infty}y_n = x\\). Dann gilt \\[\\begin{equation}\n\\{\\xi \\le x\\} = \\cap_{n = 1}^\\infty \\{\\xi \\le y_n\\}.\n\\end{equation}\\] Es gilt also \\[\\begin{equation}\nP(x)\n= \\mathbb{P}(\\{\\xi \\le x\\})\n= \\mathbb{P}(\\cap_{n = 1}^\\infty \\{\\xi \\le y_n\\})\n= \\lim_{n\\to \\infty}\\mathbb{P}(\\{\\xi \\le y_n\\})\n= P\\left(x^+\\right),\n\\end{equation}\\] wobei wir die dritte Gleichung unbegründet stehen lassen.\n\n\n\nKVFen von diskreten Zufallsvariablen\nAnhand von Abbildung 13.10 und Abbildung 13.11 wollen wir uns obige Eigenschaften von KVFen diskreter Zufallsvariablen visuell verdeutlichen. Dabei sollte man immer vor Augen haben, dass der Wert \\(P(x)\\) einer KVF für den Ergebniswert \\(x\\) der Zufallsvariable \\(x\\) der Wahrscheinlichkeit \\(\\mathbb{P}_\\xi(\\xi \\le x)\\) entspricht, also die Wahrscheinlichkeit dafür ist, dass die Zufallsvariable \\(\\xi\\) Werte kleiner oder gleich \\(x\\) annimmt. Liest man diese Wahrscheinlichkeiten entsprechend aus den Darstellungen der korrespondierenden WMF ab, so ergeben sich die funktionale Form der KVFen im Vergleich mit den entsprechenden WMFen intuitiv. Weiterhin erschließen sich auch folgende Eigenschaften der KVFen von diskreten Zufallsvariablen intuitiv: Wennn \\(a &lt; b\\) und \\(\\mathbb{P}(a &lt; \\xi &lt; b) = 0\\) ist, dann ist die KVF von \\(\\xi\\) konstant horizontal auf \\(]a,b[\\). Weiterhin gilt, dass an jedem Punkt \\(x\\) mit \\(\\mathbb{P}(\\xi=x)&gt;0\\) die KVF um den Betrag \\(\\mathbb{P}(\\xi=x)\\) springt, an dieser Stelle also linksseitig nicht stetig ist. Allgemein ist die KVF einer diskreten Zufallsvariable mit Ergebnisraum \\(\\mathbb{N}_0\\) durch \\[\\begin{equation}\nP : \\mathbb{R} \\to [0,1], x \\mapsto P(x) := \\sum_{k=0}^{\\lfloor x  \\rfloor} \\mathbb{P}(\\xi = k)\n\\end{equation}\\] gegeben, wobei \\(\\lfloor x \\rfloor\\) die Abrundungsfunktion bezeichnet.\n\n\n\n\n\n\nAbbildung 13.10: WMFen und KVFen von Bernoulli-Zufallsvariablen.\n\n\n\n\n\n\n\n\n\nAbbildung 13.11: WMFen und KVFen von Binomial-Zufallsvariablen.\n\n\n\n\n\nKVFen von kontinuierlichen Zufallsvariablen\nDie KVFen von kontinuierlichen Zufallsvariablen sind analytisch etwas zugänglicher als die KVFen von diskreten Zufallsvariablen, da sie keine Unstetigkeitsstellen aufweisen. Wir haben zunächst folgendes, vielleicht etwas überraschendes Theorem.\n\nTheorem 13.5 (Kumulative Verteilungsfunktionen von kontinuierlichen Zufallsvariablen) \\(\\xi\\) sei eine kontinuierliche Zufallsvariable mit WDF \\(p\\) und KVF \\(P\\). Dann gilt \\[\\begin{equation}\nP(x) = \\int_{-\\infty}^x p(s)\\,ds\n\\mbox{ und }\np(x) = P'(x).\n\\end{equation}\\]\n\n\nBeweis. Wir halten zunächst fest, dass weil \\(\\mathbb{P}(\\xi = x) = 0\\) für alle \\(x \\in \\mathbb{R}\\) gilt, die KVF von \\(\\xi\\) keine Sprünge hat, d.h. \\(P\\) ist stetig. Mit der Definitionen von WDF und KVF, folgt, dass \\(P\\) die Form einer Stammfunktion von \\(p\\) hat. Dass \\(p\\) die Ableitung von \\(P\\) ist folgt dann direkt aus dem ersten Hauptsatz der Differential- und Integralrechnung, Theorem 7.3.\n\nFür kontinuierliche Zufallsvariablen gilt also, dass die KVF der Zufallsvariable eine Stammfunktion der entsprechenden WDF ist, und umgekehrt, dass die WDF die Ableitung der KVF ist. Im Kontext des Theorem von Radon-Nikodym wird diese Einsicht auf generelle Maße generalisiert (vgl. Schmidt (2009)). KVFen kontinuierlicher Zufallsvariablen werden auch oft als kumulative Dichtefunktionen (KDFen) bezeichnet.\nAls Beispiel betrachten wir die KVF einer normalverteilten Zufallsvariable. Es sei \\(\\xi  \\sim N(\\mu,\\sigma^2)\\). Dann ist die WDF von \\(\\xi\\) bekanntlich durch \\[\\begin{equation}\np : \\mathbb{R} \\to \\mathbb{R}_{&gt;0}, x \\mapsto p(x)\n:= \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{1}{2\\sigma^2}(x-\\mu)^2\\right).\n\\end{equation}\\] gegeben. Für die KVF von \\(\\xi\\) folgt entsprechend, dass \\[\\begin{equation}\nP : \\mathbb{R} \\to ]0,1[, x \\mapsto P(x)\n= \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\int_{-\\infty}^x\\exp\\left(-\\frac{1}{2\\sigma^2}(s -\\mu)^2\\right)\\,ds.\n\\end{equation}\\] Interessanterweise kann das definierende Integral der KVF einer normalverteilten Zufallsvariable nur numerisch, nicht aber analytisch berechnet werden. Wir visualisieren ausgewählte WDFen und KVFen von normalverteilten Zufallsvariablen in Abbildung 13.12.\n\n\n\n\n\n\nAbbildung 13.12: WDFen und KVFen von normalverteilten Zufallsvariablen.\n\n\n\n\n\nInverse Kumulative Verteilungsfunktion\nWir beschließen dieses Kapitel mit dem Begriff der inversen kumulativen Verteilungsfunktion, einem technischen Hilfsmittel, dass insbesondere bei Konfidenzintervallen und Hypothesentests der Frequentistischen Inferenz zur Bestimmung kritischer Werte benutzt wird. Wir definieren die inverse KVF wie folgt.\n\nDefinition 13.14 (Inverse Kumulative Verteilungsfunktion) \\(\\xi\\) sei eine kontinuierliche Zufallsvariable mit KVF \\(P\\). Dann heißt die Funktion \\[\\begin{equation}\nP^{-1} : ]0,1[ \\to \\mathbb{R}, q \\mapsto P^{-1}(q) := \\{x \\in \\mathbb{R}|P(x) = q\\}\n\\end{equation}\\] die .\n\nNach Definition 13.14 gilt offenbar, dass die Funktion \\(P^{-1}\\) die Umkehrfunktion von \\(P\\) ist, also \\[\\begin{equation}\nP^{-1}(P(x)) = x.\n\\end{equation}\\] Da bekanntlich gilt, dass \\[\\begin{equation}\nP(x) = q \\Leftrightarrow \\mathbb{P}(\\xi \\le x) = q \\mbox{ für } q \\in ]0,1[,\n\\end{equation}\\] ist \\(P^{-1}(q)\\) also der Wert \\(x\\) von \\(\\xi\\), so dass \\(\\mathbb{P}(\\xi \\le x) = q\\). Wir visualisieren die KVFen und inversen KVFen normalverteilter Zufallsvariablen in Abbildung 13.13. Im Falle einer normalverteilten Zufallsvariable \\(\\xi \\sim N(0,1)\\) gilt zum Beispiel, dass \\[\\begin{equation}\nP(1.645) = 0.950\n\\Leftrightarrow\nP^{-1}(0.950) = 1.645,\n\\end{equation}\\] und \\[\\begin{equation}\nP(1.906) = 0.975\n\\Leftrightarrow\nP^{-1}(0.975) = 1.960.\n\\end{equation}\\]\n\n\n\n\n\n\nAbbildung 13.13: KVFen und inverse KVFen von normalverteilten Zufallsvariablen.",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "203-Zufallsvariablen.html#zufallsergebnisse-und-zufallsvariablen",
    "href": "203-Zufallsvariablen.html#zufallsergebnisse-und-zufallsvariablen",
    "title": "13  Zufallsvariablen",
    "section": "13.5 Zufallsergebnisse und Zufallsvariablen",
    "text": "13.5 Zufallsergebnisse und Zufallsvariablen\nMit den in Kapitel 11 diskutierten Ergebnissen und den in diesem Kapitel diskutierten Werten von Zufallsvariablen haben wir nun zwei Konzepte kennengelernt, die die Unsicherheit über einen oft numerischen Wert eines Zufallsvorgangs beschreiben können. So kann man sich zum Beispiel die Augenzahl, die ein Würfel beim einmaligen Werfen annimmt, als Realisierung eines Ergebnisses oder einer Zufallsvariable vorstellen. Tatsächlich gibt es auch keine standardisierte Antwort auf die Frage, ob man einen Zufallsvorgang nun lediglich mit einem Wahrscheinlichkeitsraum oder aber mit einem Wahrscheinlichkeitsraum, einer Zufallsvariable, und dem durch beide induzierten Wahrscheinlichkeitsraum modellieren sollte. In der Anwendung wird meist der Begriff der Zufallsvariable bevorzugt und entsprechende WMFen oder WDFen angegeben, ohne dass ein zugrundeliegender Wahrscheinlichkeitsraum oder die Abbildungsform der Zufallsvariable spezifiziert würde. Folgende Formulierung ist beispielsweise typisch:\n\\(\\xi\\) sei eine normalverteilte Zufallsvariable mit Erwartungswertparameter \\(\\mu\\) und Varianzparameter \\(\\sigma^2\\).\nImplizit werden in dieser Aussage basierend auf der Definition der normalverteilten Zufallsvariable (vgl. Definition 13.9) für \\(\\xi\\) der Ergebnisraum \\(\\mathcal{X} := \\mathbb{R}\\), die \\(\\sigma\\)-Algebra \\(\\mathcal{S} := \\mathcal{B}(\\mathbb{R})\\), und die Verteilung \\(\\mathbb{P}_\\xi := N(\\mu,\\sigma^2)\\) festgelegt, also der “induzierte” Wahrscheinlichkeitsraum \\(\\left(\\mathbb{R},\\mathcal{B}(\\mathbb{R}), N(\\mu,\\sigma^2)\\right)\\) betrachtet. Allerdings bleibt unklar, durch welchen Wahrscheinlichkeitsraum und welche Abbildungsform genau \\(\\left(\\mathbb{R},\\mathcal{B}(\\mathbb{R}), N(\\mu,\\sigma^2)\\right)\\) nun induziert wurde. Dies kann allerdings immer durch die Annahme, dass \\(\\xi\\) die Identitätsfunktion ergänzt werden. Konkret könnte man für den zugrundeliegenden Wahrscheinlichkeitsraum hier \\(\\left(\\mathbb{R},\\mathcal{B}(\\mathbb{R}), N(\\mu,\\sigma^2)\\right)\\) und für \\(\\xi\\) dann \\[\\begin{equation}\n\\xi : \\mathbb{R} \\to \\mathbb{R}, \\omega \\mapsto \\xi(\\omega) := \\omega := x\n\\end{equation}\\] wählen. Da \\(\\xi\\) dann an dem im Rahmen des zugrundeliegenden Wahrscheinlichkeitsraum zufällig realisiertem Ergebnis \\(\\omega\\) nichts ändert und dieses lediglich in \\(x\\) umbenannt wird, entspricht die Verteilung von \\(\\xi\\) dann direkt dem Wahrscheinlichkeitsmaß des zugrundeliegenden Wahrscheinlichkeitsraums.\nAllgemein mag man festhalten, dass das Modellieren von Zufallsvorgängen mithilfe von Zufallsvariablen das elementare Wahrscheinlichkeitsraummodell also als Spezialfall der Zufallsvariable als Identititätsabbildung impliziert, über dies hinausgehend aber die Möglichkeit eröffnet, durch von der Identitätsabbildung verschiedene Zufallsvariablen die Transformation von Wahrscheinlichkeitsmaßen auf unterschiedlichen Messräumen zu formalisieren.",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "203-Zufallsvariablen.html#literaturhinweise",
    "href": "203-Zufallsvariablen.html#literaturhinweise",
    "title": "13  Zufallsvariablen",
    "section": "13.6 Literaturhinweise",
    "text": "13.6 Literaturhinweise\nDie Genese des Begriffs der Zufallsvariablen ist eng mit der Entwicklung der Wahrscheinlichkeitstheorie in den letzten drei Jahrhunderten verflochten, so dass keine für den Begriff entscheidene Publikation angegeben werden kann. Die mathematischen Entwicklung des Begriffs der Normalverteilung durch Abraham De Moivre (1667-1754), Pierre Simon Laplace (1749-1827), Johann Carl Friedrich Gauss (1777-1855) und viele andere, ihre deskriptiv-statistischen Entsprechungen in der empirischen Forschung des 19. Jahrhunderts, sowie ihre multivariate Generalisierung im ausgehenden 19. Jahrhundert werden ausführlich in Stigler (1986) dargestellt.",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "203-Zufallsvariablen.html#selbstkontrollfragen",
    "href": "203-Zufallsvariablen.html#selbstkontrollfragen",
    "title": "13  Zufallsvariablen",
    "section": "13.7 Selbstkontrollfragen",
    "text": "13.7 Selbstkontrollfragen\n\nGeben Sie die Definition des Begriffs der Zufallsvariable wieder.\nErläutern Sie die Gleichung \\(\\mathbb{P}_\\xi(\\xi = x) = \\mathbb{P}(\\{\\xi = x\\})\\).\nErläutern Sie die Bedeutung von \\(\\mathbb{P}(\\xi = x)\\).\nGeben Sie die Definition des Begriffs der Wahrscheinlichkeitsmassefunktion wieder.\nGeben Sie die Definition des Begriffs der Wahrscheinlichkeitsdichtefunktion wieder.\nGeben Sie die Definition des Begriffs der kumulativen Verteilungsfunktion wieder.\nSchreiben sie die Intervallwahrscheinlichkeit einer Zufallsvariable mithilfer ihrer KVF.\nGeben Sie die Definition der WDF einer normalverteilten Zufallsvariable wieder.\nGeben Sie die Definition der KVF einer normalverteilten Zufallsvariable wieder.\nSchreiben Sie den Wert \\(P(x)\\) der KVF einer Zufallsvariable mithilfe ihrer WDF.\nSchreiben Sie den Wert \\(p(x)\\) der WDF einer Zufallsvariable mithilfe ihrer KVF.\nGeben Sie die Definition des Begriffs der inversen Verteilungsfunktion wieder.\n\n\n\n\n\nHesse, C. (2009). Wahrscheinlichkeitstheorie (2. Aufl.). Vieweg + Teubner.\n\n\nSchmidt, K. D. (2009). Maß und Wahrscheinlichkeit. Springer.\n\n\nStigler, S. M. (1986). The History of Statistics: The Measurement of Uncertainty before 1900. Belknap Press of Harvard University Press.",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "204-Zufallsvektoren.html",
    "href": "204-Zufallsvektoren.html",
    "title": "14  Zufallsvektoren",
    "section": "",
    "text": "14.1 Definition und multivariate Verteilungen\nDie Konstruktion und Definition eines Zufallsvektors ist analog zu der einer Zufallsvariable, mit dem Unterschied, dass es sich bei einer Zufallsvariable um eine skalarwertige, bei einem Zufallsvektor dagegen um eine vektorwertige Abbildung auf dem Ergebnisraum eines Wahrscheinlichkeitsraums handelt.\nDas Standardbeispiel für den Ergebisraum eines Zufallsvektors ist \\(\\mathbb{R}^n\\), das Standardbeispiel für die auf ihm definierte \\(\\sigma\\)-Algebra ist die \\(n\\)-dimensionale Borelsche \\(\\sigma\\)-Algebra \\(\\mathcal{B}(\\mathbb{R}^n)\\). Für eine explizite und formale Einführung der \\(n\\)-dimensionalen Borelschen \\(\\sigma\\)-Algebra verweisen wir auf die weiterführende Literatur (z.B. Schmidt (2009)). Wir begnügen uns hier wieder mit der (weiterhin formal falschen) Intuition der \\(n\\)-dimensionale Borelschen \\(\\sigma\\)-Algebra als Menge aller Teilmengen des \\(\\mathbb{R}^n\\). Das Standardbeispiel für einen \\(n\\)-dimensionalen Messraum ist damit \\((\\mathbb{R}^n, \\mathcal{B}(\\mathbb{R}^n))\\).\nWie bei allen vektorwertigen Funktionen nennen wir die den Zufallsvektor konstituierenden Funktionen \\(\\xi_i\\) die Komponentenfunktionen von \\(\\xi\\). Legen wir den \\(n\\)-dimensionalen Messraum \\((\\mathbb{R}^n, \\mathcal{B}(\\mathbb{R}^n))\\) dem Zufallsvektor zugrunde, so haben diese die Form \\[\\begin{equation}\n\\xi_i : \\Omega \\to \\mathbb{R}, \\omega \\mapsto \\xi_i(\\omega).\n\\end{equation}\\] Ohne Beweis halten wir fest, dass der Zufallsvektor \\(\\xi\\) messbar ist, wenn für alle \\(i = 1,...,n\\) die Funktionen \\(\\xi_i\\) messbar sind und umgekehrt. Damit sind die Komponentenfunktionen eines Zufallsvektors (letztlich nach Definition) Zufallsvariablen. Ein \\(n\\)-dimensionaler Zufallsvektor wird also als eine Konkatenation von \\(n\\) Zufallsvariablen betrachtet, für \\(n := 1\\) entspricht ein Zufallsvektor einer Zufallsvariable.\nZufallsvektoren werden manchmal auch als “multivariate Zufallsvariablen” bezeichnet. Tatsächlich stehen bei der Betrachtung von Zufallsvektoren auch zunächst primär wahrscheinlichkeitstheoretische Aspekte und nicht etwa Aspekte der geometrischen Vektorraumtheorie im Vordergrund. Die Betrachtung von Vektorraumstrukturen ist im Kontext probabilistischer Standardmodelle wie dem Allgemeinem Linearen Modell aber durchaus üblich, so dass wir hier den Begriff des Zufallsvektors bevorzugen (vgl. Christensen (2011)). Trotzdem werden wir Zufallsvektoren, wie in vielen Texten der Probabilistik üblich, auch oft in Zeilenform, also etwa als \\(\\xi:= (\\xi_1,...,\\xi_n)\\), notieren.\nDas durch die Konstruktion eines Zufallsvektors definierte Bildmaß heißt die multivariate Verteilung des Zufallsvektors, wie in folgender Definition ausgeführt (Abbildung 14.1).\nDer Einfachheit halber spricht man oft auch nur von der Verteilung des Zufallsvektors \\(\\xi\\) oder einer multivariaten Verteilung. Die Notationskonventionen für Zufallsvariablen Definition 13.3 gelten für Zufallsvektoren analog. Zum Beispiel gelten \\[\\begin{align}\n\\begin{split}\n\\mathbb{P}_\\xi(\\xi \\in S)\n& := \\mathbb{P}\\left(\\{\\xi \\in S\\} \\right)\n= \\mathbb{P}\\left(\\{\\omega \\in \\Omega|\\xi(\\omega) \\in S\\} \\right)\n\\\\\n\\mathbb{P}_\\xi(\\xi = x)\n& := \\mathbb{P}\\left(\\{\\xi = x\\} \\right)\n= \\mathbb{P}\\left(\\{\\omega \\in \\Omega|\\xi(\\omega) = x\\} \\right)\n\\\\\n\\mathbb{P}_\\xi(\\xi \\le x)\n& := \\mathbb{P}\\left(\\{\\xi \\le x\\} \\right)\n= \\mathbb{P}\\left(\\{\\omega \\in \\Omega|\\xi(\\omega) \\le x\\} \\right)\n\\\\\n\\mathbb{P}_\\xi(x_1 \\le \\xi \\le x_2)\n& := \\mathbb{P}\\left(\\{x_1 \\le \\xi \\le x_2\\} \\right)\n   = \\mathbb{P}\\left(\\{\\omega \\in \\Omega|x_1 \\le \\xi(\\omega) \\le x_2\\} \\right)\n\\end{split}\n\\end{align}\\] wobei die Relationsoperatoren \\(&lt;, \\le, &gt;, \\ge\\) werden hier komponentenweise verstanden werden. So heißt beispielsweise \\(x \\le y\\) für \\(x,y \\in \\mathbb{R}^n\\), dass für alle Komponenten \\(x_i,y_i, i = 1,...,n\\) gilt, dass \\(x_i \\le y_i\\). Eben dieser Konvention folgt auch die Definition der multivariaten kumulativen Verteilungsfunktion in Generalisierung von Definition 13.13.\nWie kumulative Verteilungsfunktionen können auch multivariate kumulative Verteilungsfunktionen zur Definition von multivariaten Verteilungen genutzt werden. Häufiger ist allerdings, wie im univariaten Fall, die Definition multivariater Verteilungen durch multivariate Wahrscheinlichkeitsmasse - oder Wahrscheinlichkeitsdichtefunktionen. Wir generalisieren die Definitionen diskreter und kontinuierlicher Zufallsvariablen und ihren assoziierten Wahrscheinlichkeitsmasse- und Wahrscheinlichkeitsdichtefunktionen (vgl. Definition 13.4 und Definition 13.8) wie folgt.\nDer Begriff der multivariaten WMF ist offenbar direkt analog zum Begriff der WMF. Wie univariate WMFen sind multivariate WMFen nicht-negativ und normiert. Der Einfachheit halber spricht man oft einfach von der WMF eines Zufallsvektors und verzichtet bei ihrer Bezeichnung, wenn der betreffende Zufallsvektor aus dem Kontext klar ist, auf das \\(\\xi\\) Subscript, schreibt also oft einfach \\(p\\) anstelle von \\(p_\\xi\\).",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Zufallsvektoren</span>"
    ]
  },
  {
    "objectID": "204-Zufallsvektoren.html#sec-definition-und-multivariate-verteilungen",
    "href": "204-Zufallsvektoren.html#sec-definition-und-multivariate-verteilungen",
    "title": "14  Zufallsvektoren",
    "section": "",
    "text": "Definition 14.1 (Zufallsvektor) \\((\\Omega, \\mathcal{A}, \\mathbb{P})\\) sei ein Wahrscheinlichkeitsraum und \\((\\mathcal{X},\\mathcal{S})\\) sei ein \\(n\\)-dimensionaler Messraum. Ein \\(n\\)-dimensionaler ist definiert als eine Abbildung \\[\\begin{equation}\n\\xi:\\Omega \\to \\mathcal{X}, \\omega \\mapsto \\xi(\\omega) :=\n\\begin{pmatrix}\n\\xi_1(\\omega) \\\\\n\\vdots      \\\\\n\\xi_n(\\omega)\n\\end{pmatrix}\n\\end{equation}\\] mit der \\[\\begin{equation}\n\\{\\omega \\in \\Omega|\\xi(\\omega) \\in S \\} \\in \\mathcal{A} \\mbox{ für alle } S \\in \\mathcal{S}.\n\\end{equation}\\]\n\n\n\n\n\n\n\n\n\n\nAbbildung 14.1: Konstruktion von Zufallsvektor und multivariater Verteilung.\n\n\n\n\n\nDefinition 14.2 (Multivariate Verteilung) \\((\\Omega, \\mathcal{A}, \\mathbb{P})\\) sei ein Wahrscheinlichkeitsraum, \\((\\mathcal{X},\\mathcal{S})\\) sei ein \\(n\\)-dimensionaler Messraum und \\[\\begin{equation}\n\\xi : \\Omega \\to \\mathcal{X}, \\omega \\mapsto \\xi(\\omega)\n\\end{equation}\\] sei ein Zufallsvektor. Dann heißt das Wahrscheinlichkeitsmaß \\(\\mathbb{P}_\\xi\\), definiert durch \\[\\begin{equation}\n\\mathbb{P}_\\xi : \\mathcal{S} \\to [0,1], S \\mapsto \\mathbb{P}_\\xi(S)\n:= \\mathbb{P}(\\xi^{-1}(S))\n= \\mathbb{P}\\left(\\{\\omega \\in \\Omega|\\xi(\\omega) \\in S\\}\\right)\n\\end{equation}\\] die .\n\n\n\nDefinition 14.3 (Multivariate kumulative Verteilungsfunktionen) \\(\\xi\\) sei ein Zufallsvektor mit Ergebnisraum \\(\\mathcal{X}\\). Dann heißt eine Funktion der Form \\[\\begin{equation}\nP_\\xi : \\mathcal{X} \\to [0,1],\\, x \\mapsto P_\\xi(x) := \\mathbb{P}_\\xi(\\xi \\le x)\n\\end{equation}\\] multivariate kumulative Verteilungsfunktion von \\(\\xi\\).\n\n\n\nDefinition 14.4 (Diskreter Zufallsvektor und multivariate Wahrscheinlichkeitsmassefunktion) Ein Zufallsvektor \\(\\xi\\) heißt diskret, wenn sein Ergebnisraum \\(\\mathcal{X}\\) endlich oder abzählbar ist und eine Funktion \\[\\begin{equation}\np_\\xi : \\mathcal{X} \\to [0,1], x \\mapsto p_\\xi(x)  \n\\end{equation}\\] existiert, für die gilt\n\n\\(\\sum_{x \\in \\mathcal{X}}p(x) = 1\\) und\n\\(\\mathbb{P}_\\xi(\\xi = x) = p(x)\\) für alle \\(x \\in \\mathcal{X}\\).\n\nEin entsprechende Funktion \\(p_\\xi\\) heißt multivariate Wahrscheinlichkeitsmassefunktion (WMF) von \\(\\xi\\).\n\n\n\nBeispiel\nZur Illustration des Begriffs des diskreten Zufallsvektors und seiner WMF wollen wir ein Beispiel betrachten. Dazu sei \\(\\xi:= (\\xi_1,\\xi_2)\\) ein Zufallsvektor, der Werte in \\(\\mathcal{X} := \\mathcal{X}_1 \\times \\mathcal{X}_2\\) annimmt, wobei \\(\\mathcal{X}_1 := \\{1,2,3\\}\\) und \\(\\mathcal{X}_2 = \\{1,2,3,4\\}\\) seien. Dann entspricht der Ergebnisraum von \\(\\xi\\) der in untenstehender Tabelle spezifizierten Menge an Tupeln \\((x_1,x_2)\\).\nEine exemplarische bivariate WMF der Form \\[\\begin{equation}\np_\\xi: \\{1,2,3\\} \\times \\{1,2,3,4\\} \\to [0,1], (x_1,x_2) \\mapsto p_\\xi(x_1,x_2)\n\\end{equation}\\] ist dann durch nachfolgende Tabelle definiert:\nMan beachte, dass die so spezifierte Funktion \\(p_\\xi\\) den Normiertheits- und Nichtnegativitätsansprüchen an eine WMF genügt. Insbesondere gilt hier \\[\\begin{equation}\n\\sum_{x \\in \\mathcal{X}} p_\\xi(x) = \\sum_{x_1 = 1}^3 \\sum_{x_2 = 1}^4 p_\\xi(x_1,x_2) = 1.\n\\end{equation}\\]\nDen Begriff des kontinuierlichen Zufallsvektors und der multivariaten Wahrscheinlichkeitsdichtefunktion definieren wir wie folgt.\n\nDefinition 14.5 (Kontinuierlicher Zufallsvektor und multivariate Wahrscheinlichkeitdichtefunktion) Ein Zufallsvektor \\(\\xi\\) heißt kontinuierlich, wenn sein Ergebnisraum durch \\(\\mathbb{R}^n\\) gegeben ist und eine Funktion\n\\[\\begin{equation}\np_\\xi : \\mathbb{R}^n \\to \\mathbb{R}_{\\ge 0}, x \\mapsto p_\\xi(x),\n\\end{equation}\\] existiert, für die gilt, dass\n\n\\(\\int_{\\mathbb{R}^n} p_\\xi(x)\\,dx = 1\\) und\n\\(\\mathbb{P}_\\xi(x_1 \\le \\xi \\le x_2)  = \\int_{x_{1_1}}^{x_{2_1}} \\cdots \\int_{x_{1_n}}^{x_{2_n}} p_\\xi(s_1,...,s_n)\\,ds_1 \\cdots ds_n\\).\n\nEine entsprechende Funktion \\(p_\\xi\\) heißt multivariate Wahrscheinlichkeitsdichtefunktion (WDF) von \\(\\xi\\).\n\nOffenbar ist der der Begriff der multivariaten WDF eines kontinuierlichen Zufallsvektors analog zum Begriff der WDF einer kontinuierlichen Zufallsvariable und wie univariate WDFen sind multivariate WDFen nicht-negativ und normiert. Der Einfachheit halber spricht man auch hier oft einfach von multivariaten WDFen und verzichtet auf die den Zufallsvektor identifizieren Subskripte. Wie für kontinuierliche Zufallsvariablen gilt für kontinuierliche Zufallsvektoren \\[\\begin{equation}\n\\mathbb{P}_\\xi(\\xi = x)\n= \\mathbb{P}_\\xi(x \\le \\xi \\le x)\n= \\int_{x_1}^{x_1} \\cdots \\int_{x_n}^{x_n} p_\\xi(s_1,...,s_n)\\,ds_1 \\cdots ds_n\n= 0.\n\\end{equation}\\]\nDas Standardbeispiel für eine multivariate WDF ist die multivariate Normalverteilung, welcher wir mit Kapitel 20 ein eigenes Kapitel widmen.",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Zufallsvektoren</span>"
    ]
  },
  {
    "objectID": "204-Zufallsvektoren.html#sec-marginalverteilungen",
    "href": "204-Zufallsvektoren.html#sec-marginalverteilungen",
    "title": "14  Zufallsvektoren",
    "section": "14.2 Marginalverteilungen",
    "text": "14.2 Marginalverteilungen\nHat man die Verteilung eines Zufallsvektors spezifiziert, so kann man sich fragen, welche Verteilungen daraus für die einzelnen Komponenten des Zufallsvektors, also die Zufallsvariablen, die zusammen den Zufallsvektor bilden, folgen. Im Kontext eines Zufallsvektors nennt man diese die univariaten Marginalverteilungen des Zufallsvektors. Folgende Definition ist grundlegend.\n\nDefinition 14.6 (Univariate Marginalverteilung) \\((\\Omega, \\mathcal{A}, \\mathbb{P})\\) sei ein Wahrscheinlichkeitsraum, \\((\\mathcal{X}, \\mathcal{S})\\) sei ein \\(n\\)-dimensionaler Messraum, \\(\\xi:\\Omega \\to \\mathcal{X}\\) sei ein Zufallsvektor, \\(\\mathbb{P}_\\xi\\) sei die Verteilung von \\(\\xi\\), \\(\\mathcal{X}_i \\subset \\mathcal{X}\\) sei der Ergebnisraum der \\(i\\)ten Komponente \\(\\xi_i\\) von \\(\\xi\\), und \\(\\mathcal{S}_i\\) sei eine \\(\\sigma\\)-Algebra auf \\(\\xi_i\\). Dann heißt die durch \\[\\begin{equation}\n\\mathbb{P}_{\\xi_i} : \\mathcal{S}_i \\to [0,1],\nS \\mapsto  \\mathbb{P}_\\xi\\left(\\mathcal{X}_1\n                     \\times\n                     \\cdots\n                     \\times\n                     \\mathcal{X}_{i-1}\n                     \\times S\n                     \\times \\mathcal{X}_{i+1}\n                     \\times \\cdots\n                     \\times \\mathcal{X}_n\\right)\n\\mbox{ für } S \\in \\mathcal{S}_i\n\\end{equation}\\] definierte Verteilung die \\(i\\)te univariate Marginalverteilung von \\(\\xi\\).\n\nKonkret kann man sowohl für diskrete als auch für kontinuierliche Zufallsvektoren die WMFen bzw. WDFen ihrer Komponenten direkt aus der entsprechenden multivariaten WMF bzw. WDF bestimmen. Dies ist die Aussage folgenden Theorems.\n\nTheorem 14.1 (Marginale Wahrscheinlichkeitsmasse und Wahrscheinlichkeitsdichtefunktionen) (1) \\(\\xi= (\\xi_1,...,\\xi_n)\\) sei ein \\(n\\)-dimensionaler diskreter Zufallsvektor mit WMF \\(p_\\xi\\) und Komponentenergebnisräumen \\(\\mathcal{X}_1, ..., \\mathcal{X}_n\\). Dann ergibt sich die WMF der \\(i\\)ten Komponente \\(\\xi_i\\) von \\(\\xi\\) als \\[\\begin{multline}\np_{\\xi_i} : \\mathcal{X}_i \\to [0,1], x_i \\mapsto\n\\\\ p_{\\xi_i}(x_i) := \\sum_{x_1} \\cdots \\sum_{x_{i-1}} \\sum_{x_{i+1}} \\cdots \\sum_{x_n} p_\\xi(x_1,...,x_{i-1},x_i,x_{i+1}, ...,x_n).\n\\end{multline}\\] (2) \\(\\xi= (\\xi_1,...,\\xi_n)\\) sei ein \\(n\\)-dimensionaler kontinuierlicher Zufallsvektor mit WDF \\(p_\\xi\\) und Komponentenergebnisraum \\(\\mathbb{R}\\). Dann ergibt sich die WDF der \\(i\\)ten Komponente \\(\\xi_i\\) von \\(\\xi\\) als \\[\\begin{multline}\np_{\\xi_i} : \\mathbb{R} \\to \\mathbb{R}_{\\ge 0},  x_i \\mapsto\n\\\\ p_{\\xi_i}(x_i) :=  \n\\int_{x_1} \\cdots \\int_{x_{i-1}} \\int_{x_{i+1}} \\cdots \\int_{x_n}\np_\\xi(x_1,..., x_{i-1},x_i,x_{i+1}, ...,x_n)\n\\,dx_1...\\,dx_{i-1}\\,dx_{i+1}...\\,dx_n.\n\\end{multline}\\]\n\nDie WMFen der univariaten Marginalverteilungen diskreter Zufallsvektoren ergeben sich also durch Summation über alle Werte der zu der jeweils betrachteten Zufallsvariable komplementären Zufallsvariablen und die WDFen der univariaten Marginalverteilungen kontinuierlicher Zufallsvektoren ergeben sich analog durch Integration über alle Werte der zu der jeweils betrachteten Zufallsvariable komplementären Zufallsvariablen. Für einen Beweis von Theorem 14.1 verweisen wir auf die weiterführende Literatur.\nBeispiel\nIn Fortführung des in Kapitel 14.1 betrachteten Beispiels eines zweidimensionalen Zufallsvektor \\(\\xi:= (\\xi_1,\\xi_2)\\) ergeben sich für die dort definierte WMF für die marginalen WMFen \\(p_{\\xi_1}\\) und \\(p_{\\xi_2}\\) die an den Rändern der unten spezifizierter Tabelle aufgelisteten WMFen anhand von \\[\\begin{equation}\np_{\\xi_1}(x_1) = \\sum_{x_2 = 1}^{4} p_\\xi(x_1,x_2) \\mbox{ und }\np_{\\xi_2}(x_2) = \\sum_{x_1 = 1}^{3} p_\\xi(x_1,x_2)\n\\end{equation}\\]\nzu\nFür die Werte von \\(p_{\\xi_1}\\) werden die entsprechenden Werte von \\(p_\\xi\\) also zeilenweise und für die Werte von \\(p_{\\xi_2}\\) spaltenweise addiert. Man beachte, dass aus der Normiertheit von \\(p_\\xi\\) die Normiertheit von \\(p_{\\xi_1}\\) und \\(p_{\\xi_2}\\) direkt folgt, da sich die Gesamtsumme an Wahrscheinlichkeitsmasse nicht ändert: \\[\\begin{equation}\n1\n= \\sum_{x_1=1}^{3}\\sum_{x_2 = 1}^{4} p_\\xi(x_1,x_2)\n= \\sum_{x_1=1}^{3} p_{\\xi_1}(x_1)\n= \\sum_{x_2=1}^{4} p_{\\xi_2}(x_2).\n\\end{equation}\\]\nEin Realisierungsbeispiel mithilfe relativer Häufigkeiten mag den Begriff der marginalen WMF intuitiv verdeutlichen. Nehmen wir an, wir hätten \\(n = 100\\) unabhängige Realisierungen von \\(\\xi\\) vorliegen. Um die Wahrscheinlichkeiten \\(p_\\xi(x_1,x_2)\\) zu schätzen, würden wir die Anzahl der Realisierungen von \\((x_1,x_2)\\) zählen und durch \\(n\\) teilen. Hätten wir beispielsweise 12 Realisierungen von \\((3,2)\\) vorliegen, so würden wir \\(p_\\xi(3,2) \\approx 12/100 = 0.12\\) schätzen. Die Frage nach der marginalen Wahrscheinlichkeit von \\(x_2 = 2\\) entspräche dann der Frage, wie oft unter den Realisierungen solche zu finden sind, bei denen \\(x_2 = 2\\) ist, irrespektive des Wertes von \\(x_1\\). Dies wäre gerade die Anzahl der Realisierungen der Form \\((1,2), (2,2)\\) und \\((3,2)\\). Gäbe es von diesen beispielsweise \\(0, 22\\) und \\(12\\) respektive, so würde man die Wahrscheinlichkeit \\(p_{\\xi_2}(2)\\) natürlicherweise durch \\[\\begin{equation}\n\\frac{0 + 22 + 12}{100} = \\frac{0}{100} + \\frac{22}{100} + \\frac{12}{100} = 0.00 + 0.22 + 0.12 = 0.34\n\\end{equation}\\] schätzen. Anstelle der Wahrscheinlichkeiten \\(p_\\xi(1,2)\\), \\(p_\\xi(2,2)\\), \\(p_\\xi(3,2)\\) addiert man hier also die entsprechenden relativen Häufigkeiten.\nMarginale Verteilungen im Fall von kontinuierlichen Zufallsvektoren behandeln wir am Standardbeispiel der multivariaten Normalverteilung in Kapitel 20.",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Zufallsvektoren</span>"
    ]
  },
  {
    "objectID": "204-Zufallsvektoren.html#sec-bedingte-verteilungen",
    "href": "204-Zufallsvektoren.html#sec-bedingte-verteilungen",
    "title": "14  Zufallsvektoren",
    "section": "14.3 Bedingte Verteilungen",
    "text": "14.3 Bedingte Verteilungen\nHat man die Verteilung eines Zufallsvektors spezifiziert, so kann man sich fragen, welche Verteilung daraus für eine einzelne Komponenten des Zufallsvektors folgt, wenn man den Wert einer anderen Komponente als bekannt annimmt. Dies führt auf den Begriff der bedingten Verteilung, welcher sich natürlicherweise aus dem Begriff der bedingten Wahrscheinlichkeit (vgl. Kapitel 12.2) ergibt. Wir erinnern uns zunächst, dass für einen Wahrscheinlichkeitsraum \\((\\Omega, \\mathcal{A}, \\mathbb{P})\\) und zwei Ereignisse \\(A,B \\in \\mathcal{A}\\) mit \\(\\mathbb{P}(B) &gt; 0\\) die bedingte Wahrscheinlichkeit von \\(A\\) gegeben \\(B\\) definiert ist als \\[\\begin{equation}\n\\mathbb{P}(A|B) = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)}.\n\\end{equation}\\] Analog wird für zwei Zufallsvariablen \\(\\xi_1,\\xi_2\\) mit Ereignisräumen \\(\\mathcal{X}_1,\\mathcal{X}_2\\) und (messbaren) Mengen \\(S_1 \\in \\mathcal{X}_1, S_2 \\in \\mathcal{X}_2\\) die bedingte Verteilung von \\(\\xi_1\\) gegeben \\(\\xi_2\\) mithilfe der Ereignisse \\[\\begin{equation}\nA := \\{\\xi_1 \\in S_1\\} \\mbox{ und } B := \\{\\xi_2 \\in S_2\\}\n\\end{equation}\\] definiert. So ergibt sich zum Beispiel die bedingte Wahrscheinlichkeit, dass \\(\\xi_1 \\in S_1\\) gegeben, dass \\(\\xi_2 \\in S_2\\) unter der Annahme, dass \\(\\mathbb{P}(\\{\\xi_2 \\in S_2\\}) &gt; 0\\), zu \\[\\begin{equation}\n\\mathbb{P}( \\{\\xi_1 \\in S_1\\}|\\{\\xi_2 \\in S_2\\})\n= \\frac{\\mathbb{P}(\\{\\xi_1 \\in S_1\\} \\cap \\{\\xi_2 \\in S_2\\})}{\\mathbb{P}(\\{\\xi_2 \\in S_2\\})}.\n\\end{equation}\\]\nWir betrachten zunächst die Definition der bedingten Verteilungen von diskreten Zufallsvektoren, die lediglich aus zwei Zufallsvariablen bestehen.\n\nDefinition 14.7 (Bedingte Wahrscheinlichkeitsmassefunktion und diskrete bedingte Verteilung) \\(\\xi:= (\\xi_1,\\xi_2)\\) sei ein diskreter Zufallsvektor mit Ergebnisraum \\(\\mathcal{X} := \\mathcal{X}_1 \\times \\mathcal{X}_2\\), WMF \\(p_\\xi = p_{\\xi_1,\\xi_2}\\) und marginalen WMFen \\(p_{\\xi_1}\\) und \\(p_{\\xi_2}\\). Die bedingte WMF von \\(\\xi_1\\) gegeben \\(\\xi_2 = x_2\\) ist dann für \\(p_{\\xi_2}(x_2) &gt; 0\\) definiert als \\[\\begin{equation}\np_{\\xi_1|\\xi_2 = x_2} : \\mathcal{X}_1 \\to [0,1],\nx_1 \\mapsto p_{\\xi_1|\\xi_2 = x_2}(x_1|x_2) := \\frac{p_{\\xi_1,\\xi_2}(x_1,x_2)}{p_{\\xi_2}(x_2)}\n\\end{equation}\\] Analog ist für \\(p_{\\xi_1}(x_1) &gt; 0\\) die bedingte WMF von \\(\\xi_2\\) gegeben \\(\\xi_1 = x_1\\) definiert als \\[\\begin{equation}\np_{\\xi_2|\\xi_1 = x_1} : \\mathcal{X}_2 \\to [0,1],\nx_2 \\mapsto p_{\\xi_2|\\xi_1 = x_2}(x_1|x_2) := \\frac{p_{\\xi_1,\\xi_2}(x_1,x_2)}{p_{\\xi_1}(x_1)}\n\\end{equation}\\] Die bedingten Verteilungen mit WMFen \\(p_{\\xi_1|\\xi_2 = x_2}\\) und \\(p_{\\xi_2|\\xi_1 = x_1}\\) heißen dann die , respektive.\n\nIn Analogie zur Definition der bedingten Wahrscheinlichkeit von Ereignissen gilt also \\[\\begin{equation}\np_{\\xi_1|\\xi_2}(x_1|x_2)\n= \\frac{p_{\\xi_1,\\xi_2}(x_1,x_2)}{p_{\\xi_2}(x_2)}\n= \\frac{\\mathbb{P}(\\{\\xi_1 = x_1\\} \\cap \\{\\xi_2 = x_2\\})}{\\mathbb{P}(\\{\\xi_2 = x_2\\})}.\n\\end{equation}\\] Es ist dabei entscheidend zu erkennen, dass bedingte Verteilungen lediglich normalisierte gemeinsame Verteilungen sind.\nBeispiel\nIn Fortführung des in Kapitel 14.1 betrachteten Beispiels eines zweidimensionalen Zufallsvektor \\(\\xi:= (\\xi_1,\\xi_2)\\) ergeben und seiner in Kapitel 14.2 bestimmten Marginalverteilungen ergeben sich folgende bedingte WMFen für \\(p_{\\xi_2|\\xi_1 = x_1}\\):\nMan beachte, dass zum einen gilt, dass \\[\\begin{equation}\n\\sum_{x_2 = 1}^4 p_{\\xi_2|\\xi_1 = x_1}(x_2|x_1) = 1 \\mbox{ für alle } x_1 \\in \\mathcal{X}_1,\n\\end{equation}\\] die bedingten WMFen sind also normiert. Zum anderen beachte man die qualitative Ähnlichkeit der WMFen \\(p_{\\xi_1,\\xi_2}(x_1,x_2)\\) und \\(p_{\\xi_2|\\xi_1}(x_2|x_1)\\), die sich einfach daraus ergibt, dass \\(p_{\\xi_1,\\xi_2}(x_1,x_2)\\) und \\(p_{\\xi_2|\\xi_1}(x_2|x_1)\\) für alle \\(x_1 \\in \\mathcal{X}_1\\) bis auf den gemeinsamen Skalierungsfaktor \\(1/p_{\\xi_1}(x_1)\\) identisch sind.\nIm Fall eines kontinuierlichen Zufallsvektors sind die analogen bedingten WDFen definiert wie folgt.\n\nDefinition 14.8 (Bedingte Wahrscheinlichkeitsdichtefunktion und kontinuierliche bedingte Verteilung) \\(\\xi:= (\\xi_1,\\xi_2)\\) sei ein kontinuierlicher Zufallsvektor mit Ergebnisraum \\(\\mathbb{R}^2\\), WDF \\(p_\\xi = p_{\\xi_1,\\xi_2}\\) und marginalen WDFen \\(p_{\\xi_1}\\) und \\(p_{\\xi_2}\\). Die bedingte WDF von \\(\\xi_1\\) gegeben \\(\\xi_2 = x_2\\) ist dann für \\(p_{\\xi_2}(x_2) &gt; 0\\) definiert als \\[\\begin{equation}\np_{\\xi_1|\\xi_2 = x_2} : \\mathbb{R} \\to \\mathbb{R}_{\\ge 0},\nx_1 \\mapsto p_{\\xi_1|\\xi_2 = x_2}(x_1|x_2) := \\frac{p_{\\xi_1,\\xi_2}(x_1,x_2)}{p_{\\xi_2}(x_2)}\n\\end{equation}\\] Analog ist für \\(p_{\\xi_1}(x_1) &gt; 0\\) die bedingte WMF von \\(\\xi_2\\) gegeben \\(\\xi_1 = x_1\\) definiert als \\[\\begin{equation}\np_{\\xi_2|\\xi_1 = x_1} : \\mathbb{R} \\to \\mathbb{R}_{\\ge 0},\nx_2 \\mapsto p_{\\xi_2|\\xi_1 = x_1}(x_2|x_1) := \\frac{p_{\\xi_1,\\xi_2}(x_1,x_2)}{p_{\\xi_1}(x_1)}\n\\end{equation}\\]\nDie Verteilungen mit WDFen \\(p_{\\xi_1|\\xi_2 = x_2}\\) und \\(p_{\\xi_2|\\xi_1 = x_1}\\) heißen dann die , respektive.\n\nMan beachte, dass im kontinuierlichen Fall zwar \\(\\mathbb{P}(\\xi = x) = 0\\), aber nicht notwendig auch \\(p_\\xi(x) = 0\\) gilt. Die bedingten Verteilungen multivariater Normalverteilungen diskutieren wir in Kapitel 20.",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Zufallsvektoren</span>"
    ]
  },
  {
    "objectID": "204-Zufallsvektoren.html#sec-unabhängige-zufallsvariablen",
    "href": "204-Zufallsvektoren.html#sec-unabhängige-zufallsvariablen",
    "title": "14  Zufallsvektoren",
    "section": "14.4 Unabhängige Zufallsvariablen",
    "text": "14.4 Unabhängige Zufallsvariablen\nÄhnlich wie die bedingte Wahrscheinlichkeiten von Ereignissen lässt sich auch das Konzept der unabhängigen Ereignisse auf Zufallsvektoren übertragen. Wir definieren zunächst den Begriff der unabhängigen Zufallsvariablen.\n\nDefinition 14.9 (Unabhängige Zufallsvariablen) \\((\\Omega, \\mathcal{A}, \\mathbb{P})\\) sei ein Wahrscheinlichkeitsraum und \\(\\xi: = (\\xi_1,\\xi_2)\\) ein zweidimensionaler Zufallsvektor. Die Zufallsvariablen \\(\\xi_1,\\xi_2\\) mit Ergebnisräumen \\(\\mathcal{X}_1, \\mathcal{X}_2\\) heißen , wenn für alle \\(S_1 \\subseteq \\mathcal{X}_1\\) und \\(S_2 \\subseteq \\mathcal{X}_2\\) gilt, dass \\[\\begin{equation}\n\\mathbb{P}_\\xi(\\xi_1 \\in S_1, \\xi_2 \\in S_2) =\n\\mathbb{P}_{\\xi_1}(\\xi_1 \\in S_1)\\mathbb{P}_{\\xi_2}(\\xi_2 \\in S_2).\n\\end{equation}\\]\n\nDefinition 14.9 besagt, dass die Ereignisse \\(\\{\\xi_1 \\in S_1\\}\\) und \\(\\{\\xi_2 \\in S_2\\}\\) unabhängig sind. Es gilt also auch, dass \\[\\begin{equation}\n\\mathbb{P}(\\{\\xi_1 \\in S_1\\})|\\{\\xi_2 \\in S_2\\}) = \\mathbb{P}(\\{\\xi_1 \\in S_1\\})\n\\end{equation}\\] und das Wissen um das Eintreten des Ereignisses \\(\\{\\xi_2 \\in S_2\\}\\) verändert die Wahrscheinlichkeit des Ereignisses \\(\\{\\xi_1 \\in S_1\\}\\) nicht. Das Faktorisierungsprinzip zur Modellierung probabilistischer Unabhängigkeit überträgt sich auf WMFen und WDFen von Zufallsvektoren. Dies ist die Aussagen folgenden Theorems.\n\nTheorem 14.2 (Unabhängigkeit und Faktorisierung)  \n\n\\(\\xi:= (\\xi_1,\\xi_2)\\) sei ein diskreter Zufallsvektor mit Ergebnisraum \\(\\mathcal{X}_1 \\times \\mathcal{X}_2\\), WMF \\(p_\\xi\\) und marginalen WMFen \\(p_{\\xi_1}, p_{\\xi_2}\\). Dann gilt \\[\\begin{multline}\n\\xi_1 \\mbox{ und } \\xi_2 \\mbox{ sind unabhängige Zufallsvariablen} \\Leftrightarrow \\\\\np_\\xi(x_1,x_2) = p_{\\xi_1}(x_1)p_{\\xi_2}(x_2) \\mbox{ für alle } (x_1,x_2) \\in \\mathcal{X}_1 \\times \\mathcal{X}_2.\n\\end{multline}\\]\n\\(\\xi:= (\\xi_1,\\xi_2)\\) sei ein kontinuierlicher Zufallsvektor mit Ergebnisraum \\(\\mathbb{R}^2\\), WDF \\(p_\\xi\\) und marginalen WDFen \\(p_{\\xi_1}, p_{\\xi_2}\\). Dann gilt \\[\\begin{multline}\n\\xi_1 \\mbox{ und } \\xi_2 \\mbox{ sind unabhängige Zufallsvariablen } \\Leftrightarrow \\\\\np_\\xi(x_1,x_2) = p_{\\xi_1}(x_1)p_{\\xi_2}(x_2) \\mbox{ für alle } (x_1,x_2) \\in \\mathbb{R}^2.\n\\end{multline}\\]\n\n\nGenerell ist die Unabhängigkeit zweier Zufallsvariablen also äquivalent zur Faktorisierung ihrer gemeinsamen WMF oder WDF. Für einen Beweis von Theorem 14.2 verweisen wir auf die weiterführende Literatur. Nichtsdestotrotz ist Theorem 14.2 für weite Aspekte der probabilistischen Modellierung grundlegend.\nBeispiel\nWir betrachten erneut den zweidimensionalen Zufallsvektor \\(\\xi:= (\\xi_1, \\xi_2)\\) aus Kapitel 14.1, dessen gemeinsame und marginale WMFen bekanntlich die untenstehende Form haben\nWir fragen zunächst, ob \\(\\xi_1\\) und \\(\\xi_2\\) wohl unabhängig sind. Dies ist nicht der Fall, da hier gilt, dass \\[\\begin{equation}\np_\\xi(1,1) = 0.10 \\neq 0.08 = 0.40\\cdot 0.20 =  p_{\\xi_1}(1)p_{\\xi_2}(1).\n\\end{equation}\\] Möchten wir basierend auf den Marginalverteilungen von \\(\\xi\\) eine gemeinsame Verteilung erzeugen, in der \\(\\xi_1\\) und \\(\\xi_2\\) unabhängig sind, so muss sich jeder Eintrag der gemeinsamen Verteilung \\(p_\\xi(\\xi_1,\\xi_2)\\) aus dem jeweiligen Produkt der Marginalwahrscheinlichkeiten ergeben. Die gemeinsame Verteilung von \\(\\xi_1\\) und \\(\\xi_2\\) unter der Annahme der Unabhängigkeit von \\(\\xi_1\\) und \\(\\xi_2\\) bei gleichen Marginalverteilungen wie im obigen Fall ergibt sich also zu\nWeiterhin ergeben sich im Falle der Unabhängigkeit von \\(\\xi_1\\) und \\(\\xi_2\\) beispielsweise die bedingten WMFen \\(p_{\\xi_2|\\xi_1}\\) zu wie folgt:\nIm Falle der Unabhängigkeit von \\(\\xi_1\\) und \\(\\xi_2\\) ändert sich die Verteilung von \\(\\xi_2\\) gegeben (oder im Wissen um) den Wert von \\(\\xi_1\\) also nicht und entspricht jeweils der Marginalverteilung von \\(\\xi_2\\). Dies entspricht natürlich der Intuition der Unabhängigkeit von Ereignissen im Kontext elementarer Wahrscheinlichkeiten.\nWir wollen den Begriff der unabhängigen Zufallsvariablen nun für mehr als zwei Zufallsvariablen definieren.\n\nDefinition 14.10 (\\(n\\) unabhängige Zufallsvariablen) \\(\\xi:= (\\xi_1,...,\\xi_n)\\) sei ein \\(n\\)-dimensionaler Zufallsvektor mit Ergebnisraum \\(\\mathcal{X}  = \\times_{i=1}^n \\mathcal{X}_i\\). Die \\(n\\) Zufallsvariablen \\(\\xi_1,...,\\xi_n\\) heißen , wenn für alle \\(S_i \\in \\mathcal{X}_i, i = 1,...,n\\) gilt, dass \\[\\begin{equation}\n\\mathbb{P}_\\xi(\\xi_1 \\in S_1, ...,\\xi_n \\in S_n) = \\prod_{i=1}^n \\mathbb{P}_{\\xi_i}(\\xi_i \\in S_i).\n\\end{equation}\\] Wenn der Zufallsvektor eine \\(n\\)-dimensionale WMF oder WDF \\(p_\\xi\\) mit marginalen WMFen oder WDFen \\(p_{\\xi_i}, i = 1,...,n\\) besitzt, dann ist die Unabhängigkeit von \\(\\xi_1,...,\\xi_n\\) gleichbedeutend mit der Faktorisierung der gemeinsamen WMF oder WDF, also mit \\[\\begin{equation}\np_\\xi(\\xi_1,...,\\xi_n) = \\prod_{i=1}^n p_{\\xi_i}(x_i).\n\\end{equation}\\]\n\nEs handelt bei Definition 14.10 also um eine direkte Generalisierung des zweidimensionalen Falls.\nSind \\(n\\) Zufallsvariablen nicht nur unabhängig, sondern haben sie auch alle die gleiche Verteilung, so nennt man sie unabhängig und identisch verteilt (u.i.v):\n\nDefinition 14.11 (Unabhängig und identisch verteilte Zufallsvariablen) \\(n\\) Zufallsvariablen \\(\\xi_1,...,\\xi_n\\) heißen unabhängig und identisch verteilt (u.i.v.), wenn\n\n\\(\\xi_1,...,\\xi_n\\) unabhängige Zufallsvariablen sind, und\ndie Marginalverteilungen der \\(\\xi_i\\) übereinstimmen, also gilt, dass \\[\\begin{equation}\n\\mathbb{P}_{\\xi_i} = \\mathbb{P}_{\\xi_j} \\mbox{ für alle } 1 \\le i,j \\le n.\n\\end{equation}\\] Wenn die Zufallsvariablen \\(\\xi_1,...,\\xi_n\\) unabhängig und identisch verteilt sind und die \\(i\\)te Marginalverteilung \\(\\mathbb{P}_\\xi := \\mathbb{P}_{\\xi_i}\\) ist, so schreibt man auch \\[\\begin{equation}\n\\xi_1,...,\\xi_n \\sim \\mathbb{P}_\\xi.\n\\end{equation}\\]\n\n\nMan sagt kurz, dass “\\(\\xi_1,...,\\xi_n\\) u.i.v.” sind. Im Englischen spricht man von independent and identically distributed (i.i.d) Zufallsvariablen. U.i.v. Zufallsvariablen spielen an vielen Stellen der probabilistischen Modellierung eine wichtige Rolle. So werden, wie wir an späterer Stelle sehen werden, additive Fehlerterme in probabilistischen Modellen meist durch u.i.v. Zufallsvariablen modelliert.\nSchließlich halten wir fest, dass \\(n\\) u.i.v. normalverteilte Zufallsvektoren werden als \\[\\begin{equation}\n\\xi_1,...,\\xi_n \\sim N(\\mu,\\sigma^2)\n\\end{equation}\\] geschrieben werden. In Kapitel 20 zeigen wir, wie genau die gemeinsame Verteilung von \\(n\\) u.i.v. normalverteilte Zufallsvektoren beschaffen ist.",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Zufallsvektoren</span>"
    ]
  },
  {
    "objectID": "204-Zufallsvektoren.html#selbstkontrollfragen",
    "href": "204-Zufallsvektoren.html#selbstkontrollfragen",
    "title": "14  Zufallsvektoren",
    "section": "14.5 Selbstkontrollfragen",
    "text": "14.5 Selbstkontrollfragen\n\nGeben Sie die Definition des Begriffs des Zufallsvektors wieder.\nGeben Sie die Definition des Begriffs der multivariaten Verteilung eines Zufallsvektors wieder.\nGeben Sie die Definition des Begriffs der multivariaten WMF wieder.\nGeben Sie die Definition des Begriffs der multivariaten WDF wieder.\nGeben Sie die Definition des Begriffs der univariaten Marginalverteilung eines Zufallsvektors wieder\nWie berechnet man die WMF der \\(i\\)ten Komponente eines diskreten Zufallsvektors?\nWie berechnet man die WDF der \\(i\\)ten Komponente eines kontinuierlichen Zufallsvektors?\nGeben Sie die Definition des Begriffs der Unabhängigkeit zweier Zufallsvariablen wieder.\nWie erkennt man an der gemeinsamen WMF oder WDF eines zweidimensionalen Zufallsvektors, ob die Komponenten des Zufallsvektors unabhängig sind oder nicht?\nGeben Sie die Definition des Begriffs der Unabhängigkeit von \\(n\\) Zufallsvariablen wieder.\nGeben Sie die Definition des Begriffs der \\(n\\) unabhängig und identisch verteilten Zufallsvariablen wieder.\n\n\n\n\n\nChristensen, R. (2011). Plane Answers to Complex Questions. Springer New York. https://doi.org/10.1007/978-1-4419-9816-3\n\n\nSchmidt, K. D. (2009). Maß und Wahrscheinlichkeit. Springer.",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Zufallsvektoren</span>"
    ]
  },
  {
    "objectID": "205-Erwartungswerte.html",
    "href": "205-Erwartungswerte.html",
    "title": "15  Erwartungswerte",
    "section": "",
    "text": "15.1 Erwartungswert\nDer Erwartungswert ist also eine skalare Zusammenfassung der Verteilung einer Zufallsvariable. Eine integrierte Definition des Erwartungswertes, die ohne eine Fallunterscheidung in kontinuierliche und diskrete Zufallsvariablen auskommt, ist möglich, erfordert aber mit der Einführung des Lebesgue-Integrals einigen technischen Aufwand. Wir verweisen dahingehend auf die weiterführende Literatur (vgl. Schmidt (2009), Meintrup & Schäffler (2005)). Intuitiv entspricht der Erwartungswert einer Zufallsvariable dem im langfristigen Mittel zu erwartenden Wert der Zufallsvariable, also etwa \\[\\begin{equation}\n\\mathbb{E}(\\xi) \\approx \\frac{1}{n}\\sum_{i=1}^n \\xi_i\n\\end{equation}\\] für eine große Zahl \\(n\\) von Kopien \\(\\xi_i\\) von \\(\\xi\\). Wir werden diese Intuition im Kontext der Gesetze der großen Zahl in Kapitel 17 weiter ausarbeiten.",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Erwartungswerte</span>"
    ]
  },
  {
    "objectID": "205-Erwartungswerte.html#sec-erwartungswert",
    "href": "205-Erwartungswerte.html#sec-erwartungswert",
    "title": "15  Erwartungswerte",
    "section": "",
    "text": "Definition 15.1 (Erwartungswert) \\((\\Omega, \\mathcal{A},\\mathbb{P})\\) sei ein Wahrscheinlichkeitsraum und \\(\\xi\\) sei eine Zufallsvariable. Dann ist der Erwartungswert von \\(\\xi\\) definiert als\n\n\\(\\mathbb{E}(\\xi) := \\sum_{x \\in \\mathcal{X}} x\\,p_\\xi(x)\\), wenn \\(\\xi : \\Omega \\to \\mathcal{X}\\) diskret mit WMF \\(p_\\xi\\) ist,\n\\(\\mathbb{E}(\\xi) := \\int_{-\\infty}^\\infty x \\,p_\\xi(x)\\,dx\\), wenn \\(\\xi : \\Omega \\to \\mathbb{R}\\) kontinuierlich mit WDF \\(p_\\xi\\) ist.\n\nMan sagt, dass der Erwartungswert einer Zufallsvariable existiert, wenn er endlich ist.\n\n\n\nBeispiele\nMit dem Erwartungswert einer Bernoulli-Zufallsvariable und dem Erwartungswert einer normalverteilten Zufallsvariable wollen wir nun zwei erste Beispiele für den Erwartungswert einer diskreten und einer kontinuierlichen Zufallsvariable betrachten.\n\nTheorem 15.1 (Erwartungswert einer Bernoulli Zufallsvariable) Es sei \\(\\xi \\sim \\mbox{Bern}(\\mu)\\). Dann gilt \\(\\mathbb{E}(\\xi) = \\mu\\).\n\n\nBeweis. \\(\\xi\\) ist diskret mit \\(\\mathcal{X} = \\{0,1\\}\\). Also gilt \\[\\begin{align}\n\\begin{split}\n\\mathbb{E}(\\xi)\n& = \\sum_{x \\in \\{0,1\\}} x\\,\\mbox{Bern}(x;\\mu) \\\\\n& = 0\\cdot \\mu^0 (1 - \\mu)^{1-0} + 1\\cdot \\mu^1 (1 - \\mu)^{1-1} \\\\\n& = 1\\cdot \\mu^1 (1 - \\mu)^{0} \\\\\n& = \\mu.\n\\end{split}\n\\end{align}\\]\n\nEs ergibt sich hier also, dass der Parameter \\(\\mu \\in [0,1]\\) der Verteilung einer Bernoulli-Zufallsvariable gleichzeitig auch ihr Erwartungswert ist.\n\nTheorem 15.2 (Erwartungswert einer normalverteilten Zufallsvariable) Es sei \\(\\xi \\sim N(\\mu,\\sigma^2)\\). Dann gilt \\(\\mathbb{E}(\\xi) = \\mu\\).\n\n\nBeweis. Die Herleitung des Erwartungswerts einer normalverteilten Zufallsvariable ist überraschend aufwändig. Wir müssen in diesem Fall einige grundlegende Eigenschaften der Exponentialfunktion als gegeben annehmen. Dazu halten wir zunächst ohne Beweis fest, dass \\[\n\\int_{-\\infty}^\\infty \\exp\\left(-x^2\\right)\\,dx = \\sqrt{\\pi}\n\\tag{15.1}\\] und dass \\[\n\\lim_{x \\to -\\infty} \\exp\\left(-x^2\\right) = 0 \\mbox{ und } \\lim_{x \\to \\infty}\\exp\\left(-x^2\\right) = 0.\n\\tag{15.2}\\] Gleichung 15.1 ist unter der Bezeichnung Gauss- oder Euler-Poisson-Integral bekannt. Mit der Definition des Erwartungswerts für kontinuierliche Zufallsvariablen gilt dann zunächst \\[\\begin{equation}\n\\mathbb{E}(\\xi)\n= \\int_{-\\infty}^\\infty x \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{1}{2\\sigma^2}(x - \\mu)^2\\right) \\,dx.\n\\end{equation}\\] Mit der allgemeinen Substitutionsregel (vgl. Theorem 7.1) \\[\\begin{equation}\n\\int_{g(a)}^{g(b)} f(x)\\,dx = \\int_a^b f(g(x))g'(x)\\,dx\n\\end{equation}\\] und der Definition von \\[\\begin{equation}\ng : \\mathbb{R} \\to \\mathbb{R}, x \\mapsto g(x) := \\sqrt{2\\sigma^2}x + \\mu\n\\mbox{ with } g'(x) = \\sqrt{2\\sigma^2},\n\\end{equation}\\] gilt dann \\[\\begin{align}\n\\begin{split}\n\\mathbb{E}(\\xi)\n& = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\n\\int_{-\\infty}^\\infty x\n\\exp\\left(-\\frac{1}{2\\sigma^2}(x - \\mu)^2\\right) \\,dx \\\\\n& = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\n\\int_{-\\infty}^\\infty (\\sqrt{2\\sigma^2}x + \\mu)\n\\exp\\left(-\\frac{1}{2\\sigma^2}\\left(\\left(\\sqrt{2\\sigma^2}x + \\mu \\right) - \\mu\\right)^2\\right)\n\\sqrt{2\\sigma^2}\\,dx \\\\\n& = \\frac{\\sqrt{2\\sigma^2}}{\\sqrt{2\\pi\\sigma^2}}\n\\int_{-\\infty}^\\infty (\\sqrt{2\\sigma^2}x + \\mu)\n\\exp\\left(-x^2\\right) \\,dx \\\\\n& = \\frac{1}{\\sqrt{\\pi}}\n\\left(\\sqrt{2\\sigma^2} \\int_{-\\infty}^\\infty x \\exp\\left(-x^2\\right) \\,dx\n      + \\mu \\int_{-\\infty}^\\infty \\exp\\left(-x^2\\right) \\,dx \\right) \\\\\n& = \\frac{1}{\\sqrt{\\pi}}\n\\left(\\sqrt{2\\sigma^2} \\int_{-\\infty}^\\infty x \\exp\\left(-x^2\\right) \\,dx\n      + \\mu \\sqrt{\\pi} \\right).\n\\end{split}\n\\end{align}\\] Eine Stammfunktion von \\(x \\exp\\left(-x^2\\right)\\) ist \\(-\\frac{1}{2}\\exp\\left(-x^2\\right)\\), weil \\[\\begin{equation}\n\\frac{d}{dx}\\left(-\\frac{1}{2}\\exp\\left(-x^2\\right)\\right)\n= -\\frac{1}{2} \\frac{d}{dx}\\exp\\left(-x^2\\right)\n= -\\frac{1}{2}\\exp\\left(-x^2\\right)(-2x)\n= x\\exp\\left(-x^2\\right)\n\\end{equation}\\] Mit Gleichung 15.2 und der Definition des uneigentlichen Integrals (vgl. Definition 7.5) verschwindet der Integralterm \\(\\int_{-\\infty}^\\infty x \\exp\\left(-x^2\\right) \\,dx\\) damit und wir erhalten \\[\\begin{align}\n\\mathbb{E}(\\xi)\n= \\frac{1}{\\sqrt{\\pi}}\\left(\\mu \\sqrt{\\pi}\\right)\n= \\mu.\n\\end{align}\\]\n\nDer Erwartungswert einer univariaten Normalverteilung ist also durch ihren Parameter \\(\\mu\\in \\mathbb{R}\\) gegeben.\nIn Verallgemeinerung von Definition 15.1 geben wir folgende Definition für den Erwartungswert einer Funktion einer Zufallsvariable\n\nDefinition 15.2 (Erwartungswert einer Funktion einer Zufallsvariable) \\((\\Omega, \\mathcal{A},\\mathbb{P})\\) sei ein Wahrscheinlichkeitsraum, \\(\\xi\\) sei eine Zufallsvariable mit Ergebnisraum \\(\\mathcal{X}\\) und \\(f: \\mathcal{X} \\to \\mathcal{Z}\\) sei eine Funktion mit Zielmenge \\(\\mathcal{Z}\\). Dann ist der Erwartungswert der Funktion \\(f\\) der Zufallsvariable \\(\\xi\\) definiert als\n\n\\(\\mathbb{E}(f(\\xi)) := \\sum_{x \\in \\mathcal{X}} f(x)\\,p_\\xi(x)\\), wenn \\(\\xi : \\Omega \\to \\mathcal{X}\\) diskret mit WMF \\(p_\\xi\\) ist,\n\\(\\mathbb{E}(f(\\xi)) := \\int_{-\\infty}^\\infty f(x) \\,p_\\xi(x)\\,dx\\), wenn \\(\\xi : \\Omega \\to \\mathbb{R}\\) kontinuierlich mit WDF \\(p_\\xi\\) ist.\n\n\nDer Erwartungswert einer Zufallsvariable ergibt sich anhand von Definition 15.2 als der Spezialfall, in dem gilt, dass \\[\\begin{equation}\nf : \\mathcal{X} \\to \\mathcal{Z}, x \\mapsto f(x) := x.\n\\end{equation}\\] In der englischsprachigen Literatur ist Definition 15.2 auch als “Law of the unconscious statistician” bekannt und wird oft auch direkt zur Definition des Erwartungswertes herangezogen.\nWeiterhin ist man wie im univariaten Fall manchmal darum bemüht, die Verteilung eines Zufallsvektors mit einigen wenigen Maßzahlen zu charakterisieren. Das multivariate Analogon des des Erwartungswerts einer Zufallsvariablen ist der Erwartungswert eines Zufallsvektors, der wie folgt definiert ist.\n\nDefinition 15.3 (Erwartungswert eines Zufallsvektors) \\(\\xi\\) sei ein \\(n\\)-dimensionaler Zufallvektor. Dann ist der Erwartungwert von \\(\\xi\\) definiert als der \\(n\\)-dimensionale reelle Vektor \\[\\begin{equation}\n\\mathbb{E}(\\xi) :=\n\\begin{pmatrix}\n\\mathbb{E}(\\xi_1) \\\\\n\\vdots            \\\\\n\\mathbb{E}(\\xi_n)\n\\end{pmatrix}\n\\end{equation}\\]\n\nDer Erwartungswert eines Zufallsvektors \\(\\xi\\) ist also der Vektor der Erwartungswerte der Komponenten \\(\\xi_1, ...,\\xi_n\\) von \\(\\xi\\), ist also direkt im Sinne von Erwartungswerten von Zufallsvariablen definiert. In Analogie zu Definition 15.2 definiert man für die Funktion eines Zufallsvektors den Erwartungswert dieser Transformation wie folgt.\n\nDefinition 15.4 (Erwartungswert einer Funktion eines Zufallsvektors) \\((\\Omega, \\mathcal{A},\\mathbb{P})\\) sei ein Wahrscheinlichkeitsraum, \\(\\xi\\) sei ein Zufallsvektor mit Ergebnisraum \\(\\mathcal{X}\\) und \\(f: \\mathcal{X} \\to \\mathcal{Z}\\) sei eine Funktion mit Zielmenge \\(\\mathcal{Z}\\). Dann ist der Erwartungswert der Funktion \\(f\\) des Zufallsvektors \\(\\xi\\) definiert als\n\n\\(\\mathbb{E}(f(\\xi)) := \\sum_{x \\in \\mathcal{X}} f(x)\\,p_\\xi(x)\\), wenn \\(\\xi : \\Omega \\to \\mathcal{X}\\) diskret mit WMF \\(p_\\xi\\) ist,\n\\(\\mathbb{E}(f(\\xi)) := \\int_{-\\infty}^\\infty f(x) \\,p_\\xi(x)\\,dx\\), wenn \\(\\xi : \\Omega \\to \\mathbb{R}\\) kontinuierlich mit WDF \\(p_\\xi\\) ist.\n\n\nFolgendes Theorem gibt nun einige Rechenregeln im Umgang mit Erwartungswerten an, die uns an vielen Stellen begegnen werden. Diese Rechenregeln folgen direkt aus der Summen- bzw. Integraldefinition des Erwartungswertes, im Beweis des Theorems betrachten wir dementsprechend lediglich den Fall kontinuierlicher Zufallsvariablen.\n\nTheorem 15.3 (Eigenschaften des Erwartungswerts)  \n\n(Linear-affine Transformation) Für eine Zufallsvariable \\(\\xi\\) und \\(a,b\\in \\mathbb{R}\\) gilt \\[\\begin{equation}\n\\mathbb{E}(a\\xi + b) = a\\mathbb{E}(\\xi) + b.\n\\end{equation}\\]\n(Linearkombination) Für Zufallsvariablen \\(\\xi_1,...,\\xi_n\\) und \\(a_1,...,a_n \\in \\mathbb{R}\\) gilt \\[\\begin{equation}\n\\mathbb{E}\\left(\\sum_{i=1}^n a_i\\xi_i \\right) = \\sum_{i = 1}^n a_i \\mathbb{E}(\\xi_i).\n\\end{equation}\\]\n(Faktorisierung bei Unabhängigkeit) Für unabhängige Zufallsvariablen \\(\\xi_1,...,\\xi_n\\) gilt \\[\\begin{equation}\n\\mathbb{E}\\left(\\prod_{i=1}^n \\xi_i \\right) = \\prod_{i = 1}^n \\mathbb{E}(\\xi_i).\n\\end{equation}\\]\n\n\n\nBeweis. Eigenschaft (1) folgt aus den Linearitätseigenschaften von Summen und Integralen. Wir betrachten nur den Fall einer kontinuierlichen Zufallsvariable \\(\\xi\\) mit WDF \\(p_\\xi\\) genauer und definieren zunächst \\(\\upsilon := a\\xi + b\\). Dann gilt\n\\[\\begin{align}\n\\begin{split}\n\\mathbb{E}(\\upsilon)\n& = \\mathbb{E}(a\\xi + b)                                        \\\\\n& = \\int_{-\\infty}^\\infty (ax + b)p_\\xi(x) \\,dx                             \\\\\n& = \\int_{-\\infty}^\\infty  axp_\\xi(x)  + b p_\\xi(x) \\,dx                        \\\\\n& = a\\int_{-\\infty}^\\infty xp_\\xi(x) \\,dx + b \\int_{-\\infty}^\\infty p_\\xi(x) \\,dx           \\\\\n& = a\\mathbb{E}(\\xi) + b.\n\\end{split}\n\\end{align}\\] Eigenschaft (2) folgt gleichfalls aus den Linearitätseigenschaften von Summen und Integralen. Wir wollen nur den Fall von zwei kontinuierlichen Zufallsvariablen \\(\\xi_1\\) und \\(\\xi_2\\) mit bivariater WDF \\(p_{\\xi_1,\\xi_2}\\) genauer betrachten. In diesem Fall gilt \\[\\begin{align}\n\\begin{split}\n\\mathbb{E}\\left(\\sum_{i=1}^2 a_i\\xi_i\\right)\n& = \\mathbb{E}(a_1\\xi_1 + a_2\\xi_2) \\\\\n& = \\iint_{\\mathbb{R}^2} (a_1x_1 + a_2x_2)p_{\\xi_1,\\xi_2}(x_1,x_2)\\,dx_1\\,dx_2  \\\\\n& = \\iint_{\\mathbb{R}^2} a_1x_1 p_{\\xi_1,\\xi_2}(x_1,x_2)\n                                   + a_2x_2 p_{\\xi_1,\\xi_2}(x_1,x_2)\\,dx_1\\,dx_2            \\\\\n& =  a_1\\iint_{\\mathbb{R}^2} x_1 p_{\\xi_1,\\xi_2}(x_1,x_2) \\,dx_1\\,dx_2\n  +  a_2\\iint_{\\mathbb{R}^2} x_2 p_{\\xi_1,\\xi_2}(x_1,x_2)\\,dx_1\\,dx_2           \\\\\n& =  a_1\\int_{-\\infty}^\\infty x_1 \\left(\\int_{-\\infty}^\\infty p_{\\xi_1,\\xi_2}(x_1,x_2) \\,dx_2 \\right)\\,dx_1\n  +  a_2\\int_{-\\infty}^\\infty x_2 \\left(\\int_{-\\infty}^\\infty p_{\\xi_1,\\xi_2}(x_1,x_2) \\,dx_1 \\right) \\,dx_2 \\\\\n& =  a_1\\int_{-\\infty}^\\infty x_1 p_{\\xi_1}(x_1) \\,dx_1\n  +  a_2\\int_{-\\infty}^\\infty x_2 p_{\\xi_2}(x_2) \\,dx_2 \\\\\n& =  a_1 \\mathbb{E}(\\xi_1) +  a_2\\mathbb{E}(\\xi_2) \\\\\n& = \\sum_{i=1}^2 a_i \\mathbb{E}(\\xi_i).\n\\end{split}\n\\end{align}\\] Ein Induktionsbeweis erlaubt dann die Generalisierung vom bivariaten auf den \\(n\\)-variaten Fall.\nZu Eigenschaft (3) betrachten wir den Fall von \\(n\\) kontinuierlichen Zufallsvariablen mit gemeinsamer WDF \\(p_{\\xi_1,...,\\xi_n}\\). Weil als \\(\\xi_1,...,\\xi_n\\) unabhängig vorausgesetzt sind, gilt \\[\\begin{equation}\np_{\\xi_1,...,\\xi_n}(x_1,...,x_n) = \\prod_{i=1}^n p_{\\xi_i}(x_i).\n\\end{equation}\\] Weiterhin gilt also \\[\\begin{align}\n\\begin{split}\n\\mathbb{E}\\left(\\prod_{i=1}^n \\xi_i \\right)\n& = \\int_{-\\infty}^\\infty\\cdots\\int_{-\\infty}^\\infty \\left(\\prod_{i=1}^n x_i\\right)\n        p_{\\xi_1,...,\\xi_n}(x_1,...,x_n) \\,dx_1...\\,dx_n    \\\\\n& = \\int_{-\\infty}^\\infty\\cdots\\int_{-\\infty}^\\infty  \\prod_{i=1}^n x_i\n         \\prod_{i=1}^n p_{\\xi_i}(x_i)\\,dx_1...\\,dx_n    \\\\\n& = \\int_{-\\infty}^\\infty\\cdots \\int_{-\\infty}^\\infty  \\prod_{i=1}^n x_i p_{\\xi_i}(x_i) \\,dx_1...\\,dx_n \\\\\n& = \\prod_{i=1}^n \\int_{-\\infty}^\\infty x_i p_{\\xi_i}(x_i) \\,dx_i   \\\\\n& = \\prod_{i=1}^n \\mathbb{E}(\\xi_i).\n\\end{split}\n\\end{align}\\]",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Erwartungswerte</span>"
    ]
  },
  {
    "objectID": "205-Erwartungswerte.html#sec-varianz-und-standardabweichung",
    "href": "205-Erwartungswerte.html#sec-varianz-und-standardabweichung",
    "title": "15  Erwartungswerte",
    "section": "15.2 Varianz und Standardabweichung",
    "text": "15.2 Varianz und Standardabweichung\nHäufig genutzte Maße für die Streuung von Verteilungen von Zufallsvariablen sind die Varianz und die Standardabweichung. Diese sind wie folgt definiert.\n\nDefinition 15.5 (Varianz und Standardabweichung) \\(\\xi\\) sei eine Zufallsvariable mit existierendem Erwartungswert \\(\\mathbb{E}(\\xi)\\).\n\nDie ist definiert als \\[\\begin{equation}\n\\mathbb{V}(\\xi) := \\mathbb{E}\\left((\\xi - \\mathbb{E}(\\xi))^2\\right).\n\\end{equation}\\]\nDie ist definiert als \\[\\begin{equation}\n\\mathbb{S}(\\xi) := \\sqrt{\\mathbb{V}(\\xi)}.\n\\end{equation}\\]\n\n\nInwiefern die Varianz und ihre Quadratwurzel als Maße für die Streuung einer Zufallsvariable dienen, werden wir in Theorem 16.2 begründen. Die Quadrierung der Abweichung der Zufallsvariable von ihrem Erwartungswert in der Definition der Varianz ist nötig, da andernfalls mit Theorem 15.3 immer gelten würde, dass \\[\\begin{equation}\n\\mathbb{E}(\\xi-\\mathbb{E}(\\xi)) = \\mathbb{E}(\\xi) - \\mathbb{E}(\\xi) = 0.\n\\end{equation}\\] Allerdings gibt es neben der Varianz durchaus weitere Maße der Streuung von Zufallsvariablen, hier seien beispielsweise die erwartete absolute Abweichung einer Zufallsvariable von ihrem Erwartungswert, \\(\\mathbb{E}(|\\xi - \\mathbb{E}(\\xi)|)\\) und die sogenannte Entropie \\(-\\mathbb{E}(\\ln p_\\xi)\\) genannt. Im Sinne von Definition 15.2 ist die Varianz der Zufallsvariable \\(\\xi: \\Omega \\to \\mathcal{X}\\) der Erwartungswert der Funktion \\[\\begin{equation}\nf : \\mathcal{X} \\to \\mathcal{Z}, x \\mapsto f(x) := (x - \\mathbb{E}(\\xi))^2.\n\\end{equation}\\] Das Berechnen von Varianzen wird durch folgendes Theorem, den sogenannten Varianzverschiebungssatz oft erleichtert, insbesondere, wenn der Erwartungswert der quadrierten Zufallsvariable leicht zu bestimmen oder bekannt ist.\n\nTheorem 15.4 (Varianzverschiebungssatz) \\(\\xi\\) sei eine Zufallsvariable. Dann gilt \\[\\begin{equation}\n\\mathbb{V}(\\xi) = \\mathbb{E}\\left(\\xi^2 \\right) - \\mathbb{E}(\\xi)^2.\n\\end{equation}\\]\n\n\nBeweis. Mit der Definition der Varianz und der Linearität des Erwartungswerts gilt \\[\\begin{align}\n\\begin{split}\n\\mathbb{V}(\\xi)\n& = \\mathbb{E}\\left((\\xi - \\mathbb{E}(\\xi))^2\\right) \\\\\n& = \\mathbb{E}\\left(\\xi^2 - 2\\xi\\mathbb{E}(\\xi) + \\mathbb{E}(\\xi)^2 \\right) \\\\\n& =    \\mathbb{E}(\\xi^2)\n    - 2\\mathbb{E}(\\xi)\\mathbb{E}(\\xi)\n    + \\mathbb{E}\\left(\\mathbb{E}(\\xi)^2\\right)   \\\\\n& = \\mathbb{E}(\\xi^2) - 2\\mathbb{E}(\\xi)^2 + \\mathbb{E}(\\xi)^2  \\\\\n& = \\mathbb{E}(\\xi^2) - \\mathbb{E}(\\xi)^2.\n\\end{split}\n\\end{align}\\]\n\nWie für den Erwartungswert gibt es auch für die Varianz einige Rechenregeln, die den Umgang mit ihr oft erleichtern. Wir fassen sie in folgendem Theorem zusammen.\n\nTheorem 15.5 (Eigenschaften der Varianz)  \n\n(Linear-affine Transformation) Für eine Zufallsvariable \\(\\xi\\) und \\(a,b\\in \\mathbb{R}\\) gelten \\[\\begin{equation}\n\\mathbb{V}(a\\xi + b) = a^2 \\mathbb{V}(\\xi)\n\\mbox{ und }\n\\mathbb{S}(a\\xi + b) = |a|\\mathbb{S}(\\xi).\n\\end{equation}\\]\n(Linearkombination bei Unabhängigkeit) Für unabhängige Zufallsvariablen \\(\\xi_1,...,\\xi_n\\) und \\(a_1,...,a_n \\in \\mathbb{R}\\) gilt \\[\\begin{equation}\n\\mathbb{V}\\left(\\sum_{i=1}^n a_i\\xi_i\\right) = \\sum_{i=1}^n a_i^2 \\mathbb{V}(\\xi_i).\n\\end{equation}\\]\n\n\n\nBeweis. Um Eigenschaft (1) zu zeigen, definieren wir zunächst \\(\\upsilon := a\\xi + b\\) und halten fest, dass \\(\\mathbb{E}(\\upsilon) = a\\mathbb{E}(\\xi) + b\\). Für die Varianz von \\(\\upsilon\\) ergibt sich dann \\[\\begin{align}\n\\begin{split}\n\\mathbb{V}(\\upsilon)\n& = \\mathbb{E}\\left((\\upsilon - \\mathbb{E}(\\upsilon))^2\\right)      \\\\\n& = \\mathbb{E}\\left((a\\xi+b-a\\mathbb{E}(\\xi)-b)^2\\right)    \\\\\n& = \\mathbb{E}\\left((a\\xi-a\\mathbb{E}(\\xi))^2\\right)        \\\\\n& = \\mathbb{E}\\left((a(\\xi - \\mathbb{E}(\\xi))^2\\right)      \\\\\n& = \\mathbb{E}\\left(a^2(\\xi - \\mathbb{E}(\\xi))^2\\right)     \\\\\n& = a^2\\mathbb{E}\\left((\\xi - \\mathbb{E}(\\xi))^2\\right)     \\\\\n& = a^2\\mathbb{V}(\\xi)                                  \\\\\n\\end{split}\n\\end{align}\\] Wurzelziehen ergibt dann das Resultat für die Standardabweichung.\nFür Eigenschaft (2) betrachten wir den Fall zweier unabhängiger Zufallsvariablen \\(\\xi_1\\) und \\(\\xi_2\\) genauer. Wir halten zunächst fest, dass in diesem Fall gilt, dass \\[\\begin{equation}\n\\mathbb{E}\\left(a_1\\xi_1 + a_2\\xi_2\\right) = a_1\\mathbb{E}(\\xi_1) + a_2\\mathbb{E}(\\xi_2).\n\\end{equation}\\] Es ergibt sich also \\[\\begin{align}\n\\begin{split}\n\\mathbb{V}\\left(\\sum_{i=1}^2 a_i \\xi_i\\right)                                                \n& = \\mathbb{V}(a_1\\xi_1 + a_2\\xi_2)                                                               \\\\\n& = \\mathbb{E}\\left((a_1\\xi_1 + a_2\\xi_2 - \\mathbb{E}\\left(a_1\\xi_1 + a_2\\xi_2\\right))^2\\right)   \\\\\n& = \\mathbb{E}\\left((a_1\\xi_1 + a_2\\xi_2 - a_1\\mathbb{E}(\\xi_1) - a_2\\mathbb{E}(\\xi_2))^2\\right)  \\\\\n& = \\mathbb{E}\\left((a_1\\xi_1 - a_1\\mathbb{E}(\\xi_1) + a_2\\xi_2  - a_2\\mathbb{E}(\\xi_2))^2\\right) \\\\\n& = \\mathbb{E}\\left(((a_1(\\xi_1 - \\mathbb{E}(\\xi_1)) + (a_2(\\xi_2 - \\mathbb{E}(\\xi_2)))^2\\right)  \\\\\n& = \\mathbb{E}\\left((a_1(\\xi_1 - \\mathbb{E}(\\xi_1)))^2\n                   + 2(a_1(\\xi_1 - \\mathbb{E}(\\xi_1))(a_2(\\xi_2 - \\mathbb{E}(\\xi_2))\n                   + (a_2(\\xi_2 - \\mathbb{E}(\\xi_2)))^2\\right) \\\\\n& = \\mathbb{E}\\left((a_1^2(\\xi_1 - \\mathbb{E}(\\xi_1))^2\n                   + 2a_1a_2(\\xi_1 - \\mathbb{E}(\\xi_1))(\\xi_2 - \\mathbb{E}(\\xi_2))\n                   + a_2^2(\\xi_2 - \\mathbb{E}(\\xi_2))^2\\right) \\\\\n& = a_1^2\\mathbb{E}\\left((\\xi_1 - \\mathbb{E}(\\xi_1))^2\\right)\n   + 2a_1a_2\\mathbb{E}\\left((\\xi_1 - \\mathbb{E}(\\xi_1))(\\xi_2 - \\mathbb{E}(\\xi_2))\\right)\n   + a_2^2\\mathbb{E}\\left((\\xi_2 - \\mathbb{E}(\\xi_2))^2\\right) \\\\\n& = a_1^2\\mathbb{V}(\\xi_1)\n   + 2a_1a_2\\mathbb{E}\\left((\\xi_1 - \\mathbb{E}(\\xi_1))(\\xi_2 - \\mathbb{E}(\\xi_2))\\right)\n   + a_2^2\\mathbb{V}(\\xi_2) \\\\\n& = \\sum_{i=1}^2 a_i^2\\mathbb{V}(\\xi_i)\n   + 2a_1a_2\\mathbb{E}\\left((\\xi_1 - \\mathbb{E}(\\xi_1))(\\xi_2 - \\mathbb{E}(\\xi_2))\\right).\n\\end{split}.\n\\end{align}\\] Weil \\(\\xi_1\\) und \\(\\xi_2\\) unabhängig sind, ergibt sich mit den Eigenschaften des Erwartungswerts für unabhängige Zufallsvariablen, dass \\[\\begin{align}\n\\begin{split}\n\\mathbb{E}\\left((\\xi_1 - \\mathbb{E}(\\xi_1))(\\xi_2 - \\mathbb{E}(\\xi_2))\\right)\n& = \\mathbb{E}\\left((\\xi_1 - \\mathbb{E}(\\xi_1))\\right)\n    \\mathbb{E}\\left((\\xi_2 - \\mathbb{E}(\\xi_2))\\right) \\\\\n& = (\\mathbb{E}(\\xi_1) - \\mathbb{E}(\\xi_1))\n    (\\mathbb{E}(\\xi_2) - \\mathbb{E}(\\xi_2)) \\\\\n& = 0\n\\end{split}\n\\end{align}\\] ist. Damit folgt also \\[\\begin{equation}\n\\mathbb{V}\\left(\\sum_{i=1}^2 a_i \\xi_i\\right)\n=  \\sum_{i=1}^2 a_i^2\\mathbb{V}(\\xi_i).\n\\end{equation}\\] Ein Induktionsbeweis erlaubt dann die Generalisierung vom bivariaten zum \\(n\\)-variaten Fall.\n\n\nBeispiele\nMit der Varianz einer Bernoulli-Zufallsvariable und der Varianz einer normalverteilten Zufallsvariable wollen wir auch hier zwei erste Beispiele für die Varianz einer diskreten und einer kontinuierlichen Zufallsvariable betrachten.\n\nTheorem 15.6 (Varianz einer Bernoulli Zufallsvariable) Es sei \\(\\xi \\sim \\mbox{Bern}(\\mu)\\). Dann ist die Varianz von \\(\\xi\\) gegeben durch \\[\\begin{equation}\n\\mathbb{V}(\\xi) = \\mu(1-\\mu).\n\\end{equation}\\]\n\n\nBeweis. \\(\\xi\\) ist eine diskrete Zufallsvariable und es gilt \\(\\mathbb{E}(\\xi) = \\mu\\). Also gilt \\[\\begin{align}\n\\begin{split}\n\\mathbb{V}(\\xi)\n& = \\mathbb{E}\\left((\\xi - \\mu)^2\\right) \\\\\n& = \\sum_{x \\in \\{0,1\\}} (x - \\mu)^2 \\mbox{Bern}(x;\\mu) \\\\\n& = (0 - \\mu)^2 \\mu^0(1-\\mu)^{1-0}  + (1 - \\mu)^2\\mu^1(1-\\mu)^{1-1}  \\\\\n& = \\mu^2 (1-\\mu)  + (1 - \\mu)^2\\mu  \\\\\n& = \\left(\\mu^2  + (1 - \\mu)\\mu\\right)(1-\\mu)  \\\\\n& = \\left(\\mu^2 + \\mu - \\mu^2\\right)(1 - \\mu) \\\\\n& = \\mu(1-\\mu).\n\\end{split}\n\\end{align}\\]\n\n\nTheorem 15.7 (Varianz einer normalverteilten Zufallsvariable) Es sei \\(\\xi \\sim N(\\mu,\\sigma^2)\\). Dann ist die Varianz von \\(\\xi\\) gegeben durch \\[\\begin{equation}\n\\mathbb{V}(\\xi) = \\sigma^2.\n\\end{equation}\\]\n\n\nBeweis. Die Herleitung der Varianz einer normalverteilten Zufallsvariable ist nicht unaufwändig, so dass wir hier auch wieder unbewiesen die Gültigkeit von Gleichung 15.1 und Gleichung 15.2 sowie weiterhin von \\[\n\\int_{-\\infty}^\\infty x \\exp(-x^2)\\,dx = 0\n\\tag{15.3}\\] annehmen wollen. Wir halten zunächst fest, dass mit dem Varianzverschiebungssatz gilt, dass \\[\\begin{align}\\label{eq:var_gauss_1}\n\\begin{split}\n\\mathbb{V}(\\xi)\n= \\mathbb{E}(\\xi^2) - \\mathbb{E}(\\xi)^2\n= \\frac{1}{2\\pi\\sigma^2}\\int_{-\\infty}^\\infty x^2 \\exp\\left(-\\frac{1}{2\\sigma^2}(x-\\mu)^2 \\right)\\,dx - \\mu^2.\n\\end{split}\n\\end{align}\\] Mit der allgemeinen Substitutionsregel (Theorem 7.1) \\[\\begin{equation}\n\\int_{a}^{b} f(g(x))g'(x)\\,dx = \\int_{g(a)}^{g(b)} f(x)\\,dx\n\\end{equation}\\] und der Definition von \\[\\begin{equation}\ng:\\mathbb{R} \\to \\mathbb{R}, x \\mapsto \\sqrt{2\\sigma^2}x + \\mu,\ng(-\\infty) := -\\infty, g(\\infty) := \\infty,\n\\mbox{ mit }\ng'(x) = \\sqrt{2\\sigma^2}\n\\end{equation}\\] kann das Integral auf der rechten Seite von Gleichung \\(\\eqref{eq:var_gauss_1}\\) dann als \\[\\begin{align}\n\\begin{split}\n\\int_{-\\infty}^\\infty x^2 \\exp\\left(-\\frac{1}{2\\sigma^2}(x - \\mu)^2 \\right) \\,dx\n& = \\int_{-\\infty}^\\infty (\\sqrt{2\\sigma^2}x + \\mu)^2 \\exp\\left(-\\frac{1}{2\\sigma^2}((\\sqrt{2\\sigma^2}x + \\mu)-\\mu)^2 \\right)\\sqrt{2\\sigma^2}\\,dx \\\\\n& = \\sqrt{2\\sigma^2}\\int_{-\\infty}^\\infty (\\sqrt{2\\sigma^2}x + \\mu)^2 \\exp\\left(-\\frac{2\\sigma^2 x^2}{2\\sigma^2} \\right)\\,dx \\\\\n& = \\sqrt{2\\sigma^2}\\int_{-\\infty}^\\infty (\\sqrt{2\\sigma^2}x + \\mu)^2 \\exp\\left(-x^2\\right)\\,dx\n\\end{split}\n\\end{align}\\] geschrieben werden. Also gilt \\[\\begin{align}\n\\begin{split}\n\\mathbb{V}(\\xi)\n& =\n\\frac{\\sqrt{2\\sigma^2}}{\\sqrt{2\\pi\\sigma^2}}\n\\int_{-\\infty}^\\infty (\\sqrt{2\\sigma^2}x + \\mu)^2 \\exp\\left(-x^2 \\right)\\,dx\n- \\mu^2\n\\\\\n&\n= \\frac{1}{\\sqrt{\\pi}}\n\\int_{-\\infty}^\\infty(\\sqrt{2\\sigma^2}x)^2 + 2\\sqrt{2\\sigma^2}x\\mu + \\mu^2) \\exp\\left(-x^2 \\right)\\,dx\n- \\mu^2\n\\\\\n&\n= \\frac{1}{\\sqrt{\\pi}}\n\\left(\n        2\\sigma^2\\int_{-\\infty}^\\infty x^2 \\exp\\left(-x^2 \\right)\\,dx +\n        2\\sqrt{2\\sigma^2}\\mu\\int_{-\\infty}^\\infty x\\exp\\left(-x^2 \\right)\\,dx +\n        \\mu^2\\int_{-\\infty}^\\infty \\exp\\left(-x^2 \\right)\\,dx\n\\right)\n- \\mu^2.\n\\end{split}\n\\end{align}\\] Mit Gleichung 15.3 ergibt sich dann \\[\\begin{align}\n\\begin{split}\n\\mathbb{V}(\\xi)\n& = \\frac{1}{\\sqrt{\\pi}}\n\\left(2\\sigma^2\\int_{-\\infty}^\\infty x^2 \\exp\\left(-x^2 \\right)\\,dx + \\mu^2\\sqrt{\\pi} \\right)\n- \\mu^2\n\\\\\n& = \\frac{2\\sigma^2}{\\sqrt{\\pi}}\n\\int_{-\\infty}^\\infty x^2 \\exp\\left(-x^2 \\right)\\,dx\n+ \\mu^2 - \\mu^2\n\\\\\n& = \\frac{2\\sigma^2}{\\sqrt{\\pi}} \\int_{-\\infty}^\\infty x^2 \\exp\\left(-x^2 \\right)\\,dx.\n\\end{split}\n\\end{align}\\] Mit der allgemeinen Form der partiellen Integrationsregel (Theorem 7.1) \\[\\begin{equation}\n\\int_{a}^{b} f'(x)g(x)\\,dx =\nf(x)g(x)|_{a}^{b} - \\int_{a}^{b} f(x)g'(x)\\,dx\n\\end{equation}\\] und der Definition von \\[\\begin{equation}\nf : \\mathbb{R} \\to \\mathbb{R}, x \\mapsto f(x) := \\exp\\left(-x^2\\right) \\mbox{ mit } f'(x) = -2\\exp\\left(-x^2\\right)\n\\end{equation}\\] und \\[\\begin{equation}\ng : \\mathbb{R} \\to \\mathbb{R}, x\\mapsto g(x) := -\\frac{1}{2}x \\mbox{ mit } g'(x) = -\\frac{1}{2},\n\\end{equation}\\] so dass \\[\\begin{equation}\nf'(x)g(x) = -2\\exp\\left(-x^2\\right)\\left(-\\frac{1}{2}x \\right) = x^2\\exp\\left(-x^2\\right),\n\\end{equation}\\] gilt, ergibt sich dann \\[\\begin{align}\n\\begin{split}\n\\mathbb{V}(\\xi)\n& = \\frac{2\\sigma^2}{\\sqrt{\\pi}} \\int_{-\\infty}^\\infty x^2 \\exp\\left(-x^2 \\right)\\,dx  \\\\\n& = \\frac{2\\sigma^2}{\\sqrt{\\pi}}\n\\left( -\\frac{1}{2}x\\exp\\left(-x^2\\right)|_{-\\infty}^{\\infty}\n- \\int_{-\\infty}^\\infty \\exp\\left(-x^2 \\right)\\left(-\\frac{1}{2} \\right)\\,dx \\right)  \\\\\n& = \\frac{2\\sigma^2}{\\sqrt{\\pi}}\n\\left(\n-\\frac{1}{2}x\\exp\\left(-x^2\\right)|_{-\\infty}^{\\infty}\n+ \\frac{1}{2}\\int_{-\\infty}^\\infty \\exp\\left(-x^2 \\right)\\,dx\n\\right).\n\\end{split}\n\\end{align}\\] Aus Gleichung 15.2 schließen wir dann, dass der erste Term in den Klammern auf der rechten Seite der obigen Gleichung gleich \\(0\\) ist. Schließlich ergibt sich damit \\[\\begin{align}\n\\begin{split}\n\\mathbb{V}(\\xi)\n= \\frac{2\\sigma^2}{\\sqrt{\\pi}} \\left(\\frac{1}{2}\\int_{-\\infty}^\\infty \\exp\\left(-x^2 \\right)\\,dx\\right)\n= \\frac{\\sigma^2}{\\sqrt{\\pi}} \\sqrt{\\pi}\n= \\sigma^2.\n\\end{split}\n\\end{align}\\]\n\nAllgemein ergeben sich die Erwartungswerte und Varianzen parametrischer Verteilungen als Funktionen ihrer Parameter. Wir fassen die Erwartungswerte uns bekannter Verteilungen in Theorem 15.8 zusammen.\n\nTheorem 15.8 (Erwartungswerte und Varianzen einiger Wahrscheinlichkeitsverteilungen)  \n\nWir verzichten auf einen Beweis.",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Erwartungswerte</span>"
    ]
  },
  {
    "objectID": "205-Erwartungswerte.html#sec-stichprobenkennzahlen",
    "href": "205-Erwartungswerte.html#sec-stichprobenkennzahlen",
    "title": "15  Erwartungswerte",
    "section": "15.3 Kennzahlen univariater Stichproben",
    "text": "15.3 Kennzahlen univariater Stichproben\nWie wir Kapitel 21 noch ausführlich diskutieren, werden ist eine Charakteristikum der probabilistischen Modellierung, beobachtete Daten als Realisierungen von Zufallsvariablen zu verstehen. Hat meine Menge \\(\\xi_1,...,\\xi_n\\) von Zufallsvariablen, so nennt man diese auch eine Stichprobe. Basierend auf einer Stichprobe kann man nun Kennzahlen berechnen, die auf den ersten Blick den Begriffen von Erwartungswert, Varianz und Standardabweichung ähneln, mit diesen aber keinesfalls zu verwechseln sind. Defacto dienen die in folgender Definition aufgeführten Stichprobenkennzahlen oft als Schätzer für die Kennzahlen von Zufallsvariablen, wie wir in Kapitel 22 ausführlich darlegen wollen. Gewissermaßen Vorgriff zur Abgrenzung der Begrifflichkeiten und auch als Grundlage für Kapitel 17 definieren wir hier einige deskriptive Stichprobenkennzahlen.\n\nDefinition 15.6 (Stichprobenmittel, Stichprobenvarianz, Stichprobenstandardabweichung) \\(\\xi_1,...,\\xi_n\\) sei eine Menge von Zufallsvariablen, genannt Stichprobe.\n\nDas Stichprobenmittel von \\(\\xi_1,...,\\xi_n\\) ist definiert als \\[\\begin{equation}\n\\bar{\\xi} := \\frac{1}{n}\\sum_{i=1}^n \\xi_i.\n\\end{equation}\\]\nDie Stichprobenvarianz von \\(\\xi_1,...,\\xi_n\\) ist definiert als \\[\\begin{equation}\nS^2 := \\frac{1}{n-1}\\sum_{i=1}^n (\\xi_i - \\bar{\\xi})^2.\n\\end{equation}\\]\nDie Stichprobenstandardabweichung ist definiert als \\[\\begin{equation}\nS := \\sqrt{S^2}.\n\\end{equation}\\]\n\n\nZur Abgrenzung erinnern wir noch einmal daran, dass Erwartungswert \\(\\mathbb{E}(\\xi)\\), Varianz \\(\\mathbb{V}(\\xi)\\) und Standardabweichung \\(\\mathbb{S}(\\xi)\\) Kennzahlen einer Zufallsvariable \\(\\xi\\) sind, wohingegen \\(\\bar{\\xi}, S^2\\), und \\(S\\) Kennzahlen einer Stichprobe \\(\\xi_1,...,\\xi_n\\) sind.\nBeispiel\nWir wollen die Bestimmung der in Definition 15.6 eingeführten Stichprobenkennzahlen an einem Beispiel erläutern. Dazu halten wir nochmals fest, dass \\(\\bar{\\xi}, S^2\\), \\(S\\) Zufallsvariablen sind und wollen ihre Realisationen im Folgenden mit \\(\\bar{x}, s^2\\) und \\(s\\) bezeichnen. Nehmen wir also an, wir haben für \\(n := 10\\) die in folgender Tabelle gezeigten Realisationen von u.i.v. nach \\(N(1,2)\\) verteilten Zufallsvariable \\(\\xi_1,...,\\xi_{10}\\), wobei für \\(i = 1,...,10\\) die Realisation von \\(\\xi_i\\) mit \\(x_i\\) bezeichnen ist:\nNach Definition 15.6 ist die Stichprobenmittelrealisation dann gegeben durch \\[\\begin{equation}\n\\bar{x}\n= \\frac{1}{10}\\sum_{i = 1}^{10}x_i\n= \\frac{6.88}{10}\n= 0.68,\n\\end{equation}\\] die Stichprobenvarianzrealisation gegeben durch \\[\\begin{equation}\ns^2\n= \\frac{1}{9}\\sum_{i=1}^{10} (x_i - \\bar{x})^2\n= \\frac{1}{9}\\sum_{i=1}^{10} (x_i - 0.68)^2\n= \\frac{25.37}{9}\n= 2.82.\n\\end{equation}\\] und die Stichprobenstandardabweichungrealisation gegeben durch \\[\\begin{equation}\ns = \\sqrt{s^2} = \\sqrt{2.82} = 1.68.\n\\end{equation}\\]",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Erwartungswerte</span>"
    ]
  },
  {
    "objectID": "205-Erwartungswerte.html#sec-kovarianz-und-korrelation",
    "href": "205-Erwartungswerte.html#sec-kovarianz-und-korrelation",
    "title": "15  Erwartungswerte",
    "section": "15.4 Kovarianz und Korrelation",
    "text": "15.4 Kovarianz und Korrelation\nHäufig genutzte Maße für den Zusammenhang zweier Zufallsvariablen sind die Kovarianz und die Korrelation. Diese sind wie folgt definiert.\n\nDefinition 15.7 (Kovarianz und Korrelation) Die Kovarianz zweier Zufallsvariablen \\(\\xi\\) und \\(\\upsilon\\) ist definiert als \\[\\begin{equation}\n\\mathbb{C}(\\xi,\\upsilon) :=\n\\mathbb{E}\\left(\\left(\\xi-\\mathbb{E}(\\xi)\\right)\\left(\\upsilon-\\mathbb{E}(\\upsilon)\\right)\\right).\n\\end{equation}\\] Die Korrelation zweier Zufallsvariablen \\(\\xi\\) und \\(\\upsilon\\) ist definiert als \\[\\begin{equation}\n\\rho(\\xi,\\upsilon)\n:= \\frac{\\mathbb{C}(\\xi,\\upsilon)}{\\sqrt{\\mathbb{V}(\\xi)}\\sqrt{\\mathbb{V}(\\upsilon)}}\n= \\frac{\\mathbb{C}(\\xi,\\upsilon)}{\\mathbb{S}(\\xi){\\mathbb{S}(\\upsilon)}}.\n\\end{equation}\\]\n\nDie Kovarianz einer Zufallsvariable \\(\\xi\\) mit sich entspricht ihrer Varianz, da \\[\\begin{equation}\n\\mathbb{C}(\\xi,\\xi) =\n\\mathbb{E}\\left(\\left(\\xi - \\mathbb{E}(\\xi) \\right)^2\\right) =\n\\mathbb{V}(\\xi).\n\\end{equation}\\] Im Gegensatz zur Varianz kann die Kovarianz aber auch negative Werte annehmen.\nBeispiel\nWir wollen beispielgebend für zwei Zufallsvariablen mit gemeinsamer diskreter Verteilung ihre Kovarianz berechnen. Dazu sei \\(\\zeta := (\\xi, \\upsilon)\\) ein Zufallsvektor mit WMF \\(p_{\\xi,\\upsilon}\\) definiert durch\nund damit \\(\\xi\\), \\(\\upsilon\\) zwei Zufallsvariablen mit einer bekannten bivariaten Verteilung. Um \\(\\mathbb{C}(\\xi,\\upsilon)\\) zu berechnen, halten wir zunächst fest, dass \\[\\begin{equation}\n\\mathbb{E}(\\xi) = \\sum_{x=1}^2 x p_{\\xi}(x) = 1\\cdot 0.3 + 2\\cdot 0.7 = 1.7\n\\end{equation}\\] und \\[\\begin{equation}\n\\mathbb{E}(\\upsilon) = \\sum_{y=1}^3 y p_{\\upsilon}(y) = 1\\cdot 0.7 + 2\\cdot 0.1 + 3\\cdot 0.2 = 1.5.\n\\end{equation}\\] Mit der Definition der Kovarianz von \\(\\xi\\) und \\(\\upsilon\\) gilt dann\n\\[\\begin{align}\n\\begin{split}\n\\mathbb{C}(\\xi,\\upsilon)                                                                                     \n& = \\mathbb{E}((\\xi - \\mathbb{E}(\\xi))(\\upsilon - \\mathbb{E}(\\upsilon)))                                \\\\\n& = \\sum_{x = 1}^2 \\sum_{y = 1}^3 (x - \\mathbb{E}(\\xi))(y - \\mathbb{E}(\\upsilon))p_{\\xi,\\upsilon}(x,y)  \\\\\n& = \\sum_{x = 1}^2 \\sum_{y = 1}^3 (x - 1.7)(y - 1.5)p_{\\xi,\\upsilon}(x,y)                           \\\\\n& = \\sum_{x = 1}^2 (x - 1.7)(1 - 1.5)p_{\\xi,\\upsilon}(x,1) +\n                   (x - 1.7)(2 - 1.5)p_{\\xi,\\upsilon}(x,2) +    \n                   (x - 1.7)(3 - 1.5)p_{\\xi,\\upsilon}(x,3)                                          \\\\\n& = \\quad   \n        (1 - 1.7)(1 - 1.5)p_{\\xi,\\upsilon}(1,1)\n    +   (1 - 1.7)(2 - 1.5)p_{\\xi,\\upsilon}(1,2)\n    +   (1 - 1.7)(3 - 1.5)p_{\\xi,\\upsilon}(1,3)                                                     \\\\\n& \\quad\n    +   (2 - 1.7)(1 - 1.5)p_{\\xi,\\upsilon}(2,1)\n    +   (2 - 1.7)(2 - 1.5)p_{\\xi,\\upsilon}(2,2)\n    +   (2 - 1.7)(3 - 1.5)p_{\\xi,\\upsilon}(2,3)                                                     \\\\\n& = \\quad\n    (-0.7)\\cdot(-0.5)   \\cdot 0.10       \n    + (-0.7)\\cdot 0.5   \\cdot 0.05       \n    + (-0.7)\\cdot 1.5   \\cdot 0.15                                                              \\\\\n& \\quad                                                               \n    + 0.3   \\cdot (-0.5) \\cdot 0.60     \n    + 0.3   \\cdot   0.5  \\cdot 0.05      \n    + 0.3   \\cdot   1.5  \\cdot 0.05                                                             \\\\\n& = 0.035\n    - 0.0175\n    - 0.1575\n    - 0.09\n    + 0.0075\n    + 0.0225                                                                                    \\\\\n& = - 0.2.\n\\end{split}\n\\end{align}\\] \nDie Kovarianz der Zufallsvariablen \\(\\xi\\) und \\(\\upsilon\\) mit der in obiger Tabelle festgelegter Verteilung ist also \\(\\mathbb{C}(\\xi,\\upsilon) = -0.2\\).\nDie Korrelation \\(\\rho(\\xi,\\upsilon)\\) zweier Zufallsvariablen entspricht ihrer anhand der Standardabweichungen der jeweiligen Zufallsvariablen standardisierten Kovarianz und wird manchmal auch als Korrelationskoeffizient von \\(\\xi\\) und \\(\\upsilon\\) bezeichnet. Ist die Korrelation \\(\\rho(\\xi,\\upsilon) = 0\\), so werden \\(\\xi\\) und \\(\\upsilon\\) unkorreliert genannt. Insbesondere ist die Korrelation im Gegensatz zur Kovarianz normalisiert, d.h. es gilt, wie wir an späterer Stellte mithilfe der Cauchy-Schwarz Ungleichung (Theorem 16.3) zeigen gilt \\[\\begin{equation}\n-1 \\le \\rho(\\xi,\\upsilon) \\le 1.\n\\end{equation}\\] Man sagt in diesem Kontext auch, dass die Korrelation im Gegensatz zur Kovarianz maßstabsunabhängig sei: wendet man auf eine Zufallsvariable eine linear-affine Transformation an, so ändert sich die Kovarianz der Zufallsvariablen, nicht aber ihre Korrelation. Das ist die Kernaussage folgenden Theorems.\n\nTheorem 15.9 (Kovarianz und Korrelation bei linear affinen Transformationen von Zufallsvariablen) \\(\\xi\\) und \\(\\upsilon\\) seien Zufallsvariablen und es seien \\(a,b,c,d \\in \\mathbb{R}\\). Dann gelten \\[\\begin{equation}\n\\mathbb{C}(a\\xi + b, c\\upsilon + d) = ac\\mathbb{C}(\\xi,\\upsilon)\n\\end{equation}\\] und \\[\\begin{equation}\n\\rho(a\\xi + b, c\\upsilon + d) = \\rho(\\xi,\\upsilon).\n\\end{equation}\\]\n\n\nBeweis. Es gilt zunächst \\[\\begin{align}\n\\begin{split}\n\\mathbb{C}(a\\xi+b,c\\upsilon+d)\n& = \\mathbb{E}((a\\xi+b-\\mathbb{E}(a\\xi+b))(c\\upsilon+d-\\mathbb{E}(c\\upsilon+d)))    \\\\\n& = \\mathbb{E}((a\\xi+b-a\\mathbb{E}(\\xi)-b)(c\\upsilon+d-c\\mathbb{E}(c\\upsilon)-d))   \\\\\n& = \\mathbb{E}(a(\\xi-\\mathbb{E}(\\xi))(c(\\upsilon -c\\mathbb{E}(\\upsilon)))             \\\\\n& = \\mathbb{E}(ac((\\xi-\\mathbb{E}(\\xi))(\\upsilon -c\\mathbb{E}(\\upsilon))))            \\\\\n&  = ac\\mathbb{C}(\\xi,\\upsilon)\n\\end{split}\n\\end{align}\\] Also folgt \\[\\begin{align}\n\\begin{split}\n\\rho(a\\xi + b, c\\upsilon + d)\n& = \\frac{\\mathbb{C}(a\\xi+b,c\\upsilon+d)}{\\sqrt{\\mathbb{V}(a\\xi+b)}\\sqrt{\\mathbb{V}(c\\upsilon+d)}} \\\\\n& = \\frac{ac\\mathbb{C}(\\xi,\\upsilon)}{\\sqrt{a^2\\mathbb{V}(\\xi)}\\sqrt{c^2\\mathbb{V}(\\upsilon)}}     \\\\\n& = \\frac{ac\\mathbb{C}(\\xi,\\upsilon)}{a\\mathbb{S}(\\xi)c\\mathbb{S}(\\upsilon)}         \\\\\n& = \\frac{\\mathbb{C}(\\xi,\\upsilon)}{\\mathbb{S}(\\xi)\\mathbb{S}(\\upsilon)}             \\\\\n& = \\rho(\\xi,\\upsilon).\n\\end{split}\n\\end{align}\\]\n\nWie das Berechnen von Varianzen wird auch das Berechnen von Kovarianzen manchmal durch folgendes Theorem, den sogenannten Kovarianzverschiebungssatz erleichtert.\n\nTheorem 15.10 (Kovarianzverschiebungssatz) \\(\\xi\\) und \\(\\upsilon\\) seien Zufallsvariablen. Dann gilt \\[\\begin{equation}\n\\mathbb{C}(\\xi,\\upsilon) = \\mathbb{E}(\\xi\\upsilon) - \\mathbb{E}(\\xi)\\mathbb{E}(\\upsilon).\n\\end{equation}\\]\n\n\nBeweis. Mit der Definition der Kovarianz gilt \\[\\begin{align}\n\\begin{split}\n\\mathbb{C}(\\xi,\\upsilon)\n& = \\mathbb{E}\\left((\\xi - \\mathbb{E}(\\xi))(\\upsilon - \\mathbb{E}(\\upsilon))\\right)                                             \\\\\n& = \\mathbb{E}\\left(\\xi\\upsilon - \\xi\\mathbb{E}(\\upsilon) - \\mathbb{E}(\\xi)\\upsilon  + \\mathbb{E}(\\xi) \\mathbb{E}(\\upsilon)\\right)          \\\\\n& = \\mathbb{E}(\\xi\\upsilon) - \\mathbb{E}(\\xi)\\mathbb{E}(\\upsilon) - \\mathbb{E}(\\xi)\\mathbb{E}(\\upsilon) + \\mathbb{E}(\\xi) \\mathbb{E}(\\upsilon)  \\\\\n& = \\mathbb{E}(\\xi\\upsilon) - \\mathbb{E}(\\xi)\\mathbb{E}(\\upsilon).\n\\end{split}\n\\end{align}\\]\n\nNatürlich ist Theorem 15.10 nur dann wirklich nützlich, wenn \\(\\mathbb{E}(\\xi\\upsilon)\\) leicht zu berechnen sind. Der Kovarianzverschiebungssatz in Theorem 15.4 ergibt sich aus Theorem 15.10 im Spezialfall, dass \\(\\upsilon := \\xi\\), da dann gilt \\[\\begin{equation}\n\\mathbb{V}(\\xi)\n= \\mathbb{C}(\\xi,\\xi)\n= \\mathbb{E}(\\xi\\xi) - \\mathbb{E}(\\xi)\\mathbb{E}(\\xi)\n= \\mathbb{E}(\\xi^2) - \\mathbb{E}(\\xi)\\mathbb{E}(\\xi)\n\\end{equation}\\]\nMithilfe des Begriffes des Kovarianz ist es möglich eine stärkere Aussage über die Varianzen von Summen und Differenzen von Zufallsvariablen zu treffen als es in Theorem 15.5 der Fall war, wo lediglich unabhängige Zufallsvariablen betrachtetet wurden. Folgende Aussagen gelten generell.\n\nTheorem 15.11 (Varianzen von Summen und Differenzen von Zufallsvariablen) \\(\\xi\\) und \\(\\upsilon\\) seien zwei Zufallsvariablen und es seien \\(a,b,c \\in \\mathbb{R}\\). Dann gilt \\[\\begin{equation}\n\\mathbb{V}(a\\xi + b\\upsilon + c) = a^2\\mathbb{V}(\\xi) + b^2\\mathbb{V}(\\upsilon) + 2ab\\mathbb{C}(\\xi,\\upsilon).\n\\end{equation}\\] Speziell gelten \\[\\begin{equation}\n\\mathbb{V}(\\xi+\\upsilon) = \\mathbb{V}(\\xi) + \\mathbb{V}(\\upsilon) + 2 \\mathbb{C}(\\xi,\\upsilon)\n\\end{equation}\\] und \\[\\begin{equation}\n\\mathbb{V}(\\xi-\\upsilon) = \\mathbb{V}(\\xi) + \\mathbb{V}(\\upsilon) - 2 \\mathbb{C}(\\xi,\\upsilon)\n\\end{equation}\\]\n\n\nBeweis. Wir halten zunächst fest, dass \\[\\begin{equation}\n\\mathbb{E}(a\\xi + b\\upsilon + c) = a\\mathbb{E}(\\xi) + b\\mathbb{E}(\\upsilon) + c.\n\\end{equation}\\]\nEs ergibt sich also \\[\\begin{align}\n\\begin{split}\n& \\mathbb{V}(a\\xi + b\\upsilon + c)                                                              \\\\\n& = \\mathbb{E}\\left((a\\xi + b\\upsilon + c - a\\mathbb{E}(\\xi) - b\\mathbb{E}(\\upsilon) - c)^2\\right)  \\\\\n& = \\mathbb{E}\\left((a(\\xi  - \\mathbb{E}(\\xi)) + b(\\upsilon  - \\mathbb{E}(\\upsilon)))^2\\right)      \\\\\n& = \\mathbb{E}\\left(a^2(\\xi - \\mathbb{E}(\\xi))^2\n                  + 2ab(\\xi - \\mathbb{E}(\\xi))(\\upsilon - \\mathbb{E}(\\upsilon)))\n                  + b^2(\\upsilon - \\mathbb{E}(\\upsilon))^2\n                  \\right)               \\\\\n& = a^2\\mathbb{E}\\left((\\xi - \\mathbb{E}(\\xi))^2\\right)\n  + b^2\\mathbb{E}\\left((\\upsilon - \\mathbb{E}(\\upsilon))^2\\right)\n  + 2ab\\mathbb{E}\\left((\\xi - \\mathbb{E}(\\xi))(\\upsilon - \\mathbb{E}(\\upsilon)))\\right)                 \\\\\n& = a^2\\mathbb{V}(\\xi)+ b^2\\mathbb{V}(\\upsilon) + 2ab\\mathbb{C}(\\xi,\\upsilon)\n\\end{split}\n\\end{align}\\] Die Spezialfälle folgen dann direkt mit \\(a := b := 1\\) und \\(a := 1, b := -1\\), respektive.\n\nIm Gegensatz zu Erwartungswerten addieren sich die Varianzen von Zufallsvariablen also nicht einfach, sondern die Varianz der Summe zweier Zufallsvariablen hängt von ihrer Kovarianz ab. Ist diese zum Beispiel im Fall der Summe zweier Zufallsvariablen positiv, so verstärkt sie die Varianz der Zufallsvariable, die sich aus der Addition der Zufallsvariablen ergibt. Intuitiv führt hierbei die Realisierung eines Extremwertes einer der Zufallvariablen häufigt auch zu der Realisierung eines Extremwertes der anderen Zufallsvariablen, so dass die Variabilität der Summe der Zufallsvariablen überproportional verstärkt wird.\nSchließlich wollen wir mit Theorem 15.12 einen ersten Eindruck zum Zusammenhang von Kovarianz und Korrelation mit dem Begriff der Unabhängigkeit von Zufallsvariablen erlangen. Es zeigt sich, dass Kovarianz und Korrelation lediglich für bestimmte Formen der Abhängigkeit von Zufallsvariablen sensitiv sind und insbesondere, dass von einer Kovarianz von Null nicht auf die Unabhängigkeit der Zufallsvariablen geschlossen werden kann. Anderseits impliziert die Unabhängigkeit zweier Zufallsvariablen immer, dass ihre Kovarianz Null und sie damit unkorreliert sind. Abhängigkeit und Unabhängigkeit von Zufallsvariablen sind also sehr viel allgemeinere Begrifflichkeiten zur Beschreibung des Zusammenhangs von Zufallsvariablen als Kovarianz und Korrelation.\n\nTheorem 15.12 (Korrelation und Unabhängigkeit) \\(\\xi\\) und \\(\\upsilon\\) seien zwei Zufallsvariablen. Wenn \\(\\xi\\) und \\(\\upsilon\\) unabhängig sind, dann ist \\(\\mathbb{C}(\\xi,\\upsilon) = 0\\) und \\(\\xi\\) und \\(\\upsilon\\) sind unkorreliert. Ist dagegen \\(\\mathbb{C}(\\xi,\\upsilon) = 0\\) und sind \\(\\xi\\) und \\(\\upsilon\\) somit unkorreliert, dann sind \\(\\xi\\) und \\(\\upsilon\\) nicht notwendigerweise unabhängig.\n\n\nBeweis. Wir zeigen zunächst, dass aus der Unabhängigkeit von \\(\\xi\\) und \\(\\upsilon\\) \\(\\mathbb{C}(\\xi,\\upsilon) = 0\\) folgt. Hierzu halten wir zunächst fest, dass für unabhängige Zufallsvariablen gilt, dass \\[\\begin{equation}\n\\mathbb{E}(\\xi\\upsilon) = \\mathbb{E}(\\xi)\\mathbb{E}(\\upsilon).\n\\end{equation}\\] Mit dem Kovarianzverschiebungssatz folgt dann \\[\\begin{equation}\n\\mathbb{C}(\\xi,\\upsilon)\n= \\mathbb{E}(\\xi\\upsilon) - \\mathbb{E}(\\xi)\\mathbb{E}(\\upsilon)\n= \\mathbb{E}(\\xi)\\mathbb{E}(\\upsilon) - \\mathbb{E}(\\xi)\\mathbb{E}(\\upsilon)\n= 0.\n\\end{equation}\\] Mit der Definition des Korrelationskoeffizienten folgt dann unmittelbar, dass \\(\\rho(\\xi,\\upsilon) = 0\\) und \\(\\xi\\) und \\(\\upsilon\\) somit unkorreliert sind.\nWir zeigen nun durch Angabe eines Beispiels, dass die Kovarianz von abhängigen Zufallsvariablen \\(\\xi\\) und \\(\\upsilon\\) Null sein kann. Zu diesem Zweck betrachten wir den Fall zweier diskreter Zufallsvariablen \\(\\xi\\) und \\(\\upsilon\\) mit Ergebnisräumen \\(\\mathcal{X} = \\{-1,0,1\\}\\) und \\(\\mathcal{\\upsilon} = \\{0,1\\}\\), marginaler WMF von \\(\\xi\\) gegeben durch \\(p_\\xi(x) := 1/3\\) für \\(x \\in \\mathcal{X}\\) und der Definition \\(\\upsilon := \\xi^2\\). Wir halten dann zunächst fest, dass \\[\\begin{equation}\n\\mathbb{E}(\\xi)\n= \\sum_{x \\in \\mathcal{X}} x p_\\xi(x)\n= -1 \\cdot \\frac{1}{3} + 0\\cdot \\frac{1}{3} + 1\\cdot\\frac{1}{3}\n= 0\n\\end{equation}\\] und \\[\\begin{equation}\n\\mathbb{E}(\\xi\\upsilon)\n= \\mathbb{E}(\\xi\\xi^2)\n= \\mathbb{E}(\\xi^3)\n= \\sum_{x \\in \\mathcal{X}} x^3 p_\\xi(x)\n= -1^3 \\cdot \\frac{1}{3} + 0^3\\cdot \\frac{1}{3} + 1^3\\cdot\\frac{1}{3}\n= 0.\n\\end{equation}\\] Mit dem Kovarianzverschiebungssatz ergibt sich dann \\[\\begin{equation}\n\\mathbb{C}(\\xi,\\upsilon)\n= \\mathbb{E}(\\xi\\upsilon) - \\mathbb{E}(\\xi)\\mathbb{E}(\\upsilon)\n= \\mathbb{E}(\\xi^3) - \\mathbb{E}(\\xi)\\mathbb{E}(\\upsilon)\n= 0 - 0\\cdot \\mathbb{E}(\\upsilon)\n= 0.\n\\end{equation}\\] Die Kovarianz von \\(\\xi\\) und \\(\\upsilon\\) ist also Null. Wie unten gezeigt faktorisiert die gemeinsame WMF von \\(\\xi\\) und \\(\\upsilon\\) jedoch nicht, und somit sind \\(\\xi\\) und \\(\\upsilon\\) nicht unabhängig. Wir halten zunächst fest, dass die Definition von \\(\\upsilon := \\xi^2\\) die folgende bedingte WMF von \\(\\upsilon\\) gegeben \\(\\xi\\) impliziert:\nDie marginale WMF \\(p_\\xi\\) und die bedingte WMF \\(p_{\\upsilon|\\xi}\\) implizieren wiederum die gemeinsame WMF\nvon \\(\\xi\\) und \\(\\upsilon\\). Es gilt also zum Beispiel \\[\\begin{equation}\np_{\\xi,\\upsilon}(-1,0) = 0 \\neq \\frac{1}{9} = \\frac{1}{3} \\cdot \\frac{1}{3} = p_{\\xi}(-1)p_{\\upsilon}( 0)\n\\end{equation}\\] und damit sind \\(\\xi\\) und \\(\\upsilon\\) nicht unabhängig.",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Erwartungswerte</span>"
    ]
  },
  {
    "objectID": "205-Erwartungswerte.html#kovarianzmatrizen",
    "href": "205-Erwartungswerte.html#kovarianzmatrizen",
    "title": "15  Erwartungswerte",
    "section": "15.5 Kovarianzmatrizen",
    "text": "15.5 Kovarianzmatrizen\nDas multivariate Analogon der Varianz einer Zufallsvariable ist die Kovarianzmatrix eines Zufallsvektors. Diese enkodiert neben den Varianzen der Komponenten des Zufallsvektors auch ihre paarweisen Kovarianzen und ist wie folgt definiert.\n\nDefinition 15.8 (Kovarianzmatrix eines Zufallsvektors) \\(\\xi\\) sei ein \\(n\\)-dimensionaler Zufallvektor. Dann ist die Kovarianzmatrix von \\(\\xi\\) definiert als die \\(n \\times n\\) Matrix \\[\\begin{equation}\n\\mathbb{C}(\\xi) := \\mathbb{E}\\left((\\xi - \\mathbb{E}(\\xi))(\\xi - \\mathbb{E}(\\xi))^T \\right).\n\\end{equation}\\]\n\nDie Kovarianzmatrix ist in Definition 15.8 formal analog zur Kovarianz zweier Zufallsvariablen definiert. Eine direkte Rückführung des Begriffs der Kovarianzmatrix eines Zufallsvektors auf den Begriff aus dem univariaten Kontext bekannten Begriff der Kovarianz zweier Zufallsvariablen erlaubt folgendes Theorem.\n\nTheorem 15.13 (Kovarianzmatrix eines Zufallsvektors) \\(\\xi\\) sei ein \\(n\\)-dimensionaler Zufallvektor und \\(\\mathbb{C}(\\xi)\\) sei seine Kovarianzmatrix. Dann gilt \\[\\begin{equation}\n\\mathbb{C}(\\xi)\n= \\left(\\mathbb{C}(\\xi_i,\\xi_j)\\right)_{1 \\le i,j \\le n}\n=\n\\begin{pmatrix}\n\\mathbb{C}(\\xi_1,\\xi_1) & \\mathbb{C}(\\xi_1,\\xi_2) & \\cdots & \\mathbb{C}(\\xi_1,\\xi_n)    \\\\\n\\mathbb{C}(\\xi_2,\\xi_1) & \\mathbb{C}(\\xi_2,\\xi_2) & \\cdots & \\mathbb{C}(\\xi_2,\\xi_n)    \\\\\n\\vdots                  & \\vdots                  & \\ddots & \\vdots                     \\\\\n\\mathbb{C}(\\xi_n,\\xi_1) & \\mathbb{C}(\\xi_n,\\xi_2) & \\cdots & \\mathbb{C}(\\xi_n,\\xi_n)    \\\\\n\\end{pmatrix}.\n\\end{equation}\\]\n\n\nBeweis. Es gilt \\[\\begin{align}\n\\mathbb{C}(\\xi)\n& := \\mathbb{E}\\left((\\xi - \\mathbb{E}(\\xi))(\\xi - \\mathbb{E}(\\xi))^T \\right) \\\\\n& =\n\\mathbb{E}\n\\left(\n\\left(\n\\begin{pmatrix}\n\\xi_1 \\\\\n\\vdots \\\\\n\\xi_n\n\\end{pmatrix}\n-\n\\begin{pmatrix}\n\\mathbb{E}(\\xi_1) \\\\\n\\vdots \\\\\n\\mathbb{E}(\\xi_n)\n\\end{pmatrix}\n\\right)\n\\left(\n\\begin{pmatrix}\n\\xi_1 \\\\\n\\vdots \\\\\n\\xi_n\n\\end{pmatrix}\n-\n\\begin{pmatrix}\n\\mathbb{E}(\\xi_1) \\\\\n\\vdots \\\\\n\\mathbb{E}(\\xi_n)\n\\end{pmatrix}\n\\right)^T\n\\right)\n\\\\\n& =\n\\mathbb{E}\n\\left(\n\\begin{pmatrix}\n\\xi_1 - \\mathbb{E}(\\xi_1) \\\\\n\\vdots \\\\\n\\xi_n - \\mathbb{E}(\\xi_n)\n\\end{pmatrix}\n\\begin{pmatrix}\n\\xi_1 - \\mathbb{E}(\\xi_1)\\\\\n\\vdots \\\\\n\\xi_n - \\mathbb{E}(\\xi_n)\n\\end{pmatrix}^T\n\\right)\n\\\\\n& =\n\\mathbb{E}\n\\left(\n\\begin{pmatrix}\n\\xi_1 - \\mathbb{E}(\\xi_1) \\\\\n\\vdots \\\\\n\\xi_n - \\mathbb{E}(\\xi_n)\n\\end{pmatrix}\n\\begin{pmatrix}\n\\xi_1 - \\mathbb{E}(\\xi_1)\n& \\dots\n& \\xi_n - \\mathbb{E}(\\xi_n)\n\\end{pmatrix}\n\\right) \\\\\n& =\n\\mathbb{E}\n\\begin{pmatrix}\n  (\\xi_1 - \\mathbb{E}(\\xi_1))(\\xi_1 - \\mathbb{E}(\\xi_1))\n& \\dots\n& (\\xi_1 - \\mathbb{E}(\\xi_1))(\\xi_n - \\mathbb{E}(\\xi_n)\n\\\\\n\\vdots\n& \\ddots\n& \\vdots\n\\\\\n  (\\xi_n - \\mathbb{E}(\\xi_n))(\\xi_1 - \\mathbb{E}(\\xi_1))\n& \\dots\n& (\\xi_n - \\mathbb{E}(\\xi_n))(\\xi_n - \\mathbb{E}(\\xi_n))\n\\\\\n\\end{pmatrix}\n\\\\\n& =\n\\left(\\mathbb{E}\\left((\\xi_i - \\mathbb{E}(\\xi_i))(\\xi_j - \\mathbb{E}(\\xi_j)) \\right) \\right)_{1 \\le i,j \\le n} \\\\\n& =\n\\left(\\mathbb{C}(\\xi_i,\\xi_j)\\right)_{1 \\le i,j \\le n}. \\\\\n\\end{align}\\]\n\nDie Kovarianzmatrix eines Zufallsvektors \\(\\xi\\) ist also die Matrix der Kovarianzen der Komponenten von \\(\\xi\\). Damit ist auch die Kovarianzmatrix direkt im Sinne des Begriffs der Kovarianz von Zufallsvektoren gegeben. Da die Kovarianz einer Zufallsvariable mit sich selbst bekanntlich ihre Varianz ist, enthält die Kovarianzmatrix auf ihrer Diagonalen die Varianzen der Komponenten von \\(\\xi\\).\nFolgendes Theorem dokumentiert eine Schreibweise für die Kovarianzmatrix eines partitionierten Zufallsvektors im Sinne von Erwartungswerten von Zufallvektorprodukten an, die zum Beispiel im Rahmen der Kanonischen Korrelationsanalyse hilfreich ist.\n\nTheorem 15.14 (Kovarianzmatrizen von Zufallsvektoren) Es seien \\[\\begin{equation}\n\\zeta = \\begin{pmatrix} \\xi \\\\ \\upsilon \\end{pmatrix}\n\\mbox{ mit }\n\\mathbb{E}(\\zeta)  := 0_m\n\\end{equation}\\] ein \\(m_\\xi + m_\\upsilon\\)-dimensionaler Zufallsvektor und sein Erwartungswertvektor, respektive. Dann kann die \\(m \\times m\\) Kovarianzmatrix von \\(\\zeta\\) geschrieben werden als \\[\\begin{equation}\n\\mathbb{C}(\\zeta) =\n\\begin{pmatrix}\n\\Sigma_{\\xi\\xi} & \\Sigma_{\\xi\\upsilon} \\\\\n\\Sigma_{\\upsilon\\xi} & \\Sigma_{\\upsilon\\upsilon} \\\\\n\\end{pmatrix}\n\\in \\mathbb{R}^{m \\times m}\n\\end{equation}\\] wobei \\[\\begin{align}\n\\begin{split}\n\\Sigma_{\\xi\\xi}   & := \\mathbb{E}\\left(\\xi\\xi^T  \\right) \\in \\mathbb{R}^{m_\\xi  \\times m_\\xi}\\\\\n\\Sigma_{\\xi\\upsilon}  & := \\mathbb{E}\\left(\\xi\\upsilon^T \\right) \\in \\mathbb{R}^{m_\\xi  \\times m_\\upsilon}\\\\\n\\Sigma_{\\upsilon\\xi}  & := \\mathbb{E}\\left(\\upsilon\\xi^T \\right) \\in \\mathbb{R}^{m_\\upsilon \\times m_\\xi}\\\\\n\\Sigma_{\\upsilon\\upsilon} & := \\mathbb{E}\\left(\\upsilon\\upsilon^T\\right) \\in \\mathbb{R}^{m_\\xi  \\times m_\\upsilon}\n\\end{split}\n\\end{align}\\]\n\n\nBeweis. Nach Definition der Kovarianzmatrix eines Zufallsvektors gilt \\[\\begin{align}\n\\begin{split}\n\\mathbb{C}(z)\n& = \\mathbb{E}\\left((\\zeta - \\mathbb{E}(\\zeta))(\\zeta - \\mathbb{E}(\\zeta))^T \\right) \\\\\n& = \\mathbb{E}\\left((\\zeta - 0_m)(\\zeta - 0_m)^T \\right) \\\\\n& = \\mathbb{E}\\left(\\zeta\\zeta^T\\right)\\\\\n& = \\mathbb{E}\\left(\\begin{pmatrix} \\xi \\\\ \\upsilon \\end{pmatrix} \\begin{pmatrix} \\xi^T & \\upsilon^T \\end{pmatrix} \\right) \\\\\n& = \\mathbb{E}\\left(\\begin{pmatrix} \\xi\\xi^T & \\xi\\upsilon^T \\\\ \\upsilon\\xi^T & \\upsilon\\upsilon^T \\end{pmatrix}\\right)\n\\\\\n& =\n\\begin{pmatrix}\n\\mathbb{E}\\left(\\xi\\xi^T\\right)   & \\mathbb{E}\\left(\\xi\\upsilon^T\\right) \\\\\n\\mathbb{E}\\left(\\upsilon\\xi^T\\right)  & \\mathbb{E}\\left(\\upsilon\\upsilon^T\\right)\n\\end{pmatrix}\n\\\\\n& =\n\\begin{pmatrix}\n\\Sigma_{\\xi\\xi}   & \\Sigma_{\\xi\\upsilon}  \\\\\n\\Sigma_{\\upsilon\\xi}  & \\Sigma_{\\upsilon\\upsilon} \\\\\n\\end{pmatrix}\n\\end{split}\n\\end{align}\\]\n\nSchließlich ist man in manchen Anwendungen an einer normalisierten, maßstabsunabhängigen Repräsentation der Kovarianzen eines Zufallsvektors interessiert. Wie im univariaten Fall bietet sich hierfür die Normalisierung der Kovarianz zweier Zufallsvariablen mithilfe ihrer jeweiligen Varianzen im Sinne einer Korrelation an. Diese Überlegung führt auf den Begriff der Korrelationsmatrix eines Zufallsvektors.\n\nDefinition 15.9 (Korrelationsmatrix) \\(\\xi\\) sei ein \\(n\\)-dimensionaler Zufallsvektor. Dann ist die Korrelationsmatrix von \\(\\xi\\) definiert als die \\(n \\times n\\) Matrix \\[\\begin{equation}\n\\mathbb{R}(\\xi)\n:= \\left(\\rho_{ij} \\right)_{1 \\le i,j\\le n}\n= \\left(\\frac{\\mathbb{C}(\\xi_i,\\xi_j)}{\\sqrt{\\mathbb{V}(\\xi_i)}\\sqrt{\\mathbb{V}(\\xi_j)}}\\right)_{1 \\le i,j\\le n}.\n\\end{equation}\\]\n\nDa es sich bei den Varianzen der Komponenten von \\(\\xi\\) um die Diagonalelement der Kovarianzmatrix von \\(\\xi\\) handelt, ist die Korrelationsmatrix natürlich in der Kovarianzmatrix implizit. Weiterhin gelten, wie immer für Korrelationen, für die Einträge \\(\\rho_{ij}, 1 \\le i,j \\le n\\) der Korrelationsmatrix, dass \\[\\begin{equation}\n\\rho_{ij} \\in [-1,1] \\mbox{ für } 1 \\le i,j \\in n \\mbox{ und } \\rho_{ii} = 1 \\mbox{ für } 1 \\le i \\le n.\n\\end{equation}\\]",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Erwartungswerte</span>"
    ]
  },
  {
    "objectID": "205-Erwartungswerte.html#stichprobenkennzahlen-von-zufallsvektoren",
    "href": "205-Erwartungswerte.html#stichprobenkennzahlen-von-zufallsvektoren",
    "title": "15  Erwartungswerte",
    "section": "15.6 Stichprobenkennzahlen von Zufallsvektoren",
    "text": "15.6 Stichprobenkennzahlen von Zufallsvektoren\nDie Begriffe des Stichprobenmittels, der Stichprobenvarianz und der Stichprobenkovarianz lassen sich auch auf den Fall multivariater Stichproben übertragen. Wir nutzen folgende Definition.\n\nDefinition 15.10 (Stichprobenmittel, -kovarianmatrix und -korrelationsmatrix) \\(\\upsilon_1,...,\\upsilon_n\\) sei eine Menge von \\(m\\)-dimensionalen Zufallsvektoren, genannt Stichprobe.\n\nDas Stichprobenmittel der \\(\\upsilon_1,...,\\upsilon_n\\) ist definiert als der \\(m\\)-dimensionale Vektor \\[\\begin{equation}\n\\bar{\\upsilon} := \\frac{1}{n} \\sum_{i=1}^n \\upsilon_i.\n\\end{equation}\\]\nDie Stichprobenkovarianzmatrix der \\(\\upsilon_1,...,\\upsilon_n\\) ist definiert als die \\(m \\times m\\) Matrix \\[\\begin{equation}\nC := \\frac{1}{n-1}\\sum_{i=1}^n (\\upsilon_i - \\bar{\\upsilon})(\\upsilon_i - \\bar{\\upsilon})^T .\n\\end{equation}\\]\nDie Stichprobenkorrelationsmatrix der \\(\\upsilon_1,...,\\upsilon_n\\) ist definiert als die \\(m \\times m\\) Matrix \\[\\begin{equation}\nD := \\left(\\frac{(C )_{ij}}{\\sqrt{ (C )_{ii}}\\sqrt{ (C )_{jj}}}\\right)_{1 \\le i,j \\le m}.\n\\end{equation}\\]\n\n\nZur konkreten Berechnung von Stichprobenmittel, Stichprobenkovarianzmatrix und Stichprobenkorrrelationsmatrix basierend auf einem multivariaten Datensatz bieten sich die Aussagen des folgenden Theorems an.\n\nTheorem 15.15 (Datenmatrix und Stichprobenstatistiken)  \nEs sei \\[\\begin{equation}\n\\upsilon :=\n\\begin{pmatrix}\n\\upsilon_1 & \\cdots & \\upsilon_n\n\\end{pmatrix}\n\\end{equation}\\] eine \\(m \\times n\\) , die durch die spaltenweise Konkatenation von \\(n\\) \\(m\\)-dimensionalen Zufallvektoren \\(\\upsilon_1, ...,\\upsilon_n\\) gegeben sei. Dann ergeben sich\n\n\nBeweis. Die Darstellung des Stichprobenmittels ergibt sich aus \\[\\begin{align}\n\\begin{split}\n\\bar{\\upsilon}\n& := \\frac{1}{n} \\sum_{i=1}^n\\upsilon_i \\\\\n&  = \\frac{1}{n}\\begin{pmatrix} \\sum_{i=1}^n\\upsilon_{i1} \\\\ \\vdots \\\\ \\sum_{i=1}^n\\upsilon_{im} \\end{pmatrix} \\\\\n&  = \\frac{1}{n}\\left(\\begin{pmatrix}\\upsilon_{11}    & \\cdots  &\\upsilon_{n1} \\\\\n                                      \\vdots    & \\ddots  & \\vdots     \\\\\n                                     \\upsilon_{1m}    & \\cdots  &\\upsilon_{nm} \\\\\n                   \\end{pmatrix}\n                   \\begin{pmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{pmatrix}\n              \\right) \\\\\n& = \\frac{1}{n}\\upsilon 1_{n}.\n\\end{split}\n\\end{align}\\] Hinsichtlich der Darstellung der Stichprobenkovarianzmatrix halten wir zunächst fest, dass nach Definition gilt, dass \\[\\begin{align}\n\\begin{split}\nC  \n& := \\frac{1}{n-1}\\sum_{i=1}^n (\\upsilon_i - \\bar{\\upsilon})(\\upsilon_i - \\bar{\\upsilon})^T \\\\\n&  = \\frac{1}{n-1}\\sum_{i=1}^n \\left(\\upsilon_i\\upsilon_i^T-\\upsilon_i\\bar{\\upsilon}^T - \\bar{\\upsilon}\\upsilon_i^T+ \\bar{\\upsilon}\\bar{\\upsilon}^T\\right) \\\\\n&  = \\frac{1}{n-1}\\left(\\sum_{i=1}^n\\upsilon_i\\upsilon_i^T- \\sum_{i=1}^n\\upsilon_i\\bar{\\upsilon}^T - \\sum_{i=1}^n \\bar{\\upsilon}\\upsilon_i^T+ \\sum_{i=1}^n \\bar{\\upsilon}\\bar{\\upsilon}^T\\right) \\\\\n&  = \\frac{1}{n-1}\\left(\\sum_{i=1}^n\\upsilon_i\\upsilon_i^T- n\\bar{\\upsilon}\\bar{\\upsilon}^T - n\\bar{\\upsilon}\\bar{\\upsilon}^T + n\\bar{\\upsilon}\\bar{\\upsilon}^T\\right) \\\\\n&  = \\frac{1}{n-1}\\left(\\sum_{i=1}^n\\upsilon_i\\upsilon_i^T- n\\bar{\\upsilon}\\bar{\\upsilon}^T\\right).\n\\end{split}\n\\end{align}\\] Mit \\(1_{n}1_{n}^T = 1_{nn}\\) ergibt sich dann weiterhin \\[\\begin{align}\n\\begin{split}\n\\upsilon\\left(I_n - \\frac{1}{n}1_{nn}\\right)\\upsilon^T\n& = \\left(\\upsilon I_n - \\frac{1}{n}\\upsilon 1_{nn}\\right)\\upsilon^T                                         \\\\\n& = \\upsilon\\upsilon^T - \\frac{1}{n}\\upsilon 1_{nn}\\upsilon^T                                                      \\\\\n& = \\begin{pmatrix} \\upsilon_1 & \\cdots & \\upsilon_n\\end{pmatrix} \\begin{pmatrix} \\upsilon_1^T \\\\ \\vdots \\\\ \\upsilon_n^T\\end{pmatrix} - \\frac{1}{n}\\upsilon 1_n 1_n^T\\upsilon^T    \\\\\n& = \\sum_{i=1}^n\\upsilon_i\\upsilon_i^T- n\\left(\\frac{1}{n}\\upsilon 1_n\\right)\\left(\\frac{1}{n}1_n^T\\upsilon^T\\right)              \\\\\n& = \\sum_{i=1}^n\\upsilon_i\\upsilon_i^T- n\\bar{\\upsilon}\\bar{\\upsilon}^T                                                          \\\\\n& = \\frac{1}{n-1}\\sum_{i=1}^n (\\upsilon_i - \\bar{\\upsilon})(\\upsilon_i - \\bar{\\upsilon})^T \\\\\n& = C.\n\\end{split}\n\\end{align}\\] Hinsichtlich der Korrelationsmatrix ergibt sich nach Definition und für ein beliebiges Indexpaar \\(i,j\\) mit \\(1 \\le i,j \\le m\\) schließlich, dass \\[\\begin{align}\n\\begin{split}\nR_{{y}_{ij}}\n& = \\frac{(C)_{ij}}{\\sqrt{ (C)_{ii}}\\sqrt{ (C)_{jj}}}             \\\\\n& = \\frac{1}{\\sqrt{(C)_{ii}}}(C)_{ij}\\frac{1}{\\sqrt{(C)_{jj}}}    \\\\\n& = (DCD)_{ij}.\n\\end{split}\n\\end{align}\\]",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Erwartungswerte</span>"
    ]
  },
  {
    "objectID": "205-Erwartungswerte.html#selbstkontrollfragen",
    "href": "205-Erwartungswerte.html#selbstkontrollfragen",
    "title": "15  Erwartungswerte",
    "section": "15.7 Selbstkontrollfragen",
    "text": "15.7 Selbstkontrollfragen\n\nGeben Sie die Definition des Erwartungswerts einer Zufallsvariable wieder.\nGeben Sie die Interpretation der Erwartungswerts einer Zufallsvariable wieder\nBerechnen Sie den Erwartungswert einer Bernoulli Zufallsvariable.\nGeben Sie das Theorem zu den Eigenschaften des Erwartungswerts wieder.\nGeben Sie die Definition der Varianz und der Standardabweichung einer Zufallsvariable wieder.\nGeben Sie die Interpretation der Varianz einer Zufallsvariable wieder.\nBerechnen Sie die Varianz einer Bernoulli Zufallsvariable.\nGeben Sie das Theorem zum Varianzverschiebungssatz wieder.\nGeben Sie das Theorem zu den Eigenschaften der Varianz wieder.\nGeben Sie die Definition des Begriffs einer Stichprobe wieder.\nGeben Sie die Definitionen von Stichprobenmittel, -varianz und -standardabweichung wieder.\nGeben Sie die Definition von Kovarianz und Korrelation zweier Zufallsvariablen wieder.\nGeben Sie das Theorem zum Kovarianzverschiebungssatz wieder.\nGeben Sie das Theorem zu Varianzen von Summen und Differenzen von Zufallsvariablen wieder.\nGeben Sie das Theorem zur Korrelation und Unabhängigkeit zweier Zufallsvariablen wieder. \n\n\n\n\n\nMeintrup, D., & Schäffler, S. (2005). Stochastik: Theorie und Anwendungen. Springer.\n\n\nSchmidt, K. D. (2009). Maß und Wahrscheinlichkeit. Springer.",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Erwartungswerte</span>"
    ]
  },
  {
    "objectID": "206-Ungleichungen.html",
    "href": "206-Ungleichungen.html",
    "title": "16  Ungleichungen",
    "section": "",
    "text": "16.1 Wahrscheinlichkeitsungleichungen\nDie Markov Ungleichung stellt einen Bezug zwischen den Überschreitungswahrscheinlichkeiten (vgl. Theorem 13.2) und dem Erwartungswert einer nicht-negativen Zufallsvariablen, also einer Zufallsvariable, für die \\(\\mathbb{P}(\\xi \\ge 0 ) = 1\\) ist, her. Im Beweis dieser Ungleichung wollen wir nur den Fall einer kontinuierlichen Zufallsvariable betrachten.\nGilt beispielweise für eine nichtnegative Zufallsvariable \\(\\xi\\), dass \\(\\mathbb{E}(\\xi) = 1\\) ist, dann folgt aus der Markov Ungleichung, dass \\[\\begin{equation}\n\\mathbb{P}(\\xi \\ge 100) \\le 0.01.\n\\end{equation}\\]\nBeispiel\nAls Beispiel für die Markov Ungleichung betrachten wir den Fall einer Gamma-Zufallsvariable \\(\\xi \\sim G(\\alpha,\\beta)\\). Gamma-Zufallsvariablen sind bekanntlich per Definition nicht-negativ (vgl. Definition 13.10) und wir haben gesehen, dass für den Erwartungswert einer Gamma-Zufallsvariable \\(\\mathbb{E}(\\xi) = \\alpha\\beta\\) gilt (vgl. Theorem 15.8). Wir betrachten konkret den Fall \\(\\alpha := 5\\) und \\(\\beta := 2\\), so dass \\(\\xi\\) auch einer \\(\\chi^2\\)-Zufallsvariable mit Freiheitsgradparameter \\(n = 10\\) entspricht. In Abbildung 16.1 A stellen wir die KVF \\(P\\) dieser Zufallsvariable dar, in Abbildung 16.1 B visualisieren wir die in der Markov Ungleichung betrachteten Größen \\(\\mathbb{E}(\\xi)/x\\) und die Überschreitungswahrscheinlichkeit \\(\\mathbb{P}(\\xi \\ge x) = 1 - P(x)\\). Offensichtlich trifft die Markov Ungleichung zu.\nDie Chebychev Ungleichung setzt die Wahrscheinlichkeit dafür, dass eine Zufallsvariable Werte weit von ihrem Erwartungswert entfernt annimmt, in Bezug zur ihrer Varianz. Die Chebychev Ungleichung liefert damit eine Begründung dafür, warum die in Definition 15.5 formulierte Größe als ein Maß für die Streuung einer Zufallsvariable verstanden werden kann. Im Beweis der Chebyshev Ungleichung wird an entscheidender Stelle auf die Markov Ungleichung zurück gegriffen.\nBeispielweise gilt für eine Zufallsvariable immer, dass die Wahrscheinlichkeit für eine absolute Abweichung vom doppelten ihrer Standardabweichung höchstens \\(1/4\\) ist, also Frequentistisch betrachtet nur für etwa ein Viertel ihrer Realisierungen zutrifft, und die Wahrscheinlichkeit für eine absolute Abweichung vom dreifachen ihrer Standardabweichung höchstens \\(1/9\\) ist, also Frequentistisch betrachtet nur für etwa ein Zehntel ihrer Realisierungen zutrifft, jeweils unabhängig davon, von welcher genauen Form die Verteilung der Zufallsvariable ist. Dies folgt mit der Chebyshev Ungleichung aus\n\\[\\begin{equation}\n\\mathbb{P}\\left(|\\xi - \\mathbb{E}(\\xi)| \\ge 2 \\sqrt{\\mathbb{V}(\\xi)}\\right)\n\\le  \\frac{\\mathbb{V}(\\xi)}{\\left(2 \\sqrt{\\mathbb{V}(\\xi)}\\right)^2} =\n\\frac{1}{4}\n\\end{equation}\\] und \\[\\begin{equation}\n\\mathbb{P}\\left(|\\xi - \\mathbb{E}(\\xi)| \\ge 3 \\sqrt{\\mathbb{V}(\\xi)}\\right)\n\\le  \\frac{\\mathbb{V}(\\xi)}{\\left(3 \\sqrt{\\mathbb{V}(\\xi)}\\right)^2} =\n\\frac{1}{9}.\n\\end{equation}\\]",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Ungleichungen</span>"
    ]
  },
  {
    "objectID": "206-Ungleichungen.html#sec-wahrscheinlichkeitsungleichungen",
    "href": "206-Ungleichungen.html#sec-wahrscheinlichkeitsungleichungen",
    "title": "16  Ungleichungen",
    "section": "",
    "text": "Theorem 16.1 (Markov Ungleichung) \\(\\xi\\) sei eine Zufallsvariable mit \\(\\mathbb{P}(\\xi \\ge 0) = 1\\). Dann gilt für alle \\(x \\in \\mathbb{R}\\), dass \\[\\begin{equation}\n\\mathbb{P}(\\xi \\ge x) \\le \\frac{\\mathbb{E}(\\xi)}{x}.\n\\end{equation}\\]\n\n\nBeweis. Wir betrachten den Fall einer kontinuierlichen Zufallsvariable \\(\\xi\\) mit WDF \\(p\\). Wir halten zunächst fest, dass \\[\\begin{equation}\n\\mathbb{E}(\\xi)\n= \\int_{-\\infty}^\\infty s \\, p(s)\\,ds\n= \\int_0^\\infty s \\, p(s)\\,ds\n= \\int_0^x s \\, p(s)\\,ds + \\int_x^\\infty s \\, p(s)\\,ds,\n\\end{equation}\\] weil \\(\\xi\\) nicht-negativ ist. Es folgt dann \\[\\begin{equation}\n\\mathbb{E}(\\xi)\n\\ge  \\int_x^\\infty s \\, p(s)\\,ds\n\\ge  \\int_x^\\infty x \\, p(s)\\,ds\n=  x\\int_x^\\infty  p(s)\\,ds\n=  x\\, \\mathbb{P}(\\xi \\ge x).\n\\end{equation}\\] Dabei gilt die erste Ungleichung, weil \\[\\begin{equation}\n\\int_{0}^x s \\, p(s)\\,ds \\ge 0,\n\\end{equation}\\] und die zweite Ungleichung gilt, weil \\(x \\le \\xi\\) für \\(\\xi \\in [x,\\infty[\\). Es folgt also, dass \\[\\begin{equation}\n\\mathbb{E}(\\xi) \\ge x\\, \\mathbb{P}(\\xi \\ge x)\n\\Leftrightarrow\n\\mathbb{P}(\\xi \\ge x) \\le \\frac{\\mathbb{E}(\\xi)}{x}.\n\\end{equation}\\]\n\n\n\n\n\n\n\n\n\n\nAbbildung 16.1: Markov Ungleichung am Beispiel einer Gamma-Zufallsvariable.\n\n\n\n\n\nTheorem 16.2 (Chebyshev Ungleichung) Es sei \\(\\xi\\) eine Zufallsvariable mit Varianz \\(\\mathbb{V}(\\xi)\\). Dann gilt für alle \\(x \\in \\mathbb{R}\\) \\[\\begin{equation}\n\\mathbb{P}(|\\xi - \\mathbb{E}(\\xi)| \\ge x) \\le \\frac{\\mathbb{V}(\\xi)}{x^2}.\n\\end{equation}\\]\n\n\nBeweis. Wir halten zunächst fest, dass für \\(a,b \\in \\mathbb{R}\\) gilt, dass aus \\(a^2 \\ge b^2\\) folgt, dass \\(|a| \\ge b\\). Dazu betrachten wir die folgenden vier möglichen Fälle.\n\n\\(a^2 \\ge b^2\\) für \\(a \\ge 0\\) und \\(b \\ge 0\\). Dann gilt \\[\\begin{equation}\na^2 \\ge b^2 \\Rightarrow \\sqrt{a^2} \\ge \\sqrt{b^2} \\Rightarrow a \\ge b \\Rightarrow |a| \\ge b.\n\\end{equation}\\]\n\\(a^2 \\ge b^2\\) für \\(a \\le 0\\) und \\(b \\ge 0\\). Dann gilt \\[\\begin{equation}\na^2 \\ge b^2 \\Rightarrow \\sqrt{a^2} \\ge \\sqrt{b^2} \\Rightarrow -a \\ge b \\Rightarrow |a| \\ge b.\n\\end{equation}\\]\n\\(a^2 \\ge b^2\\) für \\(a \\ge 0\\) und \\(b \\le 0\\). Dann gilt \\[\\begin{equation}\na^2 \\ge b^2 \\Rightarrow \\sqrt{a^2} \\ge \\sqrt{b^2} \\Rightarrow a \\ge -b \\ge b \\Rightarrow |a| \\ge b.\n\\end{equation}\\]\n\\(a^2 \\ge b^2\\) für \\(a \\le 0\\) und \\(b \\le 0\\). Dann gilt \\[\\begin{equation}\na^2 \\ge b^2 \\Rightarrow \\sqrt{a^2} \\ge \\sqrt{b^2} \\Rightarrow -a \\ge -b \\ge b \\Rightarrow |a| \\ge b.\n\\end{equation}\\] Als nächstes definieren wir \\(\\upsilon := (\\xi - \\mathbb{E}(\\xi))^2\\). Dann gilt offenbar \\(\\upsilon \\ge 0\\) und es folgt aus der Markov Ungleichung \\[\\begin{align}\n\\begin{split}\n\\mathbb{P}\\left(\\upsilon \\ge x^2\\right)\n& \\le \\frac{\\mathbb{E}(\\upsilon)}{x^2} \\\\\n\\Leftrightarrow\n\\mathbb{P}\\left((\\xi - \\mathbb{E}(\\xi))^2 \\ge x^2 \\right)\n& \\le \\frac{\\mathbb{E}\\left((\\xi - \\mathbb{E}(\\xi))^2 \\right)}{x^2} \\\\\n\\Leftrightarrow\n\\mathbb{P}(|\\xi - \\mathbb{E}(\\xi)| \\ge x)\n& \\le \\frac{\\mathbb{V}(\\xi)}{x^2}.\n\\end{split}\n\\end{align}\\]",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Ungleichungen</span>"
    ]
  },
  {
    "objectID": "206-Ungleichungen.html#sec-erwartungswertungleichungen",
    "href": "206-Ungleichungen.html#sec-erwartungswertungleichungen",
    "title": "16  Ungleichungen",
    "section": "16.2 Erwartungswertungleichungen",
    "text": "16.2 Erwartungswertungleichungen\nDie Cauchy-Schwarz Ungleichung ist eine zentrale Ungleichung der modernen Mathematik, die in verschiedenen mathematischen Bereichen wie der Analysis, der Vektorraumtheorie und eben auch der Wahrscheinlichkeitstheorie zur Anwendung kommt (vgl. Steele (2006)). In Bezug auf Erwartungswerte von Zufallsvariablen hat sie die folgende Form.\n\nTheorem 16.3 (Cauchy-Schwarz Ungleichung) \\(\\xi\\) und \\(\\upsilon\\) seien zwei Zufallsvariablen und \\(\\mathbb{E}(\\xi\\upsilon)\\) sei endlich. Dann gilt \\[\\begin{equation}\n\\mathbb{E}(\\xi\\upsilon)^2 \\le \\mathbb{E}\\left(\\xi^2\\right)\\mathbb{E}\\left(\\upsilon^2 \\right).\n\\end{equation}\\]\n\nFür einen Beweis verweisen wir auf den Beweis von Theorem 4.6.2 in DeGroot & Schervish (2012). Analog zu Theorem 16.3 gilt zum Beispiel für Vektoren \\(x,y \\in \\mathbb{R}^n\\), dass \\[\\begin{equation}\n\\langle x,y \\rangle^2 \\le \\langle x,x \\rangle \\langle x,y \\rangle.\n\\end{equation}\\]\nIm Kontext der probabilistischen Datenanalyse ist die Anwendung der Cauchy-Schwarz Ungleichung vor allem im Beweis der sogenannten Korrelationsungleichung von Relevanz.\n\nTheorem 16.4 (Korrelationsungleichung) \\(\\xi\\) und \\(\\upsilon\\) seien Zufallsvariablen mit \\(\\mathbb{V}(\\xi), \\mathbb{V}(\\upsilon) &gt; 0\\). Dann gelten \\[\\begin{equation}\n\\frac{\\mathbb{C}(\\xi,\\upsilon)^2}{\\mathbb{V}(\\xi)\\mathbb{V}(\\upsilon)} \\le 1\n\\mbox{ und }\n-1 \\le \\rho(\\xi,\\upsilon) \\le 1.\n\\end{equation}\\]\n\n\nBeweis. Mit der Cauchy-Schwarz-Ungleichung für zwei Zufallsvariablen \\(\\alpha\\) und \\(\\beta\\) gilt, dass \\[\\begin{equation}\n\\mathbb{E}(\\alpha\\beta)^2 \\le \\mathbb{E}\\left(\\alpha^2\\right)\\mathbb{E}\\left(\\beta^2\\right).\n\\end{equation}\\] Wir definieren nun \\(\\alpha := \\xi -\\mathbb{E}(\\xi)\\) und \\(\\beta := \\upsilon - \\mathbb{E}(\\upsilon)\\). Dann besagt die Cauchy-Schwarz Ungleichung gerade, dass \\[\\begin{equation}\n\\mathbb{E}\\left((\\xi -\\mathbb{E}(\\xi))(\\upsilon-\\mathbb{E}(\\upsilon))\\right)^2\n\\le  \\mathbb{E}\\left((\\xi -\\mathbb{E}(\\xi))^2 \\right) \\mathbb{E}\\left((\\upsilon-\\mathbb{E}(\\upsilon))^2 \\right).\n\\end{equation}\\] Also gilt \\[\\begin{align}\n\\begin{split}\n\\mathbb{C}(\\xi,\\upsilon)^2\n\\le  \\mathbb{V}(\\xi)\\mathbb{V}(\\upsilon)\n\\Leftrightarrow \\frac{\\mathbb{C}(\\xi,\\upsilon)^2}{\\mathbb{V}(\\xi)\\mathbb{V}(\\upsilon)}\n\\le 1.\n\\end{split}\n\\end{align}\\] Weiterhin folgt aus der Definition der Korrelation dann sofort, dass auch \\[\\begin{equation}\n\\rho(\\xi,\\upsilon)^2 \\le 1.\n\\end{equation}\\] Dann gilt aber auch \\[\\begin{equation}\n|\\rho(\\xi,\\upsilon)^2| \\le 1 \\Leftrightarrow -1 \\le \\rho(\\xi,\\upsilon) \\le 1,\n\\end{equation}\\] denn \\[\\begin{equation}\n\\rho(\\xi,\\upsilon)^2 \\le 1 \\Rightarrow \\sqrt{\\rho(\\xi,\\upsilon)^2} \\le \\sqrt{1} \\Rightarrow \\quad\\rho(\\xi,\\upsilon)  \\le 1 \\Rightarrow |\\rho(\\xi,\\upsilon)| \\le 1 \\mbox{ für } \\rho(\\xi,\\upsilon) \\ge 0\n\\end{equation}\\] und \\[\\begin{equation}\n\\rho(\\xi,\\upsilon)^2 \\le 1 \\Rightarrow \\sqrt{\\rho(\\xi,\\upsilon)^2} \\le \\sqrt{1} \\Rightarrow     -\\rho(\\xi,\\upsilon) \\le 1 \\Rightarrow |\\rho(\\xi,\\upsilon)| \\le 1 \\mbox{ für } \\rho(\\xi,\\upsilon) \\le 0\n\\end{equation}\\]\n\nDie Korrelationsungleichung wird manchmal auch als Kovarianzungleichung bezeichnet. Insbesondere besagt sie, dass die Korrelation von Zufallsvariablen normalisiert ist, also immer Werte zwischen -1 und 1 inklusive annimmt (vgl. Kapitel 15.4).\nDie Jensensche Ungleichung schließlich liefert Abschätzungen für den Erwartungswert einer durch eine konvexe oder konkave Funktion transformierte Zufallsvariable. Sie kommt in der Betrachtung von Parameterschätzereigenschaften (vgl. Kapitel 28) und insbesondere als Grundlage der Variationalen Bayesianischen Inferenz zum Einsatz. Wir erinnern daran, dass sich eine konvexe Funktion \\(g\\) dadurch auszeichnet, dass der Funktionsgraph von \\(g\\) über einem Intervall \\([x_1,x_2]\\) immer unter der verbindenden Geraden zwischen den Funktionswerten \\(g(x_1)\\) und \\(g(x_2)\\) liegt, wohingegehn bei einer konkaven Funktion \\(g\\) dieser immer über der verbindenden Geraden zwischen den Funktionswerten \\(g(x_1)\\) zu \\(g(x_2)\\) liegt. Wir visualisieren dies für eine konvexe Funktion in Abbildung 16.2.\n\nTheorem 16.5 (Jensensche Ungleichung) \\(\\xi\\) sei eine Zufallsvariable und \\(g : \\mathbb{R} \\to \\mathbb{R}\\) eine konvexe Funktion, d.h. \\[\\begin{equation}\ng(\\lambda x_1 + (1-\\lambda)x_2) \\le \\lambda g(x_1) + (1-\\lambda)g(x_2)\n\\end{equation}\\] für alle \\(x_1,x_2 \\in \\mathbb{R}, \\lambda \\in [0,1]\\). Dann gilt \\[\\begin{equation}\n\\mathbb{E}(g(\\xi)) \\ge g(\\mathbb{E}(\\xi)).\n\\end{equation}\\] Analog sei \\(g : \\mathbb{R} \\to \\mathbb{R}\\) eine konkave Funktion, d.h. \\[\\begin{equation}\ng(\\lambda x_1 + (1-\\lambda)x_2) \\ge \\lambda g(x_1) + (1-\\lambda)g(x_2)\n\\end{equation}\\] für alle \\(x_1,x_2 \\in \\mathbb{R}, \\lambda \\in [0,1]\\). Dann gilt \\[\\begin{equation}\n\\mathbb{E}(g(\\xi)) \\le g(\\mathbb{E}(\\xi)).\n\\end{equation}\\]\n\n\nBeweis. Es sei \\(g\\) eine konvexe Funktion. Dann gilt für die Tangente \\(t\\) von \\(g\\) in \\(x_0 \\in \\mathbb{R}\\) für alle \\(x \\in \\mathbb{R}\\), dass \\[\\begin{equation}\ng(x) \\ge t(x) := g(x_0) + g'(x_0)(x - x_0)\n\\end{equation}\\] Wir setzen nun \\(x := \\xi\\) und \\(x_0 := \\mathbb{E}(\\xi)\\). Dann gilt mit obiger Ungleichung, dass \\[\\begin{equation}\ng(\\xi) \\ge g(\\mathbb{E}(\\xi)) + g'(\\mathbb{E}(\\xi))(\\xi - \\mathbb{E}(\\xi))\n\\end{equation}\\] Erwartungswertbildung ergibt dann \\[\\begin{align}\n\\begin{split}\n\\mathbb{E}(g(\\xi)) & \\ge \\mathbb{E}(g(\\mathbb{E}(\\xi))) + \\mathbb{E}(g'(\\mathbb{E}(\\xi))(\\xi - \\mathbb{E}(\\xi))) \\\\\n\\Leftrightarrow\n\\mathbb{E}(g(\\xi)) & \\ge g(\\mathbb{E}(\\xi)) + g'(\\mathbb{E}(\\xi))\\mathbb{E}((\\xi - \\mathbb{E}(\\xi))) \\\\\n\\Leftrightarrow\n\\mathbb{E}(g(\\xi)) & \\ge g(\\mathbb{E}(\\xi)) + g'(\\mathbb{E}(\\xi))(\\mathbb{E}(\\xi) - \\mathbb{E}(\\xi)) \\\\\n\\Leftrightarrow\n\\mathbb{E}(g(\\xi)) & \\ge g(\\mathbb{E}(\\xi)).\n\\end{split}\n\\end{align}\\] Sei nun \\(g\\) eine konkave Funktion. Dann ist \\(-g\\) eine konvexe Funktion. Mit der Jensenschen Ungleichung für konvexe Funktionen folgt dann die Jensensche Ungleichung für konkave Funktionen aus \\[\\begin{align}\n\\begin{split}\n\\mathbb{E}(-g(\\xi)) & \\ge -g(\\mathbb{E}(\\xi)) \\\\\n\\Leftrightarrow\n-\\mathbb{E}(g(\\xi)) & \\ge -g(\\mathbb{E}(\\xi)) \\\\\n\\Leftrightarrow\n\\mathbb{E}(g(\\xi)) & \\le g(\\mathbb{E}(\\xi)).\n\\end{split}\n\\end{align}\\]\n\nIm Kontext der Variationalen Bayesianischen Inferenz ist grundlegend, dass der Logarithmus ist eine konkave Funktion ist und damit für eine beliebige Zufallsvariable \\(\\xi\\) gilt, dass \\[\\begin{equation}\n\\mathbb{E}(\\ln \\xi) \\le \\ln \\mathbb{E}(\\xi).\n\\end{equation}\\]\n\n\n\n\n\n\nAbbildung 16.2: Darstellung der konvexen Funktion \\(g(x) := x^2\\) mit \\(x_1 := -\\sqrt{1.5}, x_2 := \\sqrt{3.5}, \\lambda \\in [0,1]\\) und \\(x_0 := 1\\).",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Ungleichungen</span>"
    ]
  },
  {
    "objectID": "206-Ungleichungen.html#selbstkontrollfragen",
    "href": "206-Ungleichungen.html#selbstkontrollfragen",
    "title": "16  Ungleichungen",
    "section": "16.3 Selbstkontrollfragen",
    "text": "16.3 Selbstkontrollfragen\n\nGeben Sie die Markov Ungleichung wieder.\nGeben Sie die Chebyshev Ungleichung wieder.\nGeben Sie die Cauchy-Schwarz Ungleichung wieder.\nGeben Sie die Korrelationsungleichung wieder.\nGeben Sie die Jensensche Ungleichung wider. \n\n\n\n\n\nDeGroot, M. H., & Schervish, M. J. (2012). Probability and Statistics (4th ed). Addison-Wesley.\n\n\nSteele, J. M. (2006). The Cauchy-Schwarz Master Class: An Introduction to the Art of Mathematical Inequalities (repr). Cambridge University Press [u.a.].",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Ungleichungen</span>"
    ]
  },
  {
    "objectID": "207-Grenzwerte.html",
    "href": "207-Grenzwerte.html",
    "title": "17  Grenzwerte",
    "section": "",
    "text": "17.1 Gesetze der Großen Zahlen\nEs gibt ein Schwaches Gesetz der Großen Zahlen und ein Starkes Gesetz der Großen Zahlen. Intuitiv besagen beide Gesetze, dass sich das Stichprobenmittel von unabhängigen und identisch verteilten Zufallsvariablen für eine große Anzahl an Zufallsvariablen dem Erwartungswert der zugrundeliegenden Verteilung nähert. Das Schwache und das Starke Gesetz der Großen Zahlen unterscheiden sich in Hinblick auf die zu ihrer Formulierung benutzen Formen der Konvergenz von Zufallsvariablen. Das Schwache Gesetz basiert auf der Konvergenz in Wahrscheinlichkeit. Das Starke Gesetz basiert auf der fast sicheren Konvergenz. Wir begnügen uns hier mit dem Begriff der Konvergenz in Wahrscheinlichkeit und damit dem Schwachen Gesetz der Großen Zahlen.\n\\(\\xi_n\\xrightarrow[n \\to \\infty]{\\text{P}} \\xi\\) heißt also, dass sich die Wahrscheinlichkeit dafür, dass \\(\\xi_n\\) in dem zufälligen Intervall \\[\\begin{equation}\n]\\xi-\\epsilon, \\xi+\\epsilon[\n\\end{equation}\\] liegt, unabhängig davon, wie klein dieses Intervall sein mag, \\(1\\) nähert, wenn \\(n\\) gegen Unendlich geht. Intuitiv heißt das, dass sich für \\(n \\to \\infty\\) und eine konstante Zufallsvariable \\(\\xi := a\\) die Verteilung von \\(\\xi_n\\) mehr und mehr um \\(a\\) konzentriert, wenn \\(n\\) gegen Unendlich strebt. Mithilfe der Konvergenz in Wahrscheinlichkeit formuliert man das Schwache Gesetz der Großen Zahlen wie folgt.\nFür einen Beweis dieses Theorems verweisen wir auf die weiterführende Literatur, so zum Beispiel auf Abschnitt 5.1 in Georgii (2009). Intuitiv heißt \\[\\begin{equation}\n\\bar{\\xi}_n \\xrightarrow[n\\to\\infty]{\\mbox{P}} \\mu\n\\end{equation}\\] also, dass die Wahrscheinlichkeit, dass das Stichprobenmittel nahe dem Erwartungswert der zugrundeliegenden Verteilung liegt, sich 1 nähert, wenn \\(n\\) gegen Unendlich strebt.\nSimulation\nWir betrachten den Fall von u.i.v. normalverteilten Zufallsvariablen \\(\\xi_1,...,\\xi_n \\sim N(0,1)\\). Abbildung 17.1 A zeigt Realisationen der von Stichprobenmitteln \\(\\bar{\\xi}_n\\) als Funktion von \\(n\\). Man erkennt, dass für größeres \\(n\\) mehr Realisierungen von \\(\\bar{\\xi}_n\\) in der Nähe des Erwartungswerts der \\(\\xi_i, i = 1,...,n\\) liegen. Basierend auf diesen Stichprobenmittelrealisationen zeigt Abbildung 17.1 B Schätzungen der Wahrscheinlichkeit \\(\\mathbb{P}(|\\bar{\\xi}_n - \\mu| \\ge \\epsilon)\\) als Funktionen von \\(n\\) und \\(\\epsilon\\). Für ein großes \\(\\epsilon\\) reicht ein geringes \\(n\\) aus um die Wahrscheinlichkeit für eine absolute Abwecihung des Stichprobenmittels vom Erwartungswert klein werden zu lassen, für ein kleineres \\(\\epsilon\\) ist dafür ein größeres \\(n\\) nötig. In jedem Fall sinken die Wahrscheinlichkeiten jedoch mit größerem \\(n\\).",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Grenzwerte</span>"
    ]
  },
  {
    "objectID": "207-Grenzwerte.html#sec-gesetze-der-grossen-zahlen",
    "href": "207-Grenzwerte.html#sec-gesetze-der-grossen-zahlen",
    "title": "17  Grenzwerte",
    "section": "",
    "text": "Definition 17.1 (Konvergenz in Wahrscheinlichkeit) Eine Folge von Zufallsvariablen \\(\\xi_1,\\xi_2,...\\) konvergiert gegen eine Zufallsvariable \\(\\xi\\) in Wahrscheinlichkeit, wenn für jedes noch so kleine \\(\\epsilon &gt; 0\\) gilt, dass \\[\\begin{equation}\n\\lim_{n \\to \\infty} \\mathbb{P}(|\\xi_n - \\xi| &lt; \\epsilon)    = 1 \\Leftrightarrow\n\\lim_{n \\to \\infty} \\mathbb{P}(|\\xi_n - \\xi| \\ge \\epsilon)  = 0.\n\\end{equation}\\] Die Konvergenz von \\(\\xi_1,\\xi_2,....\\) gegen \\(\\xi\\) in Wahrscheinlichkeit wird geschrieben als \\[\\begin{equation}\n\\xi_n\\xrightarrow[n \\to \\infty]{\\mbox{P}} \\xi.\n\\end{equation}\\]\n\n\n\nTheorem 17.1 (Schwaches Gesetz der Großen Zahlen) \\(\\xi_1,...,\\xi_n\\) seien unabhängig und gleichverteilte Zufallsvariablen mit \\(\\mathbb{E}(\\xi_i) = \\mu\\) für alle \\(i = 1,...,n\\). Weiterhin bezeichne \\[\\begin{equation}\n\\bar{\\xi}_n := \\frac{1}{n}\\sum_{i=1}^n \\xi_i\n\\end{equation}\\] das Stichprobenmittel der \\(\\xi_i, i = 1,...,n\\). Dann konvergiert \\(\\bar{\\xi}_n\\) in Wahrscheinlichkeit gegen \\(\\mu\\), \\[\\begin{equation}\n\\bar{\\xi}_n \\xrightarrow[n \\to \\infty]{\\mbox{P}} \\mu.\n\\end{equation}\\]\n\n\n\n\n\n\n\n\n\n\nAbbildung 17.1: Simulation des schwachen Gesetz der Großen Zahlen.",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Grenzwerte</span>"
    ]
  },
  {
    "objectID": "207-Grenzwerte.html#sec-zentrale-grenzwertsaetze",
    "href": "207-Grenzwerte.html#sec-zentrale-grenzwertsaetze",
    "title": "17  Grenzwerte",
    "section": "17.2 Zentrale Grenzwertsätze",
    "text": "17.2 Zentrale Grenzwertsätze\nDie Zentralen Grenzwertsätze besagen intuitiv, dass die Summe von unabhängigen Zufallsvariablen mit Erwartungswert Null asymptotisch, also für unendlich viele Zufallsvariablen, normalverteilt mit Erwartungswertparameter Null ist. Modelliert man eine beliebige Messgröße \\(\\upsilon\\) also als Summe eines deterministischen Einflusses \\(\\mu\\) und der Summe \\[\\begin{equation}\n\\varepsilon := \\sum_{i=1}^n \\xi_i\n\\end{equation}\\] einer Vielzahl von unabhängigen Zufallsvariablen \\(\\xi_i, i = 1,...,n\\), die unbekannte Störeinflüsse modellieren sollen, so ist für großes \\(n\\) die Annahme \\[\n\\upsilon = \\mu + \\varepsilon \\mbox{ mit } \\varepsilon \\sim N(0,\\sigma^2)\n\\tag{17.1}\\] mathematisch gerechtfertigt. Wie wir später sehen werden, liegt die Annahme in Gleichung Gleichung 17.1 einer großen Vielzahl von probabilistischen Modellen zugrunde.\nFormal werden verschiedene Formen von Zentralen Grenzwertsätzen, je nach Ausgestaltung der zugrundeliegenden Annahmen und ihrer Beweisführung unterschieden. In der sogenannten Lindenberg und Lévy Form des Zentralen Grenzwertsatzes werden unabhängig und identische Zufallsvariablen vorausgesetzt. In der Liapunov Form dagegen werden nur unabhängige Zufallsvariablen voraussetzt. In beiden Formulierungen des Zentralen Grenzwertsatzes ist die betrachtete Konvergenz von Zufallsvariablen die Konvergenz in Verteilung, welche wir zunächst einführen.\n\nDefinition 17.2 (Konvergenz in Verteilung) Eine Folge \\(\\xi_1,\\xi_2,...\\) von Zufallsvariablen , wenn \\[\\begin{equation}\n\\lim_{n \\to \\infty} P_{\\xi_n}(x) = P_\\xi(x)\n\\end{equation}\\] für alle \\(\\xi\\) an denen \\(P_\\xi\\) stetig ist. Die Konvergenz in Verteilung von \\(\\xi_1,\\xi_2,...\\) gegen \\(\\xi\\) wird geschrieben als \\[\\begin{equation}\n\\xi_n\\xrightarrow[n\\to \\infty]{\\text{D}} \\xi.\n\\end{equation}\\] Gilt \\(\\xi_n\\xrightarrow[n\\to \\infty]{\\text{D}} \\xi\\), dann heißt die Verteilung von \\(\\xi\\) die .\n\nDie Konvergenz in Verteilung ist also eine Aussage zur Konvergenz von Funktionenfolgen, speziell von KVFen. Ohne Begründung merken wir an, dass die oben betrachtete Konvergenz in Wahrscheinlichkeit die Konvergenz in Verteilung impliziert. Wir geben nun zunächst den Zentralen Grenzwertsatz nach Lindenberg und Lévy an.\n\nTheorem 17.2 (Zentraler Grenzwertsatz nach Lindenberg und Lévy) \\(\\xi_1,...,\\xi_n\\) seien unabhängig und identisch verteilte Zufallsvariablen mit \\[\\begin{equation}\n\\mathbb{E}(\\xi_i) := \\mu \\mbox{ und }\n\\mathbb{V}(\\xi_i) := \\sigma^2 &gt; 0\n\\mbox{ für alle } i = 1,....,n.\n\\end{equation}\\] Weiterhin sei \\(\\zeta_n\\) die Zufallsvariable definiert als \\[\\begin{equation}\n\\zeta_n := \\sqrt{n}\\left(\\frac{\\bar{\\xi}_n - \\mu}{\\sigma}\\right).\n\\end{equation}\\] Dann gilt für alle \\(z \\in \\mathbb{R}\\) \\[\\begin{equation}\n\\lim_{n \\to \\infty} P_{\\zeta_n}(z) = \\Phi(z),\n\\end{equation}\\] wobei \\(\\Phi\\) die kumulative Verteilungsfunktion der Standardnormalverteilung bezeichnet.\n\nWir zeigen an späterer Stelle, dass damit für \\(n\\to\\infty\\) auch gilt, dass \\[\n\\sum_{i=1}^n \\xi_i \\sim N(\\mu, n\\sigma^2)\n\\mbox{ und }\n\\bar{\\xi}_n \\sim N\\left(\\mu,\\frac{\\sigma^2}{n}\\right).\n\\tag{17.2}\\]\nSimulation\nWir betrachten den Fall von u.i.v. \\(\\chi^2\\)-Zufallsvariablen \\(\\xi_1,...,\\xi_n \\sim \\chi^2(3)\\). Offenbar ist die funktionale Form der \\(\\chi^2(3)\\)-Verteilung von der Standardnormalverteilung recht verschieden, insbesondere nehmen \\(\\chi^2\\)-Zufallsvariablen mit von Null verschiedener Wahrscheinlichkeit nur nicht-negative Werte an (vgl. Kapitel 13.3) Nichtsdestotrotz resultiert ihre standardisierte Summe asymptotisch in einer Normalverteilung, wie in Abbildung 17.2 visualisiert. Dazu nutzen wir auf Ebene der Implementation die Tatsache, für die \\(\\chi^2\\)-Zufallsvariablen \\(\\xi_i, i = 1,...,n\\) mit Freiheitsgradparameter \\(3\\) bekanntlich gilt (vgl. Kapitel 15.1 und Kapitel 15.2) \\[\\begin{equation}\n\\mathbb{E}(\\xi_i) = 3 \\mbox{ und }\\mathbb{V}(\\xi_i) = 6\n\\end{equation}\\] Die Abbildungen in Abbildung 17.2 A zeigen Histogrammschätzer der Wahrscheinlichkeitsdichte von \\[\\begin{equation}\n\\zeta_n := \\sqrt{n}\\left(\\frac{\\bar{\\xi}_n - \\mu}{\\sigma}\\right)\n\\end{equation}\\] basierend auf 1000 Realisationen von \\(\\zeta_n\\) für \\(n = 2\\) und \\(n = 200\\), sowie die WDF von \\(N(0,1)\\). Offenbar ist die Verteilung der Realisiationen von \\(\\zeta_2\\) der Standardnormalverteilung noch sehr unähnlich, wohingegen sich die Verteilung der Realisationen von \\(\\zeta_{200}\\) der Standardnormalverteilung schon annähert. Abbildung 17.2 B zeigt die entsprechenden geschätzten KVFen über die Theorem 17.2 formal eine Aussage trifft.\n\n\n\n\n\n\nAbbildung 17.2: Simulation des Zentralen Grenzwertsatzes nach Lindenberg und Lévy.\n\n\n\n\nTheorem 17.3 (Zentraler Grenzwertsatz nach Liapounov) \\(\\xi_1,...,\\xi_n\\) seien unabhängige aber nicht notwendigerweise identisch verteilten Zufallsvariablen mit \\[\\begin{equation}\n\\mathbb{E}(\\xi_i) := \\mu_i \\mbox{ und }\n\\mathbb{V}(\\xi_i) := \\sigma^2_i &gt; 0\n\\mbox{ für alle } i = 1,....,n.\n\\end{equation}\\] Weiterhin sollen für \\(\\xi_1,...,\\xi_n\\) folgende Eigenschaften gelten: \\[\\begin{equation}\n\\mathbb{E}(|\\xi_i - \\mu_i|^3) &lt; \\infty \\mbox{ und }\n\\lim_{n \\to \\infty} \\frac{\\sum_{i=1}^n \\mathbb{E}\\left(|\\xi_i - \\mu_i|^3\\right)}{(\\sum_{i=1}^n \\sigma_i^2)^{3/2}} = 0.\n\\end{equation}\\] Dann gilt für die Zufallsvariable \\(\\zeta_n\\) definiert als \\[\\begin{equation}\n\\zeta_n := \\frac{\\sum_{i=1}^n \\xi_i - \\sum_{i=1}^n \\mu_i}{\\sqrt{\\sum_{i=1}^n \\sigma_i^2}},\n\\end{equation}\\] für alle \\(z\\in\\mathbb{R}\\), dass \\[\\begin{equation}\n\\lim_{n \\to \\infty} P_{\\zeta_n}(z) = \\Phi(z),\n\\end{equation}\\] wobei \\(\\Phi\\) KVF der Standardnormalverteilung bezeichnet.\n\nWir zeigen an späterer Stelle, dass damit für \\(n\\to\\infty\\) auch gilt, dass \\[\n\\sum_{i=1}^n \\xi_i \\sim N\\left(\\sum_{i=1}^n \\mu_i, \\sum_{i=1}^n \\sigma_i^2\\right)\n\\tag{17.3}\\]",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Grenzwerte</span>"
    ]
  },
  {
    "objectID": "207-Grenzwerte.html#literaturhinweise",
    "href": "207-Grenzwerte.html#literaturhinweise",
    "title": "17  Grenzwerte",
    "section": "17.3 Literaturhinweise",
    "text": "17.3 Literaturhinweise\nZur mathematik-geschichtlichen Genese der Zentralen Grenzwertsätze siehe z.B. Fischer (2011).",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Grenzwerte</span>"
    ]
  },
  {
    "objectID": "207-Grenzwerte.html#selbstkontrollfragen",
    "href": "207-Grenzwerte.html#selbstkontrollfragen",
    "title": "17  Grenzwerte",
    "section": "17.4 Selbstkontrollfragen",
    "text": "17.4 Selbstkontrollfragen\n\nDefinieren Sie den Begriff der Konvergenz in Wahrscheinlichkeit.\nDefinieren Sie den Begriff der Konvergenz in Verteilung.\nGeben Sie das Schwache Gesetz der Großen Zahlen wieder.\nErläutern Sie den Zentralen Grenzwertsatz nach Lindenberg und Lévy.\nErläutern Sie den Zentralen Grenzwertsatz nach Liapunov.\nWarum sind die Zentralen Grenzwertsätze für die probabilistische Modellbildung wichtig? \n\n\n\n\n\nFischer, H. (2011). A History of the Central Limit Theorem. Springer New York. https://doi.org/10.1007/978-0-387-87857-7\n\n\nGeorgii, H.-O. (2009). Stochastik: Einführung in die Wahrscheinlichkeitstheorie und Statistik (4., überarb. und erw. Aufl). de Gruyter.",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Grenzwerte</span>"
    ]
  },
  {
    "objectID": "208-Transformationstheoreme.html",
    "href": "208-Transformationstheoreme.html",
    "title": "18  Transformationstheoreme",
    "section": "",
    "text": "18.1 Univariate Transformationstheoreme\nDabei liefert Theorem 18.2 eine Formel zur Berechnung der WDF \\(p_\\upsilon\\) von \\(\\upsilon := f(\\xi)\\), wenn \\(\\xi\\) eine Zufallsvariable mit WDF \\(p_\\xi\\) ist und \\(f\\) eine bijektive Funktion ist. Theorem 18.3 gibt weiterhin eine vereinfachte Formel zur Berechnung der WDF \\(p_\\upsilon\\) von \\(\\upsilon := f(\\xi)\\) an, wenn \\(f\\) speziell eine linear-affine Funktion ist. Theorem 18.4 schließlich gibt eine Formel zur Berechnung der WDF \\(p_\\upsilon\\) von \\(\\upsilon := f(\\xi)\\) an, wenn \\(f\\) zumindest in Teilen bijektiv ist.\nEin wichtiger Anwendungsfall von Theorem 18.2 ist Theorem 18.3.\nEin wichtiger Anwendungsfall dieses Theorems ist die in Kapitel 19 betrachtete \\(Z\\)-Transformation. Das folgende Theorem, dass wir nicht beweisen wollen, verallgemeinert Theorem 18.2 auf den Fall nur stückweise bijektiver Abbildungen.\nEin wichtiger Anwendungsfall ist die in Kapitel 19 betrachtete \\(\\chi^2\\)-Transformation.",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Transformationstheoreme</span>"
    ]
  },
  {
    "objectID": "208-Transformationstheoreme.html#sec-univariate-transformationstheoreme",
    "href": "208-Transformationstheoreme.html#sec-univariate-transformationstheoreme",
    "title": "18  Transformationstheoreme",
    "section": "",
    "text": "Theorem 18.2 (Univariate WDF Transformation bei bijektiven Abbildungen) \\(\\xi\\) sei eine Zufallsvariable mit WDF \\(p_\\xi\\) für die \\(\\mathbb{P}(]a,b[) = 1\\) gilt, wobei \\(a\\) und/oder \\(b\\) entweder endlich oder unendlich seien. Weiterhin sei \\[\\begin{equation}\n\\upsilon := f(\\xi),\n\\end{equation}\\] wobei die univariate reellwertige Funktion \\(f : ]a,b[ \\to \\mathbb{R}\\) differenzierbar und bijektiv auf \\(]a,b[\\) sei. \\(f(]a,b[)\\) sei das Bild von \\(]a,b[\\) unter \\(f\\). Schließlich sei \\(f^{-1}(y)\\) der Wert der Umkehrunktion von \\(f(x)\\) für \\(y \\in f(]a,b[)\\) und \\(f'(x)\\) sei die Ableitung von \\(f\\) an der Stelle \\(x\\). Dann ist die WDF von \\(\\upsilon\\) gegeben durch \\[\\begin{equation}\np_\\upsilon : \\mathbb{R} \\to \\mathbb{R}_{\\ge 0}, y \\mapsto p_\\upsilon(y) :=\n\\begin{cases}\n\\frac{1}{\\vert  f^{'}\\left(f^{-1}(y)\\right) \\vert}p_\\xi\\left(f^{-1}(y)\\right)\n& \\mbox{ für } y \\in f(]a,b[) \\\\\n0\n& \\mbox{ für } y \\in \\mathbb{R} \\setminus f(]a,b[).\n\\end{cases}\n\\end{equation}\\]\n\n\nBeweis. Wir halten zunächst fest, dass weil \\(f\\) eine differenzierbare bijektive Funktion auf \\(]a,b[\\) ist, \\(f\\) entweder strikt wachsend oder strikt fallend ist. Nehmen wir zunächst an, dass \\(f\\) auf \\(]a,b[\\) strikt wachsend ist. Dann ist auch \\(f^{-1}\\) für alle \\(y \\in f(]a,b[)\\) wachsend, und es gilt \\[\\begin{equation*}\nP_\\upsilon(y)\n= \\mathbb{P}(\\upsilon \\le y)\n= \\mathbb{P}\\left(f(\\xi) \\le y\\right)\n= \\mathbb{P}\\left(f^{-1}(f(\\xi)) \\le f^{-1}(y)\\right)\n= \\mathbb{P}\\left(\\xi \\le f^{-1}(y)\\right)\n= P_\\xi\\left(f^{-1}(y)\\right).\n\\end{equation*}\\] \\(P_\\upsilon\\) ist also differenzierbar an allen Stellen \\(y\\), an denen sowohl \\(f^{-1}\\) als auch \\(P_\\xi\\) differenzierbar sind. Mit der Kettenregel und dem Satz von der Umkehrabbildung \\((f^{-1}(x))' = 1/f'(f^{-1}(x))\\), folgt dann, dass die WDF \\(p_\\upsilon\\) sich ergibt wie folgt: \\[\\begin{equation*}\np_\\upsilon(y)\n= \\frac{d}{dy}P_\\upsilon(y)\n= \\frac{d}{dy}P_\\xi\\left(f^{-1}(y)\\right)\n= p_\\xi\\left(f^{-1}(y)\\right)\\frac{d}{dy}f^{-1}(y)\n= \\frac{1}{f'\\left(f^{-1}(y)\\right)} p_\\xi\\left(f^{-1}(y)\\right),\n\\end{equation*}\\] Weil \\(f^{-1}\\) strikt wachsend ist, ist \\(d/dy (f^{-1}(y))\\) positiv und das Theorem trifft zu. Analog gilt, dass wenn \\(f\\) auf \\(]a,b[\\) strikt fallend ist, dann ist auch \\(f^{-1}\\) für alle \\(y \\in f(]a,b[)\\) fallend und es gilt \\[\\begin{equation*}\nP_\\upsilon(y)\n= \\mathbb{P}(f(\\xi) \\le y)\n= \\mathbb{P}\\left(f^{-1}(f(\\xi)) \\ge f^{-1}(y)\\right)\n= \\mathbb{P}\\left(\\xi \\ge f^{-1}(y)\\right)\n= 1 - P_\\xi\\left(f^{-1}(y) \\right),\n\\end{equation*}\\] Mit der Kettenregel und dem Satz von der Umkehrabbildung folgt dann \\[\\begin{equation*}\np_\\upsilon(y)\n= \\frac{d}{dy}(1 - P_\\upsilon(y))\n= -\\frac{d}{dy}P_\\xi\\left(f^{-1}(y)\\right)\n= -p_\\xi\\left(f^{-1}(y)\\right)\\frac{d}{dy}f^{-1}(y)\n= -\\frac{1}{f'\\left(f^{-1}(y)\\right)} p_\\xi\\left(f^{-1}(y)\\right).\n\\end{equation*}\\] Weil \\(f^{-1}\\) strikt fallend ist, ist \\(d/dy (f^{-1}(y))\\) negativ, so dass \\(-d/dy (f^{-1}(y))\\) gleich \\(|d/dy (f^{-1}(y))|\\) ist und das Theorem trifft zu.\n\n\n\nTheorem 18.3 (Univariates WDF Transformationstheorem bei linear-affinen Abbildungen) \\(\\xi\\) sei eine Zufallsvariable mit WDF \\(p_\\xi\\) und es sei \\[\\begin{equation}\n\\upsilon = f(\\xi) \\mbox{ mit } f(\\xi) := a\\xi + b \\mbox{ für } a\\neq 0.\n\\end{equation}\\] Dann ist die WDF von \\(\\upsilon\\) gegeben durch \\[\\begin{equation}\np_\\upsilon : \\mathbb{R} \\to \\mathbb{R}_{\\ge 0}, y \\mapsto p_\\upsilon(y) :=\n\\frac{1}{|a|}p_\\xi\\left(\\frac{y-b}{a}\\right).\n\\end{equation}\\]\n\n\nBeweis. Wir halten zunächst fest, dass \\[\\begin{equation}\nf^{-1} : \\mathbb{R} \\to \\mathbb{R}, y  \\mapsto f^{-1}(y) = \\frac{y - b}{a}\n\\end{equation}\\] ist, weil dann \\(f \\circ f^{-1} = \\mbox{id}_{\\mathbb{R}}\\) gilt, wie man anhand von \\[\\begin{equation}\nf(f^{-1}(x)) = a \\left(\\frac{x - b}{a}\\right) + b = x - b + b = x \\mbox{ für alle } x \\in \\mathbb{R}\n\\end{equation}\\] einsieht. Wir halten weiterhin fest, dass \\[\\begin{equation}\nf' : \\mathbb{R} \\to \\mathbb{R}, x \\mapsto f'(x) = \\frac{d}{dx}(ax  + b) = a.\n\\end{equation}\\] Also folgt mit Theorem 18.2, dass \\[\\begin{align}\n\\begin{split}\np_\\upsilon : \\mathbb{R} \\to \\mathbb{R}_{\\ge 0}, y \\mapsto p_\\upsilon(y)\n& = \\frac{1}{\\vert f^{'}\\left(f^{-1}(y)\\right)\\vert}p_\\xi\\left(f^{-1}(y)\\right) \\\\\n& = \\frac{1}{|a|}p_\\xi\\left(\\frac{y - b}{a}\\right).\n\\end{split}\n\\end{align}\\]\n\n\n\nTheorem 18.4 (Univariate WDF Transformation bei stückweise bijektiven Abbildungen) \\(\\xi\\) sei eine Zufallsvariable mit Ergebnisraum \\(\\mathcal{X}\\) und WDF \\(p_\\xi\\). Weiterhin sei \\[\\begin{equation}\n\\upsilon = f(\\xi),\n\\end{equation}\\] wobei \\(f\\) so beschaffen sei, dass der Ergebnisraum von \\(\\xi\\) in eine endliche Anzahl von Mengen \\(\\mathcal{X}_1,...,\\mathcal{X}_k\\) mit einer entsprechenden Anzahl von Mengen \\(\\mathcal{Y}_1 := f(\\mathcal{X}_1), ..., \\mathcal{Y}_k :=  f(\\mathcal{X}_k)\\) im Ergebnisraum \\(\\mathcal{Y}\\) von \\(\\upsilon\\) partitioniert werden kann (wobei nicht notwendigerweise \\(\\mathcal{Y}_i \\cap \\mathcal{Y}_j = \\emptyset, 1 \\le i,j \\le k\\) gelten muss), so dass die Abbildung \\(f\\) für alle \\(\\mathcal{X}_1,...,\\mathcal{X}_k\\) bijektiv ist (d.h. \\(f\\) ist eine bijektive Abbildung). Für \\(i = 1,...,k\\) bezeichne \\(f_i^{-1}\\) die Umkehrfunktion von \\(f\\) auf \\(\\mathcal{Y}_i\\). Schließlich nehmen wir an, dass die Ableitungen \\(f_i^{\\prime}\\) für alle \\(i=1,...,k\\) existieren und stetig sind. Dann ist eine WDF von \\(\\upsilon\\) durch \\[\\begin{equation}\np_\\upsilon : \\mathcal{Y} \\to \\mathbb{R}_{\\ge 0}, y \\mapsto p_\\upsilon(y) :=\n\\sum_{i=1}^k 1_{\\mathcal{Y}_i} (y) \\frac{1}{\\vert  f^{'}_i(f^{-1}_i(y)) \\vert}p_\\xi\\left(f^{-1}_i(y)\\right).\n\\end{equation}\\] gegeben.",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Transformationstheoreme</span>"
    ]
  },
  {
    "objectID": "208-Transformationstheoreme.html#sec-multivariate-wdf-transformationstheorem",
    "href": "208-Transformationstheoreme.html#sec-multivariate-wdf-transformationstheorem",
    "title": "18  Transformationstheoreme",
    "section": "18.2 Multivariate WDF Transformationstheoreme",
    "text": "18.2 Multivariate WDF Transformationstheoreme\nTheorem 18.5 liefert eine Formel zur Berechnung der WDF \\(p_\\upsilon\\) von \\(\\upsilon := f(\\xi)\\), wenn \\(\\xi\\) ein Zufallsvektor mit WDF \\(p_\\xi\\) ist und \\(f\\) eine bijektive multivariate vektorwertigeFunktion ist. Es handelt sich dabei um eine direkte Generalisierung von Theorem 18.2 und wir verzichten auf einen Beweis.\n\nTheorem 18.5 (Multivariate WDF Transformation bei bijektiven Abbildungen) \\(\\xi\\) sei ein \\(n\\)-dimensionaler Zufallsvektor mit Ergebnisraum \\(\\mathbb{R}^n\\) und WDF \\(p_\\xi\\). Weiterhin sei \\[\\begin{equation}\n\\upsilon := f(\\xi),\n\\end{equation}\\] wobei die multivariate vektorwertige Funktion \\(f : \\mathbb{R}^n \\to \\mathbb{R}^n\\) differenzierbar und bijektiv auf \\(]a,b[\\) sei. Schließlich seien \\[\\begin{equation}\nJ^f(x)\n= \\left(\\frac{\\partial}{\\partial x_j}f_i(x)\\right)_{1\\le i \\le n, 1 \\le j \\le n}\n\\in \\mathbb{R}^{n \\times n}\n\\end{equation}\\] die Jacobi-Matrix von \\(f\\) an der Stelle \\(x \\in \\mathbb{R}^n\\), \\(|J^f(x)|\\) die Determinante von \\(J^f(x)\\), und es sei \\(|J^f(x)| \\neq 0\\) für alle \\(x \\in \\mathbb{R}^n\\). Dann ist eine WDF von \\(\\upsilon\\) durch \\[\\begin{equation}\\label{eq:mpdf_transform}\np_\\upsilon : \\mathbb{R}^n \\to \\mathbb{R}_{\\ge 0}, y \\mapsto p_\\upsilon(y) :=\n\\begin{cases}\n\\frac{1}{|J^f\\left(f^{-1}(y)\\right)|}p_\\xi\\left(f^{-1}(y)\\right)\n& \\mbox{ for } y\\in f(\\mathbb{R}^n) \\\\\n0\n& \\mbox{ for } y \\in \\mathbb{R}^n \\setminus f(\\mathbb{R}^n)\n\\end{cases}\n\\end{equation}\\] gegeben.\n\nWichtige Anwendungsfälle sind die in Kapitel 19 betrachteten \\(T\\)- und \\(F\\)-Transformationen.",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Transformationstheoreme</span>"
    ]
  },
  {
    "objectID": "208-Transformationstheoreme.html#sec-operationstheoreme",
    "href": "208-Transformationstheoreme.html#sec-operationstheoreme",
    "title": "18  Transformationstheoreme",
    "section": "18.3 Operationstheoreme",
    "text": "18.3 Operationstheoreme\nDas folgende sogenannte Konvolutionstheorem liefert eine Formel zur Berechnung der WDF \\(p_\\upsilon\\) von \\(\\upsilon := \\xi_1 + \\xi_2\\), wenn \\(\\xi_1\\) und \\(\\xi_2\\) zwei Zufallsvariablen mit WDFen \\(p_{\\xi_1}\\) und \\(p_{\\xi_2}\\) sind.\n\nTheorem 18.6 (Summe unabhängiger Zufallsvariablen (Konvolution)) \\(\\xi_1\\) und \\(\\xi_2\\) seien zwei kontinuierliche unabhängige Zufallsvariablen mit WDF \\(p_{\\xi_1}\\) und \\(p_{\\xi_2}\\), respektive. \\(\\upsilon := \\xi_1 + \\xi_2\\) sei die Summe von \\(\\xi_1\\) und \\(\\xi_2\\). Dann ergibt sich eine WDF der Verteilung von \\(\\upsilon\\) als \\[\\begin{equation}\np_\\upsilon(y)\n= \\int_{-\\infty}^\\infty p_{\\xi_1}(y - x_2)p_{\\xi_2}(x_2)\\,dx_2\n= \\int_{-\\infty}^\\infty p_{\\xi_1}(x_1)p_{\\xi_2}(y - x_1)\\,dx_1\n\\end{equation}\\] Die Formel für die WDF \\(p_\\upsilon\\) heißt oder von \\(p_{\\xi_1}\\) und \\(p_{\\xi_2}\\).\n\n\nBeweis. Wir nutzen das multivariate WDF Transformationstheorem für bijektive Abbildungen. Dazu definieren wir zunächst \\[\\begin{equation}\nf : \\mathbb{R}^2 \\to \\mathbb{R}^2, x \\mapsto f(x) :=\n\\begin{pmatrix}\nx_1 + x_2 \\\\\nx_2\n\\end{pmatrix}\n:=\n\\begin{pmatrix}\nz_1 \\\\ z_2\n\\end{pmatrix}\n\\end{equation}\\] Die inverse Funktion von \\(f\\) ist dann gegeben durch \\[\\begin{equation}\nf : \\mathbb{R}^2 \\to \\mathbb{R}^2, z \\mapsto f(z) :=\n\\begin{pmatrix}\nz_1 - x_2 \\\\\nz_2\n\\end{pmatrix}\n\\end{equation}\\] weil dann \\(f \\circ f^{-1} = \\mbox{id}_{\\mathbb{R}^2}\\) gilt, wie man anhand von \\[\\begin{equation}\nf^{-1}\\left(f(x)\\right)\n=\nf^{-1}\n\\begin{pmatrix}\nx_1 + x_2 \\\\\nx_2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nx_1 + x_2 - x_2\\\\\nx_2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nx_1 \\\\\nx_2\n\\end{pmatrix}\n\\end{equation}\\] einsieht. Die Jacobimatrix von \\(f\\) ergibt sich zu \\[\\begin{equation}\nJ^{f}(x) =\n\\begin{pmatrix}\n\\frac{\\partial}{\\partial x_1} f_1(x) &  \\frac{\\partial}{\\partial x_2} f_1(x) \\\\\n\\frac{\\partial}{\\partial x_1} f_2(x) &  \\frac{\\partial}{\\partial x_2} f_2(x) \\\\\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n    \\frac{\\partial}{\\partial x_1} (x_1 + x_2)\n&   \\frac{\\partial}{\\partial x_2} (x_1 + x_2)\n\\\\\n    \\frac{\\partial}{\\partial x_1} x_2\n&   \\frac{\\partial}{\\partial x_2} x_2 \\\\\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 &  1 \\\\\n0 &  1 \\\\\n\\end{pmatrix}\n\\end{equation}\\] und die Jacobideterminante damit zu \\(|J^f(x)| = 1\\). Wir halten weiterhin fest, dass die Unabhängigkeit von \\(\\xi_1\\) und \\(\\xi_2\\) impliziert, dass \\[\\begin{equation}\np_{\\xi_1,\\xi_2}(x_1,x_2) = p_{\\xi_1}(x_1)p_{\\xi_2}(x_2)\n\\end{equation}\\] impliziert. Einsetzen und Integration hinsichtlich \\(x_2\\) ergibt dann ergibt dann für \\(z \\in f(\\mathbb{R}^2)\\) \\[\\begin{align}\n\\begin{split}\np_\\zeta(z)\n& = \\frac{1}{|J^f\\left(f^{-1}(z)\\right)|}p_\\xi\\left(f^{-1}(z)\\right)  \\\\\n& = \\frac{1}{1}p_{\\xi_1,\\xi_2}\\left(z_1 - x_2, x_2\\right)  \\\\\n& = p_{\\xi_1}(z_1 - x_2)p_{\\xi_2}(x_2)\n\\end{split}\n\\end{align}\\] Integration über \\(x_2\\) ergibt dann eine WDF für die marginale Verteilung von \\(\\zeta_1\\) \\[\\begin{align}\n\\begin{split}\np_{\\zeta_1}(z_1)\n& = \\int_{-\\infty}^{\\infty} p_{\\xi_1}(z_1 - x_2)p_{\\xi_2}(x_2)\\,dx_2\n\\end{split}\n\\end{align}\\] Mit \\(\\zeta_1 = \\xi_1 + \\xi_2 = \\upsilon\\) ergibt sich dann die erste Form des Konvlutionstheorems zu \\[\\begin{align}\np_\\upsilon(y)\n& = \\int_{-\\infty}^{\\infty} p_{\\xi_1}(y - x_2)p_{\\xi_2}(x_2)\\,dx_2.\n\\end{align}\\]\n\nWichtige in Kapitel 19 betrachtete Anwendungsfällte sind die Summentransformation und die Mittelwerttransformation.",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Transformationstheoreme</span>"
    ]
  },
  {
    "objectID": "209-Normalverteilungstransformationen.html",
    "href": "209-Normalverteilungstransformationen.html",
    "title": "19  Normalverteilungstransformationen",
    "section": "",
    "text": "19.1 Summentransformation und Mittelwertstransformation\nIn diesem Abschnitt betrachten wir die resultierenden Verteilung bei Summation und Mittelwertbildung von unabhängig und identisch normalverteilten Zufallsvariablen. Speziell besagt das Theorem 19.1 besagt, dass die Summe unabhängig normalverteilter Zufallsvariablen wiederum normalverteilt ist und gibt die Parameter dieser Verteilung an, während Theorem 19.2 besagt, dass das Stichprobenmittel unabhängig normalverteilter Zufallsvariablen wiederum normalverteilt ist und gibt die Parameter dieser Verteilung an.\nEin wichtiger Anwendungsfall von Theorem 19.1 ist das nachfolgende Theorem 19.2 sowie die in Gleichung 17.2 und Gleichung 17.3 erwähnten Generalisierungen der Zentralen Grenzwertsätze. Wir visualisieren Theorem 19.1 exemplarisch in Abbildung 19.1.\nWichtige Anwendungsfälle von Theorem 19.2 sind die Analyse von Erwartungswertschätzern in Kapitel 22 sowie die sowie die in Gleichung 17.2 erwähnte Generalisierung des Zentralen Grenzwertsatzes nach Lindenberg-Lévy. Wir visualisieren Theorem 19.2 exemplarisch in Abbildung 19.2.",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Normalverteilungstransformationen</span>"
    ]
  },
  {
    "objectID": "209-Normalverteilungstransformationen.html#sec-summentransformation-und-mittelwerttransformation",
    "href": "209-Normalverteilungstransformationen.html#sec-summentransformation-und-mittelwerttransformation",
    "title": "19  Normalverteilungstransformationen",
    "section": "",
    "text": "Theorem 19.1 (Summationstransformation) Für \\(i = 1,...,n\\) seien \\(\\xi_i \\sim N(\\mu_i,\\sigma^2_i)\\) unabhängige normalverteilte Zufallsvariablen. Dann gilt für die Summe \\(\\upsilon := \\sum_{i=1}^n \\xi_i\\) , dass \\[\\begin{equation}\n\\upsilon \\sim N\\left(\\sum_{i=1}^n \\mu_i, \\sum_{i=1}^n \\sigma^2_i\\right)\n\\end{equation}\\] Für unabhängige und identisch normalverteilte Zufallsvariablen \\(\\xi_i \\sim N(\\mu,\\sigma^2)\\) gilt folglich \\[\\begin{equation}\n\\upsilon \\sim N(n\\mu, n \\sigma^2).\n\\end{equation}\\]\n\n\nBeweis. Wir skizzieren mithilfe von Theorem 18.6, dass für \\(\\xi_1 \\sim N(\\mu_1,\\sigma^2_1)\\), \\(\\xi_2 \\sim N(\\mu_2,\\sigma^2_2)\\), und \\(\\upsilon := \\xi_1 + \\xi_2\\) gilt, dass \\(\\upsilon \\sim N(\\mu_1 + \\mu_2,\\sigma_1^2 + \\sigma_2^2)\\). Für \\(n &gt; 2\\) folgt das Theorem dann durch Iteration. Mit der Definition der WDF der Normalverteilung erhalten wir zunächst \\[\\begin{align}\n\\begin{split}\np_\\upsilon(y)\n& = \\int_{-\\infty}^\\infty p_{\\xi_1}(x_1)p_{\\xi_2}(y - x_1)\\,dx_1\n\\\\\n& = \\int_{-\\infty}^\\infty\n    \\frac{1}{\\sqrt{2 \\pi} \\sigma_1} \\exp\\left(-\\frac{1}{2}\\left(\\frac{x_1 - \\mu_1}{\\sigma_1}\\right)^2\\right)\n    \\frac{1}{\\sqrt{2 \\pi} \\sigma_2} \\exp\\left(-\\frac{1}{2}\\left(\\frac{y - x_1 - \\mu_2}{\\sigma_2}\\right)^2\\right)\n    \\,dx_1\n\\\\\n& = \\int_{-\\infty}^\\infty\n    \\frac{1}{2 \\pi \\sigma_1\\sigma_2}\\exp\n    \\left(\n    -\\frac{1}{2}\\left(\\frac{x_1 - \\mu_1}{\\sigma_1}\\right)^2\n    -\\frac{1}{2}\\left(\\frac{y - x_1 - \\mu_2}{\\sigma_2}\\right)^2\n    \\right)\n    \\,dx_1 .\n\\\\\n\\end{split}\n\\end{align}\\] Mit einigem algebraischen Aufwand erhält man die Identität \\[\\begin{multline}\n-\\frac{1}{2}\\left(\\frac{x_1 - \\mu_1}{\\sigma_1}\\right)^2\n-\\frac{1}{2}\\left(\\frac{y - x_1 - \\mu_2}{\\sigma_2}\\right)^2\n=\n-\\frac{(y - \\mu_1 - \\mu_2)^2}\n      {2(\\sigma_1^2 + \\sigma_2^2)}\n-\\frac{((\\sigma_1^2 + \\sigma_2^2)x_1 -\\sigma_1^2y + \\mu_2 \\sigma_1^2 - \\mu_1 \\sigma_2^2)^2}\n      {2\\sigma_1^2\\sigma_2^2(\\sigma_1^2 + \\sigma_2^2)},\n\\end{multline}\\] so dass weiterhin gilt, dass \\[\\begin{align}\n\\begin{split}\np_\\upsilon(y)\n& = \\int_{-\\infty}^\\infty\n    \\frac{1}{2 \\pi \\sigma_1\\sigma_2}\n    \\exp\\left(\n    -\\frac{(y - \\mu_1 - \\mu_2)^2}\n      {2(\\sigma_1^2 + \\sigma_2^2)}\n    -\\frac{((\\sigma_1^2 + \\sigma_2^2)x_1 -\\sigma_1^2y + \\mu_2 \\sigma_1^2 - \\mu_1 \\sigma_2^2)^2}\n      {2\\sigma_1^2\\sigma_2^2(\\sigma_1^2 + \\sigma_2^2)}\n    \\right)\n    \\,dx_1\n\\\\\n& = \\int_{-\\infty}^\\infty\n    \\frac{1}{2 \\pi \\sigma_1\\sigma_2}\n    \\exp\\left(\n    -\\frac{(y - \\mu_1 - \\mu_2)^2}\n      {2(\\sigma_1^2 + \\sigma_2^2)}\n    \\right)\n    \\exp\\left(\n    -\\frac{((\\sigma_1^2 + \\sigma_2^2)x_1 -\\sigma_1^2y + \\mu_2 \\sigma_1^2 - \\mu_1 \\sigma_2^2)^2}\n      {2\\sigma_1^2\\sigma_2^2(\\sigma_1^2 + \\sigma_2^2)}\n    \\right)\n    \\,dx_1\n\\\\\n& = \\frac{1}{2 \\pi \\sigma_1\\sigma_2}\n    \\exp\\left(\n    -\\frac{(y - \\mu_1 - \\mu_2)^2}\n      {2(\\sigma_1^2 + \\sigma_2^2)}\n    \\right)\n    \\int_{-\\infty}^\\infty\n    \\exp\\left(\n    -\\frac{((\\sigma_1^2 + \\sigma_2^2)x_1 -\\sigma_1^2y + \\mu_2 \\sigma_1^2 - \\mu_1 \\sigma_2^2)^2}\n      {2\\sigma_1^2\\sigma_2^2(\\sigma_1^2 + \\sigma_2^2)}\n    \\right)\n    \\,dx_1.\n\\end{split}\n\\end{align}\\] Für das verbleibende Integral zeigt man mithilfe der Integration durch Substitution, dass \\[\\begin{equation}\n\\int_{-\\infty}^\\infty\n    \\exp\\left(\n    -\\frac{((\\sigma_1^2 + \\sigma_2^2)x_1 -\\sigma_1^2y + \\mu_2 \\sigma_1^2 - \\mu_1 \\sigma_2^2)^2}\n      {2\\sigma_1^2\\sigma_2^2(\\sigma_1^2 + \\sigma_2^2)}\n    \\right)\n    \\,dx_1\n= \\frac{\\sqrt{2\\pi}\\sigma_1\\sigma_2}{\\sqrt{\\sigma_1^2 + \\sigma_2^2}}.\n\\end{equation}\\] Es ergibt sich also \\[\\begin{align}\n\\begin{split}\np_\\upsilon(y)\n& = \\frac{1}{2 \\pi \\sigma_1\\sigma_2}\n    \\frac{\\sqrt{2\\pi}\\sigma_1\\sigma_2}{\\sqrt{\\sigma_1^2 + \\sigma_2^2}}\n    \\exp\\left(\n    -\\frac{(y - \\mu_1 - \\mu_2)^2}\n      {2(\\sigma_1^2 + \\sigma_2^2)}\n    \\right)\n\\\\\n& = \\frac{(2\\pi)^{-1}(2\\pi)^2}{\\sqrt{\\sigma_1^2 + \\sigma_2^2}}\n    \\exp\\left(\n    -\\frac{(y - \\mu_1 - \\mu_2)^2}\n      {2(\\sigma_1^2 + \\sigma_2^2)}\n    \\right)\n\\\\\n& = \\frac{1}{\\sqrt{2\\pi}\\sqrt{\\sigma_1^2 + \\sigma_2^2}}\n    \\exp\\left(\n    -\\frac{(y - \\mu_1 - \\mu_2)^2}\n      {2(\\sigma_1^2 + \\sigma_2^2)}\n    \\right).\n\\end{split}\n\\end{align}\\] Schließlich folgt, dass \\[\\begin{align}\n\\begin{split}\np_\\upsilon(y)\n& = \\frac{1}{\\sqrt{2\\pi(\\sigma_1^2 + \\sigma_2^2)}}\n    \\exp\\left(-\\frac{1}{2(\\sigma_1^2 + \\sigma_2^2)}\\left(y - (\\mu_1 + \\mu_2)\\right)^2\\right)\n  = N(y; \\mu_1 + \\mu_2, \\sigma_1^2 + \\sigma_2^2)\n\\end{split}\n\\end{align}\\] Ein einfacheres Vorgehen ergibt sich vermutlich nach Fouriertransformation der WDF im Sinne der sogenannten charakteristischen Funktion einer Zufallsvariable. In diesem Fall würde die Faltung der WDFen der Multiplikation der charakteristischen Funktionen entsprechen.\n\n\n\n\n\n\n\n\nAbbildung 19.1: Summation normalverteilter Zufallsvariablen.\n\n\n\n\nTheorem 19.2 (Mittelwertstransformation) Für \\(i = 1,...,n\\) seien \\(\\xi_i \\sim N(\\mu,\\sigma^2)\\) unabhängig und identisch normalverteilte Zufallsvariablen. Dann gilt für das Stichprobenmittel \\(\\bar{\\xi}_n := \\frac{1}{n}\\sum_{i=1}^n \\xi_i\\) , dass \\[\\begin{equation}\n\\bar{\\xi}_n \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right).\n\\end{equation}\\]\n\n\nBeweis. Wir halten zunächst fest, dass mit dem Theorem zur Summe von unabhängig normalverteilten Zufallsvariablen gilt, dass \\(\\bar{\\xi}_n = \\frac{1}{n}\\upsilon\\) mit \\(\\upsilon := \\sum_{i=1}^n \\xi_i \\sim N(n\\mu,n\\sigma^2)\\). Einsetzen in Theorem 18.3 ergibt dann \\[\\begin{align}\n\\begin{split}\np_{\\bar{\\xi}_n}(\\bar{x}_n)\n& = \\frac{1}{|1/n|}N\\left(n\\bar{x}_n; n\\mu , n\\sigma^2 \\right) \\\\\n& = \\frac{n}{\\sqrt{2\\pi n\\sigma^2}}\\exp\\left(-\\frac{1}{2n\\sigma^2}\n\\left(n\\bar{x}_n - n\\mu\\right)^2 \\right) \\\\\n& = \\frac{n}{\\sqrt{2\\pi n\\sigma^2}}\\exp\\left(-\\frac{1}{2n\\sigma^2}\n\\left(n\\bar{x}_n - n\\mu\\right)^2 \\right) \\\\\n& = nn^{-\\frac{1}{2}}\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\n\\exp\\left(\n            -\\frac{(n\\bar{x}_n)^2}{2n\\sigma^2}\n            + \\frac{2(n\\bar{x}_n)(n\\mu)}{2n\\sigma^2}\n            - \\frac{(n\\mu)^2}{2n\\sigma^2}\n         \\right) \\\\\n& = \\sqrt{n}\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\n\\exp\\left(\n            -\\frac{n\\bar{x}_n^2}{2\\sigma^2}\n            + \\frac{2n\\bar{x}_n\\mu}{2\\sigma^2}\n            - \\frac{n\\mu^2}{2\\sigma^2}\n         \\right) \\\\\n& = \\frac{1}{1/\\sqrt{n}}\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\n\\exp\\left(\n            -\\frac{\\bar{x}_n^2}{2(\\sigma^2/n)}\n            + \\frac{2\\bar{x}_n\\mu}{2(\\sigma^2/n)}\n            - \\frac{\\mu^2}{2(\\sigma^2/n)}\n         \\right) \\\\\n& = \\frac{1}{\\sqrt{2\\pi(\\sigma^2/n)}}\n\\exp\\left(-\\frac{1}{2(\\sigma^2/n)}\n            (\\bar{x}_n - \\mu)^2\n         \\right) \\\\\n& = N\\left(\\bar{x}_n;\\mu,\\sigma^2/n \\right)\n\\end{split}\n\\end{align}\\]\n\n\n\n\n\n\n\n\nAbbildung 19.2: Mittelwertbildung bei normalverteilten Zufallsvariablen.",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Normalverteilungstransformationen</span>"
    ]
  },
  {
    "objectID": "209-Normalverteilungstransformationen.html#z-transformation",
    "href": "209-Normalverteilungstransformationen.html#z-transformation",
    "title": "19  Normalverteilungstransformationen",
    "section": "19.2 \\(Z\\)-Transformation",
    "text": "19.2 \\(Z\\)-Transformation\nDas Theorem 19.3 besagt, dass Subtraktion des Erwartungswertparameters und gleichzeitige Division mit der Wurzel des Varianzsparameters die Verteilung einer normalverteilten Zufallsvariable in eine Standardnormalverteilung transformiert.\n\nTheorem 19.3 (\\(Z\\)-Transformation) Es sei \\(\\upsilon \\sim N(\\mu,\\sigma^2)\\) eine normalverteilte Zufallsvariable. Dann ist die Zufallsvariable \\[\\begin{equation}\nZ := \\frac{\\upsilon - \\mu}{\\sigma}\n\\end{equation}\\] eine standardnormalverteilte Zufallsvariable, es gilt also \\(Z \\sim N(0,1)\\).\n\n\nBeweis. Wir nutzen Theorem 18.3. Dazu halten wir zunächst fest, dass die \\(Z\\)-Transformation einer Funktion der Form \\[\\begin{equation}\nf(\\upsilon) := \\frac{\\upsilon - \\mu}{\\sigma} =: Z\n\\end{equation}\\] entspricht. Wir stellen weiterhin fest, dass die Umkehrfunktion von \\(f\\) durch \\[\\begin{equation}\nf^{-1}(Z) := \\sigma Z + \\mu\n\\end{equation}\\] gegeben ist, da für alle \\(z \\in \\mathbb{R}\\) mit \\(z = {y - \\mu}{\\sigma}\\) gilt, dass \\[\\begin{equation}\n\\zeta^{-1}(z)\n= \\zeta^{-1}\\left(\\frac{y - \\mu}{\\sigma}\\right)\n= \\frac{\\sigma(y- \\mu)}{\\sigma} + \\mu\n= y - \\mu + \\mu\n= y.\n\\end{equation}\\] Schließlich stellen wir fest, dass für die Ableitung \\(f'\\) von \\(f\\) gilt, dass \\[\\begin{equation}\nf'(y)\n= \\frac{d}{dy}\\left(\\frac{y - \\mu}{\\sigma} \\right)\n= \\frac{d}{dy}\\left(\\frac{y}{\\sigma} -\\frac{\\mu}{\\sigma} \\right)\n= \\frac{1}{\\sigma}.\n\\end{equation}\\] Einsetzen in das univariate WDF Transformationstheorem für lineare Funktionen ergibt dann \\[\\begin{align}\n\\begin{split}\np_Z(z)\n& = \\frac{1}{|1/\\sigma|}N\\left(\\sigma z + \\mu; \\mu , \\sigma^2 \\right) \\\\\n& = \\frac{1}{1/\\sqrt{\\sigma^2}}\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{1}{2\\sigma^2}\\left(\\sigma z + \\mu - \\mu\\right)^2 \\right) \\\\\n& = \\frac{\\sqrt{\\sigma^2}}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{1}{2\\sigma^2}\\sigma^2 z^2\\right)\\\\\n& = \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2} z^2\\right)\\\\\n& = N(z;0,1)\n\\end{split}\n\\end{align}\\] also, dass \\(Z \\sim N(0,1)\\).\n\nWichtige Anwendungsfälle von Theorem 19.3 sind neben der häufig angewandten Standardisierung von normalverteilten Zufallsvariablen im Sinne der sogenannten \\(Z\\)-Werte (\\(Z\\)-Scores) die \\(Z\\)-Konfidenzintervallstatistik und die \\(Z\\)-Teststatistik. Wir visualisieren Theorem 19.3 exemplarisch in Abbildung 19.3.\n\n\n\n\n\n\nAbbildung 19.3: \\(Z\\)-Transformation normalverteilter Zufallsvariablen.",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Normalverteilungstransformationen</span>"
    ]
  },
  {
    "objectID": "209-Normalverteilungstransformationen.html#sec-chi-quadrat-transformation",
    "href": "209-Normalverteilungstransformationen.html#sec-chi-quadrat-transformation",
    "title": "19  Normalverteilungstransformationen",
    "section": "19.3 \\(\\chi^2\\)-Transformation",
    "text": "19.3 \\(\\chi^2\\)-Transformation\nMit der \\(\\chi^2\\)-Transformation führen wir nun eine erste Transformation unabhängig und identisch normalverteilter Zufallsvariablen ein, die nicht wiederrum auf eine Normalverteilung führt. Speziell besagt Theorem 19.4, dass die Summe quadrierter unabhängiger standardnormalverteilter Zufallsvariablen eine \\(\\chi^2\\)-verteilte Zufallsvariable ist. Dazu erinnern wir zunächst an den Begriff der \\(\\chi^2\\)-Zufallsvariable als Spezialfall der in Kapitel 13 betrachteten Gammazufallsvariablen (vgl. Definition 13.10).\n\nDefinition 19.1 (\\(\\chi^2\\)-Zufallsvariable) \\(U\\) sei eine Zufallsvariable mit Ergebnisraum \\(\\mathbb{R}_{&gt;0}\\) und WDF \\[\\begin{equation}\np : \\mathbb{R}_{&gt;0} \\to \\mathbb{R}_{&gt;0},\nu \\mapsto p(u)\n:= \\frac{1}{\\Gamma\\left(\\frac{n}{2}\\right)2^{\\frac{n}{2}}}\nu^{\\frac{n}{2}-1}\\exp\\left(-\\frac{1}{2}u\\right),\n\\end{equation}\\] wobei \\(\\Gamma\\) die Gammafunktion bezeichne. Dann sagen wir, dass \\(U\\) einer \\(\\chi^2\\)-Verteilung mit Freiheitsgradparameter \\(n\\) unterliegt und nennen \\(U\\) eine \\(\\chi^2\\)-Zufallsvariable mit Freiheitsgradparameter \\(n\\). Wir kürzen dies mit \\(U \\sim \\chi^2(n)\\) ab. Die WDF einer \\(\\chi^2\\)-Zufallsvariable bezeichnen wir mit \\[\\begin{equation}\n\\chi^2(u;n) :=\n\\frac{1}{\\Gamma\\left(\\frac{n}{2}\\right)2^{\\frac{n}{2}}}\nu^{\\frac{n}{2}-1}\\exp\\left(-\\frac{1}{2}u\\right).\n\\end{equation}\\]\n\nWir erinnern daran, dass die WDF der \\(\\chi^2\\)-Verteilung der WDF \\(G\\left(u;\\frac{n}{2},2\\right)\\) einer Gammaverteilung entspricht. In Abbildung 19.4 visualisieren wir exemplarisch einige WDFen von \\(\\chi^2\\)-Zufallsvariablen. Wir beobachten, dass mit ansteigendem \\(n\\) sich \\(\\chi^2(u;n)\\) verbreiter und Wahrscheinlichkeitsmasse zur größeren Werten von \\(u\\) verschoben wird.\n\n\n\n\n\n\nAbbildung 19.4: WDFen von \\(\\chi^2\\) Zufallsvariablen.\n\n\n\n\nTheorem 19.4 (\\(\\chi^2\\)-Transformation) \\(Z_1,...,Z_n \\sim N(0,1)\\) seien unabhängig und identisch standardnormalverteilte Zufallsvariablen. Dann ist die Zufallsvariable \\[\\begin{equation}\nU := \\sum_{i=1}^n Z_i^2\n\\end{equation}\\] eine \\(\\chi^2\\)-verteilte Zufallsvariable mit Freiheitsgradparameter \\(n\\), es gilt also \\(U \\sim \\chi^2(n)\\). Insbesondere gilt für \\(Z \\sim N(0,1)\\) und \\(U := Z^2\\), dass \\(U \\sim \\chi^2(1)\\).\n\n\nBeweis. Wir zeigen das Theorem nur für den Fall \\(n := 1\\) mithilfe von Theorem 18.4. Danach ist die WDF einer Zufallsvariable \\(U := f(Z)\\), welche aus der Transformation einer Zufallsvariable \\(Z\\) mit WDF \\(p_\\zeta\\) durch eine stückweise bijektive Abbildung hervorgeht, gegeben durch \\[\\begin{equation}\\label{eq:piecewise_pdf_transform}\np_U(u) = \\sum_{i=1}^k 1_{\\mathcal{U}_i} \\frac{1}{|f'_i(f_i^{-1}(u))|}p_\\zeta\\left(f_i^{-1} (u)\\right).\n\\end{equation}\\] Wir definieren \\[\\begin{equation}\n\\mathcal{U}_1 := ]-\\infty,0[,\n\\mathcal{U}_2 := ]0,\\infty[, \\mbox{ und }\n\\mathcal{U}_i := \\mathbb{R}_{&gt;0} \\mbox{ für } i = 1,2,\n\\end{equation}\\] sowie \\[\\begin{equation}\nf_i : \\mathcal{Z}_i \\to \\mathcal{U}_i, x \\mapsto f_i(z) := z^2 =: u \\mbox{ für } i = 1,2.\n\\end{equation}\\] Die Ableitung und die Umkehrfunktion der \\(f_i\\) ergeben sich zu \\[\\begin{equation}\nf_i' : \\mathcal{Z}_i \\to \\mathcal{Z}_i, x \\mapsto f_i'(z) = 2z \\mbox{ für } i = 1,2,\n\\end{equation}\\] und \\[\\begin{equation}\nf_1^{-1} : \\mathcal{U}_1 \\to \\mathcal{U}_1, u \\mapsto f_1^{-1}(u) = - \\sqrt{u}\n\\mbox{ und }\nf_2^{-1} : \\mathcal{U}_2 \\to \\mathcal{U}_2, u \\mapsto f_2^{-1}(u) = \\sqrt{u},\n\\end{equation}\\] respektive. Einsetzen in Gleichung \\(\\eqref{eq:piecewise_pdf_transform}\\) ergibt dann \\[\\begin{align}\n\\begin{split}\np_U(u)\n& = 1_{\\mathcal{U}_1}(u) \\frac{1}{|f'_1(f_1^{-1}(u))|}p_\\zeta\\left(f_1^{-1} (u)\\right)\n  + 1_{\\mathcal{U}_2}(u) \\frac{1}{|f'_2(f_2^{-1}(u))|}p_\\zeta\\left(f_2^{-1} (u)\\right) \\\\\n& = \\frac{1}{|2(-\\sqrt{u})|}\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}(-\\sqrt{u})^2\\right)\n  + \\frac{1}{|2( \\sqrt{u})|}\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}( \\sqrt{u})^2\\right) \\\\\n& = \\frac{1}{2\\sqrt{u}}\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}u\\right)\n  + \\frac{1}{2\\sqrt{u}}\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}u\\right)\\\\\n& = \\frac{1}{\\sqrt{2\\pi}}\\frac{1}{\\sqrt{u}}\\exp\\left(-\\frac{1}{2}u\\right).\n\\end{split}\n\\end{align}\\] Andererseits gilt, dass mit \\(\\Gamma\\left(\\frac{1}{2}\\right) = \\sqrt{\\pi}\\), die PDF einer \\(\\chi^2\\)-Zufallsvariable \\(U\\) mit \\(n = 1\\) durch \\[\\begin{equation}\n\\frac{1}{\\Gamma\\left(\\frac{1}{2}\\right)2^{\\frac{1}{2}}} u^{\\frac{1}{2}-1}\\exp\\left(-\\frac{1}{2}u\\right)\n= \\frac{1}{\\sqrt{2\\pi}}\\frac{1}{\\sqrt{u}}\\exp\\left(-\\frac{1}{2}u\\right)\n\\end{equation}\\] gegeben ist. Also gilt, dass wenn \\(Z \\sim N(0,1)\\) ist, dann ist \\(U := Z^2 \\sim \\chi^2(1)\\).\n\nWichtige Anwendungsfälle sind die \\(U\\)-Konfidenzintervallstatistik sowie die im folgenden eingeführten \\(t\\)- und \\(f\\)-Zufallsvariablen. Wir visualisieren Theorem 19.4 exemplarisch in Abbildung 19.5.\n\n\n\n\n\n\nAbbildung 19.5: \\(\\chi^2\\)-Transformation normalverteilter Zufallsvariablen.",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Normalverteilungstransformationen</span>"
    ]
  },
  {
    "objectID": "209-Normalverteilungstransformationen.html#sec-t-transformation",
    "href": "209-Normalverteilungstransformationen.html#sec-t-transformation",
    "title": "19  Normalverteilungstransformationen",
    "section": "19.4 \\(T\\)-Transformation",
    "text": "19.4 \\(T\\)-Transformation\nDas in diesem Abschnitt betrachtete Theorem geht auf Student (1908) zurück und ist das zentrale und stilprägende Resultat der Entwicklung der Frequentistischen Inferenz in der ersten Hälfte der 20. Jahrhunderts. Hald (2007) und Zabell (2008) und geben hierzu einen historischen Überblick. Das zentrale Theorem 19.5 besagt dabei, dass die Zufallsvariable, die sich durch Division einer standardnormalverteilten Zufallsvariable durch die Quadratwurzel einer \\(\\chi^2\\)-verteilten Zufallsvariable geteilt durch ein \\(n\\), ergibt, eine \\(t\\)-verteilte Zufallsvariable ist. Dabei ist eine \\(t\\)-verteilte Zufallsvariable wie folgt definiert.\n\nDefinition 19.2 (\\(t\\)-Zufallsvariable) \\(T\\) sei eine Zufallsvariable mit Ergebnisraum \\(\\mathbb{R}\\) und WDF \\[\\begin{equation}\np : \\mathbb{R} \\to \\mathbb{R}_{&gt;0}, t \\mapsto p(t)\n:= \\frac{\\Gamma\\left(\\frac{n+1}{2}\\right)}{\\sqrt{n\\pi}\\Gamma\\left(\\frac{n}{2}\\right)}\n\\left(1 + \\frac{t^2}{n} \\right)^{-\\frac{n+1}{2}},\n\\end{equation}\\] wobei \\(\\Gamma\\) die Gammafunktion bezeichne. Dann sagen wir, dass \\(T\\) einer \\(t\\)-Verteilung mit Freiheitsgradparameter \\(n\\) unterliegt und nennen \\(T\\) eine \\(t\\)-Zufallsvariable mit Freiheitsgradparameter \\(n\\). Wir kürzen dies mit \\(T \\sim t(n)\\) ab. Die WDF einer \\(t\\)-Zufallsvariable bezeichnen wir mit \\[\\begin{equation}\nT(t;n) := \\frac{\\Gamma\\left(\\frac{n+1}{2}\\right)}{\\sqrt{n\\pi}\\Gamma\\left(\\frac{n}{2}\\right)}\n\\left(1 + \\frac{t^2}{n} \\right)^{-\\frac{n+1}{2}}.\n\\end{equation}\\]\n\nIn Abbildung 19.6 visualisieren wir exemplarisch einige WDFen von \\(t\\)-Zufallsvariablen. Wir beobachten, dass die \\(t\\)-Verteilung immer um \\(0\\) symmetrisch ist und ein steigendes \\(n\\) Wahrscheinlichkeitsmasse aus den Ausläufen zum Zentrum verschiebt. Wir merken an, dass ab etwa \\(n = 30\\) gilt, dass \\(T(t;n) \\approx N(0,1)\\).\n\n\n\n\n\n\nAbbildung 19.6: WDFen von \\(T\\)-Zufallsvariablen.\n\n\n\n\nTheorem 19.5 (\\(T\\)-Transformation) \\(Z \\sim N(0,1)\\) sei eine standarnormalverteilte Zufallsvariable, \\(U \\sim \\chi^2(n)\\) sei eine \\(\\chi^2\\)-Zufallsvariable mit Freiheitsgradparameter \\(n\\), und \\(Z\\) und \\(U\\) seien unabhängig. Dann ist die Zufallsvariable \\[\\begin{equation}\nT := \\frac{Z}{\\sqrt{U/n}}\n\\end{equation}\\] eine \\(t\\)-verteilte Zufallsvariable mit Freiheitsgradparameter \\(n\\), es gilt also \\(T \\sim t(n)\\).\n\n\nBeweis. Wir halten zunächst fest, dass die zweidimensionale WDF der gemeinsamen (unabhängigen) Verteilung von \\(Z\\) und \\(U\\) durch \\[\\begin{equation}\np_{Z,U}(z,u)\n=\n\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}z^2\\right)\n\\frac{1}{\\Gamma(\\frac{n}{2})2^{\\frac{n}{2}}}u^{\\frac{n}{2}-1} \\exp\\left(-\\frac{1}{2}u\\right).\n\\end{equation}\\] gegeben ist. Wir betrachten dann die multivariate vektorwertige Abbildung \\[\\begin{equation}\nf : \\mathbb{R}^2 \\to \\mathbb{R}^2,\n(z,u)\n\\mapsto\nf(z,u)\n:=\n\\left(\\frac{z}{\\sqrt{u/n}},u\\right)\n=:\n(t,w)\n\\end{equation}\\] und benutzen das multivariate WDF Transformationstheorem für bijektive Abbildungen um die WDF von \\((t,w)\\) herzuleiten. Dazu erinnern wir uns, dass wenn \\(\\xi\\) ein \\(n\\)-dimensionaler Zufallsvektor mit WDF \\(p_\\xi\\) und \\(\\upsilon := f(\\xi)\\) für eine differenzierbare und bijektive Abbildung \\(f : \\mathbb{R}^n \\to \\mathbb{R}^n\\) ist, die WDF des Zufallsvektors \\(\\upsilon\\) durch \\[\\begin{equation}\\label{eq:pdftmv}\np_\\upsilon : \\mathbb{R}^n \\to \\mathbb{R}_{\\ge 0},\ny \\mapsto p_\\upsilon(y) :=\n\\frac{1}{|J^f\\left(f^{-1}(y)\\right)|}p_\\xi\\left(f^{-1}(y)\\right)\n\\end{equation}\\] gegeben ist. Für die im vorliegenden Fall betrachtete Abbildung halten wir zunächst fest, dass \\[\\begin{equation}\nf^{-1}:\\mathbb{R}^2 \\to \\mathbb{R}^2,\n(t,w)\n\\mapsto\nf^{-1}\n(t,w)\n:=\\left(\\sqrt{w/n}t, w\\right).\n\\end{equation}\\] Dies ergibt sich direkt aus \\[\\begin{equation}\nf^{-1}(f(z,u))\n=\nf^{-1}\\left(\\frac{z}{\\sqrt{u/n}},u\\right)\n=\n\\left(\\frac{\\sqrt{u/n}z}{\\sqrt{u/n}}, u \\right)\n=\n(z,u)\n\\mbox{ für alle }\n(z,u)\n\\in \\mathbb{R}^2.\n\\end{equation}\\] Wir halten dann fest, dass die Determinante der Jacobi-Matrix von \\(f\\) an der Stelle \\((z,u)\\) durch \\[\\begin{equation}\n|J^f(z,u)|\n=\n\\begin{vmatrix}\n  \\frac{\\partial}{\\partial z} \\left(\\frac{z}{\\sqrt{u/n}}\\right)\n& \\frac{\\partial}{\\partial u} \\left(\\frac{z}{\\sqrt{u/n}}\\right) \\\\\n  \\frac{\\partial}{\\partial z} u\n& \\frac{\\partial}{\\partial u} u\\\\\n\\end{vmatrix}\n= \\left(\\frac{v}{n}\\right)^{-1/2},\n\\end{equation}\\] gegeben ist, sodass folgt, dass \\[\\begin{equation}\n\\frac{1}{|J^f\\left(f^{-1}(z,u)\\right)|}\n= \\left(\\frac{w}{n}\\right)^{1/2}.\n\\end{equation}\\] Einsetzen in Gleichung \\(\\eqref{eq:pdftmv}\\) ergibt dann \\[\\begin{equation}\np_{T,W}(t,w) = \\left(\\frac{w}{n}\\right)^{1/2}p_{Z,V}\\left(\\sqrt{w/n}t,w\\right),\n\\end{equation}\\] Es folgt also \\[\\begin{align}\n\\begin{split}\np_T(t)\n& =\n\\int_0^\\infty  p_{T,W}(t,w)\n\\,dw                                                    \\\\\n& =\n\\int_0^\\infty\n\\left(\\frac{w}{n}\\right)^{1/2}\np_{Z,V}\\left(\\sqrt{w/n}t,w\\right)\n\\,dw  \\\\\n& =\n\\int_0^\\infty\n\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}(\\sqrt{w/n}t)^2\\right)\n\\frac{1}{\\Gamma(\\frac{n}{2})2^{\\frac{n}{2}}}w^{\\frac{n}{2}-1} \\exp\\left(-\\frac{1}{2}w\\right)\n\\left(\\frac{w}{n}\\right)^{1/2}\n\\,dw \\\\\n& =\n\\frac{1}{\\sqrt{2\\pi}}\\frac{1}{\\Gamma(\\frac{n}{2})2^{\\frac{n}{2}}n^{\\frac{1}{2}}}\n\\int_0^\\infty\n\\exp\\left(-\\frac{1}{2}\\frac{w}{n}t^2\\right)\nw^{\\frac{n}{2}-1} \\exp\\left(-\\frac{1}{2}w\\right)w^{1/2}\n\\,dw \\\\\n& =\n\\frac{1}{\\sqrt{2\\pi}}\\frac{1}{\\Gamma(\\frac{n}{2})2^{\\frac{n}{2}}n^{\\frac{1}{2}}}\n\\int_0^\\infty\n\\exp\\left(-\\frac{1}{2}\\frac{w}{n}t^2 -\\frac{1}{2}w\\right)\nw^{\\frac{n}{2}-1} w^{\\frac{1}{2}}\n\\,dw \\\\\n& =\n\\frac{1}{\\sqrt{2\\pi}}\\frac{1}{\\Gamma(\\frac{n}{2})2^{\\frac{n}{2}}n^{\\frac{1}{2}}}\n\\int_0^\\infty\n\\exp\\left(-\\frac{1}{2}\\left(\\frac{w}{n}t^2 + w\\right)\\right)\nw^{\\frac{n + 1}{2}-1}\n\\,dw \\\\\n& =\n\\frac{1}{\\sqrt{2\\pi}}\\frac{1}{\\Gamma(\\frac{n}{2})2^{\\frac{n}{2}}n^{\\frac{1}{2}}}\n\\int_0^\\infty\n\\exp\\left(-\\frac{1}{2}\\left(1 + \\frac{t^2}{n}\\right)\\right)\nw^{\\frac{n + 1}{2}-1}\n\\,dw \\\\\n\\end{split}\n\\end{align}\\] Wir stellen dann fest, dass der Integrand auf der linken Seite der obigen Gleichung dem Kern einer Gamma WDF mit Parametern \\(\\alpha = \\frac{n+1}{2}\\) und \\(\\beta = \\frac{2}{1+\\frac{t^2}{n}}\\) entspricht, wie man leicht einsieht: \\[\\begin{align*}\n\\Gamma(w;\\alpha,\\beta)\n= \\frac{1}{\\Gamma(\\alpha)\\beta^{\\alpha}}w^{\\alpha-1}\\exp\\left(-\\frac{w}{\\beta}\\right) & \\\\\n\\Rightarrow\n\\Gamma\\left(w;\\frac{n+1}{2},\\frac{2}{1+\\frac{t^2}{n}}\\right)\n& = \\frac{1}{\\Gamma(\\frac{n+1}{2})\\left(\\frac{2}{1+\\frac{t^2}{n}}\\right)^{\\frac{n+1}{2}}}\nw^{\\frac{n+1}{2}-1}\\exp\\left(-\\frac{w}{\\frac{2}{1+\\frac{t^2}{n}}}\\right) \\\\\n& = \\frac{1}{\\Gamma( \\frac{n+1}{2})\\left(\\frac{2}{1+\\frac{t^2}{n}}\\right)^{ \\frac{n+1}{2}}}\n\\exp\\left(-\\frac{1}{2}\\left(1 + \\frac{t^2}{n}\\right)\\right) w^{\\frac{n+1}{2}-1}.\n\\end{align*}\\] Es ergibt sich also \\[\\begin{equation}\np_T(t)\n=\n\\frac{1}{\\sqrt{2\\pi}}\\frac{1}{\\Gamma(\\frac{n}{2})2^{\\frac{n}{2}}n^{\\frac{1}{2}}}\n\\int_0^\\infty\n\\Gamma\\left(w;\\frac{n+1}{2},\\frac{2}{1+\\frac{t^2}{n}}\\right)\n\\,dw .\n\\end{equation}\\] Schließlich stellen wir fest, dass der Integralterm in obiger Gleichung dem Normalisierungsterm einer Gamma WDF entspricht. Abschließend ergibt sich also \\[\\begin{equation}\np_T(t) =\n\\frac{1}{\\sqrt{2\\pi}}\\frac{1}{\\Gamma(\\frac{n}{2})2^{\\frac{n}{2}}n^{\\frac{1}{2}}}\n\\Gamma\\left(\\frac{n+1}{2}\\right)\\left(\\frac{2}{1 + \\frac{t^2}{n}} \\right)^{\\frac{n+1}{2}}.\n\\end{equation}\\] Die Verteilung von \\(Z/\\sqrt{U/n}\\) hat also die WDF einer \\(t\\)-Zufallsvariable.\n\nWichtige Anwendungsfälle sind die \\(T\\)-Konfidenzintervallstatistik sowie die \\(T\\)-Teststatistiken der Theorie von Hypothesentests im Kontext des Allgemeinen Linearen Modells. Wir visualisieren Theorem 19.5 exemplarisch in Abbildung 19.7.\n\n\n\n\n\n\nAbbildung 19.7: \\(T\\)-Transformation normalverteilter Zufallsvariablen.",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Normalverteilungstransformationen</span>"
    ]
  },
  {
    "objectID": "209-Normalverteilungstransformationen.html#sec-nichtzentrale-t-transformation",
    "href": "209-Normalverteilungstransformationen.html#sec-nichtzentrale-t-transformation",
    "title": "19  Normalverteilungstransformationen",
    "section": "19.5 Nichtzentrale \\(T\\)-Transformation",
    "text": "19.5 Nichtzentrale \\(T\\)-Transformation\nIn diesem Abschnitt betrachten wir den Fall, dass der Erwartungswertparameter der Zählervariable der in Theorem 19.5 betrachteten Zufallsvariable \\(T\\) von Null verschieden ist, dass es sich bei der Zählervariable also nicht um eine nach \\(N(0,1)\\), sondern eine nach \\(N(\\mu,1)\\) verteilte Zufallsvariable für ein beliebiges \\(\\mu \\in \\mathbb{R}\\) handelt. Die so entstehende Zufallsvariable \\(T\\) folgt dann einer sogenannten nichtzentralen-t-Verteilung. Eine frühe ausführliche Diskussion dieser Verteilung findet sich zum Beispiel in Johnson & Welch (1940). Eine entsprechende nichtzentralen-\\(t\\)-verteilte Zufallsvariable ist wie folgt definiert (vgl. Lehmann (1986)).",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Normalverteilungstransformationen</span>"
    ]
  },
  {
    "objectID": "209-Normalverteilungstransformationen.html#nichtzentrale-t-zufallsvariable",
    "href": "209-Normalverteilungstransformationen.html#nichtzentrale-t-zufallsvariable",
    "title": "19  Normalverteilungstransformationen",
    "section": "19.6 Nichtzentrale \\(t\\)-Zufallsvariable",
    "text": "19.6 Nichtzentrale \\(t\\)-Zufallsvariable\n\\(T\\) sei eine Zufallsvariable mit Ergebnisraum \\(\\mathbb{R}\\) und WDF \\[\\begin{multline}\np : \\mathbb{R} \\to \\mathbb{R}_{&gt;0}, t \\mapsto p(t) :=\n\\frac{1}{2^{\\frac{n-1}{2}}\\Gamma\\left(\\frac{n}{2} \\right)(n \\pi)^{\\frac{1}{2}}} \\\\\n\\times \\int_{0}^\\infty \\tau^{\\frac{n-1}{2}} \\exp\\left(-\\frac{\\tau}{2}\\right)\n\\exp\\left(-\\frac{1}{2}\\left(t \\left(\\frac{\\tau}{n}\\right)^{\\frac{1}{2}} - \\delta \\right)^2 \\right)\\,d\\tau.\n\\end{multline}\\] Dann sagen wir, dass \\(T\\) einer nichtzentralen \\(t\\)-Verteilung mit Nichtzentralitätsparameter \\(\\delta\\) und Freiheitsgradparameter \\(n\\) unterliegt und nennen \\(T\\) eine nichtzentrale \\(t\\)-Zufallsvariable mit Nichtzentralitätsparameter \\(\\delta\\) und Freiheitsgradparameter \\(n\\). Wir kürzen dies mit \\(t(\\delta, n)\\) ab. Die WDF einer nichtzentralen \\(t\\)-Zufallsvariable bezeichnen wir mit \\(t(T;\\delta,n)\\). Die KVF und inverse KVF einer nichtzentralen \\(t\\)-Zufallsvariable bezeichnen wir mit \\(\\Psi(\\cdot; \\delta, n)\\) und \\(\\Psi^{-1}(\\cdot; \\delta, n)\\), respektive.",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Normalverteilungstransformationen</span>"
    ]
  },
  {
    "objectID": "209-Normalverteilungstransformationen.html#f-transformation",
    "href": "209-Normalverteilungstransformationen.html#f-transformation",
    "title": "19  Normalverteilungstransformationen",
    "section": "19.7 \\(F\\)-Transformation",
    "text": "19.7 \\(F\\)-Transformation\nDas in diesem Abschnitt zentrale Theorem 19.7 besagt, dass die Zufallsvariable, die sich durch Division zweier \\(\\chi^2\\) verteilter Zufallsvariablen, jeweils geteilt durch ihre jeweiligen Freiheitsgradparameter, eine \\(F\\)-verteilte Zufallsvariable ist. Dabei ist eine \\(F\\)-verteilte Zufallsvariable wie folgt definiert.\n\nDefinition 19.3 (\\(f\\)-Zufallsvariable) \\(F\\) sei eine Zufallsvariable mit Ergebnisraum \\(\\mathbb{R}_{&gt;0}\\) und WDF \\[\\begin{equation}\np_F : \\mathbb{R} \\to \\mathbb{R}_{&gt;0}, f \\mapsto p_F(f)\n:= m^{\\frac{m}{2}}n^{\\frac{n}{2}}\n   \\frac{\\Gamma\\left(\\frac{m+n}{2}\\right)}{\\Gamma\\left(\\frac{m}{2}\\right)\\Gamma\\left(\\frac{n}{2}\\right)}\n   \\frac{f^{\\frac{m}{2}-1}}{\\left(1 + \\frac{m}{n}f \\right)^{\\frac{m+n}{2}}},\n\\end{equation}\\] wobei \\(\\Gamma\\) die Gammafunktion bezeichne. Dann sagen wir, dass \\(F\\) einer \\(f\\)-Verteilung mit Freiheitsgradparametern \\(n,m\\) unterliegt und nennen \\(F\\) eine \\(f\\)-Zufallsvariable mit Freiheitsgradparametern \\(n,m\\). Wir kürzen dies mit \\(F \\sim f(n,m)\\) ab. Die WDF einer \\(f\\)-Zufallsvariable bezeichnen wir mit \\[\\begin{equation}\nF(f;n,m)\n:= m^{\\frac{m}{2}}n^{\\frac{n}{2}}\n   \\frac{\\Gamma\\left(\\frac{m+n}{2}\\right)}{\\Gamma\\left(\\frac{m}{2}\\right)\\Gamma\\left(\\frac{n}{2}\\right)}\n   \\frac{f^{\\frac{m}{2}-1}}{\\left(1 + \\frac{m}{n}f \\right)^{\\frac{m+n}{2}}}.\n\\end{equation}\\]\n\nIn Abbildung 19.10 visualisieren wir exemplarisch einige WDFen von \\(f\\)-Zufallsvariablen. Wir beobachten, dass die Form der WDFen zunächt primär durch den Freiheitsgradparameter \\(n\\) und dann sekundär durch den Freiheitsgradparameter \\(m\\) bestimmt werden.\n\n\n\n\n\n\nAbbildung 19.10: WDFen von \\(f\\)-verteilten Zufallsvariablen.\n\n\n\n\nTheorem 19.7 (\\(F\\)-Transformation) \\(V \\sim \\chi^2(n)\\) und \\(W \\sim \\chi^2(m)\\) seien zwei unabhängige \\(\\chi^2\\)-Zufallfsvariablen mit Freiheitsgradparametern \\(n\\) und \\(m\\), respektive. Dann ist die Zufallsvariable \\[\\begin{equation}\nF := \\frac{V/n}{W/m}\n\\end{equation}\\] eine \\(f\\)-verteilte Zufallsvariable mit Freiheitsgradparametern \\(n,m\\), es gilt also \\(F \\sim f(n,m)\\).\n\nDas Theorem kann bewiesen werden, in dem man zunächst ein Transformationstheorem für Quotienten von Zufallsvariablen mithilfe von Theorem 18.1 und Marginalisierung herleitet und dieses Theorem dann auf die WDF von \\(\\chi^2\\)-verteilten Zufallsvariablen anwendet. Wir visualisieren Theorem 19.7 exemplarisch in Abbildung 19.11. Wichtige Anwendungsfälle von Theorem 19.7 sind die im Rahmen der Theorie des Allgemeinen Linearen Modells betrachteten \\(F\\)-Statistiken.\n\n\n\n\n\n\nAbbildung 19.11: \\(F\\)-Transformation normalverteilter Zufallsvariablen.",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Normalverteilungstransformationen</span>"
    ]
  },
  {
    "objectID": "209-Normalverteilungstransformationen.html#selbstkontrollfragen",
    "href": "209-Normalverteilungstransformationen.html#selbstkontrollfragen",
    "title": "19  Normalverteilungstransformationen",
    "section": "19.8 Selbstkontrollfragen",
    "text": "19.8 Selbstkontrollfragen\n\nErläutern Sie die Bedeutung der in diesem Abschnitt betrachteten Transformationen von normalverteilten Zufallsvariablen für die Frequentistische Inferenz.\nGeben Sie das Theorem zur Summentransformation wieder.\nGeben Sie das Theorem zur Mittelwerttransformation wieder.\nGeben Sie das Theorem zur \\(Z\\)-Transformation wieder.\nGeben Sie das Theorem zur \\(\\chi^2\\)-Transformation wieder.\nBeschreiben Sie die WDF der \\(t\\)-Verteilung in Abhängigkeit ihrer Freiheitsgradparameter.\nGeben Sie das Theorem zur \\(T\\)-Transformation wieder.\nGeben Sie das Theorem zur \\(F\\)-Transformation wieder.\n\n\n\n\n\nHald, A. (2007). A History of Parametric Statistical Inference from Bernoulli to Fisher, 1713-1935. Springer.\n\n\nJohnson, N. L., & Welch, B. L. (1940). Applications of the Non-Central t-Distribution. Biometrika, 31(3/4), 362. https://doi.org/10.2307/2332616\n\n\nLehmann, E. L. (1986). Testing Statistical Hypotheses. Wiley Series in Probability and Statistics.\n\n\nStudent. (1908). The Probable Error of a Mean. Biometrika, 6(1), 1–25.\n\n\nZabell, S. L. (2008). On Student’s 1908 Article „The Probable Error of a Mean“. Journal of the American Statistical Association, 103(481), 1–7. https://doi.org/10.1198/016214508000000030",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Normalverteilungstransformationen</span>"
    ]
  },
  {
    "objectID": "210-Multivariate-Normalverteilungen.html",
    "href": "210-Multivariate-Normalverteilungen.html",
    "title": "20  Multivariate Normalverteilungen",
    "section": "",
    "text": "20.1 Konstruktion\nIn diesem Abschnitt wollen wir zeigen, wie ein bivariat normalverteilter Zufallsvektor durch Transformation und Konkatenation zweier univariat normalverteilter Zufallsvariablen konstruiert werden kann. Dazu erinnern wir zunächst an den Begriff der normalverteilten Zufallsvariable\nVisuell entspricht der Parameter \\(\\mu\\) einer normalverteilten Zufallsvariable dem Wert höchster Wahrscheinlichkeitsdichte und der Parameter \\(\\sigma^2\\) spezifiziert die Breite der WDF (Abbildung 20.1). Weiterhin gelten für den Erwartungswert und die Varianz einer normalverteilten Zufallsvariable bekanntlich \\[\\begin{equation}\n\\mathbb{E}(\\xi) = \\mu \\mbox{ und } \\mathbb{V}(\\xi) = \\sigma^2.\n\\end{equation}\\] Eine normalverteilte Zufallsvariable der Form \\(\\xi \\sim N(0,1)\\) schließlich heißt auch standardnormalverteilt.\nFolgendes Theorem zeigt, wie zwei unabhängige, univariat standardnormalverteilte Zufallsvariablen kombiniert werden können, um einen bivariat verteilten Zufallsvektor zu konstruieren. Die Verteilung eines ebensolchen Zufallsvektors wird dann als bivariate Normalverteilung bezeichnet.\nFür einen Beweis des Theorems verweisen wir auf DeGroot & Schervish (2012).\nBeispiel\nFolgender R Code zeichnet das obige Theorem anhand konkreter Beispielwerte für \\(\\mu_1,\\mu_2,\\sigma_1,\\sigma_2\\) und \\(\\rho\\) nach und gibt die Parameter \\(\\mu\\) und \\(\\Sigma\\) der resultierenden bivariaten WDF aus.\n# Parameterdefinitionen\nmu_1   = 5.0                                              # \\mu_1\nmu_2   = 4.0                                              # \\mu_2\nsig_1  = 1.5                                              # \\sigma_1\nsig_2  = 1.0                                              # \\sigma_2\nrho    = 0.9                                              # \\rho\n\n# Realisierungen der standardnormalverteilten ZVen\nn      = 100                                              # Anzahl Realisierungen\nzeta_1 = rnorm(n)                                         # \\zeta_1 \\sim N(0,1)\nzeta_2 = rnorm(n)                                         # \\zeta_1 \\sim N(0,1)\n\n# Evaluation von Realisierungen von \\xi_1 und \\xi_2\nxi_1   = sig_1*zeta_1 + mu_1                              # Realsierungen von zeta_1\nxi_2   = sig_2*(rho*zeta_1 + sqrt(1-rho^2)*zeta_2) + mu_2 # Realsierungen von zeta_2\n\n# Parameter der gemeinsamen Verteilung von \\xi_1 und \\xi_2\nmu     = matrix(c(mu_1,                                   # \\mu \\in \\mathbb{R}^2\n                  mu_2),\n                 nrow = 2, byrow = TRUE)\nSigma  = matrix(c(sig_1^2        , rho*sig_1*sig_2,       # \\Sigma \\in \\mathbb{R}^{2 x 2}\n                 rho*sig_1*sig_2, sig_2^2),\n                 nrow = 2, byrow = TRUE)\nprint(mu)\n\n     [,1]\n[1,]    5\n[2,]    4\n\nprint(Sigma)\n\n     [,1] [,2]\n[1,] 2.25 1.35\n[2,] 1.35 1.00\nDie durch obigen R Code generierten Realisierungen von \\(\\xi = (\\xi_1,\\xi_2)^T\\), sowie die Isokonturen der durch Theorem 20.1 postulierten WDF sind in Abbildung 20.2 dargestellt.",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Multivariate Normalverteilungen</span>"
    ]
  },
  {
    "objectID": "210-Multivariate-Normalverteilungen.html#konstruktion",
    "href": "210-Multivariate-Normalverteilungen.html#konstruktion",
    "title": "20  Multivariate Normalverteilungen",
    "section": "",
    "text": "Definition 20.1 (Normalverteilte Zufallsvariable) \\(\\xi\\) sei eine Zufallsvariable mit Ergebnisraum \\(\\mathbb{R}\\) und WDF \\[\\begin{equation}\np : \\mathbb{R} \\to \\mathbb{R}_{&gt;0}, x\\mapsto p(x)\n:= \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left(-\\frac{1}{2\\sigma^2}(x - \\mu)^2\\right).\n\\end{equation}\\] Dann sagen wir, dass \\(\\xi\\) einer Normalverteilung (oder Gauß-Verteilung) mit Erwartungswertparameter \\(\\mu \\in \\mathbb{R}\\) und Varianzparameter \\(\\sigma^2 &gt; 0\\) unterliegt und nennen \\(\\xi\\) eine normalverteilte Zufallsvariable. Wir kürzen dies mit \\(\\xi \\sim N(\\mu,\\sigma^2)\\) ab. Die WDF einer normalverteilten Zufallsvariable bezeichnen wir mit \\[\\begin{equation}\nN\\left(x;\\mu,\\sigma^2\\right) := \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left(-\\frac{1}{2\\sigma^2}(x - \\mu)^2\\right).\n\\end{equation}\\]\n\n\n\n\n\n\n\nAbbildung 20.1: Wahrscheinlichkeitsdichtefunktionen univariater Normalverteilungen.\n\n\n\n\n\n\nTheorem 20.1 (Konstruktion bivariater Normalverteilungen) \\(\\zeta_1 \\sim N(0,1)\\) und \\(\\zeta_2 \\sim N(0,1)\\) seien zwei unabhängige standardnormalverteilte Zufallsvariablen. Weiterhin seien \\(\\mu_1,\\mu_2\\in \\mathbb{R}\\), \\(\\sigma_1,\\sigma_2&gt;0\\) und \\(\\rho \\in ]-1,1[\\). Schließlich seien \\[\\begin{align}\n\\begin{split}\n\\xi_1 & := \\sigma_1\\zeta_1 + \\mu_1                                            \\\\\n\\xi_2 & := \\sigma_2\\left(\\rho\\zeta_1 + (1 -\\rho^2)^{1/2}\\zeta_2\\right) + \\mu_2.\n\\end{split}\n\\end{align}\\] Dann hat die WDF des Zufallsvektors \\(\\xi := (\\xi_1,\\xi_2)^T\\), also der gemeinsamen Verteilung von \\(\\xi_1\\) und \\(\\xi_2\\), die Form \\[\\begin{equation}\np : \\mathbb{R}^2 \\to \\mathbb{R}_{&gt;0},\\, x \\mapsto p(x) := (2\\pi)^{-\\frac{n}{2}}|\\Sigma|^{-\\frac{1}{2}}\\exp\\left(-\\frac{1}{2}(x-\\mu)^T \\Sigma^{-1} (x-\\mu)\\right),\n\\end{equation}\\] wobei \\(n:=2\\) und \\(\\mu \\in \\mathbb{R}^{2}\\) und \\(\\Sigma \\in \\mathbb{R}^{2\\times 2}\\) durch \\[\\begin{equation}\n\\mu =\n\\begin{pmatrix}\n\\mu_1 \\\\\n\\mu_2\n\\end{pmatrix}\n\\mbox{ und }\n\\Sigma =\n\\begin{pmatrix}\n\\sigma_1^2           & \\rho\\sigma_1\\sigma_2 \\\\\n\\rho\\sigma_2\\sigma_1 & \\sigma_2^2           \\\\\n\\end{pmatrix}\n\\end{equation}\\] gegeben sind.\n\n\n\n\n\n\n\n\n\n\n\n\nAbbildung 20.2: Konstruktion bivariater Normalverteilungen.\n\n\n\n\n\n\n\n\n\nAbbildung 20.3: Wahrscheinlichkeitsdichtefunktionen bivariater Normalverteilungen.",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Multivariate Normalverteilungen</span>"
    ]
  },
  {
    "objectID": "210-Multivariate-Normalverteilungen.html#definition",
    "href": "210-Multivariate-Normalverteilungen.html#definition",
    "title": "20  Multivariate Normalverteilungen",
    "section": "20.2 Definition",
    "text": "20.2 Definition\nWir wollen die multivariate Normalverteilung nun formal einführen und erste Eigenschaften angeben. Wir nutzen dazu folgende Definition.\n\nDefinition 20.2 \\(\\xi\\) sei ein \\(n\\)-dimensionaler Zufallsvektor mit Ergebnisraum \\(\\mathbb{R}^n\\) und WDF \\[\\begin{equation}\np : \\mathbb{R}^n \\to \\mathbb{R}_{&gt;0},\\, x \\mapsto p(x) := (2\\pi)^{-\\frac{n}{2}}|\\Sigma|^{-\\frac{1}{2}}\\exp\\left(-\\frac{1}{2}(x-\\mu)^T \\Sigma^{-1} (x-\\mu)\\right).\n\\end{equation}\\] Dann sagen wird, dass \\(\\xi\\) einer multivariaten (oder \\(n\\)-dimensionalen) Normalverteilung mit Erwartungswertparameter \\(\\mu \\in \\mathbb{R}^n\\) und positiv-definitem Kovarianzmatrixparameter \\(\\Sigma \\in \\mathbb{R}^{n \\times n}\\) unterliegt und nennen \\(\\xi\\) einen (multivariat) normalverteilten Zufallsvektor. Wir kürzen dies mit \\(\\xi \\sim N(\\mu,\\Sigma)\\) ab. Die WDF eines multivariat normalverteilten Zufallsvektors bezeichnen wir mit \\[\\begin{equation}\nN(x;\\mu,\\Sigma):= (2\\pi)^{-\\frac{n}{2}}|\\Sigma|^{-\\frac{1}{2}}\\exp\\left(-\\frac{1}{2}(x-\\mu)^T \\Sigma^{-1} (x-\\mu)\\right).\n\\end{equation}\\]\n\nAbbildung 20.3 A,B,C zeigen die Isokonturen der WDFen bivariat normlaverteilter Zufallsvektoren für \\(\\mu = (1,1)^T\\) und \\[\\begin{equation}\n\\Sigma_A := \\begin{pmatrix} 0.20 &  0.15  \\\\   0.15 & 0.20 \\end{pmatrix}, \\quad\n\\Sigma_B := \\begin{pmatrix} 0.20 &  0.00  \\\\   0.00 & 0.20 \\end{pmatrix}, \\quad\n\\Sigma_C := \\begin{pmatrix} 0.20 &  -0.15 \\\\  -0.15 & 0.20 \\end{pmatrix}\n\\end{equation}\\] respektive. Abbildung 20.4 A,B und C zeigen jeweils 200 Realisierungen der entsprechenden Zufallsvektoren.\n\n\n\n\n\n\nAbbildung 20.4: Realisierungen bivariat normalverteilter Zufallsvektoren.\n\n\n\nOhne Beweis halten wir fest, dass wie im Fall einer univariat normalverteilten Zufallsvariable, der Erwartungswert und die Kovarianzmatrix eines normalverteilten Zufallsvektors durch die entsprechenden Parameter gegeben sind.\n\nTheorem 20.2 (Erwartungswert und Kovarianzmatrix normalverteilter Zufallsvektoren) \\(\\xi \\sim N(\\mu,\\Sigma)\\) sei ein multivariat normalverteilter Zufallsvektor mit Erwartungswertparameter \\(\\mu \\in \\mathbb{R}^n\\) und Kovarianzmatrixparameter \\(\\Sigma \\in \\mathbb{R}^{n \\times n} \\mbox{ pd}\\). Dann gelten \\[\\begin{equation}\n\\mathbb{E}(\\xi) = \\mu \\mbox{ und } \\mathbb{C}(\\xi) = \\Sigma.\n\\end{equation}\\]\n\nWie im Falle der univariat normalverteilten Zufallsvariable entspricht der Parameter \\(\\mu \\in \\mathbb{R}^n\\) dem Wert höchster Wahrscheinlichkeitsdichte der multivariaten Normalverteilung. Analog zum Varianzparameter der univariat normalverteilten Zufallsvariable spezifizieren die Diagonalelemente von \\(\\Sigma \\in \\mathbb{R}^{n \\times n} \\mbox{ pd}\\) die Breite der WDF bezüglich der Zufallsvektorkomponenten \\(\\xi_1,...,\\xi_n\\). Allgemein spezifiziert im Falle des multivariat normalverteilten Zufallsvektors das \\(i,j\\)te Element von \\(\\Sigma \\in \\mathbb{R}^{n \\times n} \\mbox{ pd}\\) hier nun die Kovarianz der Zufallsvektorkomponenten \\(\\xi_i\\) und \\(\\xi_j\\).",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Multivariate Normalverteilungen</span>"
    ]
  },
  {
    "objectID": "210-Multivariate-Normalverteilungen.html#transformationen",
    "href": "210-Multivariate-Normalverteilungen.html#transformationen",
    "title": "20  Multivariate Normalverteilungen",
    "section": "20.3 Transformationen",
    "text": "20.3 Transformationen\nIn diesem Abschnitt stellen wir einige Resultate zu den Verteilungen transformierter normalverteilter Zufallsvektoren zusammen. Wir verzichten dabei auf Beweise.\n\nTheorem 20.3 (Invertierbare lineare Transformation eines normalverteilten Zufallsvektors) \\(\\xi \\sim N(\\mu_\\xi,\\Sigma_\\xi)\\) sei ein normalverteilter \\(n\\)-dimensionaler Zufallsvektor und es sei \\(\\zeta := A\\xi\\) mit einer invertierbaren Matrix \\(A \\in \\mathbb{R}^{n \\times n}\\). Dann gilt \\[\\begin{equation}\n\\zeta \\sim N\\left(\\mu_\\zeta, \\Sigma_\\zeta\\right)\n\\mbox{ mit }\n\\mu_\\zeta = A\\mu_\\xi \\mbox{ und }\n\\Sigma_\\zeta = A\\Sigma_\\xi A^T.\n\\end{equation}\\]\n\nNach Theorem 20.3 ergibt die invertierbare lineare Transformation eines multivariat normalverteilten Zuallsvektors also wiederum einen multivariat normalverteilten Zufallsvektor und die Parameter der Verteilung dieses normalverteilten Zufallsvektors ergeben sich aus den Parametern der Verteilung des ursprünglichen Zufallsvektors und der Transformationsmatrix.\n\nBeispiel\nAls Beispiel betrachten wir die invertierbare lineare Transformation eines bivariaten normalverteilten Zufallsvektors \\(\\xi\\). Es seien \\[\\begin{equation}\n\\mu_\\xi := \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n\\mbox{ und }\n\\Sigma_\\xi := \\begin{pmatrix} 0.20 & 0.15 \\\\ 0.15 & 0.20 \\end{pmatrix}\n\\end{equation}\\] der Erwartungswert- und Kovarianzmatrixparameter von \\(\\xi\\), respektive, und es sei \\[\\begin{equation}\nA := \\begin{pmatrix} -2 & 1 \\\\ - 1 & 2 \\end{pmatrix}\n\\end{equation}\\] die Transformationsmatrix. Da \\(|A| = -3 \\neq 0\\) ist \\(A\\) invertierbar und es gilt nach Theorem 20.3, dass \\[\\begin{equation}\n\\zeta \\sim N(\\mu_\\zeta,\\Sigma_\\zeta) \\mbox{ mit }\n\\mu_\\zeta = A\\mu_\\xi = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}\n\\mbox{ und }\nA\\Sigma_\\xi A^T = \\begin{pmatrix} 0.40 & 0.05 \\\\ 0.05 & 0.40 \\end{pmatrix}.\n\\end{equation}\\] Abbildung 20.5 A zeigt Isokonturen der WDF von \\(\\xi\\) und Realisierungen \\(x^{(i)} \\in \\mathbb{R}^2\\) von \\(\\xi\\) für \\(i = 1,...,50\\). Abbildung 20.5 B zeigt die transfomierten Realisierungen \\(z^{(i)} = Ax^{(i)} \\in \\mathbb{R}^{2}\\) von \\(\\zeta\\) sowie die Isokonturen der WDF von \\(\\zeta\\) nach Theorem 20.3.\n\n\n\n\n\n\nAbbildung 20.5: Invertierbare lineare Transformation eines normalverteilten Zufallsvektors\n\n\n\nDie Tatsache, dass ein linear transfomierter normalverteilter Zufallsvektor wiederum normalverteilt ist und dass sich die Parameter der Verteilung des transformierten Zufallsvektors aus den Parametern der Verteilung des ursprünglichen Zufallsvektors sowie den Transformationsparametern bestimmen lassen, bleibt auch im Falle einer nicht notwendigerweise invertierbaren linearen Transformation und auch im Falle einer nicht notwendigerweise invertierbaren linear-affinen Transformation wahr. Dies ist die Aussage folgenden zentralen Theorems. Für einen Beweis verweisen wir auf Anderson (2003).\n\nTheorem 20.4 (Linear-affine Transformation eines normalverteilten Zufallsvektors) \\(\\xi \\sim N(\\mu_\\xi,\\Sigma_\\xi)\\) sei ein normalverteilter \\(n\\)-dimensionaler Zufallsvektor und es sei \\[\\begin{equation}\n\\zeta := Ax + b \\mbox{ mit } A \\in \\mathbb{R}^{m \\times n} \\mbox{ und } b \\in \\mathbb{R}^m.\n\\end{equation}\\] Dann gilt \\[\\begin{equation}\n\\zeta \\sim N(\\mu_\\zeta, \\Sigma_\\zeta)\n\\mbox{ mit }\n\\mu_\\zeta    = A\\mu + b \\in \\mathbb{R}^m \\mbox{ und }\n\\Sigma_\\zeta = A\\Sigma A^T \\in \\mathbb{R}^{m \\times m}.\n\\end{equation}\\]",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Multivariate Normalverteilungen</span>"
    ]
  },
  {
    "objectID": "210-Multivariate-Normalverteilungen.html#sphärizität",
    "href": "210-Multivariate-Normalverteilungen.html#sphärizität",
    "title": "20  Multivariate Normalverteilungen",
    "section": "20.4 Sphärizität",
    "text": "20.4 Sphärizität\nFolgendes Theorem ist für die grundlegende Theorie des Allgemeinen Linearen Modells zentral.\n\nTheorem 20.5 (Sphärische multivariate Normalverteilung) Für \\(i = 1,...,n\\) seien \\(N(x_i; \\mu_i,\\sigma^2)\\) die WDFen von \\(n\\) unabhängigen univariaten normalverteilten Zufallsvariablen \\(\\xi_1,...,\\xi_n\\) mit \\(\\mu_1,...,\\mu_n \\in \\mathbb{R}\\) und \\(\\sigma^2 &gt; 0\\). Weiterhin sei \\(N(x;\\mu,\\sigma^2I_n)\\) die WDF eines \\(n\\)-variaten normalverteilten Zufallsvektors \\(\\xi\\) mit Erwartungswertparameter \\(\\mu := (\\mu_1,...,\\mu_n)^T \\in \\mathbb{R}^n\\). Dann gilt \\[\\begin{equation}\np_\\xi(x) = p_{\\xi_1,...,\\xi_n}(x_1,...,x_n) = \\prod_{i=1}^n p_{\\xi_i}(x_i)\n\\end{equation}\\] und insbesondere \\[\\begin{equation}\nN\\left(x;\\mu,\\sigma^2I_n\\right) = \\prod_{i=1}^n N\\left(x_i;\\mu_i,\\sigma^2\\right)\n\\end{equation}\\] für alle \\(x = (x_1,...,x_n)^T \\in \\mathbb{R}^n\\).\n\n\nBeweis. Wir zeigen die Identität der multivariaten WDF \\(N(x;\\mu,\\sigma^2 I_n)\\) mit dem Produkt von \\(n\\) univariaten WDFen \\(N(x_i;\\mu_i,\\sigma^2 I_n)\\), wobei \\(\\mu_i\\) der \\(i\\)te Eintrag von \\(\\mu \\in \\mathbb{R}^n\\) ist. Es ergibt sich \\[\\begin{align}\n\\begin{split}\nN\\left(x;\\mu,\\sigma^{2}I_{n} \\right)\n& = \\left(2\\pi \\right)^{-\\frac{n}{2}}\n    \\left|\\sigma^2 I_n \\right|^{-\\frac{1}{2}}\n    \\exp\\left(-\\frac{1}{2}(x-\\mu)^{T}(\\sigma^2 I_n)^{-1}(x-\\mu)\\right)\\\\\n& = \\left(\\prod_{i=1}^n 2\\pi ^{-\\frac{1}{2}} \\right)\n    \\left(\\sigma^2\\right)^{-\\frac{n}{2}}\n    \\exp\\left(-\\frac{1}{2\\sigma^2}(x-\\mu)^{T}(x-\\mu)\\right) \\\\\n& = \\left(\\prod_{i=1}^n \\left(2\\pi\\sigma^2 \\right) ^{-\\frac{1}{2}} \\right)\n    \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (x_i - \\mu_i)^2\\right) \\\\\n& = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\n    \\prod_{i=1}^n \\exp\\left(-\\frac{1}{2\\sigma^2} (x_i - \\mu_i)^2\\right) \\\\\n& = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\n                    \\exp\\left(-\\frac{1}{2\\sigma^2} (x_i - \\mu_i)^2\\right) \\\\\n& = \\prod_{i=1}^n N\\left(x_i; \\mu_i,\\sigma^2\\right).\n\\end{split}\n\\end{align}\\]\n\nEinen Kovarianzmatrixparameter der Form \\(\\Sigma = \\sigma^2 I_n\\) nennt man auch sphärisch, da die Isokonturen der WDF eines normalverteilten Zufallsvektors mit einem solchen Kovarianzmatrixparameter Sphären bilden (zum Beispiel Kreise bei \\(n = 2\\) und Kugeln bei \\(n = 3\\)). Eine multivariate Normalverteilung mit sphärische Kovarianzmatrixparameter nennt man entsprechend eine sphärische Normalverteilung. Theorem 20.5 besagt, dass die WDF eines \\(n\\)-dimensionalen normalverteilten Zufallskvektors mit sphärischem Kovarianzparameter der gemeinsamen WDF von \\(n\\) unabhängigen univariat normalverteilten Zufallsvariablen entspricht und umgekehrt. Eine Realisierung eines \\(n\\)-dimensionalen normalverteilten Zufallsvektors entspricht also den Realisierungen von \\(n\\) unabhängigen univariat normalverteilten Zufallsvariablen und umgekehrt. Man beachte, dass die Identität der Verteilungen der \\(\\xi_i, i = 1,...,n\\) hier nicht voraussgesetzt ist, insbesondere können sich ihre Erwartungswertparameter \\(\\mu_i, i = 1,...,n\\) explizit unterscheiden.",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Multivariate Normalverteilungen</span>"
    ]
  },
  {
    "objectID": "210-Multivariate-Normalverteilungen.html#marginale-und-bedingte-verteilungen",
    "href": "210-Multivariate-Normalverteilungen.html#marginale-und-bedingte-verteilungen",
    "title": "20  Multivariate Normalverteilungen",
    "section": "20.5 Marginale und bedingte Verteilungen",
    "text": "20.5 Marginale und bedingte Verteilungen\nMultivariate Normalverteilungen haben die Eigenschaft, dass auch alle anderen assoziierten Verteilungen Normalverteilungen sind und deren Erwartungswert- und Kovarianzmatrixparameter aus den Parametern der jeweils komplementären Verteilung errechnet werden können. Insbesondere gilt zum einen, dass die uni- und multivariaten Marginalverteilungen multivariater Normalverteilungen wiederum Normalverteilungen sind. Zum anderen lassen sich wie alle multivariaten Verteilungen multivariate Normalverteilungen multiplikativ in eine marginale und eine bedingte Verteilung zerlegen. Insbesondere sind nun aber bei multivariaten Normalverteilungen diese Verteilungen wiederum (multivariate) Normalverteilungen, deren Parameter aus den Parametern der gemeinsame Verteilung errechnet werden können und umgekehrt. Wir fassen obige Erkenntnisse formal in den folgenden drei Theoremen zusammen.\n\nTheorem 20.6 (Marginale Normalverteilungen) Es sei \\(n := k + l\\) und \\(\\xi = (\\xi_1,...,\\xi_n)^T\\) sei ein \\(n\\)-dimensionaler normalverteilter Zufallsvektor mit Erwartungswertparameter \\[\\begin{equation}\n\\mu =\n\\left(\\begin{matrix}\n\\mu_\\upsilon \\\\\n\\mu_\\zeta\n\\end{matrix}\\right) \\in \\mathbb{R}^n,\n\\end{equation}\\] mit \\(\\mu_\\upsilon \\in \\mathbb{R}^k\\) and \\(\\mu_\\zeta \\in \\mathbb{R}^l\\) und Kovarianzmatrixparameter \\[\\begin{equation}\n\\Sigma =\n\\left(\\begin{matrix}\n\\Sigma_{\\upsilon\\upsilon}   & \\Sigma_{\\upsilon\\zeta} \\\\\n\\Sigma_{\\zeta\\upsilon}  & \\Sigma_{\\zeta\\zeta}\n\\end{matrix}\\right) \\in \\mathbb{R}^{n \\times n},\n\\end{equation}\\] mit \\(\\Sigma_{\\upsilon\\upsilon}  \\in \\mathbb{R}^{k \\times k}\\), \\(\\Sigma_{\\upsilon\\zeta}     \\in \\mathbb{R}^{k \\times l}\\), \\(\\Sigma_{\\zeta\\upsilon}     \\in \\mathbb{R}^{l \\times k}\\), und \\(\\Sigma_{\\zeta\\zeta}        \\in \\mathbb{R}^{l \\times l}\\). Dann sind \\(\\upsilon := (\\xi_1,...,\\xi_k)^T\\) und \\(\\zeta := (\\xi_{k+1}, ...,\\xi_n)^T\\) \\(k\\)- und \\(l\\)-dimensionale normalverteilte Zufallsvektoren, respektive, und es gilt \\[\\begin{equation}\n\\upsilon \\sim N(\\mu_\\upsilon,\\Sigma_{\\upsilon\\upsilon}) \\mbox{ und } \\zeta \\sim N(\\mu_\\zeta,\\Sigma_{\\zeta\\zeta}).\n\\end{equation}\\]\n\nDie Marginalverteilungen einer multivariaten Normalverteilung sind also auch Normalverteilungen und die Parameter der Marginalverteilungen ergeben sich aus den Parametern der gemeinsamen Verteilung. Für Beweise dieses Theorems verweisen wir auf Mardia et al. (1979) und Anderson (2003). Abbildung 20.6 visualisiert Theorem 20.6 für den Fall \\(n := 2, k := 1, l := 1\\), \\[\\begin{equation}\n\\mu := \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} \\in \\mathbb{R}^2\n\\mbox{ und }\n\\Sigma := \\begin{pmatrix} 0.10 & 0.08 \\\\ 0.08 & 0.15 \\end{pmatrix}  \\in \\mathbb{R}^{2 \\times 2}.\n\\end{equation}\\] Abbildung 20.6 A zeigt dabei die WDF des bivariaten Zufallsvektors \\(\\xi\\) und Abbildung 20.6 B und C die WDFen der entsprechenden marginalen Zufallsvariablen \\(\\upsilon\\) und \\(\\zeta\\).\n\n\n\n\n\n\nAbbildung 20.6: Marginale Verteilungen eines bivariaten normalverteilten Zufallsvektor.\n\n\n\nMithilfe einer marginalen und einer bedingten multivariaten Normalverteilung lässt sich eine gemeinsame multivariate Normalverteilung konstruieren, deren Parameter sich aus den Parametern der marginalen und bedingten Verteilung ergeben. Dies ist die zentrale Aussage folgenden Theorems.\n\nTheorem 20.7 (Gemeinsame Normalverteilungen) \\(\\xi\\) sei ein \\(m\\)-dimensionaler normalverteilter Zufallsvektor mit WDF \\[\\begin{equation}\np_\\xi : \\mathbb{R}^m \\to \\mathbb{R}_{&gt;0},\\,x\\mapsto\np_\\xi(x) := N(x;\\mu_\\xi,\\Sigma_{\\xi\\xi}) \\mbox{ mit }\n\\mu_\\xi \\in \\mathbb{R}^m,\n\\Sigma_{\\xi\\xi} \\in \\mathbb{R}^{m\\times m},\n\\end{equation}\\] \\(A\\in\\mathbb{R}^{n\\times m}\\) sei eine Matrix, \\(b\\in\\mathbb{R}^n\\) sei ein Vektor und \\(\\upsilon\\) sei ein \\(n\\)-dimensionaler bedingt normalverteilter Zufallsvektor mit bedingter WDF \\[\\begin{equation}\np_{\\upsilon|\\xi}(\\cdot|x) : \\mathbb{R}^n \\to \\mathbb{R}_{&gt;0},\\, y\\mapsto\np_{\\upsilon|\\xi}(y|x) := N(y;A\\xi+b,\\Sigma_{\\upsilon\\upsilon}) \\mbox{ mit }\n\\Sigma_{\\upsilon\\upsilon} \\in \\mathbb{R}^{n\\times n}.\n\\end{equation}\\] Dann ist der \\(m+n\\)-dimensionale Zufallsvektor \\((\\xi,\\upsilon)^T\\) normalverteilt mit (gemeinsamer) WDF \\[\\begin{equation}\\label{eq:gauss_joint}\np_{\\xi,\\upsilon} : \\mathbb{R}^{m+n} \\to \\mathbb{R}_{&gt;0},\\, \\begin{pmatrix} x \\\\ y \\end{pmatrix} \\mapsto\np_{\\xi,\\upsilon}\\left(\\begin{pmatrix} x \\\\ y \\end{pmatrix}\\right) = N\\left(\\begin{pmatrix} x \\\\ y \\end{pmatrix};\n\\mu_{\\xi,\\upsilon}, \\Sigma_{\\xi,\\upsilon} \\right),\n\\end{equation}\\] mit \\(\\mu_{\\xi,\\upsilon} \\in \\mathbb{R}^{m+n}\\) und \\(\\Sigma_{\\xi,\\upsilon} \\in \\mathbb{R}^{m+n \\times m+n}\\) und insbesondere \\[\\begin{equation}\n\\mu_{\\xi,\\upsilon} = \\left( \\begin{matrix} \\mu_\\xi \\\\ A\\mu_\\xi + b \\end{matrix} \\right)\n\\mbox{ und }\n\\Sigma_{\\xi,\\upsilon} = \\left(\\begin{matrix} \\Sigma_{\\xi\\xi} & \\Sigma_{\\xi\\xi}A^T \\\\ A\\Sigma_{\\xi\\xi} & \\Sigma_{\\upsilon\\upsilon} + A\\Sigma_{\\xi\\xi}A^T \\end{matrix} \\right).\n\\end{equation}\\]\n\nInsbesondere ergeben sich die Parameter der gemeinsamen Verteilung also als linear-affine Transformation der Parameter der induzierenden Verteilungen. Abbildung 20.7 visualisiert Theorem 20.7 für den Fall \\(m := 1, n := 1, \\mu_\\xi := 1, \\Sigma_{\\xi\\xi} := 0.2, A := 1, b := 1\\) und \\(\\Sigma_{\\upsilon\\upsilon} := 0.1\\). Abbildung 20.7 A zeigt dabei die WDF der Zufallsvariable \\(\\xi\\), Abbildung 20.7 B zeigt die WDF der bedingten Verteilung der Zufallsvariable \\(\\upsilon\\) gegeben \\(\\xi\\) und Abbildung 20.6 C schließlich zeigt die WDFen des induzierten bivariaten Zufallsvektors \\((\\xi,\\upsilon)\\).\n\n\n\n\n\n\nAbbildung 20.7: Gemeinsame Verteilungen einer marginalen und einer auf dieser bedingten normalverteilten Zufallsvariable.\n\n\n\nDie Definition einer multivariaten Normalverteilung erlaubt es weiterhin, die bedingten Verteilungen aller Komponenten des entsprechenden Zufallsvektors direkt mithilfe der Parameter der multivariaten Normalverteilung zu bestimmen. Dies ist die zentrale Aussage folgenden Theorems.\n\nTheorem 20.8 (Bedingte Normalverteilungen) \\((\\xi,\\upsilon)\\) sei ein \\(m+n\\)-dimensionaler normalverteilter Zufallsvektor mit WDF \\[\\begin{equation}\np_{\\xi,\\upsilon} : \\mathbb{R}^{m + n} \\to \\mathbb{R}_{&gt;0}, \\begin{pmatrix} x \\\\ y \\end{pmatrix}\n\\mapsto p_{\\xi,\\upsilon}\\left(\\begin{pmatrix} x \\\\ y \\end{pmatrix} \\right)\n:= N\\left(\\begin{pmatrix} x \\\\ y \\end{pmatrix}; \\mu_{\\xi,\\upsilon}, \\Sigma_{\\xi,\\upsilon}\\right),\n\\end{equation}\\] mit \\[\\begin{equation}\n\\mu_{\\xi,\\upsilon}\n= \\left(\\begin{matrix} \\mu_\\xi \\\\ \\mu_\\upsilon \\end{matrix} \\right),\n\\Sigma_{\\xi,\\upsilon} = \\left(\\begin{matrix} \\Sigma_{\\xi\\xi} & \\Sigma_{\\xi\\upsilon} \\\\ \\Sigma_{\\upsilon\\xi} & \\Sigma_{\\upsilon\\upsilon} \\end{matrix} \\right),\n\\end{equation}\\] mit \\(x,\\mu_\\xi \\in \\mathbb{R}^m, y,\\mu_\\upsilon\\in\\mathbb{R}^n\\) und \\(\\Sigma_{\\xi\\xi} \\in \\mathbb{R}^{m\\times m}, \\Sigma_{\\xi\\upsilon} \\in \\mathbb{R}^{m\\times n}, \\Sigma_{\\upsilon\\upsilon} \\in \\mathbb{R}^{n \\times n}\\). Dann ist die bedingte Verteilung von \\(\\xi\\) gegeben \\(\\upsilon\\) eine \\(m\\)-dimensionale Normalverteilung mit bedingter WDF \\[\\begin{equation}\np_{\\xi|\\upsilon}(\\cdot|y) : \\mathbb{R}^m \\to \\mathbb{R}_{&gt;0}, x \\mapsto p_{\\xi|\\upsilon}(x|y) :=\nN(x;\\mu_{\\xi|\\upsilon},\\Sigma_{\\xi|\\upsilon})\n\\end{equation}\\] mit \\[\\begin{equation}\\label{eq:gauss_cond_exp}\n\\mu_{\\xi|\\upsilon} = \\mu_\\xi  + \\Sigma_{\\xi\\upsilon}\\Sigma_{\\upsilon\\upsilon}^{-1}(y-\\mu_\\upsilon) \\in \\mathbb{R}^m\n\\end{equation}\\] und \\[\\begin{equation}\\label{eq:gauss_cond_var}\n\\Sigma_{\\xi|\\upsilon} = \\Sigma_{\\xi\\xi}  - \\Sigma_{\\xi\\upsilon}\\Sigma_{\\upsilon\\upsilon}^{-1}\\Sigma_{\\upsilon\\xi} \\in \\mathbb{R}^{m\\times m}.\n\\end{equation}\\]\n\nIm Zusammenspiel mit Theorem 20.7 und Theorem 20.6 können die Parameter bedingter und marginaler Normalverteilungen also aus den Parametern der komplementären bedingten und marginalen Normalverteilungen bestimmt werden. Abbildung 20.8 visualisiert Theorem 20.8 für den Fall \\(m := 2, n := 1\\), \\[\\begin{equation}\n\\mu := \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}\n\\mbox{ und }\n\\Sigma := \\begin{pmatrix} 0.12 & 0.09 \\\\ 0.09 & 0.12 \\end{pmatrix}\n\\end{equation}\\] Dabei zeigt Abbildung 20.8 A die WDF des bivariaten Zufallsvektors \\((\\xi,\\upsilon)^T\\) und Abbildung 20.8 B und C zeigen die WDF der bedingten Verteilung der Zufallsvariable \\(\\xi\\) gegeben \\(\\upsilon = 1.5\\) und \\(\\upsilon = 2.8\\), respektive.\n\n\n\n\n\n\nAbbildung 20.8: Bedingte Normalverteilungen",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Multivariate Normalverteilungen</span>"
    ]
  },
  {
    "objectID": "210-Multivariate-Normalverteilungen.html#literaturhinweise",
    "href": "210-Multivariate-Normalverteilungen.html#literaturhinweise",
    "title": "20  Multivariate Normalverteilungen",
    "section": "20.6 Literaturhinweise",
    "text": "20.6 Literaturhinweise\nDie Entwicklung der bivariaten Normalverteilung hat ihre Ursprünge in der statistischen Literatur zur Mitte des 19. Jahrhunderts, insbesondere in den Arbeiten von Francis Galton (1822-1911). Die mathematische Formalisierung der bivariaten Normalverteilung geht dabei wohl insbesondere auf Pearson (1896) zurück (Seal (1967)). Die ursprüngliche Formulierung der multivariaten Normalverteilung wird bei Edgeworth (1892) verortet. Tong (1990) gibt eine umfassenden Überblick zur Theorie und Anwendung der multivariaten Normalverteilung.",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Multivariate Normalverteilungen</span>"
    ]
  },
  {
    "objectID": "210-Multivariate-Normalverteilungen.html#selbstkontrollfragen",
    "href": "210-Multivariate-Normalverteilungen.html#selbstkontrollfragen",
    "title": "20  Multivariate Normalverteilungen",
    "section": "20.7 Selbstkontrollfragen",
    "text": "20.7 Selbstkontrollfragen\n\nGeben Sie Definition eines Zufallsvektors wieder.\nGeben Sie Definition der multivariaten Verteilung eines Zufallsvektors wieder.\nGeben Sie Definition einer multivariaten WMF wieder.\nGeben Sie Definition einer multivariaten WDF wieder.\nGeben Sie die Definition des Erwartungswerts eines Zufallsvektors wieder.\nGeben Sie die Definition der Kovarianzmatrix eines Zufallsvektors wieder.\nWas repräsentieren die Diagonalelemente der Kovarianzmatrix eines Zufallsvektors?\nWas repräsentieren die Nichtdiagonalelemente der Kovarianzmatrix eines Zufallsvektors?\nGeben Sie die Definition der Korrelationsmatrix eines Zufallsvektors wieder.\nGeben Sie die Definition der univariaten Marginalverteilung eines Zufallsvektors wieder.\nWie berechnet man die WMF der \\(i\\)ten Komponente eines diskreten Zufallsvektors?\nWie berechnet man die WDF der \\(i\\)ten Komponente eines kontinuierlichen Zufallsvektors?\nGeben Sie Definition der bedingten WMF und der diskreten bedingten Verteilung wieder.\nGeben Sie Definition der bedingten WDF und der kontinuierlichen bedingten Verteilung wieder.\nGeben Sie die Definition der WDF eines multivariaten normalverteilten Zufallsvektors wieder.\nErläutern Sie die Komponenten der WDF eines multivariaten normalverteilten Zufallsvektors.\nGeben Sie den Erwartungswert und die Kovarianzmatrix eines normalverteilten Zufallsvektors an.\nGeben Sie das Theorem zu marginalen Normalverteilungen wieder.\nGeben Sie das Theorem zu gemeinsamen Normalverteilungen wieder.\nGeben Sie das Theorem zu bedingten Normalverteilungen wieder. \n\n\n\n\n\nAnderson, T. W. (2003). An Introduction to Multivariate Statistical Analysis (3rd ed). Wiley-Interscience.\n\n\nDeGroot, M. H., & Schervish, M. J. (2012). Probability and Statistics (4th ed). Addison-Wesley.\n\n\nEdgeworth, F. Y. (1892). The Law of Error and Correlated Averages. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 34(210), 429–438. https://doi.org/10.1080/14786449208620355\n\n\nMardia, K. V., Kent, J. T., & Bibby, J. M. (1979). Multivariate Analysis. Academic Press.\n\n\nPearson, K. (1896). Mathematical Contributions to the Theory of Evolution. III. Regression, Heredity, and Panmixia. Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character, 18, 253–318. https://www.jstor.org/stable/90707\n\n\nSeal, H. L. (1967). Studies in the History of Probability and Statistics. XV: The Historical Development of the Gauss Linear Model. Biometrika, 54(1/2), 1. https://doi.org/10.2307/2333849\n\n\nTong, Y. L. (1990). Multivariate Normal Distribution. Springer.",
    "crumbs": [
      "Wahrscheinlichkeitstheorie",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Multivariate Normalverteilungen</span>"
    ]
  },
  {
    "objectID": "301-Grundbegriffe.html",
    "href": "301-Grundbegriffe.html",
    "title": "21  Grundbegriffe",
    "section": "",
    "text": "21.1 Frequentistische Inferenzmodelle\nMit folgender Definition wollen wir zunächst einige grundlegende Begrifflichkeiten bei der Betrachtung Frequentistischer Inferenzmodelle einführen.\nVor dem Hintergrund eines Frequentistischen Inferenzmodells wird der Vorgang der Datenbeobachtung wird durch einen Zufallsvektor \\(\\upsilon\\), der Werte in \\(\\mathcal{Y}\\) annimmt und dessen Verteilung einer der prinzipiell möglichen Verteilungen \\(\\mathbb{P}_\\theta\\) entspricht, beschrieben. Man nennt diesen Zufallsvektor Daten, Beobachtung, Messung oder Stichprobe. Im Gegensatz zum Wahrscheinlichkeitsraummodell betrachtet man bei Frequentistische Inferenzmodellen also explizit zwei oder mehr Wahrscheinlichkeitsmaße, die die Verteilung von \\(\\upsilon\\) mutmaßlich bestimmen. Eine Realisierung von \\(\\upsilon\\), also konkret vorliegende Datenwerte \\(y \\in \\mathcal{Y}\\), nennt man Datensatz, Beobachtungswert, Messwert oder Stichprobenwert. Erwartungswerte und (Ko)Varianzen von \\(\\upsilon\\) bezüglich \\(\\mathbb{P}_\\theta\\) schreibt man meist als \\(\\mathbb{E}_\\theta(\\upsilon)\\), \\(\\mathbb{V}_\\theta(\\upsilon)\\) und \\(\\mathbb{C}_\\theta(\\upsilon)\\). Frequentistische Produktmodelle modellieren die \\(n\\)-fache unabhängige Wiederholung eines Zufallsvorgangs. Die entsprechende Menge von Zufallsvektoren \\(\\upsilon_1,....,\\upsilon_n\\) entspricht dann einer Menge von \\(n\\) unabhängigen Zufallsvektoren.\nIn einem konkreten Datenanalyseproblem auf Grundlage eines parameterischen Frequentistischen Produktmodells nimmt man an, dass die beobachteten Werte \\(y_1,...,y_n\\) von \\(\\upsilon_1,...,\\upsilon_n\\) durch genau ein Wahrscheinlichkeitsmaß \\(\\mathbb{P}_{\\theta}\\) mit Parameter \\(\\theta \\in \\Theta\\) generiert wurde. In der Anwendung wird dieses \\(\\theta \\in \\Theta\\) dann als wahrer, aber unbekannter, Parameterwert bezeichnet. Der wahre, aber unbekannten, Parameterwert \\(\\theta\\) bleibt dabei auch nach jeglicher Form von Inferenz unbekannt. Allgemeines Ziel von parameterischen Inferenzverfahren ist es damit, basierend auf einem vorliegenden Datensatz eine möglichst valide Aussage hinsichtlich des wahren, aber unbekannten Parameters \\(\\theta\\) zu treffen. In diesem Sinne ist der wahre, aber unbekannte Parameterwert, nur indirekt beobachtbar. Dies wird manchmal auch durch die Sprechweisen ausgedrückt, dass der wahre, aber unbekannte Parameterwert unbeobachtbar oder latent, d.h. nicht unmittelbar sichtbar oder zu erfassen, ist. In der mathematischen Analyse von Inferenzverfahren betrachtet man alle möglichen wahren, aber unbekannten, Parameterwerte, verzichtet deshalb also meist auf eine explizite notationelle Auszeichung des in einem Anwendungskontext unterstellten wahren, aber unbekannten, Parameterwerts.",
    "crumbs": [
      "Frequentistische Inferenz",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Grundbegriffe</span>"
    ]
  },
  {
    "objectID": "301-Grundbegriffe.html#sec-frequentistische-inferenzmodelle",
    "href": "301-Grundbegriffe.html#sec-frequentistische-inferenzmodelle",
    "title": "21  Grundbegriffe",
    "section": "",
    "text": "Definition 21.1 (Frequentistische Inferenzmodelle) Ein Frequentistisches Inferenzmodell ist ein Tupel \\[\\begin{equation}\n\\mathcal{M} := (\\mathcal{Y}, \\mathcal{A}, \\{\\mathbb{P}_\\theta |\\theta \\in \\Theta\\})\n\\end{equation}\\] bestehend aus einem Datenraum \\(\\mathcal{Y}\\), einer \\(\\sigma\\)-Algebra \\(\\mathcal{A}\\) auf \\(\\mathcal{Y}\\) und einer mindestens zweielementigen Menge \\(\\{\\mathbb{P}_\\theta |\\theta \\in \\Theta\\}\\) von Wahrscheinlichkeitsmaßen auf \\((\\mathcal{Y}, \\mathcal{A})\\), die durch \\(\\theta \\in \\Theta\\) indiziert sind. Wenn \\(\\Theta \\subset \\mathbb{R}^k\\) ist, heißt ein Frequentistisches Inferenzmodell auch parametrisches Frequentistisches Inferenzmodell und \\(\\Theta\\) heißt Parameterraum des Frequentistischen Inferenzmodells. Ein Frequentistisches Inferenzmodell \\(\\mathcal{M}\\) heißt ein diskretes Modell, wenn \\(\\mathcal{Y}\\) endlich oder abzählbar ist und jedes \\(\\mathbb{P}_\\theta\\) eine WMF \\(p_\\theta\\) besitzt. Ein Frequentistisches Inferenzmodell \\(\\mathcal{M}\\) heißt ein stetiges Modell, wenn \\(\\mathcal{Y} \\subset \\mathbb{R}^n\\) ist und jedes \\(\\mathbb{P}_\\theta\\) eine WDF \\(p_\\theta\\) besitzt. Wenn der Datenraum \\(\\mathcal{Y}\\) eines Frequentistischen Inferenzmodells \\(\\mathcal{M}\\) eindimensional ist, also zum Beispiel \\(\\mathcal{Y} := \\mathbb{R}\\), spricht man von einem univariaten Frequentistischen Inferenzmodell. Wenn der Datenraum \\(\\mathcal{Y}\\) eines Frequentistischen Inferenzmodells \\(\\mathcal{M}\\) mehrdimensional ist, also zum Beispiel \\(\\mathcal{Y} := \\mathbb{R}^m\\) für \\(m &gt; 1\\), spricht man von einem multivariaten Frequentistischen Inferenzmodell. Für ein Frequentistisches Inferenzmodell \\(\\mathcal{M}_0 := (\\mathcal{Y}_0, \\mathcal{A}_0, \\{\\mathbb{P}_\\theta^0 |\\theta \\in \\Theta\\})\\) wrid das Frequentistische Inferenzmodell \\(\\mathcal{M} := (\\mathcal{Y}, \\mathcal{A}, \\{\\mathbb{P}_\\theta |\\theta \\in \\Theta\\})\\), für das \\(\\mathcal{Y}\\) das \\(n\\)-fache kartesische Produkt von \\(\\mathcal{Y}_0\\) mit sich selbst, \\(\\mathcal{A}\\) die entsprechende Produkt-\\(\\sigma\\)-Algebra und \\(\\{\\mathbb{P}_\\theta |\\theta \\in \\Theta\\}\\) die entsprechende Menge an Produktmaßen ist, ein (zu \\(\\mathcal{M}_0\\) gehöriges) Frequentistisches Produktmodell genannt.\n\n\n\n\nBeispiele\nMit dem univariten Normalverteilungsmodell und dem Bernoullimodell wollen wir zwei erste Beispiel für Frequentistische Inferenzmodelle geben.\n\nDefinition 21.2 (Normalverteilungsmodell) Das univariate parametrische Produktmodell \\[\\begin{equation}\n\\mathcal{M} := \\left(\\mathcal{Y}, \\mathcal{A}, \\{\\mathbb{P}_\\theta|\\theta \\in \\Theta\\}\\right)\n\\end{equation}\\] mit \\[\\begin{equation}\n\\mathcal{Y} := \\mathbb{R}^n, \\mathcal{A} := \\mathcal{B}(\\mathbb{R}^n), \\mathbb{P}_\\theta := N(\\mu,\\sigma^2) \\theta := (\\mu, \\sigma^2), \\Theta := \\mathbb{R} \\times \\mathbb{R}_{&gt;0},\n\\end{equation}\\] also \\[\\begin{equation}\n\\{\\mathbb{P}_\\theta|\\theta \\in \\Theta\\}\n:= \\left\\lbrace \\prod_{i=1}^n N(\\mu,\\sigma^2)|(\\mu,\\sigma^2)\\in \\mathbb{R} \\times \\mathbb{R}_{&gt;0} \\right\\rbrace,\n\\end{equation}\\] und damit \\[\\begin{equation}\n\\upsilon_1,...,\\upsilon_n \\sim N(\\mu,\\sigma^2) \\mbox{ mit } (\\mu,\\sigma^2)\\in \\mathbb{R} \\times \\mathbb{R}_{&gt;0}\n\\end{equation}\\] heißt .\n\nDas Normalverteilungsmodell ist Grundlage vieler populärer statistischen Verfahren die im Rahmen des Allgemeinen Linearen Modells integrativ betrachtet werden. Man beachte, dass die Annahme normalverteilter Daten dabei durch additive normalverteilte Fehlerterme motiviert ist, wie wir in Kapitel 17 schon kurz angerissen haben und an späterer Stelle vertiefen werden.\n\nDefinition 21.3 (Bernoullimodell) Das univariate parametrische Produktmodell \\[\\begin{equation}\n\\mathcal{M} := \\left(\\mathcal{Y}, \\mathcal{A}, \\{\\mathbb{P}_\\theta|\\theta \\in \\Theta\\}\\right)\n\\end{equation}\\] mit \\[\\begin{equation}\n\\mathcal{Y} := \\{0,1\\}^n, \\mathcal{A} := \\mathcal{P}\\left(\\{0,1\\}^n\\right), \\mathbb{P}_\\theta := \\mbox{Bern}(\\mu), \\theta:= \\mu, \\Theta := ]0,1[,\n\\end{equation}\\] also \\[\\begin{equation}\n\\{\\mathbb{P}_\\theta|\\theta \\in \\Theta\\} := \\left\\lbrace \\prod_{i=1}^n \\mbox{Bern}(\\mu)|\\mu \\in ]0,1[ \\right\\rbrace,\n\\end{equation}\\] und damit \\[\\begin{equation}\n\\upsilon_1,...,\\upsilon_n \\sim \\mbox{Bern}(\\mu) \\mbox{ mit } \\mu \\in ]0,1[,\n\\end{equation}\\] heißt .",
    "crumbs": [
      "Frequentistische Inferenz",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Grundbegriffe</span>"
    ]
  },
  {
    "objectID": "301-Grundbegriffe.html#sec-statistiken-und-schätzer",
    "href": "301-Grundbegriffe.html#sec-statistiken-und-schätzer",
    "title": "21  Grundbegriffe",
    "section": "21.2 Statistiken und Schätzer",
    "text": "21.2 Statistiken und Schätzer\nVor dem Hintergrund Frequentistischer Inferenzmodelle wollen wir nun formalisieren, was unter den Begriffen einer Statistik und eines Schätzers zu verstehen ist.\n\nDefinition 21.4 (Statistik) \\(\\mathcal{M}\\) sei ein Frequentistisches Inferenzmodell und \\((\\Sigma,\\mathcal{S})\\) sei ein Messraum. Dann ist eine Statistik ein Zufallsvektor der Form \\[\\begin{equation}\nS : \\mathcal{Y} \\to \\Sigma.\n\\end{equation}\\]\n\nSowohl Daten als auch Statistiken werden in der Frequentistischen Inferenz also durch Zufallsvektoren (im univariaten Fall entsprechend durch Zufallsvariablen) modelliert. Allerdings unterscheiden sich diese Zufallvektoren hinsichtlich ihrer intuitiven Bedeutung fundamental: Daten repräsentieren den Ausgang von Messvorgängen unter Unsicherheit, Statistiken dagegen modellieren von Datenwissenschaftler:innen konstruierte Funktionen von Daten. Diese liefern im besten Fall datenbasierte Informationen, aus denen sich Schlüsse über die latenten datengenerierenden Zufallsvorgänge ziehen lassen. Die Tatsache, dass Statistiken zufällig sind ergibt sich dabei daraus, dass sie als Funktionen auf zufällige Daten angewendet werden. (vgl. etwa Theorem 13.1).\nBeispiele\n\\(\\mathcal{M}\\) sei das Normalverteilungsmodell. Dann sind zum Beispiel folgende Zufallsvariablen Statistiken:\n\nDas Stichprobenmittel \\[\\begin{equation}\n\\bar{y} : \\mathbb{R}^n \\to \\mathbb{R},\ny \\mapsto \\bar{y}(y) := \\frac{1}{n}\\sum_{i=1}^n y_i,\n\\end{equation}\\]\nDie Stichprobenvarianz \\[\\begin{equation}\ns^2  : \\mathbb{R}^n \\to \\mathbb{R}_{\\ge 0},\ny \\mapsto s^2(y) := \\frac{1}{n-1}\\sum_{i=1}^n (y_i - \\bar{y}(y))^2,\n\\end{equation}\\]\nDie Stichprobenstandardabweichung \\[\\begin{equation}\ns  : \\mathbb{R}^n \\to \\mathbb{R}_{\\ge 0},\ny \\mapsto s(y) := \\sqrt{s^2(y)},\n\\end{equation}\\]\n\nOft bleibt wie hier das Wesen von Statistiken als Zufallsvariablen oder Zufallsvektoren notationell eher implizit. Dies ändert allerdings nichts an der fundamental zu beachtetenden Tatsache, dass Statistiken als Funktionen von vom Zufall abhängigen Werten selbst wiederrum Zufallsvariablen oder Zufallsvektoren sind.\n\nDefinition 21.5 (Schätzer) \\(\\mathcal{M}\\) sei ein Frequentistisches Inferenzmodell, \\((\\Sigma, \\mathcal{S})\\) sei ein Messraum und \\(\\tau : \\Theta \\to \\Sigma\\) sei eine Abbildung, die jedem \\(\\theta \\in \\Theta\\) eine Kenngröße \\(\\tau(\\theta) \\in \\Sigma\\) zuordnet. Dann heißt eine Statistik \\[\\begin{equation}\n\\hat{\\tau} : \\mathcal{Y} \\to \\Sigma\n\\end{equation}\\] ein für \\(\\tau\\).\n\nSchätzer schätzen also Funktionen der Parameter eines parametrischen Frequentistischen Inferenzmodells. Typische Beispiele für solche Funktionen sind\n\n\\(\\tau(\\theta) := \\theta\\) für die Schätzung des Parameters \\(\\theta\\),\n\\(\\tau(\\theta) := \\theta_i\\) mit \\(\\theta \\in \\mathbb{R}^d, d &gt; 1\\) für die Schätzung einer Komponente des Parameters \\(\\theta\\),\n\\(\\tau(\\theta) := \\mathbb{E}_\\theta(y_1)\\) für die Schätzung des Erwartungswerts,\n\\(\\tau(\\theta) := \\mathbb{V}_\\theta(y_1)\\) für die Schätzung der Varianz.\n\nIm Falle \\(\\tau(\\theta) := \\theta\\), also der Schätzung von Parametern, schreibt man üblicherweise \\(\\hat{\\theta}\\). Man beachte, dass Schätzer Zahlwerte in \\(\\Sigma\\) annehmen, bei der Schätzung von Parametern etwa in \\(\\Theta\\). Sie heißen deshalb auch Punktschätzer. Dies ist ein Charaketeristikum Frequentistischer Inferenzverfahren. Im Rahmen der Bayesianischen Inferenz können Schätzer auch generalisierte Formen annehmen, zum Beispiel werden dort auch Wahrscheinlichkeitsverteilungen als Schätzer betrachtet. Schließlich ist festzuhalten, dass die Definition eines Schätzers keinerlei Aussage über die Validität von Schätzern macht. Nicht jeder Schätzer ist damit perse ein guter Schätzer. In der Frequentistischen Inferenz definiert man deshalb zusätzlich Schätzgütekriterien, wie in Kapitel 22 ausführlich dargestellt.\n\nBeispiel\n\\(\\mathcal{M}\\) sei das Normalverteilungsmodell. Dann ist zum Beispiel das Stichprobenmittel \\(\\bar{y} : \\mathbb{R}^n \\to \\mathbb{R}\\) ein Schätzer für \\[\\begin{equation}\n\\tau : \\mathbb{R} \\times \\mathbb{R}_{&gt;0} \\to \\mathbb{R},\n(\\mu, \\sigma^2) \\mapsto \\tau(\\mu,\\sigma^2) := \\mu.\n\\end{equation}\\] Ebenso ist \\(\\bar{y}\\) ein Schätzer für \\[\\begin{equation}\n\\tau: \\mathbb{R} \\times \\mathbb{R}_{&gt;0} \\to \\mathbb{R},\n(\\mu, \\sigma_2) \\mapsto \\tau(\\mu,\\sigma^2) := \\mathbb{E}_{\\mu,\\sigma^2}(y_1).\n\\end{equation}\\] Weiterhin ist die konstante Funktion \\[\\begin{equation}\n\\hat{\\tau} : \\mathbb{R}^n \\to \\mathbb{R}, y \\mapsto \\hat{\\tau}(y) := 42\n\\end{equation}\\] ein Schätzer für \\[\\begin{equation}\n\\tau : \\mathbb{R} \\times \\mathbb{R}_{&gt;0} \\to \\mathbb{R}_{&gt;0},\n(\\mu, \\sigma_2) \\mapsto \\tau(\\mu,\\sigma^2) := \\sigma^2.\n\\end{equation}\\] Dass eine Funktion \\(\\hat{\\tau} : \\mathcal{Y} \\to \\Sigma\\) ein Schätzer ist impliziert also keinesfalls, dass sie ein guter Schätzer ist.",
    "crumbs": [
      "Frequentistische Inferenz",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Grundbegriffe</span>"
    ]
  },
  {
    "objectID": "301-Grundbegriffe.html#standardannahmen-und-standardproblemstellungen",
    "href": "301-Grundbegriffe.html#standardannahmen-und-standardproblemstellungen",
    "title": "21  Grundbegriffe",
    "section": "21.3 Standardannahmen und Standardproblemstellungen",
    "text": "21.3 Standardannahmen und Standardproblemstellungen\nWir wollen die in diesem Kapitel bisher betrachteten Konzepte zunächst noch einmal unter dem Begriff der datenanalytischen Standardannahmen der Frequentistischen Inferenz zusammenfassen (vgl. auch Abbildung 21.1). Dazu sei \\(\\mathcal{M}\\) ein univariates parametrisches Frequentistisches Produktmodell und es seien \\(\\upsilon_1,...,\\upsilon_n \\sim p_\\theta\\) die Zufallsvariablen der Stichprobe, die wir etwa in einem Zufallvektor \\(\\upsilon := (\\upsilon_1,...,\\upsilon_n)\\) zusammenfassen können. Von einem konkret vorliegenden Datensatz \\(y_1,...,y_n\\) mit \\(y_i \\in \\mathbb{R}\\), \\(i = 1,...,n\\), den wir etwa in einem \\(n\\)-dimensionalen Vektor \\(y := (y_1,...,y_n)^T \\in \\mathbb{R}^n\\) zusammenfassen können, wird dann angenommen, dass er eine der möglichen Realisierungen von \\(\\upsilon\\) auf Grundlage einer Verteilung \\(\\mathbb{P}_\\theta\\) mit wahrem, aber unbekannten, Parameter \\(\\theta\\) ist. Aus Frequentistischer Sicht kann man dabei die Beobachtung eines Datensatzes unendlich oft wiederholen und zu jeder Datenrealisierung Schätzer oder Statistiken auswerten, so zum Beispiel das Stichprobenmittel:\nVor diesem Hintegrund behandelt die behandelt die Frequentistische Inferenz dann üblicheweise folgende Standardproblemstellungen:\n\nPunktschätzung. Ziel der Punktschätzung ist es, auf Grundlage beobachteter Daten einen präzisen und im Frequentistischen Sinn möglichst guten Tipp für den wahren, aber unbekannten, Parameterwert abzugeben.\nKonfidenzintervallbestimmung. Ziel der Konfidenzintervallbestimmung ist es, basierend auf der angenommenen Datenverteilung und den beobachteten Daten durch eine Intervallschätzung einen möglichst sicheren, wenn auch oft unpräzisen, Tipp für den wahren, aber unbekannten, Parameterwert abzugeben.\nHypothesentests. Ziel des Frequentistischen Hypothesentestens ist es, basierend auf der angenommenen Verteilung der Daten in einer möglichst zuverlässigen Form zu entscheiden, ob ein wahrer, aber unbekannter Parameterwert in einer von zwei sich gegenseitig ausschließenden Untermengen des Parameterraumes\n\n\n\n\n\n\n\nAbbildung 21.1: Standardannahmen und Standardproblemstellungen Frequentistischer Inferenz. Die Frequentistische Inferenz unterstellt, dass es in der Wirklichkeit einen wahren, aber unbekannten, Parameterwert des Wahrscheinlichkeitsmaßes \\(\\mathbb{P}_\\theta\\) gibt, dass als Modell für die Erhebung eines Datensatzes dient. Ein konkret vorliegender Datenzsatz \\(y = (y_1,...,y_n)\\) ist dann eine (und insbesondere nur eine) der möglichen Realisierungen des anhand \\(\\mathbb{P}_\\theta\\) verteilten Zufallsvektors \\(\\upsilon := (\\upsilon_1,...,\\upsilon_n)\\). Auf Grundage dieser Realisierung beabsichtigen die Verfahren zur Behandlung der Frequentistischen Standardproblemstellungen von Punktschätzung, Konfidenzintervallbestimmung und Hypothesentestauswertung möglichst valide Aussagen hinsichtlich des wahren, aber unbekannten Parameterwertes zu machen, in den Kapiteln Kapitel 22, Kapitel 23 und Kapitel 24 diskutiert werden soll. Der tatsächliche wahre, aber unbekannte, Parameterwert aber bleibt auch nach Abschluss eines Inferenzverfahrens immer unbekannt.\n\n\n\nVerfahren zur Lösung dieser Problemstellungen bezeichnen wir als Frequentistische Inferenzverfahren. Um die Qualität von Frequentistischen Inferenzverfahren zu beurteilen, betrachtet man in der Frequentistischen Inferenz üblicherweise die Verteilungen von Schätzern und Statistiken unter der Annahme von \\(\\upsilon = (\\upsilon_1,...,\\upsilon_n) \\sim p_\\theta\\). Man fragt zum Beispiel nach der Verteilung der oben skizzierten \\(\\bar{y}^{(1)}\\), \\(\\bar{y}^{(2)}\\), \\(\\bar{y}^{(3)}\\), \\(\\bar{y}^{(4)}\\), … also der Verteilung der Zufallsvariable \\(\\bar{\\upsilon}_n\\). Wenn ein Inferenzverfahren auf Grundlage dieser Annahmen für “gut” befunden wir, so heißt das also insbesondere nur, dass das Verfahren bei häufiger Anwendung “im Mittel gut” ist. Im Einzelfall, also im Normalfall nur eines vorliegenden Datensatzes, kann sie auch “schlecht” sein. Wir werden diese Denkweise insbesondere im Kontext der Punktschätzung (Kapitel 22) vertiefen. Ebenso beurteilt die Frequentistische Inferenz die Stärke empirischer Evidenz vor dem Hintegrund der angenommenen Verteilung von Schätzern und Statistiken in Szenarien, in denen angenommen wird, das interessierende Effekt nicht existieren (sogenannte “Nullhypothesen”). Diese Denkweise verdeutlichen wir insbesondere im Rahmen der Betrachtung von Konfidenzintervallen (Kapitel 23) und Hypothesentests (Kapitel 24).\n\n21.3.1 Anwendungsbeispiel\nFür ein erstes Anwendungsbeispiel Frequentistischer Inferenzverfahren betrachten wir die evidenzbasierte Evaluation einer Psychotherapie bei Depression. Dazu sei der in Tabelle 32.1 dargestellte Datensatz von an \\(n = 12\\) Patient:innen Differenzen von Prä- und Post-Therapie erhobenen BDI-II Scores gegeben (dBDI; Beck (1961), Beck et al. (2009)). Die dBDI Werte sollen dabei die Reduktion des BDI-II Scores der Patient:innen über den Zeitraum der Therapie wiederspiegeln. Hohe positive Werte von dBDI entsprechen also einer starken Abnahme der durch den BDI-II Score quantifizierten Depressionssymptomatik, Werte um Null entsprechen keiner wesentlichen Änderung und negative Werte entsprechen einer Zunahme der durch den BDI-II Score quantifizierten Depressionssymptomatik.\n\n\n\n\nTabelle 21.1: Prä-Post-Therapie BDI-II Reduktionsscores von n = 12 Patient:innen\n\n\n\n\n\n\ndBDI\n\n\n\n\n-1\n\n\n3\n\n\n-2\n\n\n9\n\n\n3\n\n\n-2\n\n\n4\n\n\n5\n\n\n5\n\n\n1\n\n\n9\n\n\n4\n\n\n\n\n\n\n\n\nFür jeden der \\(n := 12\\) dBDI Werte legen wir nun das Modell \\[\n\\upsilon_{i} := \\mu + \\varepsilon_{i} \\mbox{ mit } \\varepsilon_{i} \\sim N(0,\\sigma^2) \\mbox{ u.i.v. für } i = 1,...,n\n\\tag{21.1}\\] zugrunde. Damit wird der dBDI der \\(i\\)ten Patient:in also mithilfe einer über die Gruppe von Patient:innen identischen BDI-II Score Reduktion \\(\\mu \\in \\mathbb{R}\\) und einer Patient:innen-spezifischen normalverteilten BDI-II Score Reduktionsabweichung \\(\\varepsilon_{i}\\) erklärt und es wird angenommen, dass sich diese Reduktionsabweichungen zwischen Patient:innen nicht gegenseitig beeinflussen. Intuitiv wird also davon ausgegangen, dass die Therapie einen Effekt hat, der bei allen Patient:innen zur gleichen BDI-II Score Reduktion \\(\\mu\\) führt und sich die Unterschiede in den beobachteten dBDI Werten durch eine Vielzahl weiterer Zufallvorgänge, die in der Summe normalverteilt und zentriert sind erklären lässt. Alternativ mag man diese Abweichungen als Realisierungen der Unsicherheit verstehen mit der das Modell in Gleichung 21.1 behaftet ist.\nAus Gleichung 21.1 folgt dann direkt \\[\\begin{equation}\n\\upsilon_1,...,\\upsilon_n \\sim N(\\mu,\\sigma^2),\n\\end{equation}\\] denn für \\(i = 1,...,n\\) und mit \\[\\begin{equation}\n\\upsilon_i = f(\\varepsilon_i)\n\\mbox{ mit }\nf : \\mathbb{R} \\to \\mathbb{R}, e_i \\mapsto f(e_i) := e_i + \\mu.\n\\end{equation}\\] gilt für die WDFen der \\(\\upsilon_i\\), dass \\[\\begin{align}\n\\begin{split}\np_{\\upsilon_i}(y_i)\n& = \\frac{1}{|1|} p_{\\varepsilon_i}\\left(\\frac{y_i - \\mu}{1} \\right)                        \\\\\n& = N\\left(y_i - \\mu; 0, \\sigma^2\\right)                                                    \\\\\n& = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{1}{2\\sigma^2}(y_i - \\mu - 0)^2 \\right)    \\\\\n& = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{1}{2\\sigma^2}(y_i - \\mu)^2 \\right)        \\\\\n& = N(y_i; \\mu,\\sigma^2)\n\\end{split}\n\\end{align}\\]\nDie Standardproblemstellungen der Frequentistischen Inferenz führen in diesem Anwendungsszenario dann auf folgende Fragen, die wir jeweils in den Kapiteln zur Punktschätzung, Konfidenzintervallbestimmung, und Hypothesentestevaluation wieder aufgreifen wollen:\n\nWas sind sinnvolle Tipps für die wahren, aber unbekannten, Parameterwerte \\(\\mu\\) und \\(\\sigma^2\\), also den wahren, aber unbekannten, Erwartungswert der BDI-II Score Reduktion und ihre wahre, aber unbekannte, Varianz? Wie gut ist die Therapie also in diesem quantitativen Sinn, wenn wir versuchen, die Patient:innen-abhängigen Abweichungen zu berücksichtigen und wie groß ist die in der Datengeneration inhärente Unsicherheit?\nWie kann im Sinne einer Intervallschätzung eine möglichst sichere Schätzung des wahren, aber unbekannten, Erwartungswert der BDI-II Score Reduktion gelingen? Wie unpräzise muss eine solche Schätzung sein, um möglichst verlässlich zu sein?\nEntscheiden wir uns sinnvollerweise für eine der Hypothesen, dass die Therapie nicht wirksam ist (\\(\\mu = 0\\)) oder dass sie etwa im positiven (\\(\\mu &gt; 0\\)) oder auch im negativen Sinne wirksam ist (\\(\\mu &lt; 0\\))? Und wenn wir uns für eine dieser Hypothesen entscheiden sollten, mit welcher Fehlerwahrscheinlichkeit täten wir dies? Wie hoch ist also die einer solchen Entscheidung innewohnende Unsicherheit?",
    "crumbs": [
      "Frequentistische Inferenz",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Grundbegriffe</span>"
    ]
  },
  {
    "objectID": "301-Grundbegriffe.html#selbstkontrollfragen",
    "href": "301-Grundbegriffe.html#selbstkontrollfragen",
    "title": "21  Grundbegriffe",
    "section": "21.4 Selbstkontrollfragen",
    "text": "21.4 Selbstkontrollfragen\n\nGeben Sie die Definition des Begriffs des parametrischen Frequentistischen Inferenzmodells wieder.\nErläutern Sie den Begriff des parametrischen Frequentistischen Inferenzmodells.\nGeben Sie die Definition des Begriffs des parametrischen Frequentistischen Produktmodells wieder.\nErläutern Sie den Begriff des parametrischen Frequentistischen Produktmodells.\nWas ist der Unterschied zwischen univariaten und multivariaten Frequentistischen Inferenzmodellen?\nGeben Sie die Definition des Begriffs des Normalverteilungsmodells wieder.\nGeben Sie die Definition des Begriffs des Bernoullimodells wieder.\nGeben Sie die Definition des Begriffs der Statistik wieder.\nErläutern Sie den Begriff der Statistik.\nGeben Sie die Definition des Begriffs des Schätzers wieder.\nErläutern Sie den Begriff des Schätzers.\nErläutern Sie die datenanalytischen Standardannahmen der Frequentistischen Inferenz.\n\n\n\n\n\nBeck, A. T. (1961). An Inventory for Measuring Depression. Archives of General Psychiatry, 4(6), 561. https://doi.org/10.1001/archpsyc.1961.01710120031004\n\n\nBeck, A. T., Steer, R. A., & Brown, G. K. (2009). Beck-Depressions-Inventar II. Deutsche Bearbeitung von M. Hautzinger / F. Keller / C. Kühner. Hogrefe.",
    "crumbs": [
      "Frequentistische Inferenz",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Grundbegriffe</span>"
    ]
  },
  {
    "objectID": "302-Punktschätzung.html",
    "href": "302-Punktschätzung.html",
    "title": "22  Punktschätzung",
    "section": "",
    "text": "22.1 Maximum-Likelihood Schätzung\nDie Grundidee der Maximum-Likelihood Schätzung ist es, als Tipp für einen wahren, aber unbekannten, Parameterwert denjenigen Parameterwert zu wählen, für den die Wahrscheinlichkeit der beobachteten Daten maximal ist. Dafür ist es zunächst nötig, die Wahrscheinlichkeit beobachteter Daten eines Frequentistischen Inferenzmodells als Funktion des betreffenden Parameters zu betrachten. Dies ermöglichen und formalisieren die Likelihood-Funktion und ihr Logarithmus, die Log-Likelihood-Funktion. Wir definieren diese Begriffe hier für parametrische Produktmodelle.\nDie Likelihood-Funktion ist also eine Funktion des Parameters und ihre Funktionswerte sind die Werte der gemeinsamen WMF bzw. WDF beobachteter Datenwerte \\(y_1,...,y_n\\). Generell gibt es keinen Grund anzunehmen, dass eine Likelihood-Funktion über dem Parameterraum zu 1 integriert, die Likelihood-Funktion ist also im Allgemeinen keine WMF oder WDF. Die Log-Likelihood Funktion ist schlicht die logarithmierte Likelihood-Funktion. Ein nach dem Prinzip der Maximum-Likelihood Schätzung gewonnener Parameterschätzer soll nun die Likelihood-Funktion bzw. die Log-Likelihood-Funktion maximieren. Dies führt auf folgende Definition des Begriffs des Maximum-Likelihood Schätzers.\nMan beachte bei Definition 22.3, dass eine Maximumstelle der Log-Likelihood-Funktion der Maximumstelle der Likelihood-Funktion entspricht, weil die Logarithmusfunktion eine monoton steigende Funktion ist. Das Arbeiten mit der Log-Likelihood-Funktion ist allerdings oft einfacher als das direkte Arbeiten mit der Likelihood-Funktion, zum Beispiel, wenn in der WMF oder WDF des Modells Exponentialfunktionen auftauchen. Weiterhin beachte man bei Definition 22.3, dass Definition 22.2 impliziert, dass \\[\\begin{equation}\n\\hat{\\theta}^{\\mbox{\\tiny ML}}(y)\n= \\mbox{argmax}_{\\theta \\in \\Theta} \\prod_{i=1}^n p_\\theta(y_i)\n= \\mbox{argmax}_{\\theta \\in \\Theta} \\sum_{i=1}^n \\ln p_\\theta(y_i)\n\\end{equation}\\] was die Abhängigkeit eines Maximum-Likelihood Schätzers von den Daten verdeutlicht.\nMit Definition 22.3 handelt es sich bei der Maximum-Likelihood Schätzung also um das Problem, Extremalstellen einer Funktion zu bestimmen. Für diese Extremalstellen stellt die Differentialrechnung bekanntlich notwendige und hinreichende Bedingungen bereit (vgl. Kapitel 5.2). In ihrer Anwendung auf die Gewinnung von Maximum-Likelihood Schätzern begnügt man sich zumeist aufgrund der funktionellen Form der betrachteten Funktionen mit dem Erfülltsein der notwendigen Bedingung. Je nach Beschaffenheit der Log-likelihood Funktion bieten sich dann Methoden entweder der analytischen Optimierung oder der numerischen Optimierung an. In den folgenden klassischen Beispielen nutzen wir einen analytischen Zugang anhand folgendem standardisierten Vorgehen:\nIn Theorem 22.1 zeigen wir, dass der Maximum-Likelihood Schätzer für den Parameter des Bernoullimodells aus Definition 21.3 durch das entsprechende Stichprobenmittel gegeben ist und in Theorem 22.2 zeigen wir, dass die Maximum-Likelihood Schätzer für den Erwartungswert- und Varianzparameter des Normalverteilungsmodells aus Definition 21.2 durch das Stichprobenmittel und eine modifizierte Stichprobenvarianz, respektive, gegeben sind.",
    "crumbs": [
      "Frequentistische Inferenz",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Punktschätzung</span>"
    ]
  },
  {
    "objectID": "302-Punktschätzung.html#sec-maximum-likelihood-schätzung",
    "href": "302-Punktschätzung.html#sec-maximum-likelihood-schätzung",
    "title": "22  Punktschätzung",
    "section": "",
    "text": "Definition 22.2 (Likelihood-Funktion und Log-Likelihood-Funktion) \\(\\mathcal{M}\\) sei ein parametrisches Produktmodell mit WMF oder WDF \\(p_\\theta\\). Dann ist die Likelihood-Funktion definiert als \\[\\begin{equation}\nL : \\Theta \\to [0,\\infty[, \\theta \\mapsto L(\\theta) := \\prod_{i=1}^n p_\\theta(y_i)\n\\end{equation}\\] und die Log-Likelihood-Funktion ist definiert als \\[\\begin{equation}\n\\ell_n : \\Theta \\to \\mathbb{R}, \\theta \\mapsto \\ell(\\theta) := \\ln L(\\theta).\n\\end{equation}\\]\n\n\n\nDefinition 22.3 (Maximum-Likelihood Schätzer) \\(\\mathcal{M}\\) sei ein parametrisches Produktmodell mit Parameter \\(\\theta \\in \\Theta\\). Ein Maximum-Likelihood Schätzer von \\(\\theta\\) ist definiert als \\[\\begin{equation}\n\\hat{\\theta}^{\\mbox{\\tiny ML}} : \\mathcal{Y} \\to \\Theta,\ny \\mapsto \\hat{\\theta}^{\\mbox{\\tiny ML}}(y)\n:= \\mbox{argmax}_{\\theta \\in \\Theta} L(\\theta)\n= \\mbox{argmax}_{\\theta \\in \\Theta} \\ell(\\theta)\n\\end{equation}\\]\n\n\n\n\nFormulierung der Log-Likelihood-Funktion.\nBestimmung der ersten Ableitung der Log-Likelihood-Funktion und Nullsetzen.\nAuflösen nach potentiellen Maximumstellen.\n\n\n\nBeispiele\n\nTheorem 22.1 (Maximum-Likelihood Schätzer des Bernoullimodells) \\(\\mathcal{M}\\) sei das Bernoullimodell, es gelte also \\(\\upsilon_1,...,\\upsilon_n \\sim \\mbox{Bern}(\\mu)\\). Dann ist \\[\\begin{equation}\n\\hat{\\mu}^{\\mbox{\\tiny ML}} : \\{0,1\\}^n \\to [0,1],\ny \\mapsto \\hat{\\mu}^{\\mbox{\\tiny ML}}(y):= \\frac{1}{n}\\sum_{i=1}^n y_i\n\\end{equation}\\] ein Maximum-Likelihood Schätzer von \\(\\mu\\)\n\n\nBeweis. Wir formulieren zunächst die Log-Likelihood-Funktion. Für die Likelihood-Funktion gilt \\[\\begin{equation}\nL : ]0,1[ \\to ]0,1[,\n\\mu \\mapsto L(\\mu)\n:= \\prod_{i=1}^n \\mu^{y_i}(1 - \\mu)^{1-y_i}\n= \\mu^{\\sum_{i=1}^n y_i}(1 - \\mu)^{n - \\sum_{i=1}^n y_i}.\n\\end{equation}\\] Logarithmieren ergibt \\[\\begin{equation}\n\\ell : ]0,1[ \\to \\mathbb{R}, \\mu \\mapsto  \\ell(\\mu)\n= \\ln \\mu \\sum_{i=1}^n y_i + \\ln (1- \\mu) \\left(n - \\sum_{i=1}^n y_i \\right).\n\\end{equation}\\] Wir werten dann die Ableitung der Log-Likelihood-Funktion aus. Es gilt \\[\\begin{align}\n\\begin{split}\n\\frac{d}{d\\mu} \\ell(\\mu)\n& = \\frac{d}{d\\mu}\\left(\\ln \\mu \\sum_{i=1}^n y_i + \\ln (1- \\mu) \\left(n - \\sum_{i=1}^n y_i \\right)\\right)  \\\\\n& = \\frac{d}{d\\mu} \\ln \\mu \\sum_{i=1}^n y_i  + \\frac{d}{d\\mu} \\ln (1 - \\mu) \\left(n - \\sum_{i=1}^n y_i \\right)  \\\\\n& = \\frac{1}{\\mu}\\sum_{i=1}^n y_i  -  \\frac{1}{1-\\mu} \\left(n - \\sum_{i=1}^n y_i \\right).\n\\end{split}\n\\end{align}\\] Nullsetzen ergibt dann folgende als notwendige Bedingung für einen Maximum-Likelihood Schätzer im Bernoullimodell: \\[\\begin{equation}\n\\frac{1}{\\hat{\\mu}^{\\mbox{\\tiny ML}}}\\sum_{i=1}^n y_i - \\frac{1}{1-\\hat{\\mu}^{\\mbox{\\tiny ML}}} \\left(n - \\sum_{i=1}^n y_i \\right) = 0.\n\\end{equation}\\] Auflösen der Maximum-Likelihood-Gleichung nach \\(\\hat{\\mu}^{\\mbox{\\tiny ML}}\\) ergibt dann \\[\\begin{align}\n\\begin{split}\n\\frac{1}{\\hat{\\mu}^{\\mbox{\\tiny ML}}}\\sum_{i=1}^n y_i - \\frac{1}{1-\\hat{\\mu}^{\\mbox{\\tiny ML}}} \\left(n - \\sum_{i=1}^n y_i \\right) & = 0 \\\\\n\\Leftrightarrow\n\\hat{\\mu}^{\\mbox{\\tiny ML}}(1 - \\hat{\\mu}^{\\mbox{\\tiny ML}})\\left(\\frac{1}{\\hat{\\mu}^{\\mbox{\\tiny ML}}}\\sum_{i=1}^n y_i - \\frac{1}{1-\\hat{\\mu}^{\\mbox{\\tiny ML}}} \\left(n - \\sum_{i=1}^n y_i \\right) \\right) & = 0 \\\\\n\\Leftrightarrow\n\\sum_{i=1}^n y_i - \\hat{\\mu}^{\\mbox{\\tiny ML}} \\sum_{i=1}^n y_i - n \\hat{\\mu}^{\\mbox{\\tiny ML}}  + \\hat{\\mu}^{\\mbox{\\tiny ML}}\\sum_{i=1}^n y_i & = 0 \\\\\n\\Leftrightarrow\nn \\hat{\\mu}^{\\mbox{\\tiny ML}}  & = \\sum_{i=1}^n y_i \\\\\n\\Leftrightarrow\n\\hat{\\mu}^{\\mbox{\\tiny ML}}  & = \\frac{1}{n} \\sum_{i=1}^n y_i. \\\\\n\\end{split}\n\\end{align}\\] \\(\\hat{\\mu}^{\\mbox{\\tiny ML}} = \\frac{1}{n}\\sum_{i=1}^n y_i\\) ist also ein Kandidat für einen Maximum-Likelihood Schätzer von \\(\\mu\\). Dies könnte durch Betrachten der zweiten Ableitung von \\(\\ell\\) verifiziert werden, worauf wir hier aber verzichten wollen.\n\n\nTheorem 22.2 (Maximum-Likelihood Schätzer des Normalverteilungsmodells) \\(\\mathcal{M}\\) sei das Normalverteilungsmodell, es gelt also \\(\\upsilon_1,...,\\upsilon_n \\sim N\\left(\\mu,\\sigma^2\\right)\\). Dann sind \\[\\begin{equation}\n\\hat{\\mu}^{\\mbox{\\tiny ML}} :\n\\mathbb{R}^n \\to \\mathbb{R}, y \\mapsto \\hat{\\mu}^{\\mbox{\\tiny ML}}(y)\n:= \\frac{1}{n}\\sum_{i=1}^n y_i\n\\end{equation}\\] und \\[\\begin{equation}\n\\hat{\\sigma}^{2^{\\mbox{\\tiny ML}}} :\n\\mathbb{R}^n \\to \\mathbb{R}_{\\ge 0},\ny \\mapsto \\hat{\\sigma}^{2^{\\mbox{\\tiny ML}}}(y)\n:= \\frac{1}{n}\\sum_{i=1}^n \\left(y_i - \\hat{\\mu}^{\\mbox{\\tiny ML}}\\right)^2.\n\\end{equation}\\] Maximum-Likelihood Schätzer für \\(\\mu\\) und \\(\\sigma^2\\), respektive.\n\n\nBeweis. Wir formulieren zunächst die Log-Likelihood-Funktion. Für die Likelihood-Funktion ergibt sich \\[\\begin{align}\n\\begin{split}\nL : \\mathbb{R} \\times \\mathbb{R}_{&gt;0} \\to \\mathbb{R}_{&gt;0},\n(\\mu,\\sigma^2) \\mapsto L(\\mu,\\sigma^2)\n:= & \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left(-\\frac{1}{2\\sigma^2}(y_i-\\mu)^2\\right) \\\\\n= & \\left(2 \\pi \\sigma^2\\right)^{-\\frac{n}{2}}\\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(y_i-\\mu)^2\\right). \\\\\n\\end{split}\n\\end{align}\\] Logarithmieren ergibt dann \\[\\begin{equation}\n\\ell : \\mathbb{R} \\times \\mathbb{R}_{&gt;0} \\to \\mathbb{R},\n(\\mu,\\sigma^2) \\mapsto \\mathcal{\\ell}_n(\\mu,\\sigma^2)\n= -\\frac{n}{2} \\ln 2\\pi - \\frac{n}{2} \\ln \\sigma^2  -\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(y_i-\\mu)^2.\n\\end{equation}\\] Die Auswertung der partiellen Ableitungen der Log-Likelihood-Funktion ergeben dann \\[\\begin{equation}\n\\frac{\\partial}{\\partial{\\mu}} \\ell(\\mu,\\sigma^2)\n= - \\frac{\\partial}{\\partial{\\mu}} \\frac{1}{2\\sigma^2}\\sum_{i=1}^n(y_i-\\mu)^2\n= - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n \\frac{\\partial}{\\partial{\\mu}} (y_i-\\mu)^2\n= \\frac{1}{\\sigma^2}\\sum_{i=1}^n (y_i-\\mu)\n\\end{equation}\\] und \\[\\begin{align}\n\\begin{split}\n\\frac{\\partial}{\\partial\\sigma^2} \\ell(\\mu,\\sigma^2)\n= - \\frac{n}{2} \\frac{\\partial}{\\partial\\sigma^2} \\ln \\sigma^2  - \\frac{\\partial}{\\partial\\sigma^2} \\frac{1}{2\\sigma^2}\\sum_{i=1}^n(y_i-\\mu)^2\n= - \\frac{n}{2 \\sigma^2} + \\frac{1}{2\\sigma^4}\\sum_{i=1}^n(y_i-\\mu)^2.\n\\end{split}\n\\end{align}\\] Das System der Maximum-Likelihood Gleichungen als Ausdruck der notwendigen Bedingungen für Extremstellen der Log-Likelihood-Funktion hat in diesem Fall also die Form \\[\\begin{equation}\n\\sum_{i=1}^n (y_i-\\hat{\\mu}^{\\mbox{\\tiny ML}}) = 0\n\\mbox{ und }\n- \\frac{n}{2 \\hat{\\sigma}^{2^{\\mbox{\\tiny ML}}}} + \\frac{1}{2\\hat{\\sigma}^{4^{\\mbox{\\tiny ML}}}}\\sum_{i=1}^n(y_i-\\mu)^2  = 0.\n\\end{equation}\\] Lösen des Systems der Maximum-Likelihood Gleichungen ergibt dann zunächst \\[\\begin{equation}\n\\sum_{i=1}^n (y_i-\\hat{\\mu}^{\\mbox{\\tiny ML}})  = 0\n\\Leftrightarrow \\sum_{i=1}^n y_i  = n\\hat{\\mu}^{\\mbox{\\tiny ML}}\n\\Leftrightarrow \\hat{\\mu}^{\\mbox{\\tiny ML}} = \\frac{1}{n}\\sum_{i=1}^n y_i.\n\\end{equation}\\] Damit ist \\[\\begin{equation}\n\\hat{\\mu}^{\\mbox{\\tiny ML}} = \\frac{1}{n}\\sum_{i=1}^n y_i\n\\end{equation}\\] ein potentieller Maximum-Likelihood Schätzer von \\(\\mu\\). Einsetzen dieses Schätzers in die zweite Maximum-Likelihood Gleichung ergibt dann\n\\[\\begin{align}\n\\begin{split}\n- \\frac{n}{2 \\hat{\\sigma}^{2^{\\mbox{\\tiny ML}}}} + \\frac{1}{2\\hat{\\sigma}^{4^{\\mbox{\\tiny ML}}}}\\sum_{i=1}^n(y_i-\\hat{\\mu}^{\\mbox{\\tiny ML}})^2 & = 0 \\\\\n\\Leftrightarrow\n- n\\hat{\\sigma}^{2^{\\mbox{\\tiny ML}}} + \\sum_{i=1}^n(y_i-\\hat{\\mu}^{\\mbox{\\tiny ML}})^2 & = 0 \\\\\n\\Leftrightarrow\n\\hat{\\sigma}^{2^{\\mbox{\\tiny ML}}} & = \\frac{1}{n} \\sum_{i=1}^n(y_i-\\hat{\\mu}^{\\mbox{\\tiny ML}})^2.\n\\end{split}\n\\end{align}\\] Also ist \\[\\begin{equation}\n\\hat{\\sigma}^{2^{\\mbox{\\tiny ML}}} = \\frac{1}{n}\\sum_{i=1}^n\\left(y_i-\\hat{\\mu}^{\\mbox{\\tiny ML}}\\right)^2\n\\end{equation}\\] ein potentieller Maximum-Likelihood Schätzer von \\(\\sigma^2\\). Beide potentiellen Maximum-Likelihood Schätzer können durch Betrachten der zweiten Ableitung von \\(\\ell\\) verifiziert werden, worauf wir hier verzichten wollen.\n\nMan beachte bei Theorem 22.2, dass \\(\\hat{\\mu}^{\\mbox{\\tiny ML}}\\) mit dem Stichprobenmittel \\(\\bar{\\upsilon}\\) identisch ist, aber \\(\\hat{\\sigma}^{2^{\\mbox{\\tiny ML}}}\\) nicht mit der Stichprobenvarianz \\(S^2\\) übereinstimmt. Im Gegensatz zur Stichprobenvarianz findet sich im Maximum-Likelihood Schätzer von \\(\\sigma^2\\) der multiplikative Faktor \\(\\frac{1}{n}\\), nicht, wie in der Stichprobenvarianz, der multiplikative Faktor \\(\\frac{1}{n-1}\\). Wir werden auf diesen Unterschied im Kontext der Schätzereigenschaften zurückkommen.\n\n\nAnwendungsbeispiel\nZum Abschluss dieses Abschnitts wollen wir Theorem 22.2 im Kontext des Anwendungsbeispiels aus Kapitel 21.3.1 betrachten. Wir hatten dort den beobachteten dBDI Werten das Normalverteilungsmodell \\[\\begin{equation}\n\\upsilon_1,...,\\upsilon_n \\sim N(\\mu,\\sigma^2)\n\\end{equation}\\] zugrundegelegt. Die Maximum-Likelihood Schätzer für die Parameter dieses Modells lassen sich dann anhand von Theorem 22.2 mithilfe der R Stichprobenmittel- und Stichprobenvarianzfunktionen mean() und var() und unter Beachtung der Identität \\[\\begin{equation}\n\\frac{n-1}{n}s^2\n= \\frac{n-1}{n}\\cdot\\frac{1}{n-1}\\sum_{i=1}^n (y_i - \\bar{y})^2\n= \\frac{1}{n}\\sum_{i=1}^n \\left(y_i - \\hat{\\mu}^{\\mbox{\\tiny ML}}\\right)^2\n= \\hat{\\sigma}^{2^{\\mbox{\\tiny ML}}}\n\\end{equation}\\] wie in folgendem R Code auswerten.\n\nD           = read.csv(\"./_data/302-Punktschätzung.csv\") # Datensatzeinlesen \ny           = D$dBDI                                     # Datenauswahl            \nmu_hat      = mean(y)                                    # Maximum-Likelihood Schätzung des Erwartungswertparameters\nn           = length(y)                                  # Anzahl der Datenpunkte\nsigsqr_hat  = ((n-1)/n)*var(y)                           # Maximum-Likelihood Schätzung des Varianzparameters\ncat(\"mu_hat     :\", mu_hat,\"\\nsigsqr_hat :\", sigsqr_hat) # Ausgabe    \n\nmu_hat     : 3.166667 \nsigsqr_hat : 12.63889\n\n\nBasierend auf dem Prinzip der Maximum-Likelihood Schätzung und den vorliegenden \\(n = 12\\) Datenpunkten sind also \\[\\begin{equation}\n\\hat{\\mu}^{\\mbox{\\tiny ML}} = 3.17\n\\mbox{ und }\n\\hat{\\sigma}^{2^{\\mbox{\\tiny ML}}} = 12.6\n\\end{equation}\\] Tipps für die wahren, aber unbekannten, Parameter des Modells.",
    "crumbs": [
      "Frequentistische Inferenz",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Punktschätzung</span>"
    ]
  },
  {
    "objectID": "302-Punktschätzung.html#sec-schaetzereigenschaften-bei-endlichen-stichproben",
    "href": "302-Punktschätzung.html#sec-schaetzereigenschaften-bei-endlichen-stichproben",
    "title": "22  Punktschätzung",
    "section": "22.2 Schätzereigenschaften bei endlichen Stichproben",
    "text": "22.2 Schätzereigenschaften bei endlichen Stichproben\nAllgemein betreffen Frequentistische Schätzereigenschaften die Verteilung von Schätzern in Abhängigkeit der Verteilung der ihn zugrundeliegenden Daten. Weil Daten in der Frequentistischen Inferenz zufällig sind, sind auch Schätzer zufällig. Speziell werden beobachtete Datenwerte als Realisierungen von Zufallsvariablen interpretiert. Schätzer als Funktionen von Zufallsvariablen sind damit auch Zufallsvariablen, auch wenn sie natürlich bei Vorliegen eines konkreten Datensatzes nur einen konkreten Wert annehmen. Wir unterscheiden zwischen Schätzereigenschaften bei endlichen Stichproben und Asymptotischen Schätzereigenschaften. Erstere sind Inhalt dieses Abschnittes und betreffen die Eigenschaften eines Schätzer für einen festen Stichprobenumfang \\(n\\), letztere sind Inhalt von Kapitel 22.3 und betreffen die Eigenschaften eines Schätzers im Grenzfall \\(n \\to \\infty\\) von großen Stichprobenumfängen.\nEs sei zunächst \\((\\Sigma,S)\\) ein Messraum und \\(\\hat{\\tau} : \\mathcal{Y} \\to \\Sigma\\) ein Schätzer von \\(\\tau : \\Theta \\to \\Sigma\\) (vgl. Definition 21.5). In der Folge betrachten wir neben Parameterschätzern der Form \\[\\begin{equation}\n\\tau: \\Theta \\to \\Sigma, \\tau(\\theta) := \\theta\n\\end{equation}\\] auch wiederholt zunächst solche Schätzer, die bei parametrischen Produktmodellen nur Funktionen der Parameter wie den Erwartungswert, die Varianz und die Standardabweichung der Stichprobenvariablen schätzen. Da nach Annahme die Verteilungen der Stichprobenvariablen \\(\\upsilon_1,...,\\upsilon_n\\) identisch sind, handelt es sich dabei um Schätzer der Form\n\\[\\begin{align}\n\\begin{split}\n\\tau : \\Theta \\to \\Sigma,\\,\n\\theta \\mapsto \\tau(\\theta)\n\\mbox{ mit }\n\\tau(\\theta) := \\mathbb{E}_\\theta(\\upsilon_1),\n\\tau(\\theta) := \\mathbb{V}_\\theta(\\upsilon_1), \\mbox{ und }\n\\tau(\\theta) := \\mathbb{S}_\\theta(\\upsilon_1).\n\\end{split}\n\\end{align}\\]\nSpeziell wollen wir in diesem Abschnitt vier Aspekte von Schätzereigenschaften bei endlichen Stichproben beleuchten. In Kapitel 22.2.1 beschäftigen wir uns zunächst mit der Erwartungstreue eines Schätzers. Dabei heißt ein Schätzer erwartungstreu, wenn sein Erwartungswert mit dem wahren, aber unbekannten, Wert \\(\\tau(\\theta)\\) für alle \\(\\theta \\in \\Theta\\) identisch ist. In Kapitel 22.2.2 führen wir mit den Begriffen der Varianz und des Standardsfehlers eines Schätzers als Bezeichungen für die Varianz der Zufallsvariable \\(\\hat{\\tau}(\\upsilon)\\) und die Standardabweichung der Zufallsvariable \\(\\hat{\\tau}(\\upsilon)\\) zwei Maße für die Frequentistische Variabilität von Schätzern ein. Mit dem mittlere quadratischen Fehler eines Schätzers \\(\\hat{\\tau}\\) als Erwartungswert der quadrierten Abweichung von \\(\\hat{\\tau}(\\upsilon)\\) von \\(\\tau(\\theta)\\) führen wir dann in Kapitel 22.2.3 eine Schätzereigenschaft ein, die es erlaubt die Genauigkeit und die Variabilität eines Schätzers im Sinne eines sogenannten Bias-Variance-Tradeoffs miteinander in Beziehung zu setzen. Die in Kapitel 22.2.4 disktutierte Cramér-Rao-Ungleichung schließlich gibt eine untere Schranke für die Varianz erwartungstreuer Schätzer an. Ein erwartungstreuer Schätzer mit Varianz gleich der in der Cramér-Rao-Ungleichung gegebenen unteren Schranke hat die kleinstmögliche Varianz aller erwartungstreuen Schätzer und ist in diesem Sinne ein optimaler Schätzer.\n\n22.2.1 Erwartungstreue\nDer Begriff der Erwartungstreue eines Schätzers ergibt sich im Kontext des Fehlers und des systematischen Fehlers eines Schätzers wie folgt.\n\nDefinition 22.4 (Fehler, Systematischer Fehler und Erwartungstreue) \\(\\upsilon\\) sei eine Stichprobe eines Frequentischen Inferenzmodells und \\(\\hat{\\tau}\\) sei ein Schätzer für \\(\\tau\\).\n\nDer Fehler von \\(\\hat{\\tau}\\) ist definiert als \\[\\begin{equation}\n\\hat{\\tau}(\\upsilon) - \\tau(\\theta).\n\\end{equation}\\]\nDer systematische Fehler (engl. Bias) von \\(\\hat{\\tau}\\) ist definiert als \\[\\begin{equation}\n\\mbox{B}(\\hat{\\tau} ) := \\mathbb{E}_{\\theta}(\\hat{\\tau} (\\upsilon)) - \\tau(\\theta).\n\\end{equation}\\]\nDer Schätzer \\(\\hat{\\tau}\\) heißt erwartungstreu (engl. unbiased), wenn \\[\\begin{equation}\n\\mbox{B}(\\hat{\\tau} ) = 0\\Leftrightarrow\n\\mathbb{E}_{\\theta}(\\hat{\\tau} (\\upsilon)) = \\tau(\\theta) \\mbox{ für alle } \\theta \\in \\Theta \\mbox{ und alle } n \\in \\mathbb{N}.\n\\end{equation}\\] Andernfalls heißt \\(\\hat{\\tau}\\) verzerrt (engl. biased).\n\n\nMan beachte, dass in Definition 22.4 der Fehler eines Schätzers von der spezifischen Realisation der Stichprobe \\(\\upsilon\\) abhängt. Der systematische Fehler dagegen ist der erwartete Fehler über Stichprobenrealisationen und damit im Sinne eines Erwartungswerts von einer spezifischen Realisation unabhängig. Für den speziellen Fall eines Parameterpunktschätzers gilt nach Definition 22.4, dass er erwartungstreu ist, wenn gilt, dass \\[\\begin{equation}\n\\mathbb{E}_{\\theta}(\\hat{\\theta}(\\upsilon)) = \\theta.\n\\end{equation}\\]\nAls erste Beispiele für erwartungstreue Schätzer betrachten wir in folgendem Theorem das Stichprobenmittel und die Stichprobenvarianz als Schätzer für den Erwartungswert und die Varianz einer Stichprobenvariable.\n\nTheorem 22.3 (Erwartungstreue von Stichprobenmittel und Stichproenvarianz) \\(\\upsilon := (\\upsilon_1,...,\\upsilon_n)\\) sei die Stichprobe eines parametrischen Produktmodells. Dann gelten\n\nDas Stichprobenmittel \\[\\begin{equation}\n\\bar{\\upsilon} := \\frac{1}{n}\\sum_{i=1}^n \\upsilon_i\n\\end{equation}\\] ist ein erwartungstreuer Schätzer des Erwartungswerts \\(\\mathbb{E}_\\theta(\\upsilon_1)\\).\nDie Stichprobenvarianz \\[\\begin{equation}\nS^2 := \\frac{1}{n-1}\\sum_{i=1}^n (\\upsilon_i - \\bar{\\upsilon})^2\n\\end{equation}\\] ist ein erwartungstreuer Schätzer der Varianz \\(\\mathbb{V}_\\theta(\\upsilon_1)\\).\n\n\n\nBeweis. (1) Die Erwartungstreue des Stichprobenmittels ergibt mit den Eigenschaften des Erwartungswerts (vgl. Theorem 15.3) aus \\[\\begin{align}\n\\mathbb{E}_\\theta(\\bar{\\upsilon})\n= \\mathbb{E}_\\theta \\left(\\frac{1}{n}\\sum_{i=1}^n  \\upsilon_i \\right)\n= \\frac{1}{n}\\sum_{i=1}^n  \\mathbb{E}_\\theta\\left( \\upsilon_i \\right)\n= \\frac{1}{n}\\sum_{i=1}^n  \\mathbb{E}_\\theta\\left( \\upsilon_1 \\right)\n= \\frac{1}{n} n  \\mathbb{E}_\\theta\\left( \\upsilon_1 \\right)\n=  \\mathbb{E}_\\theta\\left( \\upsilon_1 \\right).\n\\end{align}\\]\n(2) Um die Erwartungstreue der Stichprobenvarianz zu zeigen, halten wir zunächst fest, dass mit den Eigenschaften der Varianz gilt, dass (vgl. Theorem 15.5) \\[\\begin{equation}\n\\mathbb{V}_\\theta(\\bar{\\upsilon})\n= \\mathbb{V}_\\theta\\left(\\frac{1}{n} \\sum_{i=1}^n \\upsilon_i \\right)\n= \\frac{1}{n^2} \\sum_{i=1}^n \\mathbb{V}_\\theta\\left( \\upsilon_i \\right)\n= \\frac{1}{n^2} \\sum_{i=1}^n  \\mathbb{V}_\\theta\\left( \\upsilon_1 \\right)\n= \\frac{1}{n^2} n \\mathbb{V}_\\theta\\left( \\upsilon_1 \\right)\n= \\frac{\\mathbb{V}_\\theta\\left( \\upsilon_1 \\right)}{n}.\n\\end{equation}\\] Weiterhin gilt für den Term der summierten quadratischen Abweichungen in der Stichprobenvarianz, dass \\[\\begin{align}\n\\sum_{i=1}^n \\left(\\upsilon_i - \\bar{\\upsilon}\\right)^2 = \\sum_{i=1}^n (\\upsilon_i - \\mathbb{E}_\\theta(\\upsilon_1))^2 - n(\\bar{\\upsilon} - \\mathbb{E}_\\theta(\\upsilon_1))^2,\n\\end{align}\\] weil \\[\\begin{align}\n\\begin{split}\n\\sum_{i=1}^n \\left(\\upsilon_i - \\bar{\\upsilon}\\right)^2\n& = \\sum_{i=1}^n \\left(\\upsilon_i - \\mathbb{E}_\\theta(\\upsilon_1) - \\bar{\\upsilon} + \\mathbb{E}_\\theta(\\upsilon_1) \\right)^2 \\\\\n& = \\sum_{i=1}^n \\left((\\upsilon_i - \\mathbb{E}_\\theta(\\upsilon_1)) - (\\bar{\\upsilon} - \\mathbb{E}_\\theta(\\upsilon_1)) \\right)^2 \\\\\n& = \\sum_{i=1}^n (\\upsilon_i-\\mathbb{E}_\\theta(\\upsilon_1))^2 - 2(\\bar{\\upsilon}-\\mathbb{E}_\\theta(\\upsilon_1))\\left(\\sum_{i=1}^n(\\upsilon_i-\\mathbb{E}_\\theta(\\upsilon_1))\\right) + \\sum_{i=1}^n (\\bar{\\upsilon}-\\mathbb{E}_\\theta(\\upsilon_1))^2 \\\\\n& = \\sum_{i=1}^n (\\upsilon_i-\\mathbb{E}_\\theta(\\upsilon_1))^2 - 2(\\bar{\\upsilon}-\\mathbb{E}_\\theta(\\upsilon_1))\\left(\\sum_{i=1}^n\\upsilon_i- n\\mathbb{E}_\\theta(\\upsilon_1)\\right) + n(\\bar{\\upsilon}-\\mathbb{E}_\\theta(\\upsilon_1))^2 \\\\\n& = \\sum_{i=1}^n (\\upsilon_i-\\mathbb{E}_\\theta(\\upsilon_1))^2 - 2(\\bar{\\upsilon}-\\mathbb{E}_\\theta(\\upsilon_1))\\left(n\\left(\\frac{1}{n}\\sum_{i=1}^n\\upsilon_i\\right)- n\\mathbb{E}_\\theta(\\upsilon_1)\\right) + n(\\bar{\\upsilon}-\\mathbb{E}_\\theta(\\upsilon_1))^2 \\\\\n& = \\sum_{i=1}^n (\\upsilon_i-\\mathbb{E}_\\theta(\\upsilon_1))^2 - 2n(\\bar{\\upsilon}-\\mathbb{E}_\\theta(\\upsilon_1))^2 + n(\\bar{\\upsilon}-\\mathbb{E}_\\theta(\\upsilon_1))^2 \\\\\n& = \\sum_{i=1}^n (\\upsilon_i - \\mathbb{E}_\\theta(\\upsilon_1))^2 - n(\\bar{\\upsilon} - \\mathbb{E}_\\theta(\\upsilon_1))^2.\n\\end{split}\n\\end{align}\\] Zusammen ergibt sich also \\[\\begin{align}\n\\mathbb{E}_\\theta\\left((n-1)S^2\\right)\n& = \\mathbb{E}_\\theta\\left(\\sum_{i=1}^n \\left(\\upsilon_i - \\bar{\\upsilon}\\right)^2 \\right) \\\\\n& = \\mathbb{E}_\\theta\\left(\\sum_{i=1}^n (\\upsilon_i - \\mathbb{E}_\\theta(\\upsilon_1))^2 - n(\\bar{\\upsilon} - \\mathbb{E}_\\theta(\\upsilon_1))^2 \\right) \\\\\n& = \\sum_{i=1}^n \\mathbb{E}_\\theta\\left((\\upsilon_i - \\mathbb{E}_\\theta(\\upsilon_1))^2\\right) - n \\mathbb{E}_\\theta\\left((\\bar{\\upsilon} - \\mathbb{E}_\\theta(\\upsilon_1))^2 \\right) \\\\\n& = n \\mathbb{V}_\\theta(\\upsilon_1) - n \\mathbb{V}_\\theta(\\bar{\\upsilon}) \\\\\n& = n \\mathbb{V}_\\theta(\\upsilon_1) - n \\frac{\\mathbb{V}_\\theta(\\upsilon_1)}{n} \\\\\n& = n \\mathbb{V}_\\theta(\\upsilon_1) - \\mathbb{V}_\\theta(\\upsilon_1) \\\\\n& = (n - 1) \\mathbb{V}_\\theta(\\upsilon_1).\n\\end{align}\\] Schließlich ergibt sich dann \\[\\begin{equation}\n\\mathbb{E}_\\theta\\left(S^2\\right)\n= \\mathbb{E}_\\theta\\left(\\frac{1}{n-1}(n-1)S^2 \\right)\n= \\frac{1}{n-1}\\mathbb{E}_\\theta\\left((n-1)S^2 \\right)\n= \\frac{1}{n-1}(n - 1)  \\mathbb{V}_\\theta(\\upsilon_1)\n= \\mathbb{V}_\\theta(\\upsilon_1)\n\\end{equation}\\] und damit die Erwartungstreue der Stichprobenvarianz als Schätzer der Varianz.\n\nNatürlich sind in Theorem 22.3 aufgrund der identischen Verteilung der Stichprobenvariablen eines parametrischen Produktmodells das Stichprobenmittel und die Stichprobenvarianz auch erwartungstreue Schätzer des Erwartungswertes und der Varianz einer beliebigen Stichprobenvariablen \\(\\upsilon_i\\) mit \\(1 \\le i \\le n\\). Man beachte, dass im Beweis der Erwartungstreue der Stichprobenvarianz der Nenner \\(n-1\\) in der Definition der Stichprobenvarianz eine entscheidende Rolle spielt.\nObwohl die Stichprobenvarianz ein unverzerrter Schätzer der Varianz einer Stichprobenvariable eines parametrischen Produktmodells ist, trifft dies auf die Stichprobenstandardabweichung als Schätzer der Standardabweichung nicht zu. Dies ist Inhalt des folgenden Theorems.\n\nTheorem 22.4 (Verzerrtheit der Stichprobenstandardabweichung) \\(\\upsilon = (\\upsilon_1,...,\\upsilon_n)\\) sei die Stichprobe eines parametrischen Produktmodells. Dann ist die Stichprobenstandard- abweichung \\[\\begin{equation}\nS := \\sqrt{S^2}\n\\end{equation}\\] ein verzerrter Schätzer der Standardabweichung \\(\\mathbb{S}_\\theta(\\upsilon_1)\\).\n\n\nBeweis. Wir halten zunächst fest, dass \\(\\sqrt{\\cdot}\\) eine strikt konkave Funktion und \\(\\sigma^2 &gt; 0\\) ist. Dann aber gilt mit der Jensenschen Ungleichung \\(\\mathbb{E}(f(\\xi)) &lt; f(\\mathbb{E}(\\xi))\\) für strikt konkave Funktionen (vgl. Theorem 16.5), dass \\[\\begin{equation}\n\\mathbb{E}_\\theta(S)\n= \\mathbb{E}_\\theta\\left(\\sqrt{S^2}\\right)\n&lt; \\sqrt{\\mathbb{E}_\\theta(S^2)}\n= \\sqrt{\\mathbb{V}_\\theta(\\upsilon_1)}\n= \\mathbb{S}_\\theta(\\upsilon_1).\n\\end{equation}\\]\n\nAllgemein führen nichtlineare Transformationen von erwartungstreuen Schätzern oft auf verzerrte Schätzer, was wir hier aber nicht weiter vertiefen wollen. Folgender R Code demonstriert exemplarisch die Begriffe der Unverzerrtheit und Verzerrtheit von Stichprobenmittel, Stichprobenvarianz und Stichprobenstandardabweichung am Beispiel eines parametrischen Produktmodells mit Stichprobenverteilung\n\\[\\begin{equation}\n\\upsilon_1,...,\\upsilon_{12} \\sim N(1.7,2)\n\\end{equation}\\] Dabei werden die Erwartungswerte der Schätzer anhand ihrer Stichprobenmittel über viele Realisierungen von \\(\\upsilon_1,...,\\upsilon_{12}\\) als Funktion der Anzahl an Realsierungen (Simulationen) geschätzt.\n\n# Modellformulierung\nset.seed(0)                          # Zufallszahlengenerator           \nmu      = 1.7                        # wahrer, aber unbekannter, Erwartungswertparameter\nsigsqr  = 2                          # wahrer, aber unbekannter, Varianzparameter\nn       = 12                         # Stichprobenumfang n\nnsim    = 5e4                        # Anzahl der Simulationen\ny_bar   = rep(NaN,nsim)              # Stichprobenmittelarray\ns_sqr   = rep(NaN,nsim)              # Stichprobenvarianzarray\ns       = rep(NaN,nsim)              # Stichprobenstandardabweichungarray\n\n# Simulationsiterationen\nfor(sim in 1:nsim){\n\n    # Stichprobenrealisation von \\upsilon_1,...,\\upsilon_{12}\n    y          = rnorm(n,mu,sqrt(sigsqr))\n\n    # Erwartungswert-, Varianz-, StandardabweichungSchätzer\n    y_bar[sim] = mean(y)             # Stichprobenmittel\n    s_sqr[sim] = var(y)              # Stichprobenvarianz\n    s[sim]     = sd(y)               # Stichprobenstandardabweichung\n}\n\n# Erwartungswertschaetzung\nE_hat_y_bar = cumsum(y_bar)/(1:nsim) # \\mathbb{E}(\\bar{\\upsilon}) Schaetzungen\nE_hat_s_sqr = cumsum(s_sqr)/(1:nsim) # \\mathbb{E}(S^2) Schaetzungen\nE_hat_s     = cumsum(s)    /(1:nsim) # \\mathbb{E}(S) Schaetzungen\n\nAbbildung 22.1 visualisiert die Ergebnisse obiger Simulation. Gezeigt sind Schätzungen der Erwartungswerte von Stichprobenmittel, Stichprobenvarianz und Stichprobenstandardabweichung als Funktion der Anzahl an Realisierungen der Stichprobenvariablen \\(\\upsilon_1,...,\\upsilon_{12}\\) sowie die wahren, aber unbekannten, Werte des Erwartungswerts, der Varianz und der Standardabweichung der \\(\\upsilon_i\\) mit \\(1\\le i \\le 12\\). Es fällt auf, dass diese Schätzungen bei geringer Realisierungsanzahl variabler ausfallen. Ab einer Schätzung basierend auf etwa 10000 Realisierungen von \\(\\upsilon_1,...,\\upsilon_{12}\\) entsprechen die Stichprobenmittel von \\(\\bar{\\upsilon}\\) und \\(S^2\\) gemäß ihrer Erwartungstreue ihren wahren, aber unbekannten, Werten. Die Stichprobenstandardabweichung dagegen zeigt gemäß ihrer Verzerrtheit auch bei weiter ansteigenden Anzahlen von der Realsierungen von \\(\\upsilon_1,...,\\upsilon_{12}\\)\nkonstant eine zu niedrige Schätzung der wahren, aber unbekannten, Standardabweichung.\n\n\n\n\n\n\nAbbildung 22.1: Simulation der Erwartungstreue von Stichprobenmittel und Stichprobenvarianz als Schätzer des Erwartungswerts und der Varianz bei normalverteilten Stichprobenvariablen und Simulation der Verzerrtheit der Stichprobenstandardabweichung als Schätzer der Standardabweichung bei normalverteilten Stichprobenvariablen\n\n\n\n\n\n22.2.2 Varianz und Standardfehler\nIm vorherigen Abschnitt haben wir den Erwartungswert eines Schätzers betrachtet. In diesem Abschnitt betrachten wir seine Varianz und seine Standardabweichung und führen die mit diesen assoziierten Begriffe ein. Wir nutzen folgende Definition.\n\nDefinition 22.5 (Varianz und Standardfehler) \\(\\upsilon = (\\upsilon_1,...,\\upsilon_n)\\) sei die Stichprobe eines Frequentistischen Inferenzmodells und \\(\\hat{\\tau}\\) sei ein Schätzer von \\(\\tau\\).\n\nDie Varianz von \\(\\hat{\\tau}\\) ist definiert als \\[\\begin{equation}\n\\mathbb{V}_\\theta(\\hat{\\tau} ) :=\n\\mathbb{E}_\\theta\n\\left((\\hat{\\tau} (\\upsilon) - \\mathbb{E}_\\theta(\\hat{\\tau} (\\upsilon)))^2\\right).\n\\end{equation}\\]\nDer Standardfehler von \\(\\hat{\\tau}\\) ist definiert als \\[\\begin{equation}\n\\mbox{SE}(\\hat{\\tau} ) := \\sqrt{\\mathbb{V}_\\theta(\\hat{\\tau})}.\n\\end{equation}\\]\n\n\nDie Varianz eines Schätzers \\(\\hat{\\tau}\\) ist also als die Varianz der Zufallsvariable \\(\\hat{\\tau}(\\upsilon)\\) definiert. Der Standardfehler eines Schätzers \\(\\hat{\\tau}\\) ist als die Standardabweichung von \\(\\hat{\\tau}(\\upsilon)\\) definiert. Als erstes Beispiel für einen Standardfehler betrachten wir den Standardfehler des Stichprobenmittels.\n\nTheorem 22.5 (Standardfehler des Stichprobenmittels) \\(\\upsilon = (\\upsilon_1,...,\\upsilon_n)\\) sei die Stichprobe eines parametrischen Produktmodells. Dann ist der gegeben durch \\[\\begin{equation}\n\\mbox{SE}(\\bar{\\upsilon}) = \\frac{\\mathbb{S}_\\theta(\\upsilon_1)}{\\sqrt{n}}.\n\\end{equation}\\].\n\n\nBeweis. Mit der Varianz des Stichprobenmittels ergibt sich \\[\\begin{equation}\n\\mbox{SE}(\\bar{\\upsilon})\n= \\sqrt{\\mathbb{V}_\\theta(\\bar{\\upsilon})}\n= \\sqrt{\\frac{\\mathbb{V}_\\theta(\\upsilon_1)}{n}}\n= \\frac{\\mathbb{S}_\\theta(\\upsilon_1)}{\\sqrt{n}}.\n\\end{equation}\\]\n\nDer Standardfehler des Mittelwerts beschreibt die Variabilität des Stichprobenmittels. Da die Standardabweichung \\(\\mathbb{S}_\\theta(\\upsilon_1)\\) unbekannt ist, ist auch der Standardfehler \\(\\mbox{SE}(\\bar{\\upsilon})\\) unbekannt, kann also nur geschätzt werden. Mit der Stichprobenstandardabweichung als verzerrter Schätzer der Standardabweichung \\(\\mathbb{S}_\\theta(\\upsilon_1)\\) ergibt sich ein ebenfalls verzerrter Schätzer für den Standardfehler des Stichprobenmittels zu \\[\\begin{equation}\n\\hat{\\mbox{SE}}(\\bar{\\upsilon}) = \\frac{S}{\\sqrt{n}}.\n\\end{equation}\\]\nAls zweites Beispiel wollen wir den Standardfehler des Maximum-Likelihood Schätzers für den Parameter eines Bernoulli-Modells betrachten.\n\nTheorem 22.6 (Standardfehler des Maximum-Likelihood Schätzers des Bernoullimodellparameters) Es sei \\(\\upsilon = (\\upsilon_1,...,\\upsilon_n)\\) die Stichproben eines Bernoullimodells und \\(\\hat{\\mu}^{\\mbox{\\tiny ML}}\\) sei der Maximum-Likelihood Schätzer für den Bernoullimodellparameter \\(\\mu\\). Dann ist der Standardfehler von \\(\\hat{\\mu}^{\\mbox{\\tiny ML}}\\) gegeben durch \\[\\begin{equation}\n\\mbox{SE}\\left(\\hat{\\mu}^{\\mbox{\\tiny ML}}\\right) = \\sqrt{\\frac{\\mu(1-\\mu)}{n}}.\n\\end{equation}\\]\n\n\nBeweis. Es gilt \\[\\begin{align}\n\\begin{split}\n\\mbox{SE}\\left(\\hat{\\mu}^{\\mbox{\\tiny ML}}\\right)\n= \\sqrt{\\mathbb{V}_\\mu\\left(\\hat{\\mu}^{\\mbox{\\tiny ML}}\\right)}\n= \\sqrt{\\mathbb{V}_\\mu\\left(\\frac{1}{n}\\sum_{i=1}^n \\upsilon_i \\right)}  \n= \\sqrt{\\frac{1}{n^2}\\sum_{i=1}^n \\mathbb{V}_\\mu(\\upsilon_i)}\n= \\sqrt{\\frac{n \\mu(1-\\mu)}{n^2}}\n= \\sqrt{\\frac{\\mu(1-\\mu)}{n}},\n\\end{split}\n\\end{align}\\] wobei die dritte Gleichung mit der Unabhängigkeit der \\(\\upsilon_i\\) und die vierte Gleichung mit der Varianz \\(\\mathbb{V}_\\mu(\\upsilon_1) = \\mathbb{V}_\\mu(\\upsilon_i) = \\mu(1-\\mu)\\) der Stichprobenvariablen folgt.\n\nWie im Falle des Standardfehlers des Stichprobenmittels ist auch der Standardfehler des Maximum-Likelihood Schätzers des Bernoullimodellparameters ein wahrer, aber unbekannter, Wert. Ein Schätzer für \\(\\mbox{SE}\\left(\\hat{\\mu}^{\\mbox{\\tiny ML}}\\right)\\) ergibt sich mit dem Maximum-Likelihood Schätzer für den Bernoullimodellparameter durch \\[\\begin{equation}\n\\hat{\\mbox{SE}}\\left(\\hat{\\mu}^{\\mbox{\\tiny ML}}\\right)\n= \\sqrt{\\frac{\\hat{\\mu}^{\\mbox{\\tiny ML}}(1-\\hat{\\mu}^{\\mbox{\\tiny ML}})}{n}}.\n\\end{equation}\\]\nFolgender R Code simuliert die Verteilung des Maximum-Likelihood Schätzers für den Parameter eines Bernoullimodells mit wahrem, aber unbekanntem, Parameterwert \\(\\mu := 0.4\\) für die Stichprobenumfänge \\(n = 20, n = 100\\) und \\(n = 200\\). Abbildung 22.2 visualisiert die resultierenden Verteilungen mithilfe von Histogrammen. Die Variabilität der Schätzwerte, also die Breite der Histogrammverteilungen, hängt dabei offenbar vom Stichprobenumfang ab und höhere Stichprobenumfänge resultieren in einer geringeren Variabilität des Schätzers. Diesen Gedanken werden wir im Abschnitt Kapitel 22.3 vertiefen.\n\n# Modellformulierung\nmu       = 0.4                                                      # wahrer, aber unbekannter, Parameterwert\nn_all    = c(20,100,200)                                            # Stichprobenumfänge n\nns       = 1e4                                                      # Anzahl der Simulationen\nmu_hat   = matrix(rep(NaN, length(n_all)*ns), nrow = length(n_all)) # Maximum-Likelihood Schätzearray\n                    \n# Stichprobenumfängeiterationen\nfor(i in seq_along(n_all)){\n\n    # Simulationsiterationen\n    for(s in 1:ns){\n        y               = rbinom(n_all[i],1,mu)                     # Stichprobenrealisation von y_1,...,y_n\n        mu_hat[i,s]  = mean(y)                                      # Stichprobenmittel\n    }\n}\n\n\n\n\n\n\n\nAbbildung 22.2: Simulation der Verteilung des Maximum-Likelihood Schätzers eines Bernoullimodells. Die Variabilität des Schätzers hängt dabei offenbar vom Stichprobenumfang \\(n\\) ab.\n\n\n\n\n\n22.2.3 Mittlerer quadratischer Fehler\nMit der Erwartungstreue und der Varianz eines Schätzers haben wir in den beiden vorherigen Abschnitten zwei unabhängige Kriterien für die Güte von Schätzern kennengelernt. Der in diesem Abschnitt eingeführte Mittlere quadratische Fehler eines Schätzers ermöglicht eine integrierte Betrachtung der Genauigkeit (Erwatungstreue) und Variabilität (Varianz) eines Schätzer im Sinne seiner sogenannten Bias-Varianz-Zerlegung. Wir definieren den mittleren quadratischen Fehler eines Schätzers zunächt wie folgt.\n\nDefinition 22.6 (Mittlerer quadratischer Fehler) \\(\\upsilon = (\\upsilon_1,...,\\upsilon_n)\\) sei die Stichprobe eines parametrischen Produktmodells und \\(\\hat{\\tau}\\) ein Schätzer für \\(\\tau\\). Dann ist der mittlere quadratischer Fehler (engl. mean squared error) von \\(\\hat{\\tau}\\) definiert als \\[\\begin{equation}\n\\mbox{MQF}(\\hat{\\tau})\n:= \\mathbb{E}_\\theta\\left((\\hat{\\tau}(\\upsilon) - \\tau(\\theta))^2\\right).\n\\end{equation}\\]\n\nDer mittlere quadratische Fehler von \\(\\hat{\\tau}\\) ist also die erwartete quadrierte Abweichung von \\(\\hat{\\tau}(\\upsilon)\\) von \\(\\tau(\\theta)\\). Man beachte, dass in Abgrenzung dazu die Varianz von \\(\\hat{\\tau}\\) die erwartete quadrierte Abweichung von \\(\\hat{\\tau}\\) von \\(\\mathbb{E}_\\theta(\\hat{\\tau}(\\upsilon))\\) ist. Dabei kann, wie in Kapitel 22.2.1 gesehen \\(\\mathbb{E}_\\theta(\\hat{\\tau}(\\upsilon))\\) mit \\(\\tau(\\theta)\\) übereinstimmen, ein Schätzer also erwartungstreu sein, er muss es aber nicht. Nutzt man den mittleren quadratischen Fehler als Gütekriterium für einen Schätzer, zum Beispiel indem man versucht, einen Schätzer mit möglichst geringem mittleren quadratischen Fehler zu konstruieren, so kann man dabei eventuelle leichte Abweichungen von der Erwartungstreue zugunsten einer geringen Schätzervarianz in Kauf nehmen. Für den mittleren quadratischen Fehler gilt nämlich folgendes Theorem.\n\nTheorem 22.7 (Zerlegung des mittleren quadratischen Fehlers) \\(\\upsilon = (\\upsilon_1,...,\\upsilon_n)\\) sei die Stichprobe eines parametrischen Produktmodells, \\(\\hat{\\tau}\\) sei ein Schätzer für \\(\\tau\\), und \\(\\mbox{MQF}(\\hat{\\tau})\\) sei der mittlere quadratische Fehler von \\(\\hat{\\tau}\\). Dann gilt \\[\\begin{equation}\n\\mbox{MQF}(\\hat{\\tau}) = \\mbox{B}(\\hat{\\tau})^2 + \\mathbb{V}_\\theta(\\hat{\\tau}).\n\\end{equation}\\]\n\n\nBeweis. Zur Vereinfachung der Notation seien \\(\\tau := \\tau(\\theta)\\), \\(\\hat{\\tau} := \\hat{\\tau}(\\upsilon)\\) und \\(\\bar{\\tau}_n := \\mathbb{E}_\\theta(\\hat{\\tau}(\\upsilon))\\). Dann gilt: \\[\\begin{align}\n\\begin{split}\n\\mathbb{E}_\\theta\\left((\\hat{\\tau} - \\tau)^2\\right)\n& = \\mathbb{E}_\\theta\\left((\\hat{\\tau} - \\bar{\\tau}_n + \\bar{\\tau}_n - \\tau)^2\\right) \\\\\n& = \\mathbb{E}_\\theta\n\\left(\n(\\hat{\\tau} - \\bar{\\tau}_n)^2 + 2(\\hat{\\tau} - \\bar{\\tau}_n)(\\bar{\\tau}_n - \\tau) + (\\bar{\\tau}_n - \\tau)^2\n\\right)\n\\\\\n& = \\mathbb{E}_\\theta\\left((\\hat{\\tau} - \\bar{\\tau}_n)^2\\right) + 2\\mathbb{E}_\\theta\\left((\\hat{\\tau} - \\bar{\\tau}_n)(\\bar{\\tau}_n - \\tau)\\right) + \\mathbb{E}_\\theta\\left((\\bar{\\tau}_n - \\tau)^2\\right) \\\\\n& = \\mathbb{E}_\\theta\\left((\\hat{\\tau} - \\bar{\\tau}_n)^2\\right) + 2\\mathbb{E}_\\theta\\left(\n\\hat{\\tau}\\bar{\\tau}_n - \\hat{\\tau}\\tau - \\bar{\\tau}_n\\bar{\\tau}_n + \\bar{\\tau}_n\\tau\n\\right) + \\mathbb{E}_\\theta\\left((\\bar{\\tau}_n - \\tau)^2\\right)\n\\\\\n& =\n\\mathbb{E}_\\theta\\left((\\hat{\\tau} - \\bar{\\tau}_n)^2\\right) + 2\\left(\n\\bar{\\tau}_n\\bar{\\tau}_n - \\bar{\\tau}_n\\tau\n\\right) + \\mathbb{E}_\\theta\\left((\\bar{\\tau}_n - \\tau)^2\\right) \\\\\n& =\n\\mathbb{E}_\\theta\\left((\\hat{\\tau} - \\bar{\\tau}_n)^2\\right) + 0 + \\mathbb{E}_\\theta\\left((\\bar{\\tau}_n - \\tau)^2\\right) \\\\\n& =\n\\mathbb{E}_\\theta\\left((\\bar{\\tau}_n - \\tau)^2\\right) + \\mathbb{E}_\\theta\\left((\\hat{\\tau} - \\bar{\\tau}_n)^2\\right) \\\\\n& =\n\\mathbb{E}_\\theta\\left((\\mathbb{E}_\\theta(\\hat{\\tau}) - \\tau)^2\\right) + \\mathbb{E}_\\theta\\left((\\hat{\\tau} - \\mathbb{E}_\\theta(\\hat{\\tau}))^2\\right) \\\\\n& =\n(\\mathbb{E}_\\theta(\\hat{\\tau}) - \\tau)^2 + \\mathbb{V}_\\theta(\\hat{\\tau}) \\\\\n& =\n\\mbox{B}(\\hat{\\tau})^2 + \\mathbb{V}_\\theta(\\hat{\\tau}).\n\\end{split}\n\\end{align}\\]\n\n\n\n22.2.4 Cramér-Rao-Ungleichung\nHat man mehrere erwartungstreue Schätzer vorliegen, so gilt, dass derjenige Schätzer mit der kleinsten Varianz am verlässlichsten seinen Zweck erfüllt. Weil aber die Stichprobenrealisierungen Frequentistischer Inferenzmodelle in aller Regel variabel sind, kann auch die Variabilität erwartungstreuer Schätzer nicht beliebig klein sein. Die Cramér-Rao-Ungleichung gibt eine untere Schranke für die Varianz erwartungstreuer Schätzer an. Ein erwartungstreuer Schätzer mit Varianz gleich dieser unteren Schranke hat damit die kleinstmögliche Varianz aller erwartungstreuer Schätzer und ist - in diesem Sinne - ein optimaler Schätzer.\nDie Cramér-Rao-Ungleichung basiert auf dem Begriff der sogenannten Fisher-Information, welche wiederrum auf dem Begriff der Scorefunktion eines Frequentischen Inferenzmodells beruht. Wir führen im Folgenden also zunächst diese beiden Begrifflichkeiten ein, bevor die Cramér-Rao-Ungleichung formuliert und bewiesen werden soll.\nDabei gelten die vorgestellten Resultate allgemein nur unter einer Reihe mathematischer Annahmen, den sogenannten Fisher-Regularitätsbedingungen. Diese bestehen für ein Frequentistisches Inferenzmodell mit WMF oder WDF \\(p_\\theta\\) und Parameterraum \\(\\Theta\\) darin, dass angenommen wird, dass (1) \\(\\Theta\\) eine offene Menge ist, der wahre, aber unbekannte, Parameterwert damit nicht an einer Parameterraumgrenze liegen kann, (2) die Teilmenge von \\(\\Theta\\), auf der \\(p_\\theta\\) von Null verschiedene Werte annimmt, nicht von \\(\\theta\\) abhängt, (3) das Modell selbst identifizierbar ist, dass also WMFen oder WDFen mit unterschiedliche Parameterwerten unterschiedliche Funktionen sind und damit unterschiedliche Stichprobenverteilungen implizieren, (4) die Likelihood-Funktion des Modells zweimal stetig differenzierbar und (5) dass für die Likelihood-Funktion Integration und Differentiation vertauscht werden dürfen. Wir setzen die Fisher-Regularitätsbedingungen also als erfüllt voraus und wollen nur Modelle mit eindimensionalen Parameterräumen \\(\\Theta \\subseteq \\mathbb{R}\\) betrachten. Wir definieren zunächst die Begriffe der Scorefunktion und der Fisher-Information wie folgt.\n\nDefinition 22.7 (Scorefunktion und Fisher-Information) \\(\\upsilon = (\\upsilon_1,...,\\upsilon_n)\\) sei die Stichprobe eines parametrischen Produktmodells mit eindimensionalem Parameter \\(\\theta \\in \\Theta \\subseteq \\mathbb{R}\\) und \\(\\ell\\) sei die zugehörige Log-Likelihood-Funktion. Dann gelten:\n\nDie erste Ableitung von \\(\\ell\\) wird Scorefunktion der Stichprobe genannt und wird mit \\[\\begin{equation}\nS(\\theta) := \\frac{d}{d\\theta}\\ell(\\theta)\n\\end{equation}\\] bezeichnet. Für \\(n = 1\\) schreiben wir \\(S(\\theta) := S_1(\\theta)\\) und nennen \\(S(\\theta)\\) Scorefunktion einer Zufallsvariable.\nDie negative zweite Ableitung von \\(\\ell\\) wird Fisher-Information der Stichprobe genannt und mit \\[\\begin{equation}\nI(\\theta) := -\\frac{d^2}{d\\theta^2}\\ell(\\theta)\n\\end{equation}\\] bezeichnet. Für \\(n  = 1\\) schreiben wir \\(I(\\theta) := I_1(\\theta)\\) und nennen \\(I(\\theta)\\) die Fisher-Information einer Zufallsvariable.\n\n\nDa Likelihood- und Log-Likelihood-Funktionen von der Realisierung einer Stichprobe abhängen, sind sie vor dem Hintegrund eines Frequentistischen Inferenzmodells zufällige Funktionen. Da die Fisher-Information als Funktion der Log-Likelihood-Funktion damit auch eine Zufallsvariable ist, muss man zwischen den beobachteten und den erwarteten Werten der Fisher-Information unterscheiden.\n\nDefinition 22.8 (Beobachtete und erwartete Fisher-Information) \\(\\upsilon = (\\upsilon_1,...,\\upsilon_n)\\) sei die Stichprobe eines parametrischen Produktmodells mit eindimensionalem Parameter \\(\\theta \\in \\Theta \\subseteq \\mathbb{R}\\), \\(\\ell\\) sei die zugehörige Log-Likelihood-Funktion und \\(\\hat{\\theta}^{\\mbox{\\tiny ML}}\\) sei ein Maximum-Likelihood-Schätzer von \\(\\theta\\). Dann gelten:\n\nDie beobachtete Fisher-Information der Stichprobe ist definiert als \\[\\begin{equation}\nI\\left(\\hat{\\theta}^{\\mbox{\\tiny ML}}\\right)\n:= -\\frac{d^2}{d\\theta^2}\\ell\\left(\\hat{\\theta}^{\\mbox{\\tiny ML}}\\right),\n\\end{equation}\\] die beobachtete Fisher-Information der Stichprobe ist also die Fisher-Information an der Stelle des Maximum-Likelihood-Schätzers \\(\\hat{\\theta}^{\\mbox{\\tiny ML}}\\).\nDie erwartete Fisher-Information der Stichprobe ist definiert als \\[\\begin{equation}\nJ(\\theta) := \\mathbb{E}_\\theta(I(\\theta)).\n\\end{equation}\\] Für \\(n = 1\\) schreiben wir \\(J(\\theta) := J_1(\\theta)\\) und nennen \\(J(\\theta)\\) die \n\n\nBevor wir diese Begrifflichkeiten anhand des Bernoullimodells (Theorem 22.10) und des Normalverteilungsmodells (Theorem 22.12 und Theorem 22.11) verdeutlichen wollen, führen wir mit der Additivität der Fisher-Information bei parametrischen Produktmodellen (Theorem 22.8) und dem Erwartungswert und der Varianz der Scorefunktion (Theorem 22.9) noch wichtige Eigenschaften der genannten Begriffe ein, die die folgende Diskussion vereinfachen.\n\nTheorem 22.8 (Additivität der Fisher-Information) \\(\\upsilon = (\\upsilon_1,...,\\upsilon_n)\\) sei die Stichprobe eines parametrischen Produktmodells mit Parameter \\(\\theta \\in \\Theta \\subseteq \\mathbb{R}\\), \\(\\ell\\) sei die zugehörige Log-Likelihood-Funktion und \\(I(\\theta)\\) und \\(J(\\theta)\\) seien die Fisher-Information und die erwartete Fisher-Information der Stichprobe, respektive. Dann gilt \\[\\begin{equation}\nI(\\theta) = nI_1(\\theta) \\mbox{ und } J(\\theta) = nJ_1(\\theta).\n\\end{equation}\\]\n\n\nBeweis. Wir zeigen das Resultat für die erwartete Fisher-Information, das Resultat für die beobachtete Fisher-Information gilt dann implizit. Mit der Linearität von Ableitungen und Erwartungswerten gilt \\[\\begin{align}\n\\begin{split}\nJ(\\theta)\n& = \\mathbb{E}_\\theta\\left(-\\frac{d^2}{d\\theta^2} \\ell(\\theta)\\right) \\\\\n& = \\mathbb{E}_\\theta\\left(-\\frac{d^2}{d\\theta^2} \\ln \\left(\\prod_{i=1}^n p_\\theta(\\upsilon_i)\\right)\\right) \\\\\n& = \\mathbb{E}_\\theta\\left(-\\frac{d^2}{d\\theta^2} \\sum_{i=1}^n \\ln p_\\theta(\\upsilon_i)\\right) \\\\\n& = \\mathbb{E}_\\theta\\left(-\\frac{d^2}{d\\theta^2} \\sum_{i=1}^n \\ln p_\\theta(y_1)\\right) \\\\\n& = \\mathbb{E}_\\theta\\left(-\\frac{d^2}{d\\theta^2} \\ell_1(\\theta)n\\right) \\\\\n& =  n \\mathbb{E}_\\theta\\left(-\\frac{d^2}{d\\theta^2}\\ell_1(\\theta))\\right) \\\\\n& =  n J(\\theta).\n\\end{split}\n\\end{align}\\]\n\nNach Theorem 22.8 genügt es zur Berechnung der beobachteten oder erwarteten Fisher-Information einer Stichprobe bei parametrischen Produktmodellen also, die beobachtete oder erwartete Fisher-Information einer der Zufallsvariablen der Stichprobe zu berechnen. Weitere Vereinfachungen in der Bestimmung von Fisher-Informationen und der Begründung der Cramér-Rao-Ungleichung ergeben sich durch die im folgenden Theorem formulierten Identitäten.\n\nTheorem 22.9 (Erwartungswert und Varianz der Scorefunktion) Der Erwartungswert der Scorefunktion einer Zufallsvariable ist \\[\\begin{equation}\n\\mathbb{E}_\\theta(S(\\theta)) = 0\n\\end{equation}\\] und die Varianz der Scorefunktion einer Zufallsvariable ist \\[\\begin{equation}\n\\mathbb{V}_\\theta(S(\\theta)) = J(\\theta).\n\\end{equation}\\]\n\n\nBeweis. Wir betrachten nur den Fall, dass \\(p_\\theta\\) eine WDF ist und zeigen zunächst, dass \\(\\mathbb{E}_\\theta(S(\\theta)) = 0\\) ist. \\[\\begin{align}\n\\begin{split}\n\\mathbb{E}_\\theta(S(\\theta))\n& = \\int S(\\theta)p_\\theta(x) \\,dx \\\\\n& = \\int \\frac{d}{d\\theta}\\ell(\\theta)p_\\theta(x) \\,dx \\\\\n& = \\int \\frac{d}{d\\theta} \\ln L(\\theta) p_\\theta(x) \\,dx \\\\\n& = \\int \\frac{1}{L(\\theta)}\\frac{d}{d\\theta}L(\\theta) p_\\theta(x) \\,dx  \\\\\n& = \\int \\frac{1}{p_\\theta(x)}\\frac{d}{d\\theta}L(\\theta) p_\\theta(x) \\,dx  \\\\\n& = \\int \\frac{d}{d\\theta}L(\\theta) \\,dx  \\\\\n& = \\frac{d}{d\\theta} \\int p_\\theta(x)\\,dx \\\\\n& = \\frac{d}{d\\theta} 1 \\\\\n& = 0.\n\\end{split}\n\\end{align}\\] Mit der Definition der Varianz folgt dann sofort, dass \\(\\mathbb{V}_\\theta(S(\\theta)) = \\mathbb{E}_\\theta(S(\\theta)^2)\\) ist. Als nächstes zeigen wir, dass \\(J(\\theta) =  \\mathbb{E}_\\theta(S(\\theta)^2)\\) und deshalb \\(\\mathbb{V}_\\theta(S(\\theta)) = J(\\theta)\\) ist. \\[\\begin{align}\n\\begin{split}\nJ(\\theta)\n& = \\mathbb{E}_\\theta\\left(-\\frac{d^2}{d\\theta^2} \\ln L(\\theta)\\right) \\\\\n& = \\mathbb{E}_\\theta\\left(-\\frac{d}{d\\theta} \\frac{\\frac{d}{d\\theta}L(\\theta)}{L(\\theta)}\\right) \\\\\n& = \\mathbb{E}_\\theta\\left(-\\frac{\\frac{d^2}{d\\theta^2} L(\\theta) L(\\theta) - \\frac{d}{d\\theta}L(\\theta)\\frac{d}{d\\theta}L(\\theta)}{L(\\theta)L(\\theta)}\\right) \\\\\n& = - \\mathbb{E}_\\theta\\left(\\frac{\\frac{d^2}{d\\theta^2} L(\\theta)}{L(\\theta)}\\right) +\n\\mathbb{E}_\\theta\\left(\\frac{\\left(\\frac{d}{d\\theta}L(\\theta)\\right)^2}{(L(\\theta))^2}\\right) \\\\\n& = - \\int \\frac{\\frac{d^2}{d\\theta^2} L(\\theta)}{L(\\theta)} p_\\theta(x) \\,dx +\n      \\int \\frac{\\left(\\frac{d}{d\\theta}L(\\theta)\\right)^2}{(L(\\theta))^2} p_\\theta(x) \\,dx \\\\\n& = - \\frac{d^2}{d\\theta^2} \\int p_\\theta(x) \\,dx +\n      \\int \\left(\\frac{1}{L(\\theta)}\\frac{d}{d\\theta}L(\\theta)\\right)^2 p_\\theta(x) \\,dx \\\\\n& = - \\frac{d^2}{d\\theta^2} 1 +\n\\int \\left(\\frac{d}{d\\theta} \\ln L(\\theta) \\right)^2 p_\\theta(x) \\,dx \\\\\n& = \\mathbb{E}_\\theta\\left(S(\\theta)^2\\right).\n\\end{split}\n\\end{align}\\]\n\nDer Erwartungswert der Ableitung der Log-Likelihood-Funktion ist also immer Null und die erwartete Fisher-Information ist immer gleich der Varianz der Scorefunktion. Wir wollen die Scorefunktion und die verschiedenen Formen der Fisher-Information nun für die uns vertrauten Frequentistischen Inferenzmodelle konkret berechnen. Nachfolgendes Theorem fasst zunächst die Ergebnisse für das Bernoullimodell zusammen.\n\nTheorem 22.10 (Scorefunktion und Fisher-Informationen des Bernoullimodells) Es sei \\(\\upsilon = (\\upsilon_1,...,\\upsilon_n)\\) die Stichprobe eines Bernoullimodells mit Parameter \\(\\mu \\in ]0,1[\\). Dann gelten:\n\nDie Scorefunktion der Stichprobe ist \\[\\begin{equation}\nS : ]0,1[ \\to \\mathbb{R}, \\mu \\mapsto S(\\mu) := \\frac{1}{\\mu}\\sum_{i=1}^n y_i  -  \\frac{1}{1-\\mu} \\left(n - \\sum_{i=1}^n y_i \\right).\n\\end{equation}\\]\nDie Fisher-Information der Stichprobe ist \\[\\begin{equation}\nI : ]0,1[ \\to \\mathbb{R}, \\mu \\mapsto I(\\mu) := I(\\mu) =  \\frac{ny}{\\mu^2} + \\frac{n(1 - y)}{1-\\mu}^{2}.\n\\end{equation}\\]\nDie beobachtete Fisher-Information der Stichprobe ist \\[\\begin{equation}\nI : ]0,1[ \\to \\mathbb{R}, \\hat{\\mu}^{\\mbox{\\tiny ML}} \\mapsto\nI\\left(\\hat{\\mu}^{\\mbox{\\tiny ML}}\\right)\n:= \\frac{ny}{\\hat{\\mu}_{n}^{{\\mbox{\\tiny ML}}^2}} + \\frac{n(1 - y)}{1-\\hat{\\mu}^{\\mbox{\\tiny ML}}}.\n\\end{equation}\\]\nDie erwartete Fisher-Information der Stichprobe ist \\[\\begin{equation}\nJ : ]0,1[ \\to \\mathbb{R}, \\mu \\mapsto J(\\mu)\n:= \\frac{n}{\\mu(1-\\mu)}.\n\\end{equation}\\]\n\n\n\nBeweis. Die Scorefunktion wurde bereits im Kontext der Maximum-Likelihood-Schätzung von \\(\\mu\\) hergeleitet. Wir betrachten die Fisher-Information einer einzelnen Bernoulli-Zufallsvariable \\(\\upsilon\\). \\[\\begin{align}\n\\begin{split}\nI(\\mu)\n& := -\\frac{d^2}{d\\mu^2} \\ell_1(\\mu)                                                                                        \\\\\n&  = -\\frac{d^2}{d\\mu^2} \\ln p_\\mu(y)                                                                                       \\\\\n&  = -\\frac{d^2}{d\\mu^2}\\left(y \\ln \\mu + (1 - y) \\ln (1-\\mu)\\right)                                                        \\\\\n&  = -\\frac{\\partial}{\\partial{\\mu}}\\left(\\frac{\\partial}{\\partial{\\mu}}\\left(y \\ln \\mu + (1 - y) \\ln (1-\\mu)\\right)\\right) \\\\\n&  = -\\frac{\\partial}{\\partial{\\mu}}\\left(\\frac{y}{\\mu} + \\frac{(1 - y)}{1-\\mu}\\right)                                      \\\\\n&  = -\\left(-\\frac{y}{\\mu^2} - \\frac{(1 - y)}{1-\\mu}^{2}\\right)                                                             \\\\\n&  =  \\frac{y}{\\mu^2} + \\frac{(1 - y)}{1-\\mu}^{2}.                                                                          \\\\\n\\end{split}\n\\end{align}\\] Damit ergibt sich die erwartete Fisher-Information der Zufallsvariable \\(\\upsilon\\) als \\[\\begin{align}\n\\begin{split}\nJ(\\mu)\n& = \\mathbb{E}_\\mu(I(\\mu))                                                                  \\\\\n& = \\mathbb{E}_\\mu \\left(\\frac{\\upsilon}{\\mu^2} + \\frac{(1 - \\upsilon)}{1-\\mu}^{2} \\right)          \\\\\n& = \\frac{\\mathbb{E}_\\mu(\\upsilon)}{\\mu^2} + \\frac{(1 - \\mathbb{E}_\\mu(\\upsilon))}{1-\\mu}^{2}       \\\\\n& = \\frac{\\mu}{\\mu^2} + \\frac{(1 - \\mu)}{1-\\mu}^{2}                                         \\\\\n& = \\frac{1}{\\mu(1-\\mu)}.                                                                   \\\\\n\\end{split}\n\\end{align}\\] Mit der Additivitätseigenschaft der Fisher-Information und der Definition der beobachteten Fisher-Information ergibt sich dann sofort \\[\\begin{equation}\nI(\\mu)\n=  \\frac{ny}{\\mu^2} + \\frac{n(1 - y)}{1-\\mu}^{2}\n\\mbox{ und }\nJ(\\mu) = \\frac{n}{\\mu(1-\\mu)}.\n\\end{equation}\\]\n\nDie Scorefunktion und die Fisher-Informationen des Normalverteilungsmodells betrachten wir lediglich unter der zusätzlichen Annahme eines bekannten Varianzparameters (Theorem 22.11) bzw. eines bekannten Erwartungswertparameters (Theorem 22.12)\n\nTheorem 22.11 (Scorefunktion und Fisher-Informationen des Normalverteilungsmodells bei bekanntem Varianzparameter) \\(\\upsilon = (\\upsilon_1,...,\\upsilon_n)\\) sei die Stichprobe eines Normalverteilungsmodells und der Varianzparameter \\(\\sigma^2\\) sei als bekannt vorausgesetzt. Dann gelten:\n\nDie Scorefunktion der Stichprobe ist \\[\\begin{equation}\nS : \\mathbb{R} \\to \\mathbb{R}, \\mu \\mapsto S(\\mu) := \\frac{1}{\\sigma^2}\\sum_{i=1}^n(y_i-\\mu).\n\\end{equation}\\]\nDie Fisher-Information der Stichprobe ist \\[\\begin{equation}\nI : \\mathbb{R} \\to \\mathbb{R}, \\mu \\mapsto I(\\mu) := \\frac{n}{\\sigma^2}.\n\\end{equation}\\]\nDie beobachtete Fisher-Information der Stichprobe ist \\[\\begin{equation}\nI(\\hat{\\mu}^{\\mbox{\\tiny ML}}_n) = \\frac{n}{\\sigma^2}.\n\\end{equation}\\]\nDie erwartete Fisher-Information der Stichprobe ist \\[\\begin{equation}\nJ : \\mathbb{R} \\to \\mathbb{R}, \\mu \\mapsto J(\\mu) := \\frac{n}{\\sigma^2}.\n\\end{equation}\\]\n\n\n\nBeweis. Wir erinnern uns, dass die Log-Likelihood-Funktion eines Normalverteilungsmodells bei bekanntem Varianzparameter \\(\\sigma^2\\) durch \\[\\begin{equation}\n\\ell : \\mathbb{R} \\to \\mathbb{R},\n\\mu \\mapsto \\ell(\\mu)\n:= -\\frac{n}{2} \\ln 2\\pi - \\frac{n}{2} \\ln \\sigma^2  - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n(y_i-\\mu)^2\n\\end{equation}\\] gegeben ist. Damit ergibt sich die Scorefunktion als \\[\\begin{align}\n\\begin{split}\nS(\\mu)\n&  = \\frac{\\partial}{\\partial{\\mu}}\\ell(\\mu)\n= \\frac{1}{\\sigma^2}\\sum_{i=1}^n(y_i-\\mu)\n\\end{split}\n\\end{align}\\] Die Fisher-Information der Stichprobe ergibt sich als \\[\\begin{align}\n\\begin{split}\nI(\\mu)\n= -\\frac{d^2}{d\\mu^2}\\ell(\\mu)\n= -\\frac{\\partial}{\\partial{\\mu}}S(\\mu)\n= -\\frac{1}{\\sigma^2}\\frac{\\partial}{\\partial{\\mu}}\\left(\\sum_{i=1}^n y_i -  n\\mu \\right)\n= \\frac{n}{\\sigma^2}.\n\\end{split}\n\\end{align}\\] Die beobachtete Fisher-Information ist die Fisher-Information an der Stelle des Maximum-Likelihood Schätzes \\(\\hat{\\mu}^{\\mbox{\\tiny ML}}_n\\). Die erwartete Fisher-Information schließlich ergibt sich als \\[\\begin{align}\n\\begin{split}\nJ(\\mu)\n= \\mathbb{E}_\\mu(I(\\mu))\n= \\mathbb{E}_\\mu\\left(\\frac{n}{\\sigma^2}\\right)\n= \\frac{n}{\\sigma^2}.\n\\end{split}\n\\end{align}\\]\n\n\nTheorem 22.12 (Scorefunktion und Fisher-Informationen des Normalverteilungsmodells bei bekanntem Erwartungswertparameter) \\(\\upsilon = (\\upsilon_1,...,\\upsilon_n)\\) sei die Stichprobe eines Normalverteilungsmodells und der Varianzparameter \\(\\sigma^2\\) sei als bekannt vorausgesetzt. Dann gelten:\n\nDie Scorefunktion der Stichprobe ist gegeben durch \\[\\begin{equation}\nS : \\mathbb{R}_{&gt;0} \\to \\mathbb{R}, \\sigma^2 \\mapsto S(\\sigma^2) := - \\frac{n}{2 \\sigma^2} + \\frac{1}{2\\sigma^4}\\sum_{i=1}^n(y_i-\\mu)^2\n\\end{equation}\\]\nDie Fisher-Information der Stichprobe ist gegeben durch \\[\\begin{equation}\nI : \\mathbb{R}_{&gt;0} \\to \\mathbb{R}, \\sigma^2 \\mapsto I(\\sigma^2) :=\n\\frac{1}{\\sigma^6}\\sum_{i=1}^n (y_i - \\mu)^2 - \\frac{n}{2\\sigma^4}\n\\end{equation}\\]\nDie beobachtete Fisher-Information der Stichprobe ist gegeben durch \\[\\begin{equation}\nI(\\hat{\\sigma}^{2\\,\\mbox{\\tiny ML}}_n) =  \\frac{n}{2\\hat{\\sigma}_{\\mbox{\\tiny ML}}^4}\n\\end{equation}\\]\nDie erwartete Fisher-Information der Stichprobe ist gegeben durch \\[\\begin{equation}\nJ : \\mathbb{R}_{&gt;0} \\to \\mathbb{R}, \\sigma^2 \\mapsto J(\\sigma^2) := \\frac{n}{2\\sigma^4}.\n\\end{equation}\\]\n\n\n\nBeweis. Wir erinnern uns, dass die Log-Likelihood-Funktion der Stichprobe eines Normalverteilungsmodells bei bekanntem Erwartungswert-Parameter \\(\\mu\\) durch \\[\\begin{equation}\n\\ell : \\mathbb{R}_{&gt;0} \\to \\mathbb{R},\n\\sigma^2 \\mapsto \\ell(\\sigma^2)\n:= -\\frac{n}{2} \\ln 2\\pi - \\frac{n}{2} \\ln \\sigma^2  - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n(y_i-\\mu)^2.\n\\end{equation}\\] gegeben ist. Die Scorefunktion ergibt sich also als \\[\\begin{align}\n\\begin{split}\nS(\\sigma^2)\n= \\frac{\\partial}{\\partial\\sigma^2}\\ell(\\sigma^2)\n= - \\frac{n}{2 \\sigma^2} + \\frac{1}{2\\sigma^4}\\sum_{i=1}^n(y_i-\\mu)^2.\n\\end{split}\n\\end{align}\\] Die Fisher-Information der Stichprobe ergibt sich als \\[\\begin{align}\n\\begin{split}\nI(\\sigma^2)\n& = -\\frac{\\partial}{\\partial\\sigma^2}S(\\sigma^2) \\\\\n& = - \\frac{\\partial}{\\partial\\sigma^2}\\left(\\frac{n}{2\\sigma^4} - \\frac{1}{\\sigma^6}\\sum_{i=1}^n (y_i - \\mu)^2\\right) \\\\\n& = \\frac{1}{\\sigma^6}\\sum_{i=1}^n (y_i - \\mu)^2 - \\frac{n}{2\\sigma^4}.\n\\end{split}\n\\end{align}\\] Die beobachtete Fisher-Information ist die Fisher-Information an der Stelle des Maximum-Likelihood Schätzes \\(\\hat{\\sigma}^{2^{\\mbox{\\tiny ML}}}\\), \\[\\begin{align}\n\\begin{split}\nI(\\hat{\\sigma}^{2\\,\\mbox{\\tiny ML}}_n)\n& =   \\frac{\\sum_{i=1}^n (y_i - \\mu)^2}{\\left(\\hat{\\sigma}^{2\\,\\mbox{\\tiny ML}}_n \\right)^3}\n    - \\frac{n}{2\\left(\\hat{\\sigma}^{2\\,\\mbox{\\tiny ML}}_n \\right)^2}                                    \\\\\n& =   \\frac{\\sum_{i=1}^n (y_i - \\mu)^2}{\\frac{1}{n^3}\\left(\\sum_{i=1}^n (y_i - \\mu)^2 \\right)^3}\n    - \\frac{n}{2\\left(\\hat{\\sigma}^{2\\,\\mbox{\\tiny ML}}_n \\right)^2}                                    \\\\\n& =   \\frac{1}{\\frac{1}{n^3}\\left(\\sum_{i=1}^n (y_i - \\mu)^2 \\right)^2}\n    - \\frac{n}{2\\left(\\hat{\\sigma}^{2\\,\\mbox{\\tiny ML}}_n \\right)^2}                                    \\\\\n& =   \\frac{n}{\\left(\\hat{\\sigma}^{2\\,\\mbox{\\tiny ML}}_n \\right)^2}\n    - \\frac{n}{2\\left(\\hat{\\sigma}^{2\\,\\mbox{\\tiny ML}}_n \\right)^2}                                \\\\\n& =  \\frac{n}{2\\left(\\hat{\\sigma}^{2\\,\\mbox{\\tiny ML}}_n \\right)^2}                                     \\\\\n& =  \\frac{n}{2\\hat{\\sigma}^{4\\,\\mbox{\\tiny ML}}_n}.\n\\end{split}\n\\end{align}\\] Die erwartete Fisher-Information ergibt sich schließlich als \\[\\begin{align}\n\\begin{split}\nJ(\\sigma^2)\n& = \\mathbb{E}_{\\sigma^2}(I(\\sigma^2))  \\\\\n& = \\mathbb{E}_{\\sigma^2}\\left(\\frac{1}{\\sigma^6}\\sum_{i=1}^n (y_i - \\mu)^2  - \\frac{n}{2\\sigma^4}\\right)   \\\\\n& = \\frac{1}{\\sigma^6}\\sum_{i=1}^n \\mathbb{E}_{\\sigma^2}\\left((y_i - \\mu)^2 \\right)   - \\frac{n}{2\\sigma^4} \\\\\n& = \\frac{1}{\\sigma^6}\\sum_{i=1}^n \\sigma^2 - \\frac{n}{2\\sigma^4}   \\\\\n& = \\frac{n\\sigma^2}{\\sigma^6} - \\frac{n}{2\\sigma^4}    \\\\\n& = \\frac{n}{\\sigma^4} - \\frac{n}{2\\sigma^4}    \\\\\n& = \\frac{n}{2\\sigma^4}.    \\\\\n\\end{split}\n\\end{align}\\]\n\nMit den oben diskutierten Eigenschaften der Scorefunktion können wir nun die Cramér-Rao-Ungleichung formulieren und beweisen.\n\nTheorem 22.13 (Cramér-Rao-Ungleichung) Gegeben sei ein Frequentistisches Inferenzmodell mit eindimensionalen Parameter \\(\\theta \\in \\Theta \\subseteq \\mathbb{R}\\), WMF oder WDF \\(p_\\theta\\) und \\(\\hat{\\tau}\\) sei ein erwartungstreuer Schätzer von \\(\\tau(\\theta)\\). Dann gilt \\[\\begin{equation}\n\\mathbb{V}_\\theta(\\hat{\\tau}) \\ge \\frac{\\left(\\frac{d}{d\\theta}\\tau(\\theta)\\right)^2}{J(\\theta)}.\n\\end{equation}\\] Speziell gilt für einen Parameterschätzer mit \\(\\tau(\\theta) := \\theta\\) und somit \\[\\begin{equation}\n\\hat{\\tau} = \\hat{\\theta} \\mbox{ und } \\left(\\frac{d}{d\\theta}\\tau(\\theta)\\right)^2 = 1,\n\\end{equation}\\] dass \\[\\begin{equation}\n\\mathbb{V}_\\theta(\\hat{\\theta}) \\ge \\frac{1}{J(\\theta)}.\n\\end{equation}\\] Die rechte Seite obiger Ungleichungen wird dabei Cramér-Rao-Schranke genannt.\n\n\nBeweis. Wir halten zunächst fest, dass für die Zufallsvariablen \\(S(\\theta)\\) und \\(\\hat{\\tau}\\) mit der Korrelationsungleichung (Theorem 16.4) und der Identität von \\(\\mathbb{V}_\\theta(S(\\theta))\\) und \\(J(\\theta)\\) (Theorem 22.9) gilt, dass \\[\\begin{equation}\n\\frac{\\mathbb{C}_\\theta(S(\\theta), \\hat{\\tau})^2}{\\mathbb{V}_\\theta(S(\\theta))\\mathbb{V}_\\theta(\\hat{\\tau})}\n\\le 1\n\\Leftrightarrow \\mathbb{V}_\\theta(\\hat{\\tau})\n\\ge \\frac{\\mathbb{C}_\\theta(S(\\theta),\\hat{\\tau})^2}{J(\\theta)}.\n\\end{equation}\\] Mit dem Kovarianzverschiebungssatz (Theorem 15.10), der Tatsache, dass der Erwartungswert der Scorefunktion immer Null ist (Theorem 22.9) und der vorausgesetzten Erwartungstreue von \\(\\hat{\\tau}\\) ergibt sich dann zunächst \\[\\begin{align}\n\\begin{split}\n\\mathbb{C}_\\theta(S(\\theta),\\hat{\\tau})\n& = \\mathbb{E}_\\theta(S(\\theta)\\hat{\\tau})  - \\mathbb{E}_\\theta(S(\\theta))\\mathbb{E}_\\theta(\\hat{\\tau}) \\\\\n& = \\mathbb{E}_\\theta(S(\\theta)\\hat{\\tau})                                                              \\\\\n& = \\int S(\\theta)\\,\\hat{\\tau}\\,p_\\theta(x) \\,dx                                                        \\\\\n& = \\int \\frac{d}{d\\theta} \\ln L(\\theta)\\,\\hat{\\tau}\\,p_\\theta(x) \\,dx                                  \\\\\n& = \\int \\frac{\\frac{d}{d\\theta} L(\\theta)}{L(\\theta)}\\,\\hat{\\tau}\\,p_\\theta(x) \\,dx                    \\\\\n& = \\int \\frac{\\frac{d}{d\\theta} L(\\theta)}{p_\\theta(x)}\\,\\hat{\\tau}\\,p_\\theta(x) \\,dx                  \\\\\n& = \\int \\frac{d}{d\\theta} L(\\theta)\\, \\hat{\\tau}  \\,dx                                                 \\\\\n& = \\frac{d}{d\\theta} \\int L(\\theta)\\, \\hat{\\tau}  \\,dx                                                 \\\\\n& = \\frac{d}{d\\theta} \\int \\hat{\\tau}\\, p_\\theta(x) \\,dx                                                \\\\\n& = \\frac{d}{d\\theta} \\mathbb{E}_\\theta(\\hat{\\tau})                                                     \\\\\n& = \\frac{d}{d\\theta} \\tau(\\theta).\n\\end{split}\n\\end{align}\\] Damit folgt dann aber direkt \\[\\begin{equation}\n\\mathbb{V}_\\theta(\\hat{\\tau}) \\ge \\frac{\\left(\\frac{d}{d\\theta}\\tau(\\theta) \\right)^2}{J(\\theta)}.\n\\end{equation}\\]\n\nFür Parameterschätzer gilt also insbesondere, dass die Varianz eines erwartungstreuen Parameterschätzers \\(\\hat{\\theta}\\) immer größer oder gleich der reziproken erwarteten Fisher-Information \\(J(\\theta)\\) ist. Im Fall, dass sogar \\[\\begin{equation}\n\\mathbb{V}_\\theta(\\hat{\\theta}) = \\frac{1}{J(\\theta)}\n\\end{equation}\\] ist, ist die Varianz des Parameterschätzers minimal und der Schätzer somit als optimaler Schätzer im Sinne der Cramér-Rao-Ungleichung nachgewiesen. Wir kommen auf diesen Gedanken in Kapitel 22.4 zurück.",
    "crumbs": [
      "Frequentistische Inferenz",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Punktschätzung</span>"
    ]
  },
  {
    "objectID": "302-Punktschätzung.html#sec-asymptotische-schaetzereigenschaften",
    "href": "302-Punktschätzung.html#sec-asymptotische-schaetzereigenschaften",
    "title": "22  Punktschätzung",
    "section": "22.3 Asymptotische Schätzereigenschaften",
    "text": "22.3 Asymptotische Schätzereigenschaften\nIn diesem Abschnitt geben wir eine Kurzeinführung in die Asymptotische Statistik (Vaart (1998)). Die Asymptotische Statistik ist der Bereich der Frequentistischen Inferenz, der sich mit dem Verhalten von Statistiken und Schätzern bei großen Stichprobenumfängen \\(n\\) beschäftigt. Dabei werden Methoden der Asymptotischen Statistik zum einen benutzt um, wie hier, qualitative Schätzereigenschaften zu studieren und andererseits um Schätzereigenschaften bei großen Stichprobenumfänge approximieren zu können. Da Stichprobenumfänge heutzutage durchaus groß sein können (“Big Data”), sind die Methoden der Asymptotischen Statistik also für die Anwendung gut motiviert und dort vielseitig einsetzbar.\nIn Fortführung von Kapitel 22.2 wollen wir in diesem Abschnitt vier asymptotische Schätzereigenschaften beleuchten. Um zu betonen, dass in diesem Abschnitt die Eigenschaften eines Schätzers \\(\\hat{\\tau}\\) vom Stichprobenumfang \\(n\\) abhängen, schreiben wir in diesem Abschnitt für einen Schätzer \\(\\hat{\\tau}_n\\). In Kapitel 22.3.1 betrachten wir die Asymptotische Erwartungstreue eines Schätzers. Dabei heißt ein Schätzer \\(\\hat{\\tau}_n\\) für \\(\\tau\\) asymptotisch erwartungstreu, wenn der Erwartungswert von \\(\\hat{\\tau}_n\\) für große Stichprobenumfänge \\(n \\to \\infty\\) mit dem wahren, aber unbekannten, Wert \\(\\tau(\\theta)\\) identisch ist. In Kapitel 22.3.2 führen wir den Begriff der Konsistenz eines Schätzers ein. Intuitiv heißt ein Schätzer \\(\\hat{\\tau}_n\\) für \\(\\tau\\) konsistent, wenn für große Stichprobenumfänge \\(n \\to \\infty\\) die Wahrscheinlichkeit dafür, dass \\(\\hat{\\tau}_n(\\upsilon)\\) vom wahren, aber unbekannten, Wert \\(\\tau(\\theta)\\) abweicht, beliebig klein wird. Für große Stichprobenumfänge resultieren die Verteilungen von Schätzern oft in Normalverteilungen. In Kapitel 22.3.3 führen wir mit dem Begriff der Asymptotischen Normalverteilung eine entsprechende Formalisierung ein. Ein Schätzer \\(\\hat{\\tau}_n\\) für \\(\\tau\\) heißt dann asymptotisch normalverteilt, wenn für große Stichprobenumfänge \\(n \\to \\infty\\), die Verteilung von \\(\\hat{\\tau}_n\\) durch eine Normalverteilung gegeben ist. In Kapitel 22.3.4 schließlich betrachten wir mit der Asymptotischen Effizienz ein Optimalitätskriterium für Schätzer bei gegen unendlich strebenden Stichprobenumfängen mit folgender Bedeutung: ein Schätzer \\(\\hat{\\tau}_n\\) für \\(\\tau\\) heißt asymptotisch effizient, wenn für große Stichprobenumfänge \\(n \\to \\infty\\) die Verteilung von \\(\\hat{\\tau}_n\\) durch eine Normalverteilung mit Erwartungswertparameter \\(\\tau(\\theta)\\) und Varianzparameter gleich der Cramér-Rao-Schranke gegeben ist.\n\n22.3.1 Asymptotische Erwartungstreue\nDie Asymptotische Erwartungstreue eines Schätzers verfeinert den Begriff des erwartungstreuen Schätzers wie folgt.\n\nDefinition 22.9 (Asymptotische Erwartungstreue) \\(\\upsilon = (\\upsilon_1,...,\\upsilon_n)\\) sei die Stichprobe eines parametrischen Produktmodells und \\(\\hat{\\tau}_n\\) sei ein Schätzer für \\(\\tau\\). \\(\\hat{\\tau}_n\\) heißt asymptotisch erwartungstreu, wenn gilt, dass \\[\\begin{equation}\n\\lim_{n\\to\\infty} \\mathbb{E}_\\theta(\\hat{\\tau}_n(\\upsilon)) = \\tau(\\theta) \\mbox{ für alle } \\theta \\in \\Theta.\n\\end{equation}\\]\n\nAsymptotisch erwartungstreue Schätzer sind also nur erwartungstreu, wenn der Stichprobenumfang gegen Unendlich geht. Erwartungstreue Schätzer sind immer auch asymptotisch erwartungstreu, da ihre Erwartungstreue vom Stichprobenumfang unabhängig sind. Als Beispiel für einen nur asymptotisch erwartungstreuen Schätzer betrachten wir den Maximum-Likelihood Schätzer des Varianzparameters des Normalverteilungsmodells.\n\nTheorem 22.14 (Asymptotische Erwartungstreue des Varianzparameterschätzers) \\(\\upsilon = (\\upsilon_1,...,\\upsilon_n)\\) sei die Stichprobe eines Normalverteilungsmodells mit Varianzparameter \\(\\sigma^2\\). Dann ist der Maximum-Likelihood Schätzer von \\(\\sigma^2\\) \\[\\begin{equation}\n\\hat{\\sigma}_n^{2^{\\mbox{\\tiny ML}}}\n:= \\frac{1}{n}\\sum_{i=1}^n \\left(\\upsilon_i - \\bar{\\upsilon}_n \\right)^2\n\\end{equation}\\] nicht erwartungstreu, aber asymptotisch erwartungstreu.\n\n\nBeweis. Mit der Erwartungstreue der Stichprobenvarianz ergibt sich zunächst (vgl. Theorem 22.3) \\[\\begin{equation}\n\\mathbb{E}_{\\mu,\\sigma^2}\\left(\\hat{\\sigma}_n^{2^{\\mbox{\\tiny ML}}} \\right)\n= \\mathbb{E}_{\\mu,\\sigma^2}\\left(\\frac{1}{n}\\sum_{i=1}^n \\left(\\upsilon_i - \\bar{\\upsilon}_n \\right)^2 \\right)\n= \\frac{1}{n}\\mathbb{E}_{\\mu,\\sigma^2}\\left(\\sum_{i=1}^n \\left(\\upsilon_i - \\bar{\\upsilon}_n \\right)^2 \\right)\n=  \\frac{n-1}{n}\\sigma^2.\n\\end{equation}\\] Also gilt \\[\\begin{equation}\n\\mathbb{E}_{\\mu,\\sigma^2}\\left(\\hat{\\sigma}_n^{2^{\\mbox{\\tiny ML}}} \\right) \\neq \\sigma^2\n\\end{equation}\\] und \\(\\hat{\\sigma}_n^{2^{\\mbox{\\tiny ML}}}\\) kein erwartungstreuer Schätzer von \\(\\sigma^2\\). Allerdings gilt auch \\[\\begin{equation}\n\\frac{n-1}{n} \\to 1 \\mbox{ für } n \\to \\infty,\n\\end{equation}\\] so dass \\[\\begin{equation}\n\\lim_{n \\to \\infty} \\mathbb{E}_{\\mu,\\sigma^2}\\left(\\hat{\\sigma}_n^{2^{\\mbox{\\tiny ML}}}\\right) =\n\\lim_{n \\to \\infty}  \\frac{n-1}{n}\\sigma^2 =\n\\sigma^2 \\lim_{n \\to \\infty}  \\frac{n-1}{n} =\n\\sigma^2\n\\end{equation}\\] gilt und der Maximum-Likelihood Schätzer von \\(\\sigma^2\\) damit asymptotisch erwartungstreu ist.\n\nFolgender R Code simuliert das Verhalten des Maximum-Likelihood Schätzers für den Varianzparameter eines Normalverteilungsmodells mit wahren, aber unbekannten, Parametern \\(\\mu = 1\\) und \\(\\sigma^2 = 2\\) in Abhängigkeit des Stichprobenumfangs.\n\n# Modellformulierung\nmu        = 1                                               # wahrer, aber unbekannter, Erwartungswertparameter\nsigsqr    = 2                                               # wahrer, aber unbekannter, Varianzparameter\nn         = seq(1,100, by = 2)                              # Stichprobenumfänge\nns        = 1e3                                             # Anzahl Simulation pro Stichprobenumfang\nsigsqr_ml = matrix(rep(NaN, length(n)*ns),ncol = length(n)) # \\hat{\\sigma^2}^{ML} Array\n            \n# Simulation\nfor(i in seq_along(n)){                                     # Stichprobenumfangsiterationen \n    for(s in 1:ns){                                         # Stichprobenrealisierungsiterationen   \n        y               = rnorm(n[i], mu, sqrt(sigsqr))     # Stichprobenrealisation\n        sigsqr_ml[s,i]  = ((n[i]-1)/n[i])*var(y)            # \\hat{\\sigma^2}^{ML}\n    }\n}\nE_sigsqr_ml = colMeans(sigsqr_ml)                           # Schätzererwartungswertschaetzung\n\nWir visualisieren den basierend auf obigen Simulationen geschätzten Erwartungswert des Schätzers in Abhängigkeit des Stichprobenumfangs in Abbildung 22.3. Für kleine Stichprobenumfänge unterschätzt der Schätzer den wahren, aber unbekannten, Varianzparameter deutlich, für größere Stichprobenumfänge dagegen nicht.\n\n\n\n\n\n\nAbbildung 22.3: Simulation des Erwartungswerts des Maximum-Likelihood Schätzers für den Varianzparameter eines Normalverteilungsmodells. Bei kleinen Stichprobenumfängen unterschätzt der Maximum-Likelihood Schätzer den wahren, aber unbekannten, Varianzparameter systematisch, bei größeren Stichprobenumfängen ist der Schätzer in etwa erwartungstreu.\n\n\n\n\n\n22.3.2 Konsistenz\nDer Begriff der Konsistenz eines Schätzers verallgemeinert das Schwache Gesetz der Großen Zahlen von Stichprobenmitteln und Erwartungswerten (vgl. Theorem 17.1) auf beliebige Schätzer und wahre, aber unbekannte, Parameterwerte.\n\nDefinition 22.10 (Konsistenz) \\(\\upsilon := (\\upsilon_1,...,\\upsilon_n)\\) sei die Stichprobe eines parametrischen Produktmodells und \\(\\hat{\\tau}_n\\) sei ein Schätzer von \\(\\tau\\). Dann heißt eine Folge von Schätzern \\(\\hat{\\tau}_1, \\hat{\\tau}_2, ...\\) eine konsistente Folge von Schätzern, wenn für jedes noch so kleine \\(\\epsilon &gt; 0\\) und jedes \\(\\theta \\in \\Theta\\) gilt, dass \\[\\begin{equation}\n\\lim_{n\\to \\infty}\n\\mathbb{P}_\\theta\\left(|\\hat{\\tau}_n(\\upsilon) - \\tau(\\theta)| \\ge \\epsilon \\right) = 0.\n\\end{equation}\\] Wenn \\(\\hat{\\tau}_1,\\hat{\\tau}_2,...\\) eine konsistente Folge von Schätzern ist, dann heißt \\(\\hat{\\tau}_n\\) ein konsistenter Schätzer.\n\nDie Intuition zu Definition 22.10 entspricht der dem Gesetz der Großen Zahlen. Für Stichprobenumfänge mit \\(n \\to \\infty\\) wird die Wahrscheinlichkeit, dass \\(\\hat{\\tau}_n(\\upsilon)\\) beliebig nah bei \\(\\tau(\\theta)\\) liegt bzw. die Wahrscheinlichkeit, dass \\(\\hat{\\tau}_n(\\upsilon)\\) weit von \\(\\tau(\\theta)\\) abweicht, klein. Allerdings müssen diese Eigenschaften für alle möglichen wahren, aber unbekannten, Parameterwerte gelten. Wie unten gezeigt werden soll ist das Stichprobenmittel ein konsistenter Schätzer für den Erwartungswertparameter eines Normalverteilungsmodells. Analog zu Kapitel 17.1 demonstrieren wir die Bedeutung der Konsistenz zunächst mithilfe der Simulation für \\(\\mu = 1\\) und \\(\\sigma^2 = 2\\) anhand folgenden R Codes.\n\n# Modellformulierung\nmu      = 1                                          # wahrer, aber unbekannter, Wert von \\mu  \nsigsqr  = 2                                          # wahrer, aber unbekannter, Wert von \\sigma^2  \nn       = seq(1,1e3,by = 10)                         # Stichprobenumfang n\neps     = c(0.15, 0.10, 0.05)                        # \\epsilon Werte\nne      = length(eps)                                # Anzahl \\epsilon Werte\nnn      = length(n)                                  # Anzahl Stichprobenumfänge\nns      = 1000                                       # Anzahl Simulationen\nE       = array(rep(NaN,nn*ne*ns),dim = c(nn,ne,ns)) # Ereignisindikatorarray             \n\n# Simulation\nfor(e in seq_along(eps)){                            # \\epsilon Iterationen\n    for(i in seq_along(n)){                          # n Iterationen\n        for(s in 1:ns){                              # Simulationsiterationen\n\n            # Stichprobenrealisationen\n            y   = rnorm(n[i], mu, sqrt(sigsqr))\n            if(abs(mean(y) - mu) &gt;= eps[e]){         # |y_bar - \\mu)| \\ge \\epsilon\n                E[i,e,s] = 1\n            } else {                                 # |y_bar - \\mu)| &lt; \\epsilon\n                E[i,e,s] = 0\n            }\n        }\n    }\n}\n\n# Schaetzung von \\mathbb{P}(|\\hat{\\tau}_n(\\upsilon)-\\tau(\\theta)| \\ge \\epsilon)\nP_hat       = apply(E, c(1,2), mean)\n\nWir visualisieren die Schätzung der Wahrscheinlichkeit \\(\\mathbb{P}(|\\hat{\\tau}_n(\\upsilon)-\\tau(\\theta)| \\ge \\epsilon)\\) für dieses Beispiel in Abbildung 22.4 in Abhängigkeit des Stichprobenumfangs und des Kriteriumwerts \\(\\epsilon\\). Bei größeren Werten von \\(\\epsilon\\) genügen geringe Stichprobenumfänge für eine geringe Wahrscheinlichkeit der Abweichung des Schätzers vom wahren, aber unbekannten, Parameterwert, bei kleineren Werten von \\(\\epsilon\\) sind dazu größere Stichprobenumfänge nötig.\n\n\n\n\n\n\nAbbildung 22.4: Simulation der Konsistenz des Stichprobenmittels als Schätzer für den Erwartungswertparameter des Normalverteilungsmodells.\n\n\n\nDie in Theorem 22.15 und Theorem 22.16 angegebenen Kriterien für die Konsistenz von Schätzern vereinfachen den Nachweis der Konsistenz eines Schätzers mithilfe des Mittleren Quadratischen Fehlers (Definition 22.6).\n\nTheorem 22.15 (Mittlerer Quadratischer Fehler Kriterium für Konsistenz) \\(\\upsilon := (\\upsilon_1,...,\\upsilon_n)\\) sei die Stichprobe eines parametrischen Produktmodells und \\(\\hat{\\tau}_n\\) sei ein Schätzer von \\(\\tau\\). Wenn gilt, dass \\[\\begin{equation}\n\\lim_{n\\to \\infty} \\mbox{MQF}(\\hat{\\tau}_n) = 0,\n\\end{equation}\\] dann ist \\(\\hat{\\tau}_n\\) ein konsistenter Schätzer.\n\n\nBeweis. Mit der Chebychev-Ungleichung gilt, dass \\[\\begin{equation}\n\\mathbb{P}_\\theta\\left(|\\hat{\\tau}_n(\\upsilon) - \\tau(\\theta)| \\ge \\epsilon \\right) \\le\n\\frac{\\mathbb{E}_\\theta\\left((\\hat{\\tau}_n(\\upsilon) -  \\tau(\\theta))^2\\right)}{\\epsilon^2}\n\\end{equation}\\] Grenzwertbildung ergibt dann \\[\\begin{equation}\n\\lim_{n\\to \\infty}\\mathbb{P}_\\theta\\left(|\\hat{\\tau}_n(\\upsilon) - \\tau(\\theta)| \\ge \\epsilon \\right) \\le\n\\frac{1}{\\epsilon^2}\\lim_{n\\to\\infty}\\mathbb{E}_\\theta\\left((\\hat{\\tau}_n(\\upsilon) - \\tau(\\theta))^2\\right).\n\\end{equation}\\] Wenn also \\(\\lim_{n\\to\\infty}\\mathbb{E}_\\theta\\left((\\hat{\\tau}_n(\\upsilon) - \\tau(\\theta))^2\\right) = 0\\) gilt, dann gilt mit \\(\\mathbb{P}_\\theta(|\\hat{\\tau}_n(\\upsilon) - \\tau(\\theta)| \\ge \\epsilon)\\ge 0\\), dass \\[\\begin{equation}\n\\lim_{n\\to \\infty}\\mathbb{P}_\\theta\\left(|\\hat{\\tau}_n(\\upsilon) - \\tau(\\theta)| \\ge \\epsilon \\right) = 0.\n\\end{equation}\\] Also ist \\(\\hat{\\tau}_n\\) ein konsistenter Schätzer.\n\n\nTheorem 22.16 (Bias-Varianz-Kriterium für Konsistenz) \\(\\upsilon := (\\upsilon_1,...,\\upsilon_n)\\) sei die Stichprobe eines parametrischen Produktmodells und \\(\\hat{\\tau}_n\\) sei ein Schätzer von \\(\\tau\\). Wenn \\[\\begin{equation}\n\\lim_{n\\to \\infty} \\mbox{B}(\\hat{\\tau}_n) = 0\n\\mbox{ und }\n\\lim_{n\\to \\infty} \\mathbb{V}_\\theta(\\hat{\\tau}_n) = 0\n\\end{equation}\\] gelten, dann ist \\(\\hat{\\tau}_n\\) ein konsistenter Schätzer\n\n\nBeweis. Wenn \\(n \\to \\infty\\), dann gilt \\(\\mbox{B}(\\hat{\\tau}_n) \\to 0\\), also auch \\(\\mbox{B}(\\hat{\\tau}_n)^2 \\to 0\\). Wenn für \\(n \\to \\infty\\) sowohl \\(\\mbox{B}(\\hat{\\tau}_n)^2 \\to 0\\) als auch \\(\\mathbb{V}_\\theta(\\hat{\\tau}_n) \\to 0\\), dann gilt auch \\(\\lim_{n\\to \\infty} \\mbox{MQF}(\\hat{\\theta}_n) = 0\\). Also gilt mit Theorem 22.15, dass \\(\\hat{\\tau}_n\\) konsistent ist.\n\nAls Anwendung von Theorem 22.16 wollen wir mit folgendem Theorem die Konsistenz des Stichprobenmittels als Schätzer für den Erwartungswert bei Normalverteilung nachweisen.\n\nTheorem 22.17 (Konsistenz des Erwartungswertschätzers bei Normalverteilung) Es sei \\(\\upsilon\\) die Stichprobe eines Normalverteilungsmodells. Dann ist \\(\\bar{\\upsilon}_n\\) ein konsistenter Schätzer von \\(\\mathbb{E}(\\upsilon_1)\\).\n\n\nBeweis. Mit der Erwartungstreue des Stichprobenmittels als Schätzer für den Erwartungswert gilt zunächst \\[\\begin{equation}\n\\lim_{n \\to \\infty} \\mbox{B}(\\bar{\\upsilon}_n) =  0.\n\\end{equation}\\] Weiterhin gilt mit der Varianz des Stichprobenmittels \\[\\begin{equation}\n\\lim_{n \\to \\infty} \\mathbb{V}_\\theta(\\bar{\\upsilon}_n) = \\lim_{n\\to \\infty} \\frac{1}{n}\\mathbb{V}(\\upsilon_1) = 0.\n\\end{equation}\\] Mit dem Bias-Varianz-Kriterium folgt dann die Konsistenz von \\(\\bar{\\upsilon}_n\\) als Schätzer von \\(\\mathbb{E}(\\upsilon_1)\\)\n\nMan beachte, dass Theorem 22.17 natürlich auch schon im Schwachen Gesetz der Großen Zahlen impliziert ist (vgl. Theorem 17.1).\n\n\n22.3.3 Asymptotische Normalität\nIn manchen Fällen nähert sich die Frequentistische Verteilung eines Schätzers bei großen Stichprobenumfängen einer Normalverteilung. Man nennt einen solchen Schätzer dann asymptotisch normalverteilt\n\nDefinition 22.11 (Asymptotisch normalverteilter Schätzer) \\(\\upsilon = (\\upsilon_1,...,\\upsilon_n)\\) sei die Stichprobe eines parametrischen Produktmodells und \\(\\hat{\\theta}_n\\) sei ein Parameterschätzer für \\(\\theta\\). Weiterhin sei \\[\\begin{equation}\n\\tilde{\\theta} \\sim N(\\mu,\\sigma^2)\n\\end{equation}\\] eine normalverteilte Zufallsvariable mit Erwartungswertparameter \\(\\mu\\) und Varianzparameter \\(\\sigma^2\\). Wenn \\(\\hat{\\theta}_n\\) in Verteilung gegen \\(\\tilde{\\theta}\\) konvergiert, wenn also für die KVFen \\(P_n\\) und \\(P\\) von \\(\\hat{\\theta}_n\\) und \\(\\tilde{\\theta}\\), respektive, gilt, dass \\[\\begin{equation}\n\\lim_{n\\to \\infty} P_n(\\hat{\\theta}_n) = P(\\tilde{\\theta}),\n\\end{equation}\\] dann heißt \\(\\hat{\\theta}_n\\) asymptotisch normalverteilt und wir schreiben \\[\\begin{equation}\n\\hat{\\theta}_n  \\stackrel{a}{\\sim}  N(\\mu,\\sigma^2).\n\\end{equation}\\]\n\nAls Beispiel für einen asymptotisch normalverteilten Schätzer betrachten wir in Kapitel 22.3.4 den Maximum-Likelihood-Schätzer des Bernoullimodellparameters. Es ist bemerkenswert, dass dieser Schätzer asymptotisch normalverteilt ist, da die Stichprobenvariablen des Bernoullimodells lediglich die Werte Null und Eins annehmen.\n\n\n22.3.4 Asymptotische Effizienz\nIn manchen Fällen lassen sich neben der asymptotischen Normalität eines Schätzers auch in der Form des Frequentistischen Modells begründete Aussagen zum Erwartungswertparameter und Varianzparameter dieser asymptotischen Normalverteilung machen. Der Begriff des effizienten Schätzers formuliert einen solchen Spezialfall.\n\nDefinition 22.12 \\(\\upsilon = (\\upsilon_1,...,\\upsilon_n)\\) sei die Stichprobe eines parametrischen Produktmodells und \\(\\hat{\\theta}_n\\) sei ein Parameterschätzer für \\(\\theta\\). Weiterhin sei \\(J(\\theta)\\) die erwartete Fisher-Information der Stichprobe \\(\\upsilon\\). Wenn gilt, dass \\[\\begin{equation}\n\\hat{\\theta}_n \\stackrel{a}{\\sim} N\\left(\\theta, J(\\theta)^{-1}\\right),\n\\end{equation}\\] dann heißt \\(\\hat{\\theta}_n\\) .\n\nEin asymptotisch effizienter Schätzer ist offenbar auch immer asymptotisch normalverteilt und erwartungstreu. Die Varianz der asymptotischen Verteilung eines Schätzers nennt man auch die asymptotische Varianz. Für einen asymptotisch effizienten Schätzers ist die asymptotische Varianz mit der Cramér-Rao-Schranke identisch und damit in der Menge der erwartungstreuen asymptotisch normalverteilten Schätzer minimal. Als Beispiel für einen asymptotisch effizienten Schätzer betrachten wir den Maximum-Likelihood-Schätzer des Bernoullimodellparameters. Folgender R simuliert die Frequentistische Verteilung dieses Schätzers und bestimmt die WDF seiner asymptotischen Verteilung\n\n# Modellformulierung\nmu          = 0.4                                                               # wahrer, aber unbekannter, Parameterwert\nn_all       = c(1e1,5e1,1e2)                                                    # Stichprobenumfang  \nns          = 1e4                                                               # Anzahl Stichprobenrealisierungen\nmu_hat   = matrix(rep(NaN, length(n_all)*ns), nrow = length(n_all))             # Maximum-Likelihood-Schätzerarray                  \nmu_hat_r = 1e3                                                                  # Maximum-Likelihood Schätzerraumaufloesung\nmu_hat_y = seq(0,1,len = mu_hat_r)                                              # Maximum-LikelihoodSchätzerraum\nmu_hat_p = matrix(rep(NaN, length(n_all)*mu_hat_r), nrow = length(n_all))       # Maximum-Likelihood WDF Array\n\n# Stichprobenumfängeiterationen\nfor(i in seq_along(n_all)){\n\n    # Simulationsiterationen\n    for(s in 1:ns){\n        y           = rbinom(n_all[i],1,mu)                                     # Stichprobenrealisation\n        mu_hat[i,s] = mean(y)                                                   # Maximum-Likelihood-Schätzerrealisation\n    }\n    mu_hat_p[i,] = dnorm(mu_hat_y, mu, sqrt(mu*(1-mu)/n_all[i]))                # WDF der asymptotischen Verteilung\n}\n\n\n\n\n\n\n\nAbbildung 22.5: Simulation der asymptotischen Effizienz des Maximum-Likelihood-Parameterschätzers für den Parameter eines Bernoullimodells. Mit steigendem Stichprobenumfang gleicht sich die durch ein Histogramm dargestellte Verteilung der simulierten Schätzerwerte der in Definition 22.12 formulierten Normalverteilung",
    "crumbs": [
      "Frequentistische Inferenz",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Punktschätzung</span>"
    ]
  },
  {
    "objectID": "302-Punktschätzung.html#sec-eigenschaften-von-maximum-likelihood-schaetzern",
    "href": "302-Punktschätzung.html#sec-eigenschaften-von-maximum-likelihood-schaetzern",
    "title": "22  Punktschätzung",
    "section": "22.4 Eigenschaften von Maximum-Likelihood Schätzern",
    "text": "22.4 Eigenschaften von Maximum-Likelihood Schätzern\nDas Maximum-Likelihood-Prinzip zur Gewinnung von Schätzern für Parameter Frequentistischer Inferenzmodelle ist durch folgendes Theorem zu den Eigenschaften von Maximum-Likelihood-Schätzern begründet.\n\nTheorem 22.18 (Eigenschaften von Maximum-Likelihood Schätzern) \\(\\upsilon\\) sei die Stichprobe eines parametrischen Produktmodells und \\(\\hat{\\theta}_n^{\\mbox{\\tiny{ML}}}\\) sei ein Maximum-Likelihood Schätzer für \\(\\theta\\). Dann gilt, dass \\(\\hat{\\theta}_n^{\\mbox{\\tiny{ML}}}\\)\n\nnicht notwendigerweise erwartungstreu, aber\nasymptotisch erwartungstreu,\nkonsistent,\nasymptotisch normalverteilt und\nasymptotisch effizient\n\nist.\n\nFür einen nicht unaufwändigen Beweis von Theorem 22.18 verweisen wir auf Held & Sabanés Bové (2014), Abschnitt 3.4. Nutzt man also das Maximum-Likelihood Prinzip zur Gewinnung eines Schätzers, so erfüllt der gewonnene Schätzer also garantiert die in Theorem 22.18 Schätzergütekriterien.",
    "crumbs": [
      "Frequentistische Inferenz",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Punktschätzung</span>"
    ]
  },
  {
    "objectID": "302-Punktschätzung.html#literaturhinweise",
    "href": "302-Punktschätzung.html#literaturhinweise",
    "title": "22  Punktschätzung",
    "section": "22.5 Literaturhinweise",
    "text": "22.5 Literaturhinweise\nDie in diesem Kapitel vorgestellten Resultate gehen in ganz wesentlicher Weise auf Fisher (1922) zurück. Aldrich (1997) gibt dazu eine historische Einordnung.",
    "crumbs": [
      "Frequentistische Inferenz",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Punktschätzung</span>"
    ]
  },
  {
    "objectID": "302-Punktschätzung.html#selbstkontrollfragen",
    "href": "302-Punktschätzung.html#selbstkontrollfragen",
    "title": "22  Punktschätzung",
    "section": "22.6 Selbstkontrollfragen",
    "text": "22.6 Selbstkontrollfragen\n\nGeben Sie die Definition des Begriffs eines Parameterpunktschätzers wieder.\nErläutern Sie den Begriff des Parameterpunktschätzers.\nGeben Sie Definition der Begriffe der Likelihood-Funktion und der Log-Likelihood-Funktion wieder.\nGeben Sie Definition des Begriffs des Maximum-Likelihood Schätzes wieder.\nErläutern Sie das Vorgehen zur Gewinnung von Maximum-Likelihood-Schätzern.\nGeben Sie das Theorem zum Maximum-Likelihood-Schätzer des Bernoullimodellparameters wieder.\nGeben Sie das Theorem zu den Maximum-Likelihood-Schätzern der Normalverteilungsmodellparameter wieder.\nGeben Sie die Definition des Begriffs der Erwartungstreue eines Schätzers wieder.\nErläutern Sie den Begriff der Erwartungstreue eines Schätzers.\nGeben Sie Definition der Begriffe der Varianz und des Standardfehlers eines Schätzers wieder.\nErläutern Sie den Begriff der asymptotischen Erwartungstreue eines Schätzers.\nErläutern Sie den Begriff der Konsistenz eines Schätzers.\nErläutern Sie den Begriff der asymptotischen Normalität eines Schätzers.\nGeben Sie das Theorem zu den Eigenschaften von Maximum-Likelihood-Schätzern wieder. \n\n\n\n\n\nAldrich, J. (1997). R.A. Fisher and the Making of Maximum Likelihood 1912-1922. Statistical Science, 12(3), 162–176. https://doi.org/10.1214/ss/1030037906\n\n\nFisher, R. A. (1922). On the Mathematical Foundations of Theoretical Statistics. Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character, 222(594-604), 309–368. https://doi.org/10.1098/rsta.1922.0009\n\n\nHeld, L., & Sabanés Bové, D. (2014). Applied Statistical Inference. Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-642-37887-4\n\n\nVaart, A. W. van der. (1998). Asymptotic Statistics. Cambridge University Press.",
    "crumbs": [
      "Frequentistische Inferenz",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Punktschätzung</span>"
    ]
  },
  {
    "objectID": "303-Konfidenzintervalle.html",
    "href": "303-Konfidenzintervalle.html",
    "title": "23  Konfidenzintervalle",
    "section": "",
    "text": "23.1 Definition\nMan beachte in Definition 23.1 dass, wie in allen Frequentistischen Inferenzmodellen, der Parameter \\(\\theta\\) ein wahrer, aber unbekannter, Wert und damit insbesondere fest, nicht zufällig, ist. Weil die oberen und unteren Grenzen eines Konfidenzintervalls als Funktionen der zufälligen Stichprobe Zufallsvariablen sind, ist das durch sie definierte Konfidenzintervall ein zufälliges Intervall. Die etwas ungewöhnliche Schreibweise \\(\\kappa(\\upsilon) \\ni \\theta\\) bedeutet schlicht \\(\\theta \\in \\kappa(\\upsilon)\\). Da \\(\\kappa(\\upsilon)\\) in dem Ausdruck \\(\\mathbb{P}_\\theta\\left(\\kappa(\\upsilon) \\ni \\theta\\right)\\) wie beschrieben die zufällige Entität ist, steht \\(\\kappa(\\upsilon)\\) konventionellerweise links, man denke zum Beispiel an einen Ausdruck wie \\(\\mathbb{P}(\\xi = x)\\). Ein \\(\\delta\\)-Konfidenzintervall überdeckt den wahren, aber unbekannten, Parameter \\(\\theta\\) nach Definition mit Wahrscheinlichkeit \\(\\delta\\). Dabei wird oft eine hohe Überdeckungswahrscheinlichkeit von \\(\\delta := 0.95\\) gewählt, in diesem Fall spricht man von einem \\(95\\%\\)-Konfidenzintervall.\nIntuitiv mag man \\(\\delta\\)-Konfidenzintervalle auf zwei Arten interpretieren. Im ersten Fall geht man von der Wiederholung der unabhängigen Realisierung von Stichproben bei immer identischen wahren, aber unbekannten, Parameter \\(\\theta\\) aus. Wiederholt man die Realisierung von Daten also “unter immer den gleichen Umständen” und bei identischen wahren, aber, unbekannten, Parameter \\(\\theta\\), so überdeckt ein \\(\\delta\\)-Konfidenzintervall diesen wahren, aber unbekannten, Parameter im langfristigen Mittel in \\(\\delta\\cdot 100 \\%\\) der realisierten Fälle. Alternativ gilt diese Frequentistische Wahrscheinlichkeit für die Überdeckung des wahren, aber unbekannten, Parameters nach Definition 23.1 jedoch auch für jeden beliebigen wahren, aber unbekannten, Parameterwert \\(\\theta_i, i = 1,2,...\\). Auch wenn man also unterschiedliche, wahre, aber unbekannte, Parameterwerte \\(\\theta_1,\\theta_2,...\\) betrachtet und in jedem Fall eine, von den anderen Realisierungen unabhängige, Realisierung der Stichprobe erfasst, so überdecken die entsprechenden \\(\\delta\\)-Konfidenzintervalle diese wahren, aber unbekannten, Parameter im langfristigen Mittel in \\(\\delta\\cdot 100 \\%\\) der Fälle. Intuitiv braucht man also “eine Studie”, also die Untersuchung eines wahren, aber unbekannten, Parameterwerts, nicht unter den gleichen Umständen “unendlich oft wiederholen”, um von der Überdeckungswahrscheinlichkeit eines Konfidenzintervalls zu profitieren, sondern es genügt in “unterschiedlichen Studien”, also den Untersuchungen unterschiedlicher wahrer, aber unbekannten, Parameter, Konfidenzintervalle gemäß Definition 23.1 zu bestimmen, auch im diesen Fall ist ihre Überdeckungswahrscheinlichkeit für die wahren, aber unbekannten Parameter, gesichert. Wir demonstrieren diese beiden Interpretationen in der Folge mithilfe einer Simulation.\nUm nun für gegeben Frequentistische Inferenzmodelle \\(\\delta\\)-Konfidenzintervalle durch eine konkrete Angabe der Statistiken \\(G_u(\\upsilon)\\) und \\(G_o(\\upsilon)\\) zu konstruieren, geht man vor wie folgt. Zunächst definiert man das Frequentistische Inferenzmodell und legt damit die Verteilung der Stichprobe \\(\\upsilon\\) fest. In einem zweiten Schritt definiert man eine Statistik, also eine Funktion der Stichprobe, die als Grundlage für \\(G_u(\\upsilon)\\) und \\(G_o(\\upsilon)\\) dienen mag und analysiert ihre, auf der Stichprobenverteilung basierende, Verteilung. Hat man die entsprechende Verteilung gefunden, so kann man diese dazu nutzen, die Überdeckungswahrscheinlichkeit des wahren, aber unbekannten, Parameters durch ein entsprechend definiertes Konfidenzintervall zu sichern. Wir zeichnen dieses Verfahren in der Entwicklung und den konstruktiven Beweisen der folgenden Beispiele nach.",
    "crumbs": [
      "Frequentistische Inferenz",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Konfidenzintervalle</span>"
    ]
  },
  {
    "objectID": "303-Konfidenzintervalle.html#sec-definition",
    "href": "303-Konfidenzintervalle.html#sec-definition",
    "title": "23  Konfidenzintervalle",
    "section": "",
    "text": "Definition 23.1 (\\(\\delta\\)-Konfidenzintervall) Es sei \\(\\upsilon\\) die Stichprobe eines Frequentistischen Inferenzmodells mit wahrem, aber unbekannten Parameter, \\(\\theta \\in \\Theta\\), es sei \\(\\delta \\in \\,]0,1[\\) und es seien \\(G_u(\\upsilon)\\) und \\(G_o(\\upsilon)\\). Dann heißt ein Intervall der Form \\[\\begin{equation}\n\\kappa(\\upsilon) := [G_u(\\upsilon), G_o(\\upsilon)],\n\\end{equation}\\] so dass \\[\\begin{equation}\n\\mathbb{P}_\\theta\\left(\\kappa(\\upsilon) \\ni \\theta\\right) =\n\\mathbb{P}_\\theta\\left(G_u(\\upsilon) \\le \\theta \\le G_o(\\upsilon) \\right) =\n\\delta \\mbox{ für alle } \\theta \\in \\Theta \\mbox{ gilt}\n\\end{equation}\\] ein \\(\\delta\\)-Konfidenzintervall für \\(\\theta\\). \\(\\delta\\) ist die Überdeckungswahrscheinlichkeit von \\(\\kappa(\\upsilon)\\) für \\(\\theta\\) und wird meist Konfidenzlevel genannt. Die Statistiken \\(G_u(\\upsilon)\\) und \\(G_o(\\upsilon)\\) heißen die unteren und oberen Grenzen des Konfidenzintervalls, respektive.",
    "crumbs": [
      "Frequentistische Inferenz",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Konfidenzintervalle</span>"
    ]
  },
  {
    "objectID": "303-Konfidenzintervalle.html#sec-beispiele-fuer-konfidenzintervalle",
    "href": "303-Konfidenzintervalle.html#sec-beispiele-fuer-konfidenzintervalle",
    "title": "23  Konfidenzintervalle",
    "section": "23.2 Beispiele für Konfidenzintervalle",
    "text": "23.2 Beispiele für Konfidenzintervalle\n\nKonfidenzintervall für den Erwartungswertparameter des Normalverteilungsmodells\nWir betrachten die Konstruktion eines \\(\\delta\\)-Konfidenzintervalls für den Erwartungswertparameter des Normalverteilungsmodells. Zu diesem Zweck definieren wir zunächst folgende Konfidenzintervallstatistik.\n\nDefinition 23.2 (\\(T\\)-Konfidenzintervallstatistk) Gegeben sei das Normalverteilungsmodell \\[\\begin{equation}\n\\upsilon_1,...,\\upsilon_n \\sim N\\left(\\mu,\\sigma^2\\right)\n\\end{equation}\\] Dann heißt die mit dem Stichprobenmittel und der Stichprobenstandardabweichung \\[\\begin{equation}\n\\bar{\\upsilon} := \\frac{1}{n}\\sum_{i=1}^n \\upsilon_i \\mbox{ und } S := \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n(\\upsilon_i - \\bar{\\upsilon})^2},\n\\end{equation}\\] definierte Statistik \\[\\begin{equation}\nT := \\sqrt{n}\\frac{\\bar{\\upsilon} - \\mu}{S}\n\\end{equation}\\] \\(T\\)-Konfidenzintervallstatistik.\n\nFür die Verteilung der \\(T\\)-Konfidenzintervallstatistik gilt folgendes Theorem.\n\nTheorem 23.1 (Verteilung der \\(T\\)-Konfidenzintervallstatistik) Die \\(T\\)-Konfidenzintervallstatistik ist eine \\(t\\)-verteilte Zufallsvariable mit Parameter \\(n-1\\), es gilt also \\[\\begin{equation}\nT \\sim t(n-1)\n\\end{equation}\\]\n\n\nBeweis. \n\nMan beachte, dass die \\(T\\)-Konfidenzintervallstatistik nach Definition 23.2 eine Funktion der Stichprobe ist, während ihre Verteilung nach Theorem 23.1 unabhängig von den wahren, aber unbekannten, Parametern der Stichprobenverteilung ist. Man nennt dies auch die Pivoteigenschaft der \\(T\\)-Konfidenzeigenschaft. Für die folgenden Entwicklungen erinnern wir daran, dass wir die WDF einer \\(t\\)-verteilten Zufallvariable mit \\(t\\), die KVF einer \\(t\\)-verteilten Zufallvariable mit \\(\\Psi\\) und die inverse KVF einer \\(t\\)-verteilten Zufallvariable mit \\(\\Psi^{-1}\\) bezeichnen. Folgender R Code simuliert zunächst die Verteilung der \\(T\\)-Konfidenzintervallstatistitk.\n\n# Modellformulierung\nmu       = 10                                         # wahrer, aber unbekannter, Erwartungswertparameter\nsigsqr   = 4                                          #  wahrer, aber unbekannter, Varianzparameter\nn        = 12                                         # Stichprobenumfang\nns       = 1e4                                        # Anzahl Stichprobenrealisierungen\nres      = 1e3                                        # Ausgangsraumaufloesung\n\n# analytische Definitionen und Resultate\nyx       = seq(3,17,len = res)                        # \\upsilonilon_i Raum\nssqrx    = seq(0,20,len = res)                        # S^2 Raum\ntx       = seq(-4,4,len = res)                        # T Raum\np_y_i    = dnorm(yx,mu,sqrt(sigsqr))                  # \\upsilonilon_i WDF\np_y_bar  = dnorm(yx,mu,sqrt(sigsqr/n))                # \\upsilonilon_bar WDF\np_sqr    = dchisq(ssqrx,n-1)                          # S^2 WDF\np_t      = dt(tx,n-1)                                 # T WDF\n\n# Simulation\ny_i      = rep(NaN,ns)                                # y_i Array\ny_bar    = rep(NaN,ns)                                # \\bar{y} Array\nS        = rep(NaN,ns)                                # S Array  \nTKS      = rep(NaN,ns)                                # T-Konfidenzintervallstatistik Array\nfor(s in 1:ns){                                       # Simulationsiterationen\n  y         = rnorm(n,mu,sqrt(sigsqr))                # Stichprobenrealisierung\n  y_i[s]    = y[1]                                    # Stichprobenrealisierung \\upsilonilon_i mit i = 1\n  y_bar[s]  = mean(y)                                 # Stichprobenmittelrealisierung\n  S[s]      = sd(y)                                   # Stichprobenstandardabweichungrealisierung\n  TKS[s]    = sqrt(n)*((y_bar[s]-mu)/S[s])            # T-Konfidenzintervallstatistikrealisierung\n}\n\n\n\n\n\n\n\nAbbildung 23.1: Simulation der Verteilung der \\(T\\)-Konfidenzintervallstatistik und der ihr zugrundeliegenden Verteilungen der Stichprobenvariable, des Stichprobenmittels und der Stichprobenvarianz.\n\n\n\nIn Abbildung 23.1 visualisieren wir die Verteilung der \\(T\\)-Konfidenzintervallstatistik als Resultat der ihr zugrundeliegenden Verteilungen der Stichprobenvariablen, des Stichprobenmittels und der Stichprobenvarianz (vgl. Kapitel 19.4). Mithilfe der Verteilung der \\(T\\)-Konfidenzintervallstatistik können wir jetzt folgendes Theorem zum Konfidenzintervall für den Erwartungswertparameter des Normalverteilungsmodells beweisen.\n\nTheorem 23.2 (Konfidenzintervall für den Erwartungswertparameter des Normalverteilungsmodells) Gegeben sei das Normalverteilungsmodell \\[\\begin{equation}\n\\upsilon_1,...,\\upsilon_n \\sim N(\\mu,\\sigma^2)\n\\end{equation}\\] mit wahren, aber unbekannten, Parametern \\(\\mu\\) und \\(\\sigma^2\\), es sei \\(\\delta \\in ]0,1[\\) und es sei \\[\\begin{equation}\nt_\\delta := \\Psi^{-1}\\left(\\frac{1+\\delta}{2}; n-1\\right).\n\\end{equation}\\] mit der inversen KVF \\(\\Psi^{-1}\\) einer \\(t\\)-verteilten Zufallsvariable. Dann gilt für das Intervall \\[\\begin{equation}\n\\kappa(\\upsilon) :=\n\\left[\\bar{\\upsilon} - \\frac{S}{\\sqrt{n}}t_\\delta,\n\\bar{\\upsilon} + \\frac{S}{\\sqrt{n}}t_\\delta\\right],\n\\end{equation}\\] mit dem Stichprobenmittel und der Stichprobenstandardabweichung \\[\\begin{equation}\n\\bar{\\upsilon} := \\frac{1}{n}\\sum_{i=1}^n \\upsilon_i \\mbox{ und } S := \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n(\\upsilon_i - \\bar{\\upsilon})^2},\n\\end{equation}\\] respektive, dass \\[\\begin{equation}\n\\mathbb{P}_{\\mu}(\\kappa(\\upsilon) \\ni \\mu) = \\delta.\n\\end{equation}\\]\n\n\nBeweis. Für \\(\\delta \\in ]0,1[\\) seien zunächst \\[\\begin{equation}\nt_1 := \\Psi^{-1}\\left(\\frac{1 - \\delta}{2}; n - 1\\right)\n\\mbox{ und }\nt_2 := \\Psi^{-1}\\left(\\frac{1 + \\delta}{2}; n-1\\right)\n\\end{equation}\\] definiert. Dann gilt \\[\\begin{equation}\n\\frac{1+\\delta}{2} - \\frac{1-\\delta}{2} = \\delta\n\\end{equation}\\] und weiterhin gilt mit der Symmetrie der WDF der \\(t\\)-Verteilung, dass \\[\\begin{equation}\nt_1 = - t_2.\n\\end{equation}\\] Per Definition gilt dann aber mit Definition 23.2 und Theorem 23.1, dass \\[\\begin{equation}\n\\mathbb{P}_\\mu\\left(-t_{\\delta} \\le T \\le t_{\\delta} \\right) = \\delta.\n\\end{equation}\\] Damit folgt dann aber direkt \\[\\begin{align}\n\\begin{split}\n\\delta\n& =\n\\mathbb{P}_\\mu\\left(-t_\\delta \\le T \\le t_\\delta \\right)                                                                            \\\\\n& =\n\\mathbb{P}_\\mu\\left(-t_\\delta \\le \\frac{\\sqrt{n}}{S}(\\bar{\\upsilon} - \\mu) \\le t_\\delta \\right)                                         \\\\\n& =\n\\mathbb{P}_\\mu\\left(-\\frac{S}{\\sqrt{n}}t_\\delta \\le \\bar{\\upsilon} - \\mu \\le \\frac{S}{\\sqrt{n}}t_\\delta \\right)                     \\\\\n& =\n\\mathbb{P}_\\mu\\left(-\\bar{\\upsilon} -\\frac{S}{\\sqrt{n}}t_\\delta \\le - \\mu \\le - \\bar{\\upsilon} + \\frac{S}{\\sqrt{n}}t_\\delta \\right)     \\\\\n& =\n\\mathbb{P}_\\mu\\left(\\bar{\\upsilon} + \\frac{S}{\\sqrt{n}}t_\\delta \\ge \\mu \\ge \\bar{\\upsilon} - \\frac{S}{\\sqrt{n}}t_\\delta \\right)         \\\\\n& =\n\\mathbb{P}_\\mu\\left(\\bar{\\upsilon} - \\frac{S}{\\sqrt{n}}t_\\delta \\le \\mu \\le \\bar{\\upsilon} + \\frac{S}{\\sqrt{n}}t_\\delta \\right)         \\\\\n& =\n\\mathbb{P}_\\mu\\left(\\left[\\bar{\\upsilon} - \\frac{S}{\\sqrt{n}}t_\\delta, \\bar{\\upsilon} + \\frac{S}{\\sqrt{n}}t_\\delta\\right] \\ni \\mu \\right).      \\\\\n& =\n\\mathbb{P}_\\mu\\left(\\kappa(\\upsilon) \\ni \\mu \\right).       \\\\\n\\end{split}\n\\end{align}\\]\n\nDer entscheidene Schritt zur Sicherung der Überdeckungswahrscheinlichkeit \\(\\delta\\) des wahren, aber unbekannten, Erwartungswertparameters durch das in Theorem 23.2 definierte Konfidenzintervall ist die Definition von\n\\[\\begin{equation}\nt_\\delta := \\Psi^{-1}\\left(\\frac{1+\\delta}{2}; n-1\\right).\n\\end{equation}\\] Wie im Beweis von Theorem 23.2 nachgezeichnet ist die Überdeckungswahrscheinlichkeit des Konfidenzintervalls für den wahren, aber unbekannten, Erwartungswertparameter äquivalent zu der Tatsache, dass bei Wahl eben dieses \\(t_\\delta\\) die \\(T\\)-Konfidenzintervallstatistik eine Wahrscheinlichkeit von \\(\\delta\\) dafür hat, einen Wert im Intervall \\([-t_\\delta, t_\\delta]\\) anzunehmen. Wir visualisieren die Wahl von \\(t_\\delta\\) für Fall \\(\\delta := 0.95\\) und \\(n := 5\\) in Abbildung 23.2. In diesem Fall ergibt sich \\[\\begin{equation}\n-t_\\delta = \\Psi^{-1}(0.025;4) = -2.57 \\mbox{ und }\nt_\\delta = \\Psi^{-1}(0.975;4) = 2.57.\n\\end{equation}\\] Abbildung 23.2 A zeigt diese Wahl aus Perspektive der WDF der \\(T\\)-Konfidenintervallstatistik. Die von \\(-t_\\delta\\) und \\(t_\\delta\\) eingeschlossene Wahrscheinlichkeitsmasse beträgt nach Konstruktion \\(\\delta\\), \\(T\\) nimmt mit einer Wahrscheinlichkeit von \\(\\delta\\) also einen Wert zwischen \\(-t_\\delta\\) und \\(t_\\delta\\) an. Abbildung 23.2 B zeigt die entsprechende Perspektive der KVF der \\(T\\)-Konfidenintervallstatistik. Basierend auf der Vorgabe von \\(\\frac{1-\\delta}{2}\\) und \\(\\frac{1+\\delta}{2}\\) werden anhand der inversen KVF \\(\\Psi^{-1}\\) die entsprechenden Werte für \\(-t_\\delta\\) und \\(t_\\delta\\) bestimmt. Man beachte, dass die hier gegebene Zentralität der Wahrscheinlichkeitsmasse in Definition 23.1 nicht implizit ist, sondern sich aus den Gegebenheiten der Verteilung der \\(T\\)-Konfidenzintervallstatistik, insbesondere ihrer Symmetrie um 0, ergibt.\n\n\n\n\n\n\nAbbildung 23.2: Sicherung der Überdeckungswahrscheinlichkeit des Konfidenzintervalls für den Erwartungswertparameter des Normalverteilungsmodells für \\(\\delta := 0.95\\) und \\(n := 5\\) aus Perspektive der WDF (A) und der KVF (B) der Verteilung der \\(T\\)-Konfidenzintervallstatistik.\n\n\n\nAbschließend wollen wir die Überdeckungswahrscheinlichkeit des durch Theorem 23.2 gegebenen Konfidenzintervalls mithilfe einer Simulation demonstrieren. Wir betrachten dabei lediglich die erste Interpretation eines Konfidenzintervalls bei konstantem, wahrem, aber unbekanntem, Parameter. Folgender R Code bestimmt in diesem Sinne zu jeder Stichprobenrealisierung das entsprechende Konfidenzintervall.\n\n# Modellformulierung\nset.seed(1)                                        # Random number generator seed\nmu      = 2                                        # wahrer, aber unbekannter, Erwartungswertparameter\nsigsqr  = 1                                        # wahrer, aber unbekannter, Varianzparameter\nsigma   = sqrt(sigsqr)                             # wahrer, aber unbekannter, Standardabweichungsparameter\nn       = 12                                       # Stichprobenumfang\ndelta   = 0.95                                     # Konfidenzbedingung\nt_delta = qt((1+delta)/2,n-1)                      # \\Psi^-1((\\delta + 1)/2, n-1)\n\n# Stichprobenrealisierungen\nns      = 1e2                                      # Anzahl Stichprobenrealisierungen \ny_bar   = rep(NaN,ns)                              # Stichprobenmittelarray\nS       = rep(NaN,ns)                              # Standardabweichungsarray\nkappa   = matrix(rep(NaN,2*ns), ncol = 2)          # Konfidenzintervallarray\nfor(i in 1:ns){                                     \n   y          = rnorm(n,mu,sigma)                  # Stichprobenrealisierung\n   y_bar[i]   = mean(y)                            # Stichprobenmittel\n   S[i]       = sd(y)                              # Stichprobenstandardabweichung\n   kappa[i,1] = y_bar[i] - (S[i]/sqrt(n))*t_delta  # untere Konfidenzintervallgrenze\n   kappa[i,2] = y_bar[i] + (S[i]/sqrt(n))*t_delta  # obere Konfidenzintervallgrenze\n}\n\nWir visualisieren die Ergebnisse dieser Simulation in Abbildung 23.3.\n\n\n\n\n\n\nAbbildung 23.3: Simulation der Überdeckungswahrscheinlichkeit des Konfidenzintervalls für den Erwartungswertparameter des Normalverteilungsmodells bei konstanten, wahren, aber unbekannten, Erwartungswertparameter \\(\\mu := 2\\) für \\(\\sigma^2 := 2, n := 12\\) und einer gewünschten Überdeckungswahrscheinlichkeit von \\(\\delta := 0.95\\). Die Abbildung zeigt für jede Stichprobenrealisierung das Konfidenzintervall und den entsprechenden Erwartungswertparameterschätzer. In der vorliegenden Simulation überdecken die Konfidenzintervalle den durch eine graue Linie eingezeichneten immer gleichen wahren, aber unbekannten, Erwartungswertparameter \\(\\mu:=2\\) in 96 von 100 Fällen. Die Stichprobenrealisierungen, für die dies nicht der Fall sind, sind mit einen orangen Kreis markiert\n\n\n\n\n# Anzahl Simulationen mit \\theta_1, \\theta_2,...\nset.seed(1)                                        # random number generator seed\nns      = 1e2                                      # Anzahl Simulationen\nmu      = 2*seq(0,1,len = ns)                      # wahrer, aber unbekannter, Erwartungswertparameter\nsigsqr  = 1                                        # wahrer, aber unbekannter, Varianzparameter\nsigma   = sqrt(sigsqr)                             # wahrer, aber unbekannter, Standardabweichungsparameter\nn       = 12                                       # Stichprobenumfang\ndelta   = 0.95                                     # Konfidenzbedingung\nt_delta = qt((1+delta)/2,n-1)                      # \\Psi^-1((\\delta + 1)/2, n-1)\n\n# Simulation\ny_bar   = rep(NaN,ns)                              # Stichprobenmittelarray\nS       = rep(NaN,ns)                              # Standardabweichungsarray\nkappa   = matrix(rep(NaN,2*ns), ncol = 2)          # Konfidenzintervallarray\nfor(i in 1:ns){\n   y          = rnorm(n,mu[i],sigma)               # Stichprobenrealisierung\n   y_bar[i]   = mean(y)                            # Stichprobenmittel\n   S[i]       = sd(y)                              # Stichprobenstandardabweichung\n   kappa[i,1] = y_bar[i] - (S[i]/sqrt(n))*t_delta  # untere Konfidenzintervallgrenze\n   kappa[i,2] = y_bar[i] + (S[i]/sqrt(n))*t_delta  # obere Konfidenzintervallgrenze\n}\n\nWir visualisieren die Ergebnisse dieser Simulation in Abbildung 23.4.\n\n\n\n\n\n\nAbbildung 23.4: Simulation der Überdeckungswahrscheinlichkeit des Konfidenzintervalls für den Erwartungswertparameter des Normalverteilungsmodells bei variablem, wahren, aber unbekannten, Erwartungswertparameter \\(\\mu\\) für \\(\\sigma^2 := 2, n := 12\\) und einer gewünschten Überdeckungswahrscheinlichkeit von \\(\\delta := 0.95\\). Die Abbildung zeigt für jede Stichprobenrealisierung das Konfidenzintervall und den entsprechenden Erwartungswertparameterschätzer. In der vorliegenden Simulation überdecken die Konfidenzintervalle den durch eine graue Linie eingezeichneten variablen wahren, aber unbekannten, Erwartungswertparameter \\(\\mu\\) in 95 von 100 Fällen. Die Stichprobenrealisierungen, für die dies nicht der Fall sind, sind mit einen orangen Kreis markiert\n\n\n\n\n\nKonfidenzintervall für den Varianzparameter des Normalverteilungsmodells\nWir betrachten die Konstruktion eines \\(\\delta\\)-Konfidenzintervalls für den Varianzparameter des Normalverteilungsmodells. Zu diesem Zweck definieren zunächst folgende Konfidenzintervallstatistik.\n\nDefinition 23.3 (\\(U\\)-Konfindenzintervallstatistik) Gegeben sei das Normalverteilungsmodell \\[\\begin{equation}\n\\upsilon_1,...,\\upsilon_n \\sim N(\\mu,\\sigma^2)\n\\end{equation}\\] Dann heißt die mit der Stichprobenvarianz \\[\\begin{equation}\n\\frac{1}{n-1}\\sum_{i=1}^n \\left(\\upsilon_i - \\bar{\\upsilon}\\right)^2\n\\end{equation}\\] definierte Statistik \\[\\begin{equation}\nU := \\frac{n-1}{\\sigma^2}S^2  \n\\end{equation}\\] \\(U\\)-Konfidenzintervallstatistik.\n\nFür die Verteilung der \\(U\\)-Konfidenzintervallstatistik gilt folgendes Theorem.\n\nTheorem 23.3 (Verteilung der \\(U\\)-Konfidenzintervallstatistik) Die \\(U\\)-Konfidenzintervallstatistik ist eine \\(\\chi^2\\)-verteilte Zufallsvariable mit Parameter \\(n-1\\), es gilt also \\[\\begin{equation}\nU \\sim \\chi^2(n-1)\n\\end{equation}\\]\n\nFür einen Beweis von Theorem 23.3 verrweisen wir auf Casella & Berger (2012). Wie die \\(T\\)-Konfidenzintervallstatistik besitzt auch die \\(U\\)-Konfidenzintervallstatistik die Pivoteigenschaft, da sie eine Funktion der Stichprobe ist, aber ihre Verteilung nach Theorem 23.3 von den wahren, aber unbekannten, Verteilungsparametern der Stichprobe nicht abhängt. Für die folgenden Entwicklungen erinnern wir daran, dass wir die WDF einer \\(\\chi^2\\)-verteilten Zufallvariable mit \\(\\chi^2\\), die KVF einer \\(\\chi^2\\)-verteilten Zufallvariable mit \\(\\Xi\\) und die inverse KVF einer \\(\\chi^2\\)-verteilten Zufallvariable mit \\(\\Xi^{-1}\\) bezeichnen. Folgender R Code simuliert zunächst die Verteilung der \\(U\\)-Konfidenzintervallstatistik.\n\n# Modellformulierung\nmu      = 10                                    # wahrer Erwartungswertparameter\nsigsqr  = 4                                     # wahrer bekannter Varianzparameter\nn       = 12                                    # Stichprobenumfang\nns      = 1e4                                   # Anzahl Stichprobenrealisierungen\nres     = 1e3                                   # Ausgangsraumaufloesung\n\n# analytische Definitionen und Resultate\nyx      = seq(3,17,len = res)                   # \\upsilonilon_i Raum\nux      = seq(0,30,len = res)                   # U Raum\np_y_i   = dnorm(yx,mu,sqrt(sigsqr))             # \\upsilonilon_i WDF\np_y_bar = dnorm(yx,mu,sqrt(sigsqr/n))           # \\upsilonilon_bar WDF\np_u     = dchisq(ux,n-1)                        # U WDF\n\n# Simulation\ny_i     = rep(NaN,ns)                           # y_i Array\ny_bar   = rep(NaN,ns)                           # \\bar{y} Array\nS_sqr   = rep(NaN,ns)                           # S^2 Array\nUKS     = rep(NaN,ns)                           # U-Konfidenzintervallstatistik Array\nfor(s in 1:ns){                                 # Simulationsiterationen\n  y         = rnorm(n,mu,sqrt(sigsqr))          # Stichprobenrealisierung\n  y_i[s]    = y[1]                              # Stichprobenrealisierung \\upsilonilon_i mit i = 1\n  y_bar[s]  = mean(y)                           # Stichprobenmittelrealisierung\n  S_sqr[s]  = var(y)                            # Stichprobenvarianzrealisierung\n  UKS[s]    = ((n-1)/sigsqr)*S_sqr[s]           # U-Konfidenzintervallstatistikrealisierung\n} \n\n\n\n\n\n\n\nAbbildung 23.5: Simulation der Verteilung der \\(U\\)-Konfidenzintervallstatistik und der ihr zugrundeliegenden Verteilungen der Stichprobenvariable, des Stichprobenmittels und der Stichprobenvarianz.\n\n\n\nMithilfe der Verteilung der \\(U\\)-Konfidenzintervallstatistik können wir jetzt folgendes Theorem zum Konfidenzintervall für den Varianzparameter des Normalverteilungsmodells beweisen.\n\nTheorem 23.4 (Konfidenzintervall für den Varianzparameter des Normalverteilungsmodells) Gegeben sei das Normalverteilungsmodell \\[\\begin{equation}\n\\upsilon_1,...,\\upsilon_n \\sim N(\\mu,\\sigma^2)\n\\end{equation}\\] mit wahren, aber unbekannten, Parametern \\(\\mu\\) und \\(\\sigma^2\\), es sei \\(\\delta \\in ]0,1[\\) und es seien \\[\\begin{equation}\nu_{\\delta} := \\Xi^{-1}\\left(\\frac{1 - \\delta}{2}; n -1  \\right)\n\\mbox{ und }\nu_{\\delta}' := \\Xi^{-1}\\left(\\frac{1 + \\delta}{2};n-1 \\right)\n\\end{equation}\\] mit der inversen KVF \\(\\Xi^{-1}\\) einer \\(\\chi^2\\)-verteilten Zufallsvariable. Dann gilt für das Intervall \\[\\begin{equation}\n\\kappa(\\upsilon) := \\left[\\frac{(n-1)S^2}{u_{\\delta}'}, \\frac{(n-1)S^2}{u_{\\delta}}\\right].\n\\end{equation}\\] mit der Stichprobenvarianz \\[\\begin{equation}\nS^2 := \\frac{1}{n-1}\\sum_{i=1}^n(\\upsilon_i - \\bar{\\upsilon})^2,\n\\end{equation}\\] dass \\[\\begin{equation}\n\\mathbb{P}_{\\sigma^2}(\\kappa(\\upsilon) \\ni \\sigma^2) = \\delta.\n\\end{equation}\\]\n\n\nBeweis. Per Definition gilt mit Definition 23.3 und Theorem 23.3, dass \\[\\begin{equation}\n\\mathbb{P}_{\\sigma^2}\\left(u_{\\delta} \\le U \\le u_{\\delta}' \\right) = \\delta.\n\\end{equation}\\] Damit folgt dann aber direkt \\[\\begin{align}\n\\begin{split}\n\\delta\n& = \\mathbb{P}_{\\sigma^2}\\left(u_{\\delta} \\le U \\le u_{\\delta}' \\right)                                                                     \\\\\n& = \\mathbb{P}_{\\sigma^2}\\left(u_{\\delta} \\le \\frac{n-1}{\\sigma^2}S^2  \\le u_{\\delta}' \\right)                                        \\\\\n& = \\mathbb{P}_{\\sigma^2}\\left(u_{\\delta}^{-1} \\ge \\frac{\\sigma^2}{(n-1)S^2} \\ge {u_{\\delta}'}^{-1} \\right)                               \\\\\n& = \\mathbb{P}_{\\sigma^2}\\left(\\frac{(n-1)S^2}{u_{\\delta}} \\ge \\sigma^2 \\ge \\frac{(n-1)S^2}{u_{\\delta}'} \\right)                   \\\\\n& = \\mathbb{P}_{\\sigma^2}\\left(\\frac{(n-1)S^2}{u_{\\delta}'} \\le \\sigma^2 \\le \\frac{(n-1)S^2)}{u_{\\delta}} \\right)                 \\\\\n& = \\mathbb{P}_{\\sigma^2}\\left(\\left[\\frac{(n-1)S^2}{u_{\\delta}'}, \\frac{(n-1)S^2)}{u_{\\delta}}\\right] \\ni \\sigma^2 \\right).    \\\\\n\\end{split}\n\\end{align}\\]\n\nWie im Falle von Theorem 23.2 ist der entscheidene Schritt zur Sicherung der Überdeckungswahrscheinlichkeit \\(\\delta\\) des wahren, aber unbekannten, Varianzparameters durch das in Theorem 23.4 definierte Konfidenzintervall die Definition von\n\\[\\begin{equation}\nu_{\\delta} := \\Xi^{-1}\\left(\\frac{1-\\delta}{2}; n-1\\right)\n\\mbox{ und }\nu_{\\delta}' := \\Xi^{-1}\\left(\\frac{1 + \\delta}{2};n-1 \\right)\n\\end{equation}\\] Wie im Beweis von Theorem 23.4 nachgezeichnet ist die Überdeckungswahrscheinlichkeit des Konfidenzintervalls für den wahren, aber unbekannten, Varianzparameter äquivalent zu der Tatsache, dass bei Wahl eben dieser Werte von \\(u_\\delta\\) und \\(u_{\\delta}'\\) die \\(U\\)-Konfidenzintervallstatistik eine Wahrscheinlichkeit von \\(\\delta\\) dafür hat, einen Wert im Intervall \\([u_{\\delta}, u_{\\delta}']\\) anzunehmen. Wir visualisieren die Wahl von \\(u_\\delta\\) und \\(u_\\delta'\\) für Fall \\(\\delta := 0.95\\) und \\(n := 10\\) in Abbildung 23.6. In diesem Fall ergibt sich \\[\\begin{equation}\nu_{\\delta}  := \\Xi^{-1}\\left(0.025;9\\right) = 2.70 \\mbox{ und }\nu_{\\delta}' := \\Xi^{-1}\\left(0.975;9\\right) = 19.0.\n\\end{equation}\\] Abbildung 23.6 A zeigt diese Wahl aus Perspektive der WDF der \\(U\\)-Konfidenintervallstatistik. Die von \\(u_\\delta\\) und \\(u_\\delta'\\) eingeschlossene Wahrscheinlichkeitsmasse beträgt nach Konstruktion \\(\\delta\\), \\(U\\) nimmt mit einer Wahrscheinlichkeit von \\(\\delta\\) also einen Wert zwischen \\(u_\\delta\\) und \\(u_\\delta'\\) an. Abbildung 23.6 B zeigt die entsprechende Perspektive der KVF der \\(U\\)-Konfidenintervallstatistik. Basierend auf der Vorgabe von \\(\\frac{1-\\delta}{2}\\) und \\(\\frac{1+\\delta}{2}\\) werden anhand der inversen KVF \\(\\Psi^{-1}\\) die entsprechenden Werte für \\(u_\\delta\\) und \\(u_\\delta'\\) bestimmt. Man beachte, dass in diesem Fall die Wahrscheinlichkeitsmasse recht arbiträr hinsichtlich des Modalwerts der Verteilung der \\(U\\)-Konfidenzintervallstatistik lokalisiert ist. Dementsprechend gibt es weitergehende Verfahren, die Überdeckungswahrscheinlichkeit einer Konfidenzintervallstatistik so zu lokalisieren, dass sie beispielsweise ein maximales Intervall in ihrem Ergebnisraum einnimmt oder eine Symmetrieeigenschaft um den Erwartungswert erfüllt, die wir hier aber nicht vertiefen wollen.\n\n\n\n\n\n\nAbbildung 23.6: Sicherung der Überdeckungswahrscheinlichkeit des Konfidenzintervalls für den Varianzparameter des Normalverteilungsmodells für \\(\\delta := 0.95\\) und \\(n := 10\\) aus Perspektive der WDF (A) und der KVF (B) der Verteilung der \\(U\\)-Konfidenzintervallstatistik.\n\n\n\nAbschließend wollen wir die Überdeckungswahrscheinlichkeit des durch Theorem 23.4 gegebenen Konfidenzintervalls mithilfe einer Simulation demonstrieren. Dazu betrachten wir zunächst die in Kapitel 23.1 gegebene erste Interpretation eines Konfidenzintervalls bei immer gleichem wahrem, aber unbekanntem, Parameter. Folgender R Code bestimmt in diesem Sinne zu jeder Stichprobenrealisierung das entsprechende Konfidenzintervall.\n\n# Modellformulierung\nset.seed(1)                                     # random number generator seed\nmu          = 2                                 # wahrer, aber unbekannter,  Erwartungswertparameter\nsigsqr      = 2                                 # wahrer, aber unbekannter,  Varianzparameter\nn           = 12                                # Stichprobenumfang\ndelta       = 0.95                              # Konfidenzbedingung\nu_delta_u   = qchisq((1-delta)/2, n - 1)        # \\Xi^2((1-\\delta)/2; n - 1)\nu_delta_o   = qchisq((1+delta)/2, n - 1)        # \\Xi^2((1+\\delta)/2; n - 1)\n\n# Stichprobenrealisierungen\nns          = 1e2                               # Anzahl Simulationen\ny_bar       = rep(NaN,ns)                       # Stichprobenmittelarray\nS2          = rep(NaN,ns)                       # Stichprobenvarianzarray\nkappa       = matrix(rep(NaN,2*ns), ncol = 2)   # Konfidenzintervallarray\nfor(i in 1:ns){                                 # Simulationsiterationen\n   y        = rnorm(n,mu,sqrt(sigsqr))          # Stichprobenrealisierung\n   S2[i]    = var(y)                            # Stichprobenvarianz\n   kappa[i,1]   = (n-1)*S2[i]/u_delta_o         # untere Konfidenzintervallgrenze\n   kappa[i,2]   = (n-1)*S2[i]/u_delta_u         # obere Konfidenzintervallgrenze\n}\n\n\n\n\n\n\n\nAbbildung 23.7: Simulation der Überdeckungswahrscheinlichkeit des Konfidenzintervalls für den Varianzparameter des Normalverteilungsmodells bei konstanten, wahren, aber unbekannten, Varianzparameter \\(\\sigma^2 := 2\\) für \\(\\mu := 2, n := 12\\) und einer gewünschten Überdeckungswahrscheinlichkeit von \\(\\delta := 0.95\\). Die Abbildung zeigt für jede Stichprobenrealisierung das Konfidenzintervall und den entsprechenden Varianzparameterschätzer. In der vorliegenden Simulation überdecken die Konfidenzintervalle den durch eine graue Linie eingezeichneten immer gleichen wahren, aber unbekannten, Varianzparameter \\(\\sigma^2 :=2\\) in 95 von 100 Fällen. Die Stichprobenrealisierungen, für die dies nicht der Fall sind, sind mit einen orangen Kreis markiert. Man beachte, dass die Konfidenzintervalle nicht symmetrisch um den den Varianzparameterschätzer angeordnet sind",
    "crumbs": [
      "Frequentistische Inferenz",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Konfidenzintervalle</span>"
    ]
  },
  {
    "objectID": "303-Konfidenzintervalle.html#sec-anwendungsbeispiel",
    "href": "303-Konfidenzintervalle.html#sec-anwendungsbeispiel",
    "title": "23  Konfidenzintervalle",
    "section": "23.3 Anwendungsbeispiel",
    "text": "23.3 Anwendungsbeispiel\nZum Abschluss dieses Abschnitts wollen wir die Evaluation von Konfidenzintervallen für den Erwartungswert und den Varianzparameter bei Normalverteilung nun im Kontext des Anwendungsbeispiels von Kapitel 21.3.1. Dazu werten wir zunächst einmal die unverzerrten Punktschätzer von \\(\\mu\\) und \\(\\sigma^2\\), also das Stichprobenmittel und die Stichprobenvarianz des Datensatzes mithilfe folgenden R Codes aus.\n\nD           = read.csv(\"./_data/303-Konfidenzintervalle.csv\")  # Datensatzeinlesen \ny           = D$dBDI                                           # Datenauswahl            \nmu_hat      = mean(y)                                          # Stichprobenmittel \nsigsqr_hat  = var(y)                                           # Stichprobenvarianz\ncat(\"mu_hat     :\", mu_hat,\"\\nsigsqr_hat :\", sigsqr_hat)       # Ausgabe    \n\nmu_hat     : 3.166667 \nsigsqr_hat : 13.78788\n\n\nBasierend auf diesen Schätzern und den vorliegenden \\(n = 12\\) Datenpunkten sind also \\[\\begin{equation}\n\\hat{\\mu} = 3.17 \\mbox{ und } \\hat{\\sigma}^{2} = 13.8\n\\end{equation}\\] sinnvolle Tipps für \\(\\mu\\) und \\(\\sigma^2\\). Um neben diesen Punktschätzern, die zwar sehr genau sind, mit einer Wahrscheinlichkeit von 0 aber den wahren, aber unbekannten Parametern, exakt entsprechen, werten wir zusätzlich die 95%-Konfidenzintervallschätzungen für \\(\\mu\\) und \\(\\sigma^2\\) aus. Folgender R Code bestimmt das 95%-Konfidenzintervall für den Erwartungswertparameter.\n\n# Konfidenzintervall für den Erwartungswertparameter\ndelta    = 0.95                                    # Konfidenzlevel\nn        = length(y)                               # Anzahl Datenpunkte\nt_delta  = qt((1+delta)/2,n-1)                     # \\psi^-1((\\delta+1)/2,n-1)\ny_bar    = mean(y)                                 # Stichprobenmittel\ns        = sd(y)                                   # Stichprobenstandardabweichung\nmu_hat   = y_bar                                   # Erwartungswertparameterschätzer\nkappa_u  = y_bar - (s/sqrt(n))*t_delta             # untere Konfidenzintervallgrenze\nkappa_o  = y_bar + (s/sqrt(n))*t_delta             # obere  Konfidenzintervallgrenze\ncat(\"kappa_u:\", kappa_u, \"\\nkappa_o:\", kappa_o)    # Ausgabe  \n\nkappa_u: 0.8074098 \nkappa_o: 5.525923\n\n\nDas 0.95-Konfidenzintervall für den Erwartungswertparameter ist also \\[\\begin{equation}\n\\kappa(y) = [0.80,5.52].\n\\end{equation}\\] Im langfristigen Mittel überdeckt ein auf diese Weise berechnetes Konfidenzintervall den wahren, aber unbekannten, Erwartungswertparameter in 95 von 100 Fällen. In diesem Sinne liegt der wahre, aber unbekannte, Therapieeffekt also sehr sicher in einem Intervall zwischen 0.80 und 5.52 BDI-II Score Pre-Post-Differenzen.\nFolgender R Code bestimmt das 95%-Konfidenzintervall für den Varianzparameter.\n\n# Konfidenzintervall für den Varianzwertparameter\ndelta       =  0.95                                # Konfidenzlevel\nn           = length(y)                            # Anzahl Datenpunkte\nu_delta_u   = qchisq((1-delta)/2, n - 1)           # \\Xi^2((1-\\delta)/2; n - 1)\nu_delta_o   = qchisq((1+delta)/2, n - 1)           # \\Xi^2((1+\\delta)/2; n - 1)\ns2          = var(y)                               # Stichprobenstandardabweichung\nsigsqr_hat  = s2                                   # Varianzparameterschätzer\nkappa_u     = (n-1)*s2/u_delta_o                   # untere Konfidenzintervallgrenze\nkappa_o        = (n-1)*s2/u_delta_u                # obere Konfidenzintervallgrenze\ncat(\"kappa_u:\", kappa_u, \"\\nkappa_o:\", kappa_o)    # Ausgabe  \n\nkappa_u: 6.919084 \nkappa_o: 39.74756\n\n\nDas 0.95-Konfidenzintervall für den Varianzparameter ist also \\[\\begin{equation}\n\\kappa(y) = [6.91,39.74].\n\\end{equation}\\] Im langfristigen Mittel überdeckt ein auf diese Weise berechnetes Konfidenzintervall den wahren, aber unbekannten, Varianzparameter in 95 von 100 Fällen. In diesem Sinne liegt die wahre, aber unbekannte, Therapieeffektstreuung also sehr sicher in einem Intervall zwischen 6.91 und 39.74 quadriertern BDI-II Score Pre-Post-Differenzen.",
    "crumbs": [
      "Frequentistische Inferenz",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Konfidenzintervalle</span>"
    ]
  },
  {
    "objectID": "303-Konfidenzintervalle.html#literaturhinweise",
    "href": "303-Konfidenzintervalle.html#literaturhinweise",
    "title": "23  Konfidenzintervalle",
    "section": "23.4 Literaturhinweise",
    "text": "23.4 Literaturhinweise\nDie in diesem Kapitel vorgestellten Ergebnisse gehen in ganz wesentlicher Weise auf Neyman (1937) zurück.",
    "crumbs": [
      "Frequentistische Inferenz",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Konfidenzintervalle</span>"
    ]
  },
  {
    "objectID": "303-Konfidenzintervalle.html#selbstkontrollfragen",
    "href": "303-Konfidenzintervalle.html#selbstkontrollfragen",
    "title": "23  Konfidenzintervalle",
    "section": "23.5 Selbstkontrollfragen",
    "text": "23.5 Selbstkontrollfragen\n\nGeben Sie die Definition des Begriffs eines \\(\\delta\\)-Konfidenzintervalls wieder.\nErläutern Sie die zwei Interpretationen eines \\(\\delta\\)-Konfidenzintervalls.\nErläutern Sie die typischen Schritte zur Konstruktion eines \\(\\delta\\)-Konfidenzintervalls.\nGeben Sie das Theorem zum \\(\\delta\\)-Konfidenzintervall für den Erwartungswert der Normalverteilung wieder.\nGeben Sie das Theorem zum \\(\\delta\\)-Konfidenzintervall für den Varianzparameter der Normalverteilung wieder.\n\n\n\n\n\nCasella, G., & Berger, R. (2012). Statistical Inference. Duxbury.\n\n\nNeyman, J. (1937). Outline of a Theory of Statistical Estimation Based on the Classical Theory of Probability. Statistical Stimation.",
    "crumbs": [
      "Frequentistische Inferenz",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Konfidenzintervalle</span>"
    ]
  },
  {
    "objectID": "304-Hypothesentests.html",
    "href": "304-Hypothesentests.html",
    "title": "24  Hypothesentests",
    "section": "",
    "text": "24.1 Testhypothesen und Tests\nIm Kontext von Frequentistischen Hypothesentests wird der Begriff des Frequentistischen Inferenzmodells (vgl. Definition 21.1) zunächt durch die sogenannten Testhypothesen zu einem Testszenario erweitert. Wir nutzen folgende Definition.\nJe nach Beschaffenheit von \\(\\Theta_0\\) und \\(\\Theta_1\\) unterscheidet man einerseits einfache und zusammengesetzte und andererseits einseitige und zweiseitige Testhypothesen.\nMan beachte, dass da nach Annahme der wahre, aber unbekannte, Parameter \\(\\theta\\) die Verteilung \\(\\mathbb{P}_\\theta\\) der Stichprobe festlegt, eine einfache Testhypothese der Festlegung der Verteilung der Stichprobe auf genau eine Verteilung entspricht. Eine zusammengesetzte entspricht dagegen einer Menge möglicher Verteilungen der Stichprobe. Ein Beispiel für eine einfache Testhypothese in einem Testszenario mit Parameterraum \\(\\Theta := \\mathbb{R}\\) ist \\[\\begin{equation}\n\\Theta_0 := \\{0\\},\n\\end{equation}\\] die entsprechend zusammengesetzte Alternativhypothese ist dann gegeben durch \\[\\begin{equation}\n\\Theta_1 = \\mathbb{R} \\setminus \\{0\\}.\n\\end{equation}\\] Die Nullhypothese, also die Aussage “\\(\\theta \\in \\Theta_0\\)” entspricht dann der Aussage “\\(\\theta = 0\\)”, da \\(\\Theta_0\\) nur eben dieses eine Element enthält.\nIst wie in diesem Beispiel der Parameterraum eindimensional, so unterscheidet man weiterhin einseitige und zweiseitige Null- und Alternativhypothesen.\nVor dem Hintergrund eines Testszenarios definieren wir nun den Begriff des Hypothesentests, den wir kurz einfach als Test bezeichnen wollen.\nDie Formalisierung des Testbegriffs ist nicht trivial, da Tests, wie Schätzer und Konfidenzintervalle, Funktionen von Zufallsvariablen, nämlich gerade den Stichprobenvariablen sind. Eigentlich sind Tests damit auf Zufallsvektorräumen definiert. Der Einfachheit halber betrachten wir in Definition 24.4 eine konkrete Realisierung \\(y \\in \\mathcal{Y}\\) der Stichprobe \\(\\upsilon\\), die durch \\(\\phi\\) in die Menge \\(\\{0,1\\}\\) abgebildet wird. Der Funktionswert \\(\\phi(y)\\) von \\(\\phi\\) ist vor diesem Hintergrund also eine Realisierung der Zufallsvariable \\(\\phi(\\upsilon)\\).\nIn der Anwendung ist man oft an Tests interessiert, die eine bestimmte Struktur haben, wir formalisieren diese unter dem Begriff der Standardtests.\nWie oben angemerkt gibt es auch bei Definition 24.5 zu beachten, dass die Teststatistik eigentlich eine Funktion der Stichprobenvariablen, also von Zufallsvariablen ist, die wir hier als Funktion der Werte dieser Zufallsvariablen in \\(\\mathcal{Y}\\) definiert haben. Ebenso gibt es zu beachten, dass die Entscheidungsregel eine Funktion der somit zufälligen Teststatistik ist, die wir hier gleichfalls als Funktion der Werte dieser Zufallsvariable mit Ergebnisraum \\(\\Gamma\\) geschrieben haben. Sowohl Teststatistik und Entscheidungsregel sind in einem Testszenario also Zufallsvariablen. Entsprechend ist, wenn \\(y\\) eine Realisierung der Stichprobe \\(\\upsilon\\) ist, \\(\\gamma(y) \\in \\Gamma\\) eine Realisierung von \\(\\gamma(\\upsilon)\\) und \\((\\delta \\circ \\gamma)(y)\\) eine Realisierung von \\((\\delta \\circ \\gamma)(\\upsilon)\\).\nDie verteilungstheoretischen Eigenschaften eines Tests ergeben sich aus den ihnen zugrundeliegenden verteilungstheoretischen Eigenschaften des entsprechenden Frequentistischen Inferenzmodells und damit natürlich insbesondere der Verteilung der Stichprobenvariablen. Eine wichtige Brücke zwischen diesen beiden Ebenen der Verteilung der Stichprobenvariablen auf der einen Seite und der Verteilung der Testergebnisse auf der anderen Seite bilden die Begriffe des kritischen Bereichs und des Ablehnungsbereichs eines Tests.\nMan beachte, dass vor dem Hintergrund von Definition 24.6 die zufälligen Ereignisse \\(\\{\\upsilon\\in K\\}\\) und \\(\\{\\phi(\\upsilon) = 1\\}\\), also dass die Stichprobe einen Wert im kritischen Bereichs des Tests annimmt bzw. dass der Test den Wert 1 annimmt, äquivalent sind und damit insbesondere auch die gleiche Wahrscheinlichkeit haben. Fragt man also nach der Wahrscheinlichkeit, dass ein Test den Wert 1 annimmt, also die Nullhypothese abgelehnt wird, so entspricht diese Wahrscheinlichkeit genau der Wahrscheinlichkeit, dass die Stichprobe einen Wert im kritischen Bereichs des Tests annimmt. Da die Verteilung der Stichprobe aber als bekannt vorausgesetzt ist, kann die Wahrscheinlichkeit für das Ablehnen der Nullhypothese darauf basierend bestimmt werden. Hat man insbesondere einen Standardtest vorliegen, so überträgt sich das Gesagte unmittelbar auch auf die zwischen Stichprobe und Test geschaltete Teststatistik. Dies führt auf die folgende Definition.\nWie zum Begriff des kritischen Bereichs angemerkt gilt auch hier, dass die Ereignisse \\(\\{\\phi(\\upsilon) = 1\\}\\) und \\(\\{\\gamma(\\upsilon) \\in A\\}\\) äquivalent sind und damit insbesondere auch die gleiche Wahrscheinlichkeit besitzen. Insgesamt gelten mit Definition 24.6 und Definition 24.7 für einen Standardtest also \\[\\begin{equation}\n\\{\\upsilon\\in K\\} \\Leftrightarrow \\{\\gamma(\\upsilon) \\in A\\} \\Leftrightarrow \\{\\phi(\\upsilon) = 1\\}\n\\end{equation}\\] und \\[\\begin{equation}\n\\mathbb{P}_{\\theta}\\left(\\{\\upsilon\\in K\\}\\right)\n=\n\\mathbb{P}_{\\theta}\\left(\\{\\gamma(\\upsilon) \\in A\\}\\right)\n=\n\\mathbb{P}_{\\theta}\\left(\\{\\phi(\\upsilon) = 1\\}\\right),\n\\end{equation}\\] wobei das Subskript \\(\\theta\\) bei der Verteilung der Teststatistik und des Tests andeuten soll, dass diese Verteilungen durch den Parameter der Stichprobenverteilung festgelegt sind.\nIn der Anwendung basiert die in Definition 24.5 allgemein angebene Form der Entscheidungsregel eines Standardtest meist darauf, dass eine beobachtete Teststatistik mit Ergebnisraum \\(\\Gamma := \\mathbb{R}\\) einen bestimmten sogenannten kritischen Wert \\(k\\in \\mathbb{R}\\) überschreitet oder unterschreitet. Dies führt auf die Konzepte der einseitigen und zweiseitigen kritischen Wert-basierte Tests.\nMit der Definition kritischer Wert-basierter Tests ist die praktische Durchführung eines Hypothesentests nun vorgezeichnet. Wie immer in der Frequentistischen Inferenz legt man vorliegenden Daten zunächst ein Frequentistisches Inferenzmodell zugrunde, nimmt also an, dass die vorliegenden Daten eine Realisierung einer Stichprobe sind. Basierend auf dieser Realisierung berechnet man eine Teststatistik und vergleicht diese abschließend mit einem kritischen Wert, um dann entweder die Nullhypothese nicht abzulehnen oder die Nullhypothese abzulehnen. Im folgenden Abschnitt wollen wir nun der Frage nachgehen, wie vor dem Hintergrund von Null- und Alternativhypothese dabei der kritische Wert eines kritischen Wert-basierten Tests so bestimmt werden kann, dass man im Sinne der Frequentistischen Wahrscheinlichkeit möglichst gute Testentscheidungen trifft.",
    "crumbs": [
      "Frequentistische Inferenz",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Hypothesentests</span>"
    ]
  },
  {
    "objectID": "304-Hypothesentests.html#sec-testhypothesen-und-tests",
    "href": "304-Hypothesentests.html#sec-testhypothesen-und-tests",
    "title": "24  Hypothesentests",
    "section": "",
    "text": "Definition 24.1 (Testhypothesen und Testszenario) Gegeben sei ein Frequentistisches Inferenzmodell mit Stichprobe \\(\\upsilon\\), Ergebnisraum \\(\\mathcal{Y}\\) und Parameterraum \\(\\Theta\\). Weiterhin sei \\(\\{\\Theta_0,\\Theta_1\\}\\) eine Partition des Parameterraums, so dass \\[\\begin{equation}\n\\Theta = \\Theta_0 \\cup \\Theta_1 \\mbox{ und } \\Theta_0 \\cap \\Theta_1 = \\emptyset.\n\\end{equation}\\] Dann ist eine Testhypothese eine Aussage über den wahren, aber unbekannten, Parameterwert \\(\\theta\\) in Hinblick auf die Untermengen \\(\\Theta_0\\) und \\(\\Theta_1\\) des Parameterraums. Speziell werden die Aussagen\n\n\\(\\theta \\in \\Theta_0\\) als Nullhypothese und\n\\(\\theta \\in \\Theta_1\\) als Alternativhypothese\n\nbezeichnet. Der Einfachheit halber bezeichnet man auch \\(\\Theta_0\\) und \\(\\Theta_1\\) direkt als Nullhypothese und Alternativhypothese, respektive. Die Einheit aus Frequentistischem Inferenzmodell und Testhypothesen wird als Testszenario bezeichnet.\n\n\n\nDefinition 24.2 (Einfache und zusammengesetzte Testhypothesen) Für die Testhypothesen \\(\\Theta_i\\) mit \\(i = 0,1\\) gilt:\n\nEnthält \\(\\Theta_i\\) nur ein einziges Element, so heißt \\(\\Theta_i\\) einfach.\nEnthält \\(\\Theta_i\\) mehr als ein Element, so heißt \\(\\Theta_i\\) zusammengesetzt.\n\n\n\n\n\nDefinition 24.3 (Einseitige und zweiseitige Testhypothesen) Gegeben sei ein Testszenario mit eindimensionalem Parameteraum \\(\\Theta := \\mathbb{R}\\) und es sei \\(\\theta_0 \\in \\Theta\\). Dann werden zusammengesetzte Nullhypothesen der Form \\(\\Theta_0 := ]-\\infty,\\theta_0]\\) oder \\(\\Theta_0 := [\\theta_0,\\infty[\\) einseitige Nullhypothesen genannt und auch in der Form \\(H_0:\\theta \\le \\theta_0\\) bzw. \\(H_0 : \\theta \\ge \\theta_0\\) geschrieben. Die entsprechenden Alternativhypothesen haben dabei die Form \\(\\Theta_1 := ]\\theta_0,\\infty[\\) bzw. \\(\\Theta_1:= ]-\\infty, \\theta_0[\\), auch geschrieben als \\(H_1:\\theta&gt;\\theta_0\\) bzw. \\(H_1:\\theta &lt; \\theta_0\\). Bei einer einfachen Nullhypothese der Form \\(\\Theta_0 := \\{\\theta_0\\}\\), auch geschrieben als \\(H_0:\\theta = \\theta_0\\), wird die Alternativhypothese \\(\\Theta_1 := \\Theta \\setminus \\{\\theta_0\\}\\), auch geschrieben als \\(H_1:\\theta \\neq \\theta_0\\), zweiseitige Alternativhypothese genannt.\n\n\n\nDefinition 24.4 (Test) Gegeben sei ein Testszenario. Dann ist ein Test eine Abbildung \\(\\phi\\) aus dem Ergebnisraum der Stichprobe \\(\\mathcal{Y}\\) in die Menge \\(\\{0,1\\}\\), also \\[\\begin{equation}\n\\phi : \\mathcal{Y} \\to \\{0,1\\}, y \\mapsto \\phi(y),\n\\end{equation}\\] wobei\n\n\\(\\phi(y) = 0\\) den Vorgang des Nichtablehnens der Nullhypothese und\n\\(\\phi(y) = 1\\) den Vorgang des Ablehnens der Nullhypothese\n\nrepräsentieren.\n\n\n\n\nDefinition 24.5 (Standardtest) Gegeben sei ein Testszenario. Dann ist ein Standardtest \\(\\phi\\) definiert als die Verkettung einer Teststatistik \\[\\begin{equation}\n\\gamma : \\mathcal{Y} \\to \\Gamma\n\\end{equation}\\] und einer Entscheidungsregel \\[\\begin{equation}\n\\delta : \\Gamma \\to \\{0,1\\}\n\\end{equation}\\] kann also geschrieben werden als \\[\\begin{equation}\n\\phi := \\delta \\circ \\gamma : \\mathcal{Y} \\to \\{0,1\\}.\n\\end{equation}\\]\n\n\n\n\nDefinition 24.6 (Kritischer Bereich eines Tests) Gegeben sei ein Testszenario und ein Test \\(\\phi\\). Dann heißt die Untermenge \\(K\\) des Ergebnisraums \\(\\mathcal{Y}\\) der Stichprobe \\(\\upsilon\\), für die der Test den Wert 1 annimmt, kritischer Bereich des Tests, formal \\[\\begin{equation}\nK := \\{y \\in \\mathcal{Y} |\\phi(y) = 1 \\} \\subset \\mathcal{Y}.\n\\end{equation}\\]\n\n\n\nDefinition 24.7 (Ablehnungsbereich eines Standardtests) Gegeben sei ein Testszenario und ein Standardtest \\(\\phi\\) mit Teststatistik \\(\\gamma\\). Die Untermenge \\(A\\) des Ergebnisraums \\(\\Gamma\\) der Teststatistik, für die der Test den Wert 1 annimmt, Ablehnungsbereich des Tests, formal \\[\\begin{equation}\nA := \\{\\gamma(y) \\in \\Gamma |\\phi(y) = 1 \\} \\subset \\Gamma.\n\\end{equation}\\]\n\n\n\n\nDefinition 24.8 (Kritischer Wert-basierte Tests) Ein kritischer Wert-basierter Test ist ein Standardtest, bei dem die Entscheidungsregel \\(\\delta\\) von einem kritischen Wert \\(k\\) der Teststatistik mit Ergebnisraum \\(\\mathbb{R}\\) abhängt. Speziell ist\n\nein einseitiger kritischer Wert-basierter Test von der Form \\[\\begin{equation}\n\\phi : \\mathcal{Y} \\to \\{0,1\\}, y \\mapsto \\phi(y) := 1_{\\{\\gamma(y) \\ge k\\}} =\n\\begin{cases}\n1 & \\gamma(y) \\ge k \\\\\n0 & \\gamma(y) &lt; k\n\\end{cases},\n\\end{equation}\\]\nein zweiseitiger kritischer Wert-basierter Test von der Form \\[\\begin{equation}\n\\phi : \\mathcal{Y} \\to \\{0,1\\}, y \\mapsto \\phi(y) := 1_{\\{|\\gamma(y)| \\ge k\\}} =\n\\begin{cases}\n1 & |\\gamma(y)| \\ge k \\\\\n0 & |\\gamma(y)| &lt; k\n\\end{cases}.\n\\end{equation}\\]",
    "crumbs": [
      "Frequentistische Inferenz",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Hypothesentests</span>"
    ]
  },
  {
    "objectID": "304-Hypothesentests.html#sec-testguetekriterien-und-testkonstruktion",
    "href": "304-Hypothesentests.html#sec-testguetekriterien-und-testkonstruktion",
    "title": "24  Hypothesentests",
    "section": "24.2 Testgütekriterien und Testkonstruktion",
    "text": "24.2 Testgütekriterien und Testkonstruktion\nDie Tatsache, dass in einem Testszenario der wahre, aber unbekannte, Parameter im Bereich der Nullhypothese oder der Alternativhypothese liegen kann und man gleichzeitig basierend auf dem Wert des Tests die Nullhypothese entweder ablehnen oder nicht ablehnen kann, impliziert, dass eine Testentscheidung richtig oder falsch sein kann. Untenstehende Definition soll dahingehend zunächst begriffliche Klarheit schaffen.\n\nDefinition 24.9 (Richtige Testentscheidungen und Testfehler) Gegeben seien ein Testszenario und ein Test. Dann gibt es mit dem Nichtablehnen der Nullhypothese \\(\\phi(y) = 0\\), wenn die Nullhypothese \\(\\theta \\in \\Theta_0\\) zutrifft und dem Ablehnen der Nullhypothese \\(\\phi(y) = 1\\), wenn die Alternativhypothese \\(\\theta \\in \\Theta_1\\) zutrifft zwei Formen der richtigen Testentscheidung. Ebenso gibt es zwei Arten von Testfehlern: Das Ablehnen der Nullhypothese \\(\\phi(y) = 1\\), wenn die Nullhypothese \\(\\theta \\in \\Theta_0\\) zutrifft, heißt Typ I Fehler und das Nichtablehen der Nullhypothese, wenn die Alternativhypothese \\(\\theta \\in \\Theta_1\\) zutrifft, heißt Typ II Fehler.\n\n\n\n\n\n\n\nAbbildung 24.1: Richtige Testentscheidungen und Typ I und Typ II Fehler\n\n\n\nAbbildung 24.1 gibt eine Übersicht zu den möglichen richtigen Testentscheidungen und Testfehlern bei Durchführung eines Tests. Natürlich möchte man generell meist eine richtige Testentscheidung treffen. Das entscheidene Werkzeug, um vor dem Frequentistischen Hintergrund des Testszenarios gute Tests zu konstruieren, ist die sogenannte Testgütefunktion.\n\nDefinition 24.10 (Testgütefunktion) Gegeben sei ein Testszenario und ein Test \\(\\phi\\). Dann ist die Testgütefunktion von \\(\\phi\\) definiert als \\[\\begin{equation}\nq_{\\phi} : \\Theta \\to [0,1], \\theta \\mapsto q_{\\phi}(\\theta) := \\mathbb{P}_\\theta(\\phi(\\upsilon) = 1).\n\\end{equation}\\] Für \\(\\theta \\in \\Theta_1\\) heißt \\(q_\\phi\\) auch Trennschärfefunktion oder Powerfunktion.\n\nMan beachte, dass \\(\\mathbb{P}_\\theta\\) in Definition 24.10 die Verteilung der Zufallsvariable \\(\\phi(\\upsilon)\\) unter der Annahme, dass die Verteilung von \\(\\upsilon\\) durch \\(\\theta\\) festgelegt ist, bezeichnen soll. Für jedes \\(\\theta \\in \\Theta\\) liefert \\(q_\\phi\\) also die die Wahrscheinlichkeit dafür, dass die Nullhypothese durch den Test \\(\\phi\\) abgelehnt wird. Für diese Wahrscheinlichkeiten gelten insbesondere mit den Begriffen des kritischen Bereichs (vgl. Definition 24.6) und des Ablehnungsbereichs (vgl. Definition 24.7) wie bereits gesehen \\[\\begin{equation}\n\\mathbb{P}_\\theta(\\phi(\\upsilon) = 1)\n= \\mathbb{P}_\\theta(\\gamma \\in A)\n= \\mathbb{P}_\\theta (\\upsilon \\in K).\n\\end{equation}\\] Die Testgütefunktion ist spezifisch für einen gegebenen Test. Ändert sich der Test, zum Beispiel, weil bei einem kritischen Wert-basierten Test ein anderer kritischer Wert gewählt wird, ändern sich obige Wahrscheinlichkeiten und damit die Testgütefunktion.\nMithilfe der Testgütefunktion folgt die Testkonstruktion dann folgenden Überlegungen. Im Idealfall hätte man einen Test \\(\\phi\\) mit \\[\\begin{equation}\nq_\\phi(\\theta) = \\mathbb{P}_\\theta(\\phi(\\upsilon) = 1) = 0 \\mbox{ für } \\theta \\in \\Theta_0 \\mbox{ und }\nq_\\phi(\\theta) = \\mathbb{P}_\\theta(\\phi(\\upsilon) = 1) = 1 \\mbox{ für } \\theta \\in \\Theta_1.\n\\end{equation}\\] Die Testentscheidung eines solchen Tests wäre mit Wahrscheinlichkeit 1 richtig, da ein solcher Test die Nullhypothese mit Wahrscheinlichkeit 0 ablehnt, wenn sie zutrifft, und die Nullhypothese mit Wahrscheinlichkeit 1 ablehnt, wenn sie nicht zutrifft. Allgemeiner sind natürlich kleine Werte von \\(q_\\phi\\) für \\(\\theta \\in \\Theta_0\\), also kleine Wahrscheinlichkeiten dafür, die Nullhypothese abzulehnen, wenn sie zutrifft, und große Werte von \\(q_\\phi\\) für \\(\\theta \\in \\Theta_1\\), also große Wahrscheinlichkeiten dafür, die Nullhypothese abzulehnen, wenn sie nicht zutrifft, zur Testfehlerminimierung günstig. Allerdings bestehen im Allgemeinen Abhängigkeiten zwischen den Werten der Testgütefunktion für \\(\\theta \\in \\Theta_0\\) und \\(\\theta \\in \\Theta_1\\), wie folgende Beispiele illustrieren sollen.\nBeispiel (A) Es sei \\(\\phi_a\\) ein Test definiert durch \\[\\begin{equation}\n\\phi_a : \\mathcal{Y} \\to \\{0,1\\},  y \\mapsto \\phi_a(y) := 0.\n\\end{equation}\\] \\(\\phi_a\\) sei also ein Test, der die Nullhypothese unabhängig von den beobachteten Daten nie ablehnt. Für \\(\\phi_a\\) gilt dann \\[\\begin{equation}\nq_{\\phi_a}(\\theta) = \\mathbb{P}_\\theta(\\phi(\\upsilon) = 1) = 0 \\mbox{ für } \\theta \\in \\Theta_0.\n\\end{equation}\\] Allerdings gilt für \\(\\phi_a\\) dann auch automatisch \\[\\begin{equation}\nq_{\\phi_a}(\\theta) = \\mathbb{P}_\\theta(\\phi(\\upsilon) = 1) = 0 \\mbox{ für } \\theta \\in \\Theta_1.\n\\end{equation}\\] \\(\\phi_a\\) hat also eine minimale Sensitivität dafür, die Tatsache, dass die Alternativhypothese zutrifft, zu detektieren.\nBeispiel (B) Andersherum sei \\(\\phi_b\\) ein Test definiert durch \\[\\begin{equation}\n\\phi_b : \\mathcal{Y} \\to \\{0,1\\},  y \\mapsto \\phi_b(y) := 1.\n\\end{equation}\\] \\(\\phi_b\\) sei also ein Test, der die Nullhypothese, unabhängig von den beobachteten Daten immer ablehnt. Für \\(\\phi_b\\) gilt dann \\[\\begin{equation}\nq_{\\phi_b}(\\theta) = \\mathbb{P}_\\theta(\\phi(\\upsilon) = 1) = 1 \\mbox{ für } \\theta \\in \\Theta_1.\n\\end{equation}\\] \\(\\phi_b\\) ist also maximal sensitiv für das Zutreffen der Alternativhypothese. Allerdings gilt für \\(\\phi_b\\) dann auch automatisch \\[\\begin{equation}\nq_{\\phi_b}(\\theta) = \\mathbb{P}_\\theta(\\phi(\\upsilon) = 1) = 0 \\mbox{ für } \\theta \\in \\Theta_0,\n\\end{equation}\\] und \\(\\phi_b\\) resultiert auch immer in der Ablehnung der Nullhypothese, wenn diese zutrifft und generiert in diesem Sinne viele falsch positive Resultate.\nVor dem Hintergrund dieser beiden Extremszenarien muss es also das Ziel der Testkonstruktion sein, eine angemessene Balance zwischen kleinen Werten der Testgütefunktion bei Zutreffen der Nullhypothese und großen Werten der Testgütefunktion bei Zutreffen der Alternativhypothese zu finden. Die populärste Methode, dies zu erreichen ist es, in einem ersten Schritt einen kleinen Wert \\(\\alpha_0 \\in [0,1]\\) zu wählen und sicherzustellen, dass \\[\\begin{equation}\\label{eq:significance}\nq_\\phi(\\theta) \\le \\alpha_0 \\mbox{ für alle } \\theta \\in \\Theta_0,\n\\end{equation}\\] dass also die Wahrscheinlichkeit für das Ablehnen der Nullhypothese, wenn diese zutrifft, also die Wahrscheinlichkeit für einen Typ I Fehler, höchstens \\(\\alpha_0\\) beträgt. Konventionelle Werte für ein solches \\(\\alpha_0\\) sind zum Beispiel \\(\\alpha_0 := 0.001\\) und \\(\\alpha_0 := 0.05\\). Unter allen Tests (und, bei Optimierung von Stichprobengrößen, Frequentistischen Inferenzmodellen), die die Ungleichung \\(\\eqref{eq:significance}\\) erfüllen, sucht man dann in einem zweiten Schritt einen Test, für den \\(q_\\phi(\\theta)\\) für \\(\\theta \\in \\Theta_1\\) so groß wie möglich ist. Dieses zweischrittige Vorgehen ist nicht alternativlos, man könnte ja beispielsweise auch eine lineare Kombinationen von Typ I und Typ II Fehlern simultan minimieren. Allerdings ist das skizzierte zweischrittige Vorgehen das in der Anwendung populärste, so dass wir uns in der Folge darauf beschränken wollen. Ungleichung \\(\\eqref{eq:significance}\\) motiviert dann zunächst die Definition der Begriffe des Level-\\(\\alpha_0\\)-Tests, des Signifikanzlevels \\(\\alpha_0\\) und des Testumfangs \\(\\alpha\\).\n\nDefinition 24.11 (Level-\\(\\alpha_0\\)-Test, Signifikanzlevel \\(\\alpha_0\\) und Testumfang \\(\\alpha\\)) Gegeben seien ein Testszenario, ein Test \\(\\phi\\), seine Testgütefunktion \\(q_\\phi\\) und ein \\(\\alpha_0 \\in [0,1]\\). \\(\\phi\\) heißt ein Level-\\(\\alpha_0\\)-Test, wenn gilt, dass \\[\\begin{equation}\nq_\\phi(\\theta) \\le \\alpha_0 \\mbox{ für alle } \\theta \\in \\Theta_0.\n\\end{equation}\\] Wenn \\(\\phi\\) ein Level-\\(\\alpha_0\\)-Test ist, nennt man den Wert \\(\\alpha_0\\) auch das Signifikanzlevel des Tests. Weiterhin heißt die Zahl \\[\\begin{equation}\n\\alpha := \\max_{\\theta \\in \\Theta_0} q_\\phi(\\theta) \\in [0,1]\n\\end{equation}\\] der Testumfang von \\(\\phi\\).\n\nNach Definition 24.11 ist der Testumfang \\(\\alpha\\) die maximale Wahrscheinlichkeit für einen Typ I Fehler und ein Test ist dann, und nur dann, ein Level-\\(\\alpha_0\\)-Test, wenn diese maximale Wahrscheinlichkeit kleiner oder gleich dem Signifikanzlevel \\(\\alpha_0\\) ist. Es ist dabei für die Anwendung wichtig, sich die feinen begrifflichen Unterschiede zwischen der Wahrscheinlichkeit eines Typ I Fehlers, dem Testumfang und dem Signifikanzlevels eines Tests zu verdeutlichen. Vor dem Hintergrund des Unterschiedes von einfachen und zusammengesetzten Nullhypothesen (vgl. Definition 24.2) muss man zunächst die Begriffe der Typ I Fehler Wahrscheinlichkeit und des Testumfangs differenzieren. Bei einer einfachen Nullhypothese \\(\\Theta_0\\) ist der Testumfang immer gleich der Wahrscheinlichkeit eines Typ I Fehlers, da gilt dass \\[\\begin{equation}\n\\alpha\n:= \\max_{\\theta \\in \\Theta_0} q_\\phi(\\theta)\n=  \\max_{\\theta \\in \\{\\theta_0\\}} q_\\phi(\\theta)\n= q_\\phi(\\theta_0)\n= \\mathbb{P}_{\\theta_0}(\\phi = 1).\n\\end{equation}\\] Bei einer zusammengesetzten Nullhypothese \\(\\Theta_0\\) gibt es je nach Wert von \\(\\theta \\in \\Theta_0\\) verschiedene Wahrscheinlichkeiten für einen Typ I Fehler. Die größte dieser Wahrscheinlichkeiten ist der Testumfang \\[\\begin{equation}\n\\alpha\n:= \\max_{\\theta \\in \\Theta_0} q_\\phi(\\theta)\n= \\max_{\\theta \\in \\Theta_0} \\mathbb{P}_{\\theta}(\\phi = 1).\n\\end{equation}\\] Ebenso klar sollte man die Begriffe des Signifikanzlevels \\(\\alpha_0\\) und des Testumfangs \\(\\alpha\\) voneinander abgrenzen. Ein Signifikanzlevel ist eine frei gewählte obere Grenze für die maximale Wahrscheinlichkeit eines Typ I Fehlers. Die tatsächliche maximale Wahrscheinlichkeit für einen Typ I Fehler, kann mit dieser identisch sein, wie in den meisten Fällen der Kapitel 24.3 diskutierten Beispiele, muss es aber nicht, wie zum Beispiel in multiplen Testszenarien mit nicht unabhängigen Stichprobenvariablen. Man nennt dementsprechend einen Test exakt, wenn sein Testumfang mit seinem Signifikanzlevel identisch ist, wenn also \\[\\begin{equation}\n\\alpha = \\alpha_0.\n\\end{equation}\\] Ein Test, für den der Testumfang kleiner als sein Signifikanzlevel ist, für den also gilt \\[\\begin{equation}\n\\alpha &lt; \\alpha_0\n\\end{equation}\\] wird konservativ genannt. Ein Test schließlich, dessen Testumfang größer als sein Signifikanzlevel ist, \\[\\begin{equation}\n\\alpha &gt; \\alpha_0\n\\end{equation}\\] und der damit natürlich kein Level-\\(\\alpha_0\\)-Test sein kann, wird liberal genannt.\n\np-Wert\nEin definierendes Charakterstikum eines Tests ist es wie gesehen, dass die Wertemenge eines Tests binär ist, Resultat eines Tests ist entweder \\(\\phi = 0\\), die Nullhypothese wird nicht abgelehnt, oder \\(\\phi = 1\\), die Nullhypothese wird abgelehnt. Als finales Resultat einer Datenanalyse wird dabei die einem Datensatz inhärente Information sehr stark komprimiert. Insbesondere supprimiert das alleinige Berichten des Testergbnisses interessante Information über das Signal-zu-Rauschen-Verhältnis des betrachteten Datensatzes. So ist es ja beispielsweise möglich, dass die Nullhypothese im Kontext eines kritischen Wert-basierten abgelehnt wird, weil die Teststatistik den kritischen Wert nur um wenige Nachkommastellen übertroffen hat oder aber, dass die Testsstatistik ein Vielfaches des kritischen Werts angenommen hat. In beiden Fällen wäre das Testergebnis mit \\(\\phi = 1\\) identisch. Neben der reinen Testumfangkontrolle eines Tests und des Berichtens des binären Testergebnisses hat es sich deshalb für kritische Wert-basierte Tests eingebürgert, basierend auf dem beobachteten Wert der Teststatistik auch alle Werte des Signifikanzlevels \\(\\alpha_0\\), für die ein Level-\\(\\alpha_0\\)-Test das Ergebnis \\(\\phi = 1\\) hätte, für die die Nullhypothese also abgelehnt werden würden, zu betrachten. Diese Überlegung führt auf folgende allgemeine Definition des sogenannten p-Werts, wobei p für probability steht.\n\nDefinition 24.12 (p-Wert) \\(\\phi\\) sei ein Test. Dann ist der das kleinste Signifikanzlevel \\(\\alpha_0\\), bei dem die Nullhypothese basierend auf einem vorliegendem Wert der Teststatistik abgelehnt werden würde.\n\nInsbesondere in einfachen Anwendungsbeispielen, wie dem in Kapitel 24.3.1 betrachteten Einstichproben-T-Test-Szenario spiegeln p-Werte dann die Antwort auf die intuitive Frage, wie wahrscheinlich es im Frequentistischen Sinne wäre, den beobachteten oder einen extremeren Wert der Teststatistik unter der Annahme eines Nullmodels zu observieren. Dabei ist in vielen Bereichen der Grundlagenwissenschaft das Berichten von p-Werten sehr populär, aber auch umstritten (vgl. Wasserstein et al. (2019)). Dabei gilt es insbesondere, p-Werte nicht zu überinterpretieren. Basierend auf dem Gesagten gibt es keinen Grund dies anzunehmen, trotzdem weisen wir vorsorglich daraufhin, dass p-Werte nicht die Wahrscheinlichkeit dafür quantifizieren, dass die Nullhypothese wahr ist, man aufgrund eines p-Wertes kleiner als \\(0.05\\) nicht darauf schließen kann, dass die Alternativhypothese zutrifft, und man aufgrund eines p-Wertes von größer als \\(0.05\\) nicht darauf schließen kann, dass die Nullhypothese zutrifft. Ebenso wie der Wert einer Teststatistik und eines Tests quantifizieren p-Werte lediglich das in einem vorliegenden Datensatz beobachtete Signal-zu-Rauschen-Verhältnis - nicht weniger, aber auch nicht mehr.\n\n\nAnmerkungen zur Wahl von Null- und Alternativhypothese\nWir wollen diesen Abschnitt mit einigen Anmerkungen zur Durchführung von Hypothesentests in der Wissenschaft beschließen. Vor dem Hintergrund der skizzierten Theorie der Hypothesentests stellt sich zunächst die Frage, wie man in einem gegebenen Anwendungskontext die Zuordnung von Null- und Alternativhypothese zu den Gegenständen des wissenschaftlichen Interesses, also zweier wissenschafltichen Hypothesen vornimmt. Möchte man zum Beispiel einen Test durchführen, um im Sinne der Frequentistischen Inferenz zu entscheiden, ob ein bestimmtes Psychotherapieverfahren in einer klinischen Studie wirksam war oder nicht, so stellt sich die Frage, ob man dabei die Abwesenheit eines Therapieeffekts dabei als die Null- oder als die Alternativhypothese verstehen sollte. Dazu sei angemerkt, dass das oben beschriebene zweischrittige Vorgehen zur Testkonstruktion, in dem zunächst durch die Wahl eines Signifikanzlevels der Testumfang begrenzt wird und erst in einem zweiten Schritt dafür gesorgt wird, dass die Wahrscheinlichkeit, die Nullhypothese abzulehnen, wenn die Alternativhypothese zutrifft, möglichst groß ist, eine deutliche Asymmetrie in der Behandlung von Null- und Alternativhypothese impliziert: Man wichtet mit diesem Vorgehen Typ I Fehler als schwerwiegender als Typ II Fehler. Dies wiederum impliziert eine mögliche Strategie zur Festlegung von Null- und Alternativhypothese: Die Nullhypothese ist die wissenschaftliche Hypothese, hinsichtlich deren assoziierter Testentscheidung man eher keinen Fehler machen möchte bzw. deren Fehlerwahrscheinlichkeit man primär kontrollieren möchte. In der Wissenschaft ist es ein gebräuchlicher Standard, die falsche Konfirmation der von einem selbst favorisierten Theorie (also zum Beispiel die falsche Konfirmation, dass ein selbstentwickeltes Psychotherapieverfahren besser wirkt als ein anderes) als einen schwerwiegenderen Fehler als die falsche Ablehnung der eigenen Theorie zu werten. Damit sollte die falsche Konfirmation der eigenen Theorie ein Typ I Fehler, das falsche Ablehnen der eigenen Theorie ein Typ II Fehler sein. Damit nun die falsche Konfirmation der eigenen Theorie einen Typ I Fehler, also das Ablehnen der Nullhypothese bei Zutreffen der Nullhypothese, darstellt, muss die eigene Theorie als Alternativhypothese aufgestellt werden, die Alternativhypothese fälschlichweise abzulehnen wird damit ein Typ II Fehler. Intuitiv ergibt sich also folgende Zuordnung:\n\n\nAnmerkungen zu Hypothesentests in Entscheidungskontexten und Grundlagenwissenschaft\nZum zweiten stellt sich die Frage, ob man zur Evaluation wissenschaftlicher Hypothesen überhaupt einen Hypothesentest durchführen sollte. Oberflächliche betrachtet liefern Hypothesentests zunächst einmal einfache binäre Aussagen der Form “Die Hypothese ist gegeben die Evidenz abzulehnen oder zu akzeptieren”. Solche Aussagen sind in einem konkreten Entscheidungskontext hilfreich, wenn tatsächlich eine Entscheidung getroffen werden muss. Allerdings sei dazu angemerkt, dass wie gesehen, Frequentistische Hypothesentests ohne explizite Entscheidungsnutzenfunktion formuliert sind und potentielle Entscheidungskosten damit nicht explizit in die Entscheidungswahl einbezogen werden. Speziell für diesen Zweck gibt es eine Reihe sehr zugänglicher Theorien, die es erlauben, im langfristigen Mittel gute Entscheidungen unter Unsicherheit zu treffen, vgl. zum Beispiel Pratt et al. (1995), Puterman (2005), oder Kochenderfer et al. (2022).\nOrientiert man sich von praktisch relevanten Entscheidungskontexten in den Bereich der Grundlagenwissenschaften, deren Wesen es ja gerade ist, keine finalen Wahrheiten zu kennen, sondern sondern nur das Maß an Unsicherheit über den gerade vorherrschenden Theoriestand zu quantifizieren und zu kommunizieren, erscheint die Binarität der Hypothesentestentscheidung im besten Fall überflüssig, im schlimmsten Fall grob irreführend. Prinzipiell sollten Fragestellungen der Grundlagenwissenschaften deshalb gerade nicht als Entscheidungsprobleme formuliert werden. Trotz der weit verbreiteten Meinung, dass Bayesianische Herangehensweisen wie Positive Predictive Values oder Bayes Factors hier Vorteile bieten würden, ist dem nicht so, so lange die mit einer gewissen Modellpräferenz assoziierte Unsicherheit nicht klar mitkommuniziert wird. Nichtsdestotrotz bleibt das das Frequentistische Hypothesentesten auch in der grundlagenorientierten Wissenschaftsgemeinschaft weiterhin sehr populär, manchmal allerdings nur unter dem Deckmantel der Rufe nach Grundlagenstudien mit “höherer Power”. Um einen Zugang zur psychologisch-naturwissenschaftlichen Literatur zu haben, ist es daher bisher unumgänglich, sich auch mit dem grundlagenwissenschaftlich betrachtet eigentlich wenig sinnvollen Hypothesentesten zu beschäftigen.",
    "crumbs": [
      "Frequentistische Inferenz",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Hypothesentests</span>"
    ]
  },
  {
    "objectID": "304-Hypothesentests.html#sec-testbeispiele",
    "href": "304-Hypothesentests.html#sec-testbeispiele",
    "title": "24  Hypothesentests",
    "section": "24.3 Testbeispiele",
    "text": "24.3 Testbeispiele\n\n24.3.1 Einstichproben-T-Test\nDas Anwendungsszenario eines Einstichproben-T-Test ist dadurch gekennzeichnet, dass \\(n\\) univariate Datenpunkte einer Stichprobe (Gruppe) randomisierter experimenteller Einheiten betrachtet werden, von denen angenommen wird, dass sie Realisierungen von \\(n\\) unabhängigen und identisch normalverteilten Zufallsvariablen sind. Hinsichtlich der identischen univariaten Normalverteilungen \\(N(\\mu,\\sigma^2)\\) dieser Zufallsvariablen wird angenommen, dass sowohl der Erwartungswertparameter \\(\\mu\\) als auch der Varianzparameter \\(\\sigma^2\\) unbekannt sind. Schließlich wird vorausgesetzt, dass ein Interesse an einem inferentiellen Vergleich des unbekannten Erwartungswertparameters \\(\\mu\\) mit einen vorgebenenen Wert \\(\\mu_0\\) im Sinne eines Hypothesentests besteht.\nDabei gibt es allerdings mindestens vier Szenarien, die potentiell von Interesse sein können. Ein erster Fall wäre das Szenario einer einfachen Nullhypothese und einer einfachen Alternativhypothese, \\[\\begin{equation}\nH_0:\\mu = \\mu_0 \\mbox{ und }  H_1: \\mu = \\mu_1\n\\end{equation}\\] Dieser Fall ist in der Theorie sehr gut verstanden und Grundlage des sogenannten Neymann-Pearson-Lemmas (Neyman & Pearson (1933)). Seine praktische Relevanz ist aber eher gering, da die Alternativhypothese von einer genauen Spezifikation des Erwartungswertparameters ausgeht. Ein zweiter Fall ist das Szenario einer einfachen Nullhypothese und einer zusammengesetzten Alternativhypothese \\[\\begin{equation}\nH_0:\\mu = \\mu_0 \\mbox{ und } H_1:\\mu \\neq \\mu_0\n\\end{equation}\\] In diesem Fall spricht man auch von einer ungerichteten Hypothese und nutzt in der Regel einen zweiseitigen Test. Intuitiv entspricht dies der ungerichteten Frage nach inferentieller Evidenz für einen Unterschied. Es ist dieser Fall, den wir im Folgenden detailliert betrachten werden. Schließlich gibt es noch mindestens Szenarien mit zusammengesetzten Null- und Alternativhypothesen, etwa der Form \\[\\begin{equation}\nH_0:\\mu \\le \\mu_0 \\mbox{ und } H_1:\\mu &gt; \\mu_0 \\mbox{ oder }\nH_0:\\mu \\ge \\mu_0 \\mbox{ und } H_1:\\mu &lt; \\mu_0\n\\end{equation}\\] Man spricht in diesem Fall auch von gerichteten Hypothesen und nutzt in der Regel einseitige Tests. Diese Fall betrachten wir im Folgenden jedoch nicht.\n\nFrequentistisches Inferenzmodell\n\nDefinition 24.13 (Frequentistisches Inferenzmodell des Einstichproben-T-Tests) Das Frequentistische Inferenzmodell des Einstichproben-T-Tests ist gegeben durch das Normalverteilungsmodell (vgl. Definition 21.2) \\[\\begin{equation}\n\\upsilon_1,...,\\upsilon_n \\sim N(\\mu,\\sigma^2) \\mbox{ mit } (\\mu,\\sigma^2)\\in \\mathbb{R} \\times \\mathbb{R}_{&gt;0}\n\\end{equation}\\]\n\nWir erinnern daran, dass aus generativer Sicht das Normalverteilungsmodell dem Modell \\[\\begin{equation}\n\\upsilon_i = \\mu + \\varepsilon_i \\mbox{ mit } \\varepsilon_i \\sim N(0,\\sigma^2) \\mbox{ für } i = 1,...,n\n\\end{equation}\\] entspricht (vgl. Kapitel 21.3.1). Die Annahme unabhängig und identisch normalverteilter Zufallsvariablen als Grundlage der Modellierung der Beobachtung von \\(n\\) Datenpunkten ist wie in Kapitel 21.3.1 gesehen äquivalent zu der Annahme, dass sich jede einen Datenpunkt modellierende Zufallsvariable \\(\\upsilon_i\\) als Summe aus einem festen, wahren, aber unbekannten, über Zufallsvariablen konstanten Wert \\(\\mu\\) und aus einem Zufallsvariablen- bzw. Datenpunkt-spezifischen Abweichungsterm \\(\\varepsilon_i\\) ergibt. Dabei modelliert, wie gesehen, \\(\\mu\\) den tatsächlichen im wissenschaftlichen Anwendungskontext angenommen Effekt von Interesse und \\(\\varepsilon_i\\) den Aspekt der Datenvariabilität, der nicht durch diesen Effekt erklärt werden kann und im Sinne des Zentralen Grenzwertsatzes aus der Summation unendlich vieler Störeinflüsse hervorgeht und damit als Unsicherheit über die Erklärung der Datenvariabilität durch den festen Wert \\(\\mu\\) verbleibt.\n\n\nTesthypothesen\nWie oben diskutiert betrachten wir hier den Fall des Einstichproben-T-Tests mit einfacher Nullhypothese und zusammengesetzter Alternativhypothese.\n\nDefinition 24.14 (Einfache Nullhypothese und zusammengesetzte Alternativhypothese des Einstichproben-T-Tests) Gegeben sei das Frequentistiche Inferenzmodell des Einstichproben-T-Tests \\[\\begin{equation}\n\\upsilon_1,...,\\upsilon_n \\sim N(\\mu,\\sigma^2) \\mbox{ mit } (\\mu,\\sigma^2)\\in \\mathbb{R} \\times \\mathbb{R}_{&gt;0}\n\\end{equation}\\] und es sei \\(\\Theta := \\mathbb{R}\\) der Parameterunteraum des Parameters von Interesse \\(\\mu\\). Dann sind für den Nullhypothesenparameterwert \\(\\mu_0 \\in \\mathbb{R}\\) die einfache Nullhypothese und die zusammengesetzte Alternativhypothese des Einstichproben-T-Tests gegeben durch \\[\\begin{equation}\n\\Theta_0 := \\{\\mu_0\\} \\Leftrightarrow H_0 : \\mu = \\mu_0\n\\mbox{ und }\n\\Theta_1 := \\mathbb{R} \\setminus \\{\\mu_0\\} \\Leftrightarrow  H_1 : \\mu \\neq \\mu_0.\n\\end{equation}\\]\n\nMan beachte, dass die einfache Nullhypothese und die zusammengesetzte Alternativhypothese durch den Wert \\(\\mu_0 \\in \\mathbb{R}\\) parameterisiert sind. Je nach Wahl von \\(\\mu_0\\) ergeben sich also verschiedene Hypothesenszenarien. Wird beispielsweise \\(\\mu_0 := 0\\) gewählt, so entspricht die Nullhypothese \\(\\Theta_0 := \\{0\\}\\) der Aussage, dass der wahre, aber unbekannte, Parameter \\(\\mu\\) gleich \\(0\\) ist und die Alternativhypothese \\(\\Theta_1 := \\mathbb{R} \\setminus \\{0\\}\\) der Aussage, dass der wahre, aber unbekannte, Parameter \\(\\mu\\) ungleich \\(0\\) ist. Wird dagegen beispielsweise \\(\\mu_0 := 2\\) gewählt, so entspricht die Nullhypothese \\(\\Theta_0 := \\{2\\}\\) der Aussage, dass der wahre, aber unbekannte, Parameter \\(\\mu\\) gleich \\(2\\) ist und die Alternativhypothese \\(\\Theta_1 := \\mathbb{R} \\setminus \\{2\\}\\) der Aussage, dass der wahre, aber unbekannte, Parameter \\(\\mu\\) ungleich \\(2\\) ist. Im Anwendungskontext ist \\(\\mu_0\\) dementsprechend ein frei gewählter und damit natürlich auch bekannter Parameter des Einstichproben-T-Tests ist, wohingegen \\(\\mu\\) bekanntlich wahr, aber unbekannt ist und bleibt.\n\n\nDefinition der Teststatistik\nMit der Einstichproben-T-Test-Statistik definieren wir nun eine Teststatistik, die als Grundlage eines kritischen Wert-basierten Tests dienen kann und deren Betrag eine Abweichung von der Nullhypothese indiziert.\n\nDefinition 24.15 (Einstichproben-T-Test-Statistik) Gegeben sei das Testszenario eines Einstichproben-T-Tests mit Stichprobe \\(\\upsilon_1,...,\\upsilon_n\\), Stichprobenmittel \\(\\bar{\\upsilon}\\), Stichprobenstandard- abweichung \\(S\\) und Nullhypothesenparameter \\(\\mu_0\\). Dann ist die Einstichproben-T-Test-Statistik definiert als \\[\\begin{equation}\nT := \\sqrt{n}\\frac{\\bar{\\upsilon} - \\mu_0}{S}.\n\\end{equation}\\]\n\nOffenbar hat die Einstichproben-T-Test-Statistik eine hohe Ähnlichkeit mit der T-Konfidenzintervallstatistik (vgl. Definition 23.2). Man beachte allerdings, dass im Fall der Einstichproben-T-Test-Statistik der Nullhypothesenparameter \\(\\mu_0\\) nicht identisch mit dem in der T-Konfidenzintervallstatistik auftauchendem wahren, aber unbekannten, Parameterwert \\(\\mu\\) sein muss.\nDa die Einstichproben-T-Test-Statistik im Kontext des Einstichproben-T-Tests zentral ist, macht es Sinn, sich ihrer intuitiven Mechanik bewusst zu sein. Im Zähler des Bruches der Einstichproben-T-Test-Statistik tritt zunächst die Differenz des Stichprobenmittels \\(\\bar{\\upsilon}\\) zum angenommenen Nullhypothesenparameter \\(\\mu_0\\) auf. Wie gesehen ist das Stichprobenmittel ein unverzerrter Schätzer des Erwartungswertparameters \\(\\mu\\) der Stichprobe. Die Differenz \\(\\bar{\\upsilon} - \\mu_0\\) entspricht also einer Schätzung der Abweichung des wahren, aber unbekannten, Erwartungswertsparameters vom Nullhypothesenparameter und damit dem Betrage nach der Evidenz für eine Abweichung des wahren, aber unbekannten, Erwartungswertparameters von der Nullhypothese. Grob betrachtet hat man mit dem Zähler \\(\\bar{\\upsilon} - \\mu_0\\) also ein Maß für das der Stichprobe innewohnende “Signal” im Sinne der Abweichung von der Nullhypothese oder “systematischer Variabilität”. Der Nenner \\(S\\) erlaubt es dann, dieses Signal in Einheiten der Stichprobenstandardabweichung auszudrücken. Gilt zum Beispiel \\(\\bar{\\upsilon} - \\mu_0 = 2\\) und ist \\(S = 1\\), so beträgt die Abweichung des Stichprobenmittels vom Nullhypothesenparameter gerade zwei Standardabweichungen, ist dagegen \\(S = 2\\) so beträgt die entsprechende Abweichung gerade eine Standardabweichung. Weiterhin entspricht der Nenner \\(S\\) ja einem Maß für die beobachtete Datenvariabilität und einem Schätzer für die Standardabweichung \\(\\sigma\\) der Fehlerterme in der generativen Form des Einstichproben-T-Test Modells. Grob betrachtet hat man also im Nenner der Einstichproben-T-Test-Statistik ein Maß für das den Daten innewohnende “Rauschen” oder ihrer “unsystematischen Variabilität”. Insgesamt kann man den Bruch \\(\\frac{\\bar{\\upsilon} - \\mu_0}{S}\\) also als eine Schätzung des “Signal-zu-Rauschen-Verhältnis” der Daten verstehen. Schließlich wird in der Einstichproben-T-Test-Statistik dieses Verhältnis mit der Wurzel der Stichprobengröße \\(\\sqrt{n}\\) gewichtet. Intuitiv entspricht diese Wichtung der Tatsache, dass man einem gegebenen Signal-zu-Rauschen-Verhältnis mehr Validität zumessen kann, wenn es auf einer höheren Anzahl von Datenpunkten basiert, als wenn es auf einer geringen Anzahl von Datenpunkten basiert. Insgesamt hat man mit der Einstichproben-T-Test-Statistik eine skalare Zusammenfassung der den Daten innewohnenden Evidenz gegen die Nullhypothese, bei der sowohl die Datenvariabilität als auch der Datenumfang betrachtet werden.\n\n\nVerteilung der Teststatistik\nFür die Verteilung der Einstichproben-T-Test-Statistik gilt nun folgendes Theorem.\n\nTheorem 24.1 (Verteilung der Einstichproben-T-Test-Statistik) Gegeben sei das Testszenario eines Einstichproben-T-Tests mit Stichprobe \\(\\upsilon_1,...,\\upsilon_n\\), Stichprobenmittel \\(\\bar{\\upsilon}\\), Stichprobenstandard- abweichung \\(S\\) , Nullhypothesenparameter \\(\\mu_0\\) und Einstichproben-T-Test-Statistik definiert als \\[\\begin{equation}\nT := \\sqrt{n}\\frac{\\bar{\\upsilon} - \\mu_0}{S}.\n\\end{equation}\\] Dann ist \\(T\\) eine nichtzentrale \\(t\\)-Zufallsvariable mit Nichtzentralitätsparameter \\[\\begin{equation}\nd = \\sqrt{n}\\frac{\\mu - \\mu_0}{\\sigma}\n\\end{equation}\\] und Freiheitsgradparameter \\(n-1\\), es gilt also \\(T \\sim t(d,n-1)\\)\n\n\nBeweis. \n\nMan beachte, dass im Falle des Zutreffens der Nullhypothese der Nullhypothesenparameter \\(\\mu_0\\) mit dem wahren, aber unbekannten, Erwartungswertparameter \\(\\mu\\) identisch ist und der Nichtzentralitätsparameter der Verteilung der Einstichproben-T-Test-Statistik den Wert \\(d = 0\\) annimmt. Im Falle des Zutreffens der Nullhypothese des Einstichproben-T-Testszenarios ist die Einstichproben-T-Test-Statistik also eine \\(t\\)-verteilte Zufallsvariable mit Freiheitsgradparameter \\(n-1\\). Wir visualisieren die Verteilung der Einstichproben-T-Test-Statistik exemplarisch für ein Einstichproben-T-Testszenario mit \\(n = 12\\), wahren, aber unbekannten, Parametern \\(\\mu = 3\\) und \\(\\sigma^2 = 2\\) und Nullhypothesenparameter \\(\\mu_0 = 0\\) in Abbildung 24.2 (B), Die Parameter dieser Verteilung ergeben sich dabei zu\n\\[\\begin{equation}\nd\n= \\sqrt{n}\\frac{\\mu - \\mu_0}{\\sigma}\n= \\sqrt{12}\\frac{3 - 0}{\\sqrt{2}}\n\\approx 7.34\n\\mbox{ und }\nn - 1 = 11\n\\end{equation}\\] ergeben.\n\n\n\n\n\n\nAbbildung 24.2: Verteilung der Einstichproben-T-Test-Statistik für \\(n = 12, \\mu = 3, \\sigma^2\\) und \\(\\mu_0 = 0\\). (A) Verteilung der Stichprobenvariablen. (B) Verteilung der Einstichproben-T-Test-Statstik\n\n\n\n\n\nTestdefinition\nWir können nun den zweiseitigen Einstichproben-T-Test mit einfacher Nullhypothese und zusammengesetzter Alternativhypothese definieren und seine Testgütefunktion analysieren.\n\nDefinition 24.16 (Zweiseitiger Einstichproben-T-Test mit einfacher Nullhypothese und zusammengesetzter Alternativhypothese) Gegeben seien das Frequentistische Inferenzmodell des Einstichproben-T-Tests mit einfacher Nullhypothese und zusammengesetzter Alternativhypothese und \\(T\\) bezeichne die Einstichproben-T-Test-Statistik mit Werten \\(t \\in \\mathbb{R}\\). Dann ist der zweiseitige Einstichproben-T-Test mit einfacher Nullhypothese und zu- sammengesetzter Alternativhypothese definiert als der zweiseitige kritische Wert-basierte Test \\[\\begin{equation}\n\\phi : \\mathcal{Y} \\to \\{0,1\\}, y \\mapsto \\phi(y) := 1_{\\{|t| \\ge k\\}} =\n\\begin{cases}\n1 & |t| \\ge k \\\\\n0 & |t| &lt; k\n\\end{cases}.\n\\end{equation}\\]\n\nDer zweiseitige Einstichproben-T-Test mit einfacher Nullhypothese und zusammengesetzter Alternativhypothese nimmt also den Wert \\(0\\) an, wenn der Betrag der Einstichproben-T-Test-Statistik kleiner als der kritische Wert ist und er nimmt den Wert \\(1\\) an, wenn der Betrag der Einstichproben-T-Test-Statistik gleich oder größer als der kritische Wert ist.\n\n\n\nTestgütefunktion\nFür die Kontrolle des Testumfangs durch Wahl eines kritischen Werts und zur Bestimmung der Powerfunktion dieses Tests ist nun folgendes Theorem maßgeblich.\n\nTheorem 24.2 (Testgütefunktion des zweiseitigen Einstichproben-T-Test mit einfacher Nullhypothese und zusammengesetzter Alternativhypothese) \\(\\phi\\) sei der zweiseitige Einstichproben-T-Test mit einfacher Nullhypothese und zusammengesetzter Alternativhypothese. Dann ist die Testgütefunktion von \\(\\phi\\) gegeben durch \\[\\begin{equation}\nq_{\\phi} : \\mathbb{R} \\to [0,1],\n\\mu \\mapsto q_{\\phi}(\\mu)\n:= 1 - \\Psi(k;d_\\mu,n-1) + \\Psi(-k;d_\\mu,n-1),\n\\end{equation}\\] wobei \\(\\Psi(\\cdot; d_\\mu, n-1)\\) die KVF der nichtzentralen \\(t\\)-Verteilung mit Nichtzentralitätsparameter \\[\\begin{equation}\nd_\\mu := \\sqrt{n}\\frac{\\mu - \\mu_0}{\\sigma}\n\\end{equation}\\] und Freiheitsgradparameter \\(n-1\\) bezeichnet.\n\n\nBeweis. Die Testgütefunktion des betrachteten Test im vorliegenden Testszenario ist definiert als \\[\\begin{equation}\nq_{\\phi} : \\mathbb{R} \\to [0,1],\n\\mu \\mapsto q_{\\phi}(\\mu) := \\mathbb{P}_{\\mu}(\\phi = 1).\n\\end{equation}\\] Da die Wahrscheinlichkeiten für \\(\\phi = 1\\) und dafür, dass die zugehörige Teststatistik im Ablehnungsbereich des Tests liegt gleich sind, benötigen wir also zunächst die Verteilung der Teststatistik. Wir haben oben bereits gesehen, dass die Einstichproben-T-Test-Statistik \\[\\begin{equation}\nT := \\sqrt{n}\\frac{\\bar{\\upsilon}-\\mu_0}{S}  \n\\end{equation}\\] unter der Annahme \\(\\upsilon_1,...,\\upsilon_n \\sim N(\\mu,\\sigma^2)\\) anhand einer nichtzentralen \\(t\\)-Verteilung \\(t(d_\\mu,n-1)\\) mit Nichtzentralitätsparameter \\[\\begin{equation}\nd_\\mu := \\sqrt{n}\\frac{\\mu - \\mu_0}{\\sigma}\n\\end{equation}\\] verteilt ist. Der Ablehnungsbereich des zweiseitigen Einstichproben-T-Tests ist \\[\\begin{equation}\nA  = \\,]-\\infty, -k]\\, \\cup \\,]k,\\infty[.\n\\end{equation}\\] Mit diesem Ablehungsbereich ergibt sich dann \\[\\begin{align}\n\\begin{split}\nq_\\phi(\\mu)\n& = \\mathbb{P}_{\\mu}(\\phi = 1)                                                   \\\\\n& = \\mathbb{P}_{\\mu}\\left(T \\in ]-\\infty, -k]\\,\n                         \\cup \\,]k,\\infty[ \\right)                               \\\\\n& = \\mathbb{P}_{\\mu}\\left(T \\in ]-\\infty, -k]\\right)\n  + \\mathbb{P}_{\\mu}\\left(T \\in [k,\\infty[ \\right)                               \\\\\n& = \\mathbb{P}_{\\mu}(T \\le -k)  + \\mathbb{P}_{\\mu}(T \\ge k)                      \\\\\n& = \\mathbb{P}_{\\mu}(T \\le -k)  + (1-\\mathbb{P}_{\\mu}(T \\le k))                  \\\\\n& = 1 - \\mathbb{P}_{\\mu}(T \\le k)  + \\mathbb{P}_{\\mu}(T \\le - k)                 \\\\\n& = 1 - \\Psi(k; d_\\mu, n-1)  + \\Psi(-k;d_\\mu,n-1),\n\\end{split}\n\\end{align}\\] wobei \\(\\Psi(\\cdot; d_\\mu,n-1)\\) die KVF der nichtzentralen T-Verteilung mit Nichtzentralitätsparameter \\(d_\\mu\\) und Freiheitsgradparameter \\(n-1\\) bezeichnet.\n\nIn Abbildung 24.3 visualisieren wir die Testgütefunktion des zweiseitigen Einstichproben-T-Tests mit einfacher Nullhypothese und zusammengesetzter Alternativhypothese aus Theorem 24.2 für \\(\\sigma^2 = 9\\) und \\(\\mu_0 = 4\\) in Abhängigkeit vom kritischen Wert \\(k\\). Man beachte dabei zunächst, dass die Testgütefunktion als Funktion von \\(\\mu\\) sowohl das Szenario des Zutreffens der Nullhypothese \\(\\mu = \\mu_0\\) als auch das Szenario des Zutreffens der Alternativhypothese \\(\\mu \\neq \\mu_0\\) abdeckt. Man beachte weiterhin, dass der Wert der Testgütefunktion, also die Wahrscheinlichkeit dafür, dass der Test den Wert 1 annimmt, sowohl bei positiven als auch bei negativen Abweichungen des wahren, aber unbekannten, Erwartungswertparameters \\(\\mu\\) vom Nullhypothesenparameter \\(\\mu_0\\) ansteigt. Dies ist natürlich der Tatsache geschuldet ist, dass die Testentscheidung auf dem Betrag der Teststatistik beruht. Schließlich ist die genaue Form und Lage der Testgütefunktion von der Wahl des kritischen Werts \\(k\\) abhängig. Wird dieser größer gewählt, ist also ein größerer absoluter Wert der Teststatistik für dafür notwendig, dass der Test den Wert 1 annimmt, so ist die Wahrscheinlichkeit dafür, bei ansonsten konstanten Parametern, kleiner als bei kleineren Werten des kritischen Werts.\n\n\n\n\n\n\nAbbildung 24.3: Testgütefunktion des zweiseitigen Einstichproben-T-Tests mit einfacher Nullhypothese und zusammengesetzter Alternativhypothese für \\(\\sigma^2 = 9, \\mu_0 = 4, n = 12\\) und \\(k = 1,2,3\\).\n\n\n\n\nTestumfangkontrolle\nDie Werte der Testgütefunktion bei \\(\\mu = \\mu_0\\) in Abbildung 24.3 geben einen visuellen Eindruck davon, wie der kritische Wert den Testumfang kontrolliert. Die exakte Bestimmung des kritischen Werts bei einem gewünschten Testumfang ist Inhalt folgenden Theorems.\n\nTheorem 24.3 (Testumfangkontrolle für den zweiseitigen Einstichproben-T-Test mit einfacher Nullhypothese und zusammengesetzter Alternativhypothese) \\(\\phi\\) sei der zweiseitige Einstichproben-T-Test mit einfacher Nullhypothese und zusammengesetzter Alternativhypothese. Dann ist \\(\\phi\\) ein Level-\\(\\alpha_0\\)-Test mit Testumfang \\(\\alpha_0\\), wenn der kritische Wert definiert ist durch \\[\\begin{equation}\nk_{\\alpha_0} := \\Psi^{-1}\\left(1 - \\frac{\\alpha_0}{2}; n-1 \\right),\n\\end{equation}\\] wobei \\(\\Psi^{-1}(\\cdot; n-1)\\) die inverse KVF der \\(t\\)-Verteilung mit Freiheitsgradparameter \\(n-1\\) bezeichnet.\n\n\nBeweis. Damit der betrachtete Test ein Level-\\(\\alpha_0\\)-Test ist, muss bekanntlich \\(q_\\phi(\\mu) \\le \\alpha_0\\) für alle \\(\\mu \\in \\{\\mu_0\\}\\), also hier \\(q_\\phi(\\mu_0)\n\\le \\alpha_0\\), gelten. Weiterhin ist der Testumfang des betrachteten Tests durch \\(\\alpha = \\max_{\\mu \\in \\{\\mu_0\\}} q_\\phi(\\mu)\\), also hier durch \\(\\alpha =\nq_\\phi(\\mu_0)\\) gegeben. Wir müssen also zeigen, dass die Wahl von \\(k_{\\alpha_0}\\) garantiert, dass \\(\\phi\\) ein Level-\\(\\alpha_0\\)-Test mit Testumfang \\(\\alpha_0\\) ist. Dazu merken wir zunächst an, dass für \\(\\mu = \\mu_0\\) gilt, dass \\[\\begin{align}\n\\begin{split}\nq_\\phi(\\mu_0)\n& =  1 - \\Psi(k;d_{\\mu_0},n-1) + \\Psi(-k;d_{\\mu_0},n-1)                          \\\\\n& =  1 - \\Psi(k;0,n-1) + \\Psi(-k;0,n-1)                                          \\\\\n& =  1 - \\Psi(k;n-1) + \\Psi(-k;n-1),                                             \\\\\n\\end{split}\n\\end{align}\\] wobei \\(\\Psi(\\cdot;d,n-1)\\) und \\(\\Psi(\\cdot;n-1)\\) die KVF der nichtzentralen \\(t\\)-Verteilung mit Nichtzentralitätsparameter \\(d\\) und Freiheitsgradparameter \\(n-1\\) sowie der \\(t\\)-Verteilung mit Freiheitsgradparameter \\(n-1\\), respektive, bezeichnen. Sei nun also \\(k := k_{\\alpha_0}\\). Dann gilt \\[\\begin{align}\n\\begin{split}\nq_\\phi(\\mu_0)\n& = 1 - \\Psi(k_{\\alpha_0};n-1) + \\Psi(-k_{\\alpha_0};n-1)                             \\\\\n& = 1 - \\Psi(k_{\\alpha_0};n-1) + (1 - \\Psi(k_{\\alpha_0};n-1)                         \\\\\n& = 2(1-\\Psi(k_{\\alpha_0};n-1))                                                      \\\\\n& = 2\\left(1-\\Psi\\left(\\Psi^{-1}\\left(1- \\frac{\\alpha_0}{2} , n-1\\right), n-1\\right)\\right) \\\\\n& = 2\\left(1 - 1 + \\frac{\\alpha_0}{2}\\right)                                    \\\\\n& = \\alpha_0,\n\\end{split}\n\\end{align}\\] wobei die zweite Gleichung mit der Symmetrie der \\(t\\)-Verteilung folgt. Es folgt also direkt, dass bei der Wahl von \\(k = k_{\\alpha_0}\\), \\(q_\\phi(\\mu_0)\\le \\alpha_0\\) ist und der betrachtete Test somit ein Level-\\(\\alpha_0\\)-Test ist. Weiterhin folgt direkt, dass der Testumfang des betrachteten Tests bei der Wahl von \\(k = k_{\\alpha_0}\\) gleich \\(\\alpha_0\\) ist.\n\nMan beachte, dass nach Theorem 24.3 der hier betrachtete Tests inbesondere exakt ist, der Testumfang also mit dem Signifikanzlevel identisch ist. In Abbildung 24.4 visualisieren wir die Wahl des kritischen Werts \\(k_{\\alpha_0}\\) in einem zweiseitigen Einstichproben-T-Test-Szenario mit einfacher Nullhypothese und zusammengesetzter Alternativhypothese für \\(\\alpha_0 := 0.05\\) und \\(n =12\\).\n\n\n\n\n\n\nAbbildung 24.4: Bestimmung des kritischen Werts \\(k_{\\alpha_0}\\) für den zweiseitigen Einstichproben-T-Tests mit einfacher Nullhypothese für \\(n =12\\) und \\(\\alpha_0 := 0.05\\) zu Kontrolle des Testumfangs (A) Für ein gewähltes \\(\\alpha_0\\) ist der kritische Wert des betrachteten Tests nach Theorem 24.3 durch den Wert der inversen KVF der \\(t\\)-Verteilung mit Freiheitsgradparameter \\(n-1\\) an der Stelle \\(1 - \\frac{\\alpha_0}{2}\\) gegeben. Die Abbildung zeigt die Bestimmung dieses Werts zu \\(k_{0.05} = 2.2\\) anhand der KVF der \\(t\\)-Verteilung mit Freiheitsgradparameter \\(n-1\\). (B) Die Abbildung zeigt den aus der Wahl von \\(k_{\\alpha_0}\\) resultierenden Ablehunungsbereich des betrachteten Tests als grau hinterlegte Flächen unter der WDF der \\(t\\)-Verteilung mit Freiheitsgradparameter \\(n-1\\). Die symmetrische Verteilung der Teilmengen des Ablehnungsbereichs in den Ausläufern der WDF ergibt sich dabei aus der Definition des Tests als Funktion des Betrages der Einstichproben-T-Test-Statistik, also insbesondere des zweiseitigen Charakters des hier betrachteten Tests.\n\n\n\nFolgender R Code demonstriert die Bestimmung des kritischen Werts des zweiseitigen Einstichproben-T-Tests mit einfacher Nullhypothese und zusammengesetzter Alternativhypothese mithilfe der inversen KVF der \\(t\\)-Verteilung, die in R als die Funktion qt() implementiert ist. Darüberhinaus simuliert der Code \\(10^6\\) Stichprobenrealisationen für das hier betrachteten Testszenario bei Zutreffen der Nullhypothese und wertet den betrachteten Test aus. Es zeigt sich, dass die geschätzte Wahrscheinlichkeit dafür, dass der Test bei Zutreffen der Nullhypothese den Wert 1 annimmt mit dem gewünschten Wert von \\(\\alpha_0 = 0.05\\) sehr gut übereinstimmt.\n\n# Modellparameter\nn         = 12                                           # Anzahl der Datenpunkte\nmu        = 0                                            # wahrer, aber unbekannter, Erwartungswertparameter\nsigsqr    = 2                                            # wahrer, aber unbekannter, Varianzparameter\n\n# Testparameter\nmu_0      = 0                                            # Nullhypothesenparameter, hier \\mu = \\mu_0\nalpha_0   = 0.05                                         # Signifikanzlevel\nk_alpha_0 = qt(1-alpha_0/2,n-1)                          # Kritischer Wert\n\n# Simulation der Testumfangkontrolle\nset.seed(1)                                              # Random number generator seed\nnsim      = 1e6                                          # Anzahl Simulationen\nphi       = rep(NaN,nsim)                                # Testentscheidungsarray\nfor(j in 1:nsim){                                        # Simulationsiterationen\n    y      = rnorm(n,mu,sigsqr)                          # \\upsilon_i \\sim N(\\mu,\\Sigma), i = 1,...,n\n    y_bar  = mean(y)                                     # Stichprobenmittel\n    s      = sd(y)                                       # Stichprobenstandardabweichung\n    Tee    = sqrt(n)*((y_bar - mu_0)/s)                  # Einstichproben-T-Test-Statistik\n    if(abs(Tee) &gt; k_alpha_0){                            # Test 1_{\\vert t \\vert &gt;= k_alpha_0}\n        phi[j] = 1                                       # Ablehnen der Nullhypothese\n    } else {\n        phi[j] = 0                                       # Nichtablehnen der Nullhypothese\n    }\n}\n\n# Ausgabe\ncat(\"Kritischer Wert              =\", k_alpha_0,\n    \"\\nGeschätzter Testumfang alpha =\", mean(phi))\n\nKritischer Wert              = 2.200985 \nGeschätzter Testumfang alpha = 0.049755\n\n\n\n\np-Wert\nDer mit einem vorliegenden Wert der Teststatistik des zweiseitigen Einstichproben-T-Tests mit einfacher Nullhypothese und zusammengesetzter Alternativhypothese assoziierte p-Wert ergibt aus folgendem Theorem wie folgt.\n\nTheorem 24.4 (p-Wert des zweiseitigen Einstichproben-T-Test mit einfacher Nullhypothese und zusammengesetzter Alternativhypothese) Gegeben sei der zweiseitige Einstichproben-T-Test mit einfacher Nullhypothese und zusammengesetzter Alternativhypothese und \\(t\\) sei ein Wert der Einstichproben-T-Test-Statistik \\(T\\). Dann gilt \\[\\begin{equation}\n\\mbox{p-Wert} = 2(1 - \\Psi(\\vert t \\vert;n-1))\n\\end{equation}\\] wobei \\(\\Psi(\\cdot; n-1)\\) die KVF der \\(t\\)-Verteilung mit Freiheitsgradparameter \\(n-1\\) bezeichnet.\n\n\nBeweis. Nach Definition 24.12 ist der p-Wert das kleinste Signifikanzlevel \\(\\alpha_0\\) bei für den betrachteten Test die Nullhypothese basierend auf dem Wert von \\(t\\) abgelehnt werden würde. Im vorliegenden Fall würde die Nullhypothese für jedes \\(\\alpha_0\\) mit \\[\\begin{equation}\n\\vert t \\vert \\ge \\Psi^{-1}\\left(1- \\frac{\\alpha_0}{2};n-1\\right)\n\\end{equation}\\] abgelehnt werden, vgl. Theorem 24.3. Für diese \\(\\alpha_0\\) gilt, dass \\[\\begin{equation}\n\\alpha_0 \\ge 2(1 - \\Psi(\\vert t \\vert;n-1)),\n\\end{equation}\\] denn \\[\\begin{align}\n\\begin{split}\n\\vert t \\vert\n&\n\\ge \\Psi^{-1}\\left(1 - \\frac{\\alpha_0}{2}; n-1\\right)           \n\\\\\\Leftrightarrow\n\\Psi(\\vert t \\vert; n-1)\n&\n\\ge \\Psi\\left(\\Psi^{-1}\\left(1 - \\frac{\\alpha_0}{2}; n-1\\right); n-1\\right)\n\\\\\\Leftrightarrow\n\\Psi(\\vert t \\vert; n-1)\n& \\ge 1 - \\frac{\\alpha_0}{2}\n\\\\\\Leftrightarrow\n\\mathbb{P}(T \\le \\vert t \\vert)\n&\n\\ge 1 - \\frac{\\alpha_0}{2}\n\\\\\\Leftrightarrow\n\\frac{\\alpha_0}{2}\n& \\ge 1 - \\mathbb{P}(T \\le \\vert t \\vert)\n\\\\\\Leftrightarrow\n\\frac{\\alpha_0}{2}\n&\n\\ge \\mathbb{P}(T \\ge \\vert t \\vert)\n\\\\\\Leftrightarrow\n\\alpha_0\n&\n\\ge 2 \\mathbb{P}(T \\ge \\vert t \\vert)\n\\\\\\Leftrightarrow\n\\alpha_0\n& \\ge\n2(1 - \\Psi(\\vert t \\vert;n-1)).\n\\end{split}\n\\end{align}\\] Das kleinste \\(\\alpha_0 \\in [0,1]\\) mit \\[\\begin{equation}\n\\alpha_0 \\ge 2 \\mathbb{P}(T \\ge \\vert t \\vert)\n\\end{equation}\\] ist dann entsprechend \\[\\begin{equation}\n\\alpha_0 = 2(1 - \\Psi(\\vert t \\vert;n-1)).\n\\end{equation}\\]\n\nIn Abbildung 24.5 visualisieren wir die Bestimmung von p-Werten für \\(t = 2.26\\) und für \\(t = 3.81\\), welche sich zu \\(\\mbox{p} = 0.045\\) und \\(\\mbox{p} = 0.003\\), respektive, ergeben. Man beachte, dass zum Beispiel der p-Wert zu \\(t = -2.26\\) auch \\(\\mbox{p} = 0.045\\) beträgt.\n\n\n\n\n\n\nAbbildung 24.5: p-Werte für zwei mögliche Werte der Einstichproben-T-Test-Statistik bei \\(n = 12\\). Die Bereiche im Ergebnisraum der Einstichproben-T-Test-Statistik über denen roten Flächen eingezeichnet sind, markieren die Bereiche mit \\(T \\ge |t|\\) gilt. Die summierten ihnen zugeordneten Wahrscheinlichkeiten, also die entsprechenden roten Flächen unter der WDF der \\(t\\)-Verteilungen entsprechen dem p-Wert\n\n\n\n\n\nPowerfunktion\nDie Powerfunktion eines Tests entspricht der Testgütefunktion eines Tests für den Bereich des Parameterraums, der der Alternativhypothese entspricht. Änderungen im Wert der Powerfunktion eines Tests, oft einfach als Power des Tests bezeichnet, ergeben sich also zunächst einmal durch Änderungen des Wertes des wahren, aber unbekannten, Parameters im Bereich der Alternativhypothese. Allerdings hat es sich eingebürgert, die Wahrscheinlichkeit dafür, dass der Test den Wert 1 annimmt, also die Nullhypothese abgelehnt wird, nicht ausschließlich als Funktion des wahren, aber unbekannten, Parameters, sondern auch weiterer und in der praktischen Anwendung relevanter Parameter eines Testszenarios zu betrachten. An erster Stelle ist hier der Stichprobenumfangs \\(n\\) von Interesse. Im Kontext der praktischen Durchführung von Poweranalysen fragt man dann meist danach, welcher Stichprobenumfang bei Annahme eines Wertes für den wahren, aber unbekannten, Parameter im Bereich der Alternativhypothese mit einer bestimmten Wahrscheinlichkeit dafür, die Nullhypothese abzulehnen, assoziiert ist. Basierend auf der asymmetrischen Behandlung von Typ I und Typ II Fehlerwahrscheinlichkeiten (vgl. Kapitel 24.2) setzt man dahingehend zunächst ein Signifikanzlevel \\(\\alpha_0\\) zur Kontrolle des Testumfangs fest. Für den hier diskutierten zweiseitigen Einstichproben-T-Test mit einfacher Nullhypothese und zusammengesetzter Alternativhypothese betrachten wir also die Testgütefunktion \\[\\begin{equation}\nq_\\phi : \\mathbb{R} \\to [0,1],\n\\mu \\mapsto q_\\phi(\\mu)\n:= 1 - \\Psi(k_{\\alpha_0}; d_\\mu, n-1) + \\Psi(-k_{\\alpha_0}; d_\\mu, n-1)\n\\end{equation}\\] bei kontrolliertem Testumfang, also für \\[\\begin{equation}\nk_{\\alpha_0} := \\Psi^{-1}\\left(1-\\frac{\\alpha_0}{2};n-1\\right)\n\\end{equation}\\] mit festem \\(\\alpha_0\\) als Funktion des Nichtzentralitätsparameters \\(d\\) und des Stichprobenumfangs \\(n\\). Insbesondere hängt dabei der Nichtzentralitätsparameter \\(d\\) vom Verhältnis der wahren, aber unbekannten, Parameter \\(\\mu\\) und \\(\\sigma^2\\), also dem wahren, aber unbekannten, Signal-zu-Rauschen-Verhältnis des Testszenarios und \\(k_{\\alpha_0}\\) von \\(n\\) ab. Diese Überlegungen führen auf folgende Definition.\n\nDefinition 24.17 (Powerfunktion des zweiseitigen Einstichproben-T-Tests mit einfacher Nullhypothese und zusammengesetzter Alternativhypothese) Gegeben sei der zweiseitige Einstichproben-T-Test mit einfacher Nullhypothese und zusammengesetzter Alternativhypothese. Dann ist die Powerfunktion des Tests gegeben durch \\[\\begin{equation}\n\\pi : \\mathbb{R} \\times \\mathbb{N} \\to [0,1],\n(d,n) \\mapsto\n\\pi(d,n) := 1 - \\Psi(k_{\\alpha_0}; d, n-1) + \\Psi(-k_{\\alpha_0}; d, n-1)\n\\end{equation}\\]\n\nFolgender R Code demonstriert exemplarisch die Auswertung der Powerfunktion des zweiseitigen Einstichproben-T-Tests mit einfacher Nullhypothese und zusammengesetzter Alternativhypothese in einem Szenario mit \\(\\alpha_0 := 0.05\\) mithilfe der R Implementationen der KVF und der inversen KVF der nichtzentralen \\(t\\)-Verteilung pt() und qt(), respektive.\n\nalpha_0     = 0.05                                                          # Signifikanzlevel\nd_min       = -5                                                            # minimaler Nichtzentralitätsparameter                   \nd_max       =  5                                                            # maximaler Nichtzentralitätsparameter  \nd_res       = 50                                                            # Auflösung Nichtzentralitätsparameter  \nd           = seq(d_min, d_max, len = d_res)                                # Nichtzentralitätsparameterraum   \nn_min       = 1                                                             # minimaler Stichprobenumfang \nn_max       = 30                                                            # maximaler Stichprobenumfang \nn_res       = 50                                                            # Auflösung Stichprobenumfang \nn           = seq(n_min,n_max, len = n_res)                                 # maximaler Stichprobenumfang \npi          = matrix(rep(NaN, d_res*n_res), nrow = d_res)                   # Powerfunktionsarray\nfor(i in 1:d_res){                                                          # Nichtzentralitätsparameteriterationen\n  for(j in 1:n_res){                                                        # Stichprobenumfangiterationen\n    k_alpha_0 = qt(1 - alpha_0/2, n[j]-1)                                   # kritischer Wert\n    pi[i,j]   = 1-pt(k_alpha_0, n[j]-1, d[i])+pt(-k_alpha_0, n[j]-1, d[i])  # Auswertung der Powerfunktion\n  }\n}\n\nWir visualisieren die Abhängigkeit der Powerfunktion des zweiseitigen Einstichproben-T-Tests mit einfacher Nullhypothese und zusammengesetzter Alternativhypothese vom Nichtzentralitätsparameter und Stichprobenumfang in Abbildung 24.6 und Abbildung 24.7. Generell steigt die Powerfunktion des betrachteten Tests mit positiver oder negativer Abweichung des Nichtzentralitätsparameters vom Nullhypothesenszenario \\(d = 0\\) und steigendem Stichprobenumfang \\(n\\) monoton. Je nach Wahl des Signifikanzlevels erfolgt dieser Anstieg steiler oder weniger steil.\n\n\n\n\n\n\nAbbildung 24.6: Powerfunktionen des zweiseitigen Einstichproben-T-Tests mit einfacher Nullhypothese und zusammengesetzter Alternativhypothese. (A) Abhängigkeit der Powerfunktion \\(\\pi\\) vom Nichtzentralitätsparameter \\(d\\) und Stichprobenumfang \\(n\\) bei Wahl von \\(\\alpha_0 := 0.05\\). (B) Abhängigkeit der Powerfunktion \\(\\pi\\) vom Nichtzentralitätsparameter \\(d\\) und Stichprobenumfang \\(n\\) bei Wahl von \\(\\alpha_0 := 0.001\\)\n\n\n\n\n\n\n\n\n\nAbbildung 24.7: Schnitte der Powerfunktionen des zweiseitigen Einstichproben-T-Tests mit einfacher Nullhypothese und zusammengesetzter Alternativhypothese. (A) Abhängigkeit der Powerfunktion \\(\\pi\\) vom Nichtzentralitätsparameter \\(d\\) bei konstantem Stichprobenumfang \\(n = 12\\) und Wahl von \\(\\alpha_0 := 0.05\\). (B) Abhängigkeit der Powerfunktion vom Stichprobenumfang \\(n\\) \\(\\pi\\) bei Nichtzentralitätsparameter \\(d = 3\\) und Wahl von \\(\\alpha_0 := 0.05\\). (C) Abhängigkeit der Powerfunktion \\(\\pi\\) vom Nichtzentralitätsparameter \\(d\\) bei konstantem Stichprobenumfang \\(n = 12\\) und Wahl von \\(\\alpha_0 := 0.001\\). (D) Abhängigkeit der Powerfunktion vom Stichprobenumfang \\(n\\) \\(\\pi\\) bei Nichtzentralitätsparameter \\(d = 3\\) und Wahl von \\(\\alpha_0 := 0.001\\).\n\n\n\n\n\nPraktische Durchführung\nVor dem Hintergrund der in den bisherigen Abschnitten diskutierten Theorie des zweiseitigen Einstichproben-T-Tests mit einfacher Nullhypothese und zusammengesetzter Alternativhypothese ergibt sich dann zunächst folgendes routiniertes Durchführen des Tests im Rahmen einer Datenanalyse.\nMan nimmt an, dass ein vorliegender univariater Datensatz \\(y_1,...,y_n\\) eine Realisierung des Frequentistischen Inferenzmodells \\(\\upsilon_1,...,\\upsilon_n \\sim N(\\mu,\\sigma^2)\\) des Einstichproben-T-Tests mit wahren, aber unbekannten, Parameter \\(\\mu\\) und \\(\\sigma^2 &gt; 0\\) ist. Man nimmt ferner an, dass man entscheiden muss ob für einen gewählten Nullhypothesenparameter \\(\\mu_0\\) eher die Nullhypothese \\(H_0 : \\mu = \\mu_0\\) oder die Alternativhypothese \\(H_1: \\mu \\neq \\mu_0\\) zutrifft. Um den Testumfang über viele Wiederholungen dieser Testprozedur zu kontrollieren, wählt ein Signifikanzlevel \\(\\alpha_0\\) und bestimmt den zugehörigen kritischen Wert \\(k_{\\alpha_0}\\), so dass zum Beispiel bei einem Stichprobenumfang von \\(n=12\\) und der Wahl von \\(\\alpha_0  := 0.05\\) ein kritischer Wert von \\(k_{0.05} = 2.20\\) gewählt wird. Anhand des Stichprobenumfangs \\(n\\), des Nullhypothesenparameters \\(\\mu_0\\), des Stichprobenmittels \\(\\bar{\\upsilon}\\) und der Stichprobenstandardabweichung \\(s\\) berechnet man sodann den Wert der Einstichproben-T-Test-Statistik durch \\[\\begin{equation}\nt := \\sqrt{n}\\left(\\frac{\\bar{y} - \\mu_0}{s}\\right).\n\\end{equation}\\] Wenn dieses für den vorliegenden Datensatz so bestimmte \\(t\\) größer als \\(k_{\\alpha_0}\\) ist oder wenn \\(t\\) kleiner als \\(-k_{\\alpha_0}\\) ist, lehnt man die Nullhypothese ab, andernfalls lehnt man sie nicht ab. Die oben entwickelte Theorie garantiert dann, dass man im langfristigen Mittel in höchstens \\(\\alpha_0 \\cdot 100\\) von \\(100\\) Fällen die Nullhypothese fälschlicherweise ablehnt. Weiterhin bestimmt man basierend auf dem vorliegenden Wert der Einstichproben-T-Test-Statistik den zugehörigen p-Wert durch \\[\\begin{equation}\n\\mbox{p-Wert} = 2(1 - \\Psi(\\vert t \\vert;n-1))\n\\end{equation}\\] Folgender R Code demonstriert dieses Vorgehen bei Annahme eines vorliegenden Datenvektors y der Länge n.\n\nn           = length(y)                                   # Stichprobenumfang\nmu_0        = 0                                           # Nullhypothesenparameter\nalpha_0     = 0.05                                        # Signifikanzlevel\nk_alpha_0   = qt(1-alpha_0/2,n-1)                         # kritischer Wert\nTee         = sqrt(n)*((mean(y) - mu_0)/sd(y))            # Einstichproben-T-Test-Statistik\nif(abs(Tee) &gt; k_alpha_0){phi = 1} else {phi = 0}          # Testauswertung\np = 2*(1 - pt(Tee,n-1))                                   # p-Wert Evaluation\n\nWill man im Rahmen einer Studienplanung eine Poweranalyse zur Optimierung des Stichprobenumfangs im vorliegenden Testszenario durchführen, so gilt natürlich zunächst grundsätzlich, dass mit steigendem Stichprobenumfang die Powerfunktion des Tests ansteigt. Vor dem Gesichtspunkt der Power des Tests ist ein größerer Stichprobenumfang also immer besser als ein kleinerer Stichprobenumfang. Allerdings bleiben dabei mögliche Kosten für die Erhöhung des Stichprobenumfangs, wie zum Beispiel mögliche Risiken für die Studienteilnehmer:innen, unberücksichtigt. Weiterhin ist der Wert, den die Powerfunktion bei einem gewählten Stichprobenumfang immer von den wahren, aber unbekannten, Parameterwerten \\(\\mu\\) und \\(\\sigma\\), die in den Wert des Nichtzentralitätsparameters \\(d\\) einfließen, abhängig. Würde man diese Werte in einem gegebenen Anwendungskontext schon sehr genau kennen, so würde man vermutlich keine Studie durchführen wollen. Generell wird im Rahmen der Studienplanung deshalb folgendes Vorgehen favorisiert. Zunächst entscheided man sich für ein Signifikanzlevel \\(\\alpha_0\\) zur Kontrolle des Testumfangs und evaluiert die Powerfunktion. Man überlegt sich dann einen Nichtzentralitätswert \\(d^*\\), den man mit einer Power von mindestens \\(\\beta\\) detektieren möchte, wobei ein typischer konventioneller \\(\\beta = 0.8\\) ist. Man wertet dann die für einen Powerfunktionswert \\[\\begin{equation}\n\\pi(d = d^*,n) = \\beta\n\\end{equation}\\] nötige Stichprobengröße aus. Aufgrund der Monotonie der Powerfunktion des zweiseitigen Einstichproben-T-Tests mit einfacher Nullhypothese und zusammengesetzter Alternativhypothese im Bereich nicht-negativer Nichtzentralitätsparameter ist dann gewährleistet, dass die Power des Tests für Nichtzentralitätsparameter, die größer als \\(d^*\\) sind, größer oder gleich \\(\\beta\\) sind. Folgender R Code implementiert dieses Vorgehen zur Optimierung des Stichprobenumfangs und Abbildung 24.8 visualisiert es.\n\n# Powerfunktionsbasierte Stichprobenumfangsoptimierung\nalpha_0   = 0.05                                                        # Signifikanzlevel\nbeta      = 0.8                                                         # gewünschter Powerfunktionswert\nd_stern   = 3                                                           # fester Nichtzentralitätsparameter\nn_min     = 2                                                           # minimal betrachteter Stichprobenumfang  \nn_max     = 20                                                          # maximal betrachteter Stichprobenumfang  \nn_res     = 1e2                                                         # Auflösung des Stichprobenumfangraums\nn         = seq(n_min,n_max, len = n_res)                               # Stichprobenumfangraum\nk_alpha_0 = qt(1-alpha_0/2, n-1)                                        # kritische Werte in Abhängigkeit vom Stichprobenumfang\npi_n      = 1-pt(k_alpha_0, n-1, d_stern)+pt(-k_alpha_0, n-1, d_stern)  # Powerfunktion bei festem Nichtzentralitätsparameter\ni         = 1                                                           # Indexinitialisierung\nn_min     = NaN                                                         # minimales n Initialisierung\nwhile(pi_n[i] &lt; beta){                                                  # Solange \\pi(d*,n) &lt; \\beta\n    n_min = n[i]                                                        # Aufnahme des minimal nötigen ns\n    i     = i + 1}                                                      # und Erhöhung des Indexes\ncat(\"Minimal nötiges n =\", ceiling(n_min))                              # Ausgabe\n\nMinimal nötiges n = 16\n\n\n\n\n\n\n\n\nAbbildung 24.8: Praktische Durchführung einer Powerfunktionsbasiertern Stichprobenumfangoptimierung. Bei gewählten Signifikanzlevel \\(\\alpha_0\\) und fest angenommenen Nichtzentralitätsparameter \\(d*\\) evaluiert man den Stichprobenumfang für den die Testpowerfunktion einen Wert von \\(\\beta\\) oder größer hat.\n\n\n\n\n\nAnwendungsbeispiel\nAbschließen wollen wir oben skizziertes Vorgehen zur Durchführung eines zweiseitigen Einstichproben-T-Tests mit einfacher Nullhypothese und zusammengesetzter Alternativhypothese noch an dem in Kapitel 21.3.1 eingeführten Anwendungsbeispiel demonstrieren. Inhaltlich entsprich in diesem Fall die einfache Nullhypothese \\(H_0 : \\mu = 0\\) der Hypothese, dass die Therapie keinen Effekt auf BDI-II Reduktionsscores hat und die zusammengesetzte Alternativhypothese \\(H_1 : \\mu \\neq 0\\), dass die Therapie einen systematischen von Null verschiedenen Effekt auf BDI-II Reduktionsscores hat. Untenstehender R Code wendet das oben demonstrierte Verfahren zur Evaluation der Hypothesen auf den Prä-Post-Therapie BDI-II Reduktionsscore Datensatz von \\(n = 12\\) Patient:innen an und evaluiert darüberhinaus zusätzlich das 95%-Konfidenzintervall für den Erwartungswertparameter.\n\nD         = read.csv(\"./_data/304-Hypothesentests.csv\")     # Datensatzeinlesen\ny         = D$dBDI                                          # Datenauswahl\nn         = length(y)                                       # Stichprobenumfang\nmu_hat    = mean(y)                                         # Erwartungswertparameterschätzer\ndelta     = 0.95                                            # Konfidenzlevel\nt_delta   = qt((1+delta)/2,n-1)                             # \\Psi^-1((\\delta + 1)/2, n-1)\nG_u       = mean(y) - (sd(y)/sqrt(n))*t_delta               # untere Konfidenzintervallgrenze\nG_o       = mean(y) + (sd(y)/sqrt(n))*t_delta               # obere  Konfidenzintervallgrenze\nmu_0      = 0                                               # Nullhypothesenparameter, hier \\mu = \\mu_0\nalpha_0   = 0.05                                            # Signifikanzlevel\nk_alpha_0 = qt(1-alpha_0/2,n-1)                             # kritischer Wert\nTee       = sqrt(n)*((mean(y) - mu_0)/sd(y))                # T-Teststatistik\nif(abs(Tee) &gt; k_alpha_0){phi = 1} else {phi = 0}            # Test 1_{\\vert t \\vert &gt;= k_alpha_0}                             \np         = 2*(1 - pt(Tee,n-1))                             # p-Wert\ncat(\"Parameterschätzwert    =\", mu_hat,                     # Ausgabe\n    \"\\n95%-Konfidenzintervall =\", G_u, G_o,\n    \"\\nSignifikanzlevel       =\", alpha_0,\n    \"\\nKritischer Wert        =\", k_alpha_0,\n    \"\\nTeststatistik          =\", Tee,\n    \"\\nTestwert               =\", phi,\n    \"\\np-Wert                 =\", p)\n\nParameterschätzwert    = 3.166667 \n95%-Konfidenzintervall = 0.8074098 5.525923 \nSignifikanzlevel       = 0.05 \nKritischer Wert        = 2.200985 \nTeststatistik          = 2.95423 \nTestwert               = 1 \np-Wert                 = 0.01310986\n\n\nDie gleiche Analyse kann auch mit der in R implementierten Funktion t.test() durchgeführt werden, die Syntax zu ihrer Benutzung und die Formatierung der durch sie bestimmten Ergebnisse finden sich untensstehend\n\nt.test(y)             \n\n\n    One Sample t-test\n\ndata:  y\nt = 2.9542, df = 11, p-value = 0.01311\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.8074098 5.5259235\nsample estimates:\nmean of x \n 3.166667 \n\n\nIm vorliegen Fall würde man die Nullhypothese also bei einem Signifikanzlevel von \\(\\alpha_0 = 0.05\\) ablehnen. Ob die Nullhypothese allerdings im vorliegenden Fall zutrifft oder nicht bleibt, wie der wahre, aber unbekannte, Erwartungswertparameter unbekannt. Im langfristigen Mittel jedoch lehnt man basierend auf den oben beschriebenen Annahmen die Nullhypothese in nur 5 von 100 Fällen fälschlicherweise ab.",
    "crumbs": [
      "Frequentistische Inferenz",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Hypothesentests</span>"
    ]
  },
  {
    "objectID": "304-Hypothesentests.html#sec-konfidenzintervalle-und-hypothesentests",
    "href": "304-Hypothesentests.html#sec-konfidenzintervalle-und-hypothesentests",
    "title": "24  Hypothesentests",
    "section": "24.4 Konfidenzintervalle und Hypothesentests",
    "text": "24.4 Konfidenzintervalle und Hypothesentests\nIn diesem Abschnitt untersuchen wir, inwieweit Konfidenzintervalle und Hypothesentests als äquivalent angesehen werden können. Wir wollen dabei von dem Szenario eines Konfidenzintervalls ausgehen.\n\nTheorem 24.5 (Dualität von Konfidenzintervallen und Hypothesentests) Es sei \\(\\upsilon\\) die Stichprobe eines Frequentistischen Inferenzmodells mit Ergebnisraum \\(\\mathcal{Y}\\) und Parameterraum \\(\\Theta\\). Weiterhin sei für ein \\(\\delta \\in \\,]0,1[\\) mit \\([G_u(\\upsilon), G_o(\\upsilon)]\\) ein \\(\\delta\\)-Konfidenzintervall für den wahren, aber unbekannten, Parameter \\(\\theta \\in \\Theta\\) definiert. Dann gilt, dass der Hypothesentest definiert durch \\[\\begin{equation}\n\\phi_\\theta : \\mathcal{Y} \\to \\{0,1\\},\ny \\mapsto \\phi(y)\n:=\n\\begin{cases}\n0, & [G_u(y), G_o(y)]   \\ni      \\theta_0 \\\\\n1, & [G_u(y), G_o(y)] \\,\\niton\\, \\theta_0 \\\\\n\\end{cases}\n\\end{equation}\\] ein Hypothesentest vom Signifikanzlevel \\(\\alpha_0 = 1 - \\delta\\) für die Hypothesen \\[\\begin{equation}\n\\Theta_0 := \\{\\theta_0\\} \\mbox{ und } \\Theta_1 := \\Theta \\setminus \\{\\theta_0\\}.\n\\end{equation}\\]\n\n\nBeweis. Aufgrund der einfachen Nullhypothese und somit \\(\\alpha_0 = \\alpha\\) folgt \\[\\begin{equation}\n\\alpha_0\n= \\alpha\n= \\mathbb{P}_{\\theta_0}(\\phi(\\upsilon) = 1)\n= \\mathbb{P}_{\\theta_0}([G_u(y), G_o(y)]     \\,\\niton\\, \\theta)\n= 1 - \\mathbb{P}_{\\theta_0}([G_u(y), G_o(y)]    \\ni     \\theta)\n= 1 - \\delta.\n\\end{equation}\\]\n\nTheorem 24.5 besagt also, dass man mithilfe eines \\(\\delta\\)-Konfidenzintervalls einen Hypothesentest mit Signifikanzlevel \\(\\alpha_0 = 1 -\\delta\\) mit einfacher Nullhypothese und zusammengesetzter Alternativhypothese konstruieren kann. Dazu ist die bei diesem Test die Nullhypothese \\(\\theta = \\theta_0\\) jeweils abzulehnen, wenn das Konfidenzintervall den Nullhypotheseparameter \\(\\theta_0\\) nicht überdeckt und ansonsten nicht. Anhand folgenden Theorems wollen wir Theorem 24.5 für das in Kapitel 23.2 betrachtete Konfidenzintervall für den Erwartungswertparameter des Normalverteilungsmodells und den in Kapitel 24.3.1 betrachteten Einstichproben-T-Test konkretisieren.\n\nTheorem 24.6 (Dualität von Erwartungswertkonfidenzintervall und Einstichproben-T-Test) Gegeben sei das Normalverteilungsmodell und es sei \\[\\begin{equation}\n\\kappa :=\n\\left[\\bar{\\upsilon} - \\frac{S}{\\sqrt{n}}t_\\delta,\\bar{\\upsilon} + \\frac{S}{\\sqrt{n}}t_\\delta\\right].\n\\end{equation}\\] das mithilfe von \\[\\begin{equation}\nt_\\delta := \\Psi^{-1}\\left(\\frac{1 + \\delta}{2}; n-1 \\right)\n\\end{equation}\\] in Theorem 23.2 definierte \\(\\delta\\)-Konfidenzintervall für den Erwartungswertparameter. Dann ist mit Theorem 24.5 der Test\n\\[\\begin{equation}\n\\phi : \\mathcal{Y} \\to \\{0,1\\},\ny \\mapsto \\phi(y)\n:=\n\\begin{cases}\n0, & \\left[\\bar{\\upsilon} - \\frac{S}{\\sqrt{n}}t_\\delta,\\bar{\\upsilon} + \\frac{S}{\\sqrt{n}}t_\\delta\\right]\n           \\ni \\mu_0\n          \\\\  \n1, &\\left[\\bar{\\upsilon} - \\frac{S}{\\sqrt{n}}t_\\delta,\\bar{\\upsilon} + \\frac{S}{\\sqrt{n}}t_\\delta\\right]\n          \\,\\niton\\, \\mu_0\n          \\\\\n\\end{cases}\n\\end{equation}\\] ein Test der einfachen Nullhypothese \\(H_0: \\mu = \\mu_0\\) und der zusammengesetzten Alternativhypothese \\(H_1: = \\mu_0 \\neq \\mu\\) mit Signifikanzlevel \\(\\alpha_0 = 1 - \\delta\\).\n\n\nBeweis. Es gilt \\[\\begin{align}\n\\begin{split}\n\\mathbb{P}_{\\mu_0}\\left(\\phi(\\upsilon) = 1 \\right)\n= 1 - \\mathbb{P}_{\\mu_0}\\left(\\phi(\\upsilon) = 0 \\right)\n= 1 - \\mathbb{P}_{\\mu_0}\\left(\n            \\left[\\bar{\\upsilon} - \\frac{S}{\\sqrt{n}}t_\\delta,\n                  \\bar{\\upsilon} + \\frac{S}{\\sqrt{n}}t_\\delta\\right]\n            \\ni \\mu_0\\right)\n= 1 - \\delta.\n\\end{split}\n\\end{align}\\]\n\nFolgender R Code simuliert diesen Konfidenzintervall-basierten Hypothesentest bei Zutreffen der Nullhypothese und gibt Schätzungen für das Konfidenzlevel und das Signifikanzlevel über 100 Realisierungen einer Stichproben vom Stichprobenumfang \\(n = 12\\) mit wahren, aber unbekannten, Parametern \\(\\mu = 2\\) und \\(\\sigma^2 = 1\\) an.\n\nn       = 12                                          # Stichprobenumfang\nmu      = 2                                           # wahrer, aber unbekannter, Erwartungswertparameter\nsigsqr  = 1                                           # wahrer, aber unbekannter, Varianzparameter\ndelta   = 0.95                                        # Konfidenzlevel\nt_delta = qt((1+delta)/2, n-1)                        # \\Psi^{-1}((\\delta + 1)/2, n-1)\nmu_0    = mu                                          # Nullhypothesenparameter bei Zutreffen von H_0 \nset.seed(1)                                           # random number generator seed\nns      = 1e2                                         # Anzahl Simulationen\ny_bar   = rep(NaN,ns)                                 # Stichprobenmittelarray\ns       = rep(NaN,ns)                                 # Stichprobenstandardabweichungarray\nkappa   = matrix(rep(NaN,2*ns), ncol = 2)             # Konfidenzintervallarray\nkfn     = rep(NaN,ns)                                 # Überdeckungsindikatorarray\nphi     = rep(NaN,ns)                                 # Testarray\nfor(i in 1:ns){                                       # Simulationsiterationen\n  y          = rnorm(n,mu_0,sqrt(sigsqr))             # Stichprobenrealisierung\n  y_bar[i]   = mean(y)                                # Stichprobenmittel\n  s[i]       = sd(y)                                  # Stichprobenstandardabweichung\n  kappa[i,1] = y_bar[i] - (s[i]/sqrt(n))*t_delta      # untere Konfidenzintervallgrenze\n  kappa[i,2] = y_bar[i] + (s[i]/sqrt(n))*t_delta      # obere Konfidenzintervallgrenze\n  if(kappa[i,1] &lt;= mu_0 & mu_0 &lt;= kappa[i,2]){\n      kfn[i] = 1} else{kfn[i] = 0}                    # Überdeckungsindikatorevaluation\n  if(kappa[i,1] &lt;= mu_0 & mu_0 &lt;= kappa[i,2]){\n      phi[i] = 0} else{phi[i] = 1}}                   # Testevaluation\ncat(   \"Geschätztes Konfidenzniveau =\", mean(kfn),    # Ausgabe\n     \"\\nGeschätzter Testumfang      =\", mean(phi))\n\nGeschätztes Konfidenzniveau = 0.96 \nGeschätzter Testumfang      = 0.04\n\n\nWir visualisieren die Ergebnisse dieser Simulation in Abbildung 24.9.\n\n\n\n\n\n\nAbbildung 24.9: Dualität von Konfidenzintervall und Hypothesentest am Beispiel des Normalverteilungsmodells. Nutzt man das \\(\\delta\\)-Konfidenzintervall für den Erwartungswertparameter um bei Nichtübedeckung des Nullhypothesenparameters die Nullhypothese abzulehnen, so ergibt sich in diesem Fall ein Hypothesentest mit Signifikanzlevel \\(\\alpha_0 = 1 - \\delta\\).",
    "crumbs": [
      "Frequentistische Inferenz",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Hypothesentests</span>"
    ]
  },
  {
    "objectID": "304-Hypothesentests.html#literaturhinweise",
    "href": "304-Hypothesentests.html#literaturhinweise",
    "title": "24  Hypothesentests",
    "section": "24.5 Literaturhinweise",
    "text": "24.5 Literaturhinweise\nDie hier präsentierte Theorie der Hypothesentests geht im Wesentlichen auf Neyman & Pearson (1928) und Neyman & Pearson (1933) zurück. Gigerenzer (2004) und Lehmann (2011) geben historische Einordnungen der Genese des Hypothesentestbegriffs.",
    "crumbs": [
      "Frequentistische Inferenz",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Hypothesentests</span>"
    ]
  },
  {
    "objectID": "304-Hypothesentests.html#selbstkontrollfragen",
    "href": "304-Hypothesentests.html#selbstkontrollfragen",
    "title": "24  Hypothesentests",
    "section": "24.6 Selbstkontrollfragen",
    "text": "24.6 Selbstkontrollfragen\n\nErläutern Sie die grundlegende Logik Frequentistischer Hypothesentests.\nGeben Sie die Definition der Begriffe der Testhypothesen und des Testszenario wieder.\nGeben Sie die Definition der Begriffe der einfachen und zusammengesetzten Testhypothesen wieder.\nGeben Sie die Definition der Begriffe einseitigen und zweiseitigen Testhypothesen wieder.\nGeben Sie die Definition des Begriff des Tests wieder.\nGeben Sie die Definition des Begriffs des Standardtests wieder.\nGeben Sie die Definition des Begriffs des kritischen Bereichs wieder.\nGeben Sie die Definition des Begriffs des Ablehungsbereichs wieder\nGeben Sie die Definition des Begriffs des kritischen Wert-basierten Tests wieder.\nGeben Sie die Definition der Begriffe der richtigen Testentscheidungen und der Testfehler wieder.\nGeben Sie die Definition des Begriffs der Testgütefunktion wieder.\nErläutern Sie die Bedeutung der Testgütefunktion im Rahmen der Konstruktion von Hypothesentests.\nGeben Sie die Definition der Begriffe des Level-\\(\\alpha_0\\)-Tests und des Signifikanzlevels \\(\\alpha_0\\) wieder\nGeben Sie die Definition des Begriffs des Testumfangs \\(\\alpha\\) wieder.\nGeben Sie die Definition des Begriffs des p-Werts wieder. \n\n\n\n\n\nAmrhein, V., & Greenland, S. (2018). Remove, Rather than Redefine, Statistical Significance. Nature Human Behaviour, 2(1), 4–4. https://doi.org/10.1038/s41562-017-0224-0\n\n\nGigerenzer, G. (2004). Mindless Statistics. The Journal of Socio-Economics, 33(5), 587–606. https://doi.org/10.1016/j.socec.2004.09.033\n\n\nKochenderfer, M. J., Wheeler, T. A., & Wray, K. H. (2022). Algorithms for Decision Making. The MIT Press.\n\n\nLehmann, E. L. (2011). Fisher, Neyman, and the Creation of Classical Statistics. Springer New York. https://doi.org/10.1007/978-1-4419-9500-1\n\n\nMcShane, B. B., Gal, D., Gelman, A., Robert, C., & Tackett, J. L. (2019). Abandon Statistical Significance. The American Statistician, 73(sup1), 235–245. https://doi.org/10.1080/00031305.2018.1527253\n\n\nNeyman, J., & Pearson, E. S. (1928). On the Use and Interpretation of Certain Test Criteria for Purposes of Statistical Inference: Part I. Biometrika, 20A(1/2), 175. https://doi.org/10.2307/2331945\n\n\nNeyman, J., & Pearson, E. S. (1933). On the Problem of the Most Efficient Tests of Statistical Hypotheses. Phil. Trans. R. Soc. Lond. A, 231(694-706), 289–337.\n\n\nPratt, J., Raiffa, H., & Schlaifer, R. (1995). Statistical Decision Theory. MIT Press.\n\n\nPuterman, M. (2005). Markov Decision Processes. Wiley-Interscience.\n\n\nWasserstein, R. L., Schirm, A. L., & Lazar, N. A. (2019). Moving to a World Beyond „ p \\(&lt;\\) 0.05“. The American Statistician, 73(sup1), 1–19. https://doi.org/10.1080/00031305.2019.1583913",
    "crumbs": [
      "Frequentistische Inferenz",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Hypothesentests</span>"
    ]
  },
  {
    "objectID": "401-Regression.html",
    "href": "401-Regression.html",
    "title": "25  Regression",
    "section": "",
    "text": "25.1 Methode der kleinsten Quadrate\nWir definieren zunächst den Begriff der Ausgleichsgerade.\nBei der Ausgleichsgerade handelt es sich also um eine linear-affine Funktion der Form \\[\\begin{equation}\nf_{\\beta}: \\mathbb{R} \\rightarrow \\mathbb{R}, x \\mapsto f_{\\beta}(x):=\\beta_{0}+\\beta_{1} x\n\\end{equation}\\] Abbildung 25.2 zeigt drei durch jeweils andere Werte von \\(\\beta_{0}\\) und \\(\\beta_{1}\\) parameterisierte linearaffine Funktionen zusammen mit der Wertemenge des Beispieldatensatzes.\nWie bei allen linear-affinen Funktionen entspricht bei \\(f_{\\beta}\\) der Wert von \\(\\beta_{0}\\) dem Wert, den \\(f_{\\beta}\\) für \\(x=0\\) annimmt, \\[\\begin{equation}\nf_{\\beta}(0)=\\beta_{0}+\\beta_{1} \\cdot 0=\\beta_{0}\n\\end{equation}\\] und damit graphisch dem Schnittpunkt des Funktionsgraphen mit der \\(y\\)-Achse. Da \\(\\beta_{0}\\) damit dem Versatz (engl. offset) des Funktionsgraphen von \\(y=0\\) an der Stelle \\(x=0\\) entspricht, nennt man \\(\\beta_{0}\\) auch häufig den Offsetparameter. Analog entspricht wie bei allen linear-affinen Funktionen der Wert von \\(\\beta_{1}\\) dem Wert der Funktionswertdifferenz pro Argumenteinheitsdifferenz. Beispielsweise gilt etwa für \\(\\beta_{0}=5\\) und \\(\\beta_{1}=0.5\\), dass \\[\\begin{equation}\n\\begin{aligned}\n& f_{\\beta}(2)-f_{\\beta}(1)=(5+0.5 \\cdot 2)-(5+0.5 \\cdot 1)=1-0.5=0.5 \\\\\n& f_{\\beta}(9)-f_{\\beta}(8)=(5+0.5 \\cdot 9)-(5+0.5 \\cdot 8)=9.5-8=0.5\n\\end{aligned}\n\\end{equation}\\] Für eine Argumentdifferenz von 1 ergibt sich also eine Funktionswertdifferenz von 0.5. \\(\\beta_{1}\\) enkodiert also die Stärke der Änderung der Funktionswerte pro Argumentseinheitsdifferenz und damit die Steigung (engl. slope) des Graphen der linear-affinen Funktion. Entsprechend wird \\(\\beta_{1}\\) Steigungsparameter oder Slopeparameter genannt.\nNach Definition ist die Ausgleichsgerade nun allerdings nicht eine beliebige linearaffine Funktion der Form \\(f_{\\beta}\\), sondern eben jene, die für einen gegebenen Datensatze \\(\\left\\{\\left(x_{1}, y_{1}\\right), \\ldots,\\left(x_{n}, y_{n}\\right)\\right\\}\\) die Summe der quadrierten vertikalen Abweichnungen \\[\\begin{equation}\nq(\\beta):=\\sum_{i=1}^{n}\\left(y_{i}-\\left(\\beta_{0}+\\beta_{1} x_{i}\\right)\\right)^{2}\n\\end{equation}\\] minimiert. Für eine fest vorgegebenen Datensatz von \\(\\left(x_{i}, y_{i}\\right)\\) Paaren ist der Wert dieser Summe abhängig von den Werten von \\(\\beta_{0}\\) und \\(\\beta_{1}\\) und kann deshalb durch Wahl geeigneter\nWerte von \\(\\beta_{0}\\) und \\(\\beta_{1}\\) minimiert werden. Da hierbei eine Summe von quadrierten Abweichungen zwischen Datenpunkten und Werten der Ausgleichsgerade minimiert wird, spricht man auch oft etwas ungenau von der Methode der kleinsten Quadrate (engl. method of least squares). Abbildung 1.3 zeigt die vertikalen Abweichungen zwischen \\(y_{i}\\) und \\(\\beta_{0}+\\beta_{1} x_{i}\\) für \\(i=1, \\ldots, n\\) des Beispieldatensatzes als orange Linien sowie die Summe ihrer Quadrate \\(q(\\beta)\\) im Titel. Für die Parameterwerte \\(\\beta_{0}=-6.2\\) und \\(\\beta_{1}=1.7\\) (vgl. Abbildung 25.2) immt diese Summe ihren kleinsten Wert an.\nKonkrete Formeln zur Bestimmung der Parameterwerte der Ausgleichsgerade stellt Theorem 25.1 bereit.\nTheorem 25.1 besagt, dass die Parameterwerte, die für einen gegebenen Datensatz \\(\\left\\{\\left(x_{1}, y_{1}\\right), \\ldots,\\left(x_{n}, y_{n}\\right)\\right\\}\\) die Summe der quadrierten vertikalen Abweichungen für eine linear-affine Funktion minimieren mithilfe der Stichprobenmittel der \\(x_{i^{-}}\\)und \\(y_{i}\\)-Werte, der Stichprobenvarianz der \\(x_{i}\\)-Werte und der Stichprobenkovarianz der \\(x_{i}\\) - und \\(y_{i}\\)-Werte berechnet werden können. Die Terminologie orientiert sich hier an den Begrifflichkeiten der deskriptiven Statistik, insbesondere werden die \\(x_{i}\\)-Werte häufig nicht als Realisationen von Zufallsvariablen verstanden, der Begriff der Stichprobe wird jedoch trotzdem verwendet. Aus der Anwendungsperspektive können nach Theorem 25.1 die Parameter der Ausgleichsgerade also mithilfe der bekannten Funktionen für die Auswertung deskriptiver Statistiken bestimmt werden. Folgender R Code demonstriert dies.\n# Einlesen des Beispieldatensatzes\nD           = read.csv(\"./_data/401-regression.csv\")\n\n# Stichprobenstatistiken\nx_bar       = mean(D$x_i)                                                       # Stichprobenmittel der x_i-Werte\ny_bar       = mean(D$y_i)                                                       # Stichprobenmittel der y_i-Werte\ns2x         = var(D$x_i)                                                        # Stichprobenvarianz der  x_i-Werte\ncxy         = cov(D$x_i, D$y_i)                                                 # Stichprobenkovarianz der (x_i,y_i)-Werte\n\n# Ausgleichsgeradenparameter\nbeta_1_hat  = cxy/s2x                                                           # \\hat{\\beta}_1, Steigungsparameter\nbeta_0_hat  = y_bar - beta_1_hat*x_bar                                          # \\hat{\\beta}_0, Offset Parameter\n\n# Ausgabe\ncat(\"beta_0_hat:\", beta_0_hat,\n    \"\\nbeta_1_hat:\", beta_1_hat)\n\nbeta_0_hat: -6.194704 \nbeta_1_hat: 1.657055\nEine typische Visualisierung der Ausgleichsgerade eines Datensatzes wie in Abbildung 25.4 implementiert folgender \\(\\mathbf{R}\\) Code.\n# Datenwerte\nplot(\nD$x_i,\nD$y_i,\npch         = 16,\nxlab        = \"Anzahl Therapiestunden (x)\",\nylab        = \"Symptomreduktion (y)\",\nxlim        = c(0,21),\nylim        = c(-10, 40),\nmain        = TeX(\"$\\\\hat{\\\\beta}_0 =  -6.19, \\\\hat{\\\\beta}_1 = 1.66$\"))\n\n# Ausgleichsgerade\nabline(\ncoef        = c(beta_0_hat, beta_1_hat),\nlty         = 1,\ncol         = \"black\")\n\n# Legende\nlegend(\n\"topleft\",\nc(TeX(\"$(x_i,y_i)$\"), TeX(\"$f(x) = \\\\hat{\\\\beta}_0 + \\\\hat{\\\\beta}_1x$\")),\nlty        = c(0,1),\npch        = c(16, NA),\nbty        = \"n\")\nDie Idee, bei einem gegebenen Datensatz von \\(\\left(x_{i}, y_{i}\\right)\\) Paaren die Summe der quadrierten vertikalen Abweichungen zwischen einer Funktion der \\(x_{i}\\)-Werte und den \\(y_{i}\\)-Werten zu minimieren und so eine Funktion möglichst gut an eine Wertemenge anzupassen, ist nicht auf linear-affine Funktionen beschränkt. Folgende Definition verallgemeinert die Definition der Ausgleichsgerade auf Polynomfunktionen beliebigen Grades.\nDie Ausgleichsgerade ist damit das Ausgleichspolynom ersten Grades. Wir wollen den Begriff des Ausgleichspolynoms hier nicht weiter vertiefen und werden insbesondere die Parameterwerte \\(\\hat{\\beta}_{0}, \\ldots, \\hat{\\beta}_{k}\\), für die die Funktion \\(q\\) ihr Minimum annimt an späterer Stelle im Rahmen der Theorie des Allgemeinen Linearen Modells allgemein bestimmen. In Abbildung 25.5 visualisieren beispielhaft die Ausgleichspolynome ersten bis vierten Grades für den Beispieldatensatz wobei der Wert der Funktion \\(q\\) an der Minimumsstelle jeweils im Titel vermerkt ist.",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "401-Regression.html#sec-methode-der-kleinsten-quadrate",
    "href": "401-Regression.html#sec-methode-der-kleinsten-quadrate",
    "title": "25  Regression",
    "section": "",
    "text": "Definition 25.1 (Ausgleichsgerade) Für \\(\\beta:=\\left(\\beta_{0}, \\beta_{1}\\right)^{T} \\in \\mathbb{R}^{2}\\) heißt die linear-affine Funktion \\[\\begin{equation}\nf_{\\beta}: \\mathbb{R} \\rightarrow \\mathbb{R}, x \\mapsto f_{\\beta}(x):=\\beta_{0}+\\beta_{1} x\n\\end{equation}\\] für die für einen Datensatz \\(\\left\\{\\left(x_{1}, y_{1}\\right), \\ldots,\\left(x_{n}, y_{n}\\right)\\right\\} \\subset \\mathbb{R}^{2}\\) die Funktion \\[\\begin{equation}\nq: \\mathbb{R}^{2} \\rightarrow \\mathbb{R}_{\\geq 0}, \\beta \\mapsto q(\\beta):=\\sum_{i=1}^{n}\\left(y_{i}-f_{\\beta}\\left(x_{i}\\right)\\right)^{2}=\\sum_{i=1}^{n}\\left(y_{i}-\\left(\\beta_{0}+\\beta_{1} x_{i}\\right)\\right)^{2}\n\\end{equation}\\] der quadrierten vertikalen Abweichungen der \\(y_{i}\\) von den Funktionswerten \\(f_{\\beta}\\left(x_{i}\\right)\\) ihr Minimum annimt, Ausgleichsgerade für den Datensatz \\(\\left\\{\\left(x_{1}, y_{1}\\right), \\ldots,\\left(x_{n}, y_{n}\\right)\\right\\}\\).\n\n\n\n\n\n\n\n\n\n\nAbbildung 25.2: Linear-affine Funktionen mit unterschiedlichen Parameterwerten vor dem Hintegrund des Beispieldatensatzes.\n\n\n\n\n\n\n\n\n\n\nAbbildung 25.3: Vertikale Abweichungen und Quadratsummen bei unterschiedlichen Parameterwerten.\n\n\n\n\n\nTheorem 25.1 (Ausgleichsgerade) Für einen Datensatz \\(\\left\\{\\left(x_{1}, y_{1}\\right), \\ldots,\\left(x_{n}, y_{n}\\right)\\right\\} \\subset \\mathbb{R}^{2}\\) hat die Ausgleichsgerade die Form \\[\\begin{equation}\nf_{\\beta}: \\mathbb{R} \\rightarrow \\mathbb{R}, x \\mapsto f_{\\beta}(x):=\\hat{\\beta}_{0}+\\hat{\\beta}_{1} x\n\\end{equation}\\] wobei mit der Stichprobenkovarianz \\(c_{x y}\\) der \\(\\left(x_{i}, y_{i}\\right)\\)-Werte, der Stichprobenvarianz \\(s_{x}^{2}\\) der \\(x_{i}\\)-Werte und den Stichprobenmitteln \\(\\bar{x}\\) und \\(\\bar{y}\\) der \\(x_{i}\\) - und \\(y_{i}\\)-Werte, respektive, gilt, dass \\[\\begin{equation}\n\\hat{\\beta}_{1}=\\frac{c_{x y}}{s_{x}^{2}} \\text { und } \\hat{\\beta}_{0}=\\bar{y}-\\hat{\\beta}_{1} \\bar{x} \\text {. }\n\\end{equation}\\]\n\n\nBeweis. Wir betrachten die Summe der quadrierten vertikalen Abweichungen der \\(y_{i}\\) von den Funktionswerten \\(f\\left(x_{i}\\right)\\) als Funktion von \\(\\beta_{0}\\) und \\(\\beta_{1}\\) und bestimmen Werte \\(\\hat{\\beta}_{0}\\) und \\(\\hat{\\beta}_{1}\\), für die diese Funktion ihr Minimum annimmt, die Summe der quadrierten vertikalen Abweichungen der \\(y_{i}\\) von den Funktionswerten \\(f\\left(x_{i}\\right)\\) also minimal wird. Wir betrachten die Funktion \\[\\begin{equation}\nq: \\mathbb{R}^{2} \\rightarrow \\mathbb{R},\\left(\\beta_{0}, \\beta_{1}\\right) \\mapsto q\\left(\\beta_{0}, \\beta_{1}\\right):=\\sum_{i=1}^{n}\\left(y_{i}-\\left(\\beta_{0}+\\beta_{1} x\\right)\\right)^{2}\n\\end{equation}\\] Um das Minimum dieser Funktion zu bestimmen, berechnen wir zunächst die partiellen Ableitungen hinsichtlich \\(\\beta_{0}\\) und \\(\\beta_{1}\\) und setzen diese gleich 0. Es ergibt sich zunächst \\[\\begin{equation}\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\beta_{0}} q\\left(\\beta_{0}, \\beta_{1}\\right) & =\\frac{\\partial}{\\partial \\beta_{0}}\\left(\\sum_{i=1}^{n}\\left(y_{i}-\\left(\\beta_{0}+\\beta_{1} x_{i}\\right)\\right)^{2}\\right) \\\\\n& =\\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\beta_{0}}\\left(y_{i}-\\left(\\beta_{0}+\\beta_{1} x_{i}\\right)\\right)^{2} \\\\\n& =\\sum_{i=1}^{n} 2\\left(y_{i}-\\left(\\beta_{0}+\\beta_{1} x_{i}\\right)\\right) \\frac{\\partial}{\\partial \\beta_{0}}\\left(y_{i}-\\beta_{0}-\\beta_{1} x_{i}\\right) \\\\\n& =-2 \\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-\\beta_{1} x_{i}\\right) .\n\\end{aligned}\n\\end{equation}\\] Weiterhin ergibt sich \\[\\begin{equation}\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\beta_{1}} q\\left(\\beta_{0}, \\beta_{1}\\right) & =\\frac{\\partial}{\\partial \\beta_{1}}\\left(\\sum_{i=1}^{n}\\left(y_{i}-\\left(\\beta_{0}+\\beta_{1} x_{i}\\right)\\right)^{2}\\right) \\\\\n& =\\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\beta_{1}}\\left(y_{i}-\\left(\\beta_{0}+\\beta_{1} x_{i}\\right)\\right)^{2} \\\\\n& =\\sum_{i=1}^{n} 2\\left(y_{i}-\\left(\\beta_{0}+\\beta_{1} x_{i}\\right)\\right) \\frac{\\partial}{\\partial \\beta_{1}}\\left(y_{i}-\\beta_{0}-\\beta_{1} x_{i}\\right) \\\\\n& =-2 \\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-\\beta_{1} x_{i}\\right) x_{i} .\n\\end{aligned}\n\\end{equation}\\] Nullsetzen beider partieller Ableitungen ergibt dann \\[\\begin{equation}\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\beta_{0}} q\\left(\\beta_{0}, \\beta_{1}\\right) & =0 \\text { und } \\frac{\\partial}{\\partial \\beta_{1}} q\\left(\\beta_{0}, \\beta_{1}\\right)=0 \\\\\n\\Leftrightarrow-2 \\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-\\beta_{1} x_{i}\\right) & =0 \\text { und }-2 \\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-\\beta_{1} x_{i}\\right) x_{i}=0 \\\\\n\\Leftrightarrow \\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-\\beta_{1} x_{i}\\right) & =0 \\text { und } \\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-\\beta_{1} x_{i}\\right) x_{i}=0\n\\end{aligned}\n\\end{equation}\\] und weiter \\[\\begin{equation}\n\\begin{gathered}\n\\sum_{i=1}^{n} y_{i}-\\sum_{i=1}^{n} \\beta_{0}-\\beta_{1} \\sum_{i=1}^{n} x_{i}=0 \\text { und } \\sum_{i=1}^{n} y_{i} x_{i}-\\sum_{i=1}^{n} \\beta_{0} x_{i}-\\beta_{1} \\sum_{i=1}^{n} x_{i}^{2}=0 \\\\\n\\Leftrightarrow \\beta_{0} n+\\beta_{1} \\sum_{i=1}^{n} x_{i}=\\sum_{i=1}^{n} y_{i} \\text { und } \\beta_{0} \\sum_{i=1}^{n} x_{i}+\\beta_{1} \\sum_{i=1}^{n} x_{i}^{2}=\\sum_{i=1}^{n} y_{i} x_{i}\n\\end{gathered}\n\\end{equation}\\] Das sich hier ergebende Gleichungssystem \\[\\begin{equation}\n\\begin{aligned}\n\\beta_{0} n+\\beta_{1} \\sum_{i=1}^{n} x_{i} & =\\sum_{i=1}^{n} y_{i} \\\\\n\\beta_{0} \\sum_{i=1}^{n} x_{i}+\\beta_{1} \\sum_{i=1}^{n} x_{i}^{2} & =\\sum_{i=1}^{n} y_{i} x_{i}\n\\end{aligned}\n\\end{equation}\\] wird System der Normalengleichungen genannt und beschreibt die notwendige Bedingung für ein Minimum von \\(q\\). Aufösen dieses Gleichungssystems nach \\(\\beta_{0}\\) und \\(\\beta_{1}\\) liefert dann die Werte \\(\\hat{\\beta}_{0}\\) und \\(\\hat{\\beta}_{1}\\) des Theorems. Um dies zu sehen, halten wir zunächst fest, dass mit der ersten Gleichung des Systems der Normalengleichungen gilt \\[\\begin{equation}\nn \\hat{\\beta}_{0}+\\hat{\\beta}_{1} \\sum_{i=1}^{n} x_{i}=\\sum_{i=1}^{n} y_{i} \\Leftrightarrow \\hat{\\beta}_{0}+\\hat{\\beta}_{1} \\bar{x}=\\bar{y} \\Leftrightarrow \\hat{\\beta}_{0}=\\bar{y}-\\hat{\\beta}_{1} \\bar{x}\n\\end{equation}\\] Einsetzen der Form von \\(\\hat{\\beta}_{0}\\) in die zweite Gleichung des Systems der Normalengleichungen ergibt dann zunächst \\[\\begin{equation}\n\\begin{aligned}\n\\hat{\\beta}_{0} \\sum_{i=1}^{n} x_{i}+\\hat{\\beta}_{1} \\sum_{i=1}^{n} x_{i}^{2} & =\\sum_{i=1}^{n} y_{i} x_{i} \\\\\n\\Leftrightarrow\\left(\\bar{y}-\\hat{\\beta}_{1} \\bar{x}\\right) \\sum_{i=1}^{n} x_{i}+\\hat{\\beta}_{1} \\sum_{i=1}^{n} x_{i}^{2} & =\\sum_{i=1}^{n} y_{i} x_{i} \\\\\n\\Leftrightarrow \\bar{y} \\sum_{i=1}^{n} x_{i}-\\hat{\\beta}_{1} \\bar{x} \\sum_{i=1}^{n} x_{i}+\\hat{\\beta}_{1} \\sum_{i=1}^{n} x_{i}^{2} & =\\sum_{i=1}^{n} y_{i} x_{i} \\\\\n\\Leftrightarrow-\\hat{\\beta}_{1} \\bar{x} \\sum_{i=1}^{n} x_{i}+\\hat{\\beta}_{1} \\sum_{i=1}^{n} x_{i}^{2} & =\\sum_{i=1}^{n} y_{i} x_{i}-\\bar{y} \\sum_{i=1}^{n} x_{i} \\\\\n\\Leftrightarrow \\hat{\\beta}_{1}\\left(\\sum_{i=1}^{n} x_{i}^{2}-\\bar{x} \\sum_{i=1}^{n} x_{i}\\right) & =\\sum_{i=1}^{n} y_{i} x_{i}-\\bar{y} \\sum_{i=1}^{n} x_{i} .\n\\end{aligned}\n\\end{equation}\\] Wir halten nun zunächst fest, dass gilt \\[\\begin{equation}\n\\begin{aligned}\n\\sum_{i=1}^{n} x_{i}^{2}-\\bar{x} \\sum_{i=1}^{n} x_{i} & =\\sum_{i=1}^{n} x_{i}^{2}-2 \\bar{x} \\sum_{i=1}^{n} x_{i}+\\bar{x} \\sum_{i=1}^{n} x_{i} \\\\\n& =\\sum_{i=1}^{n} x_{i}^{2}-2 \\bar{x} \\sum_{i=1}^{n} x_{i}+n\\left(\\frac{1}{n} \\sum_{i=1}^{n} x_{i}\\right) \\bar{x} \\\\\n& =\\sum_{i=1}^{n} x_{i}^{2}-2 \\bar{x} \\sum_{i=1}^{n} x_{i}+n \\bar{x}^{2} \\\\\n& =\\sum_{i=1}^{n}\\left(x_{i}^{2}-2 \\bar{x} x_{i}+\\bar{x}^{2}\\right) \\\\\n& =\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}\n\\end{aligned}\n\\end{equation}\\] Weiterhin halten wir zunächst fest, dass gilt \\[\\begin{equation}\n\\begin{aligned}\n\\sum_{i=1}^{n} y_{i} x_{i}-\\bar{y} \\sum_{i=1}^{n} x_{i} & =\\sum_{i=1}^{n} y_{i} x_{i}-\\bar{y} \\sum_{i=1}^{n} x_{i}-n \\bar{y} \\bar{x}+n \\bar{y} \\bar{x} \\\\\n& =\\sum_{i=1}^{n} y_{i} x_{i}-\\bar{y} \\sum_{i=1}^{n} x_{i}-\\sum_{i=1}^{n} y_{i} \\bar{x}+\\sum_{i=1}^{n} \\bar{y} \\bar{x} \\\\\n& =\\sum_{i=1}^{n} y_{i} x_{i}-\\sum_{i=1}^{n} y_{i} \\bar{x}-\\sum_{i=1}^{n} \\bar{y} x_{i}+\\sum_{i=1}^{n} \\bar{y} \\bar{x} \\\\\n& =\\sum_{i=1}^{n}\\left(y_{i} x_{i}-y_{i} \\bar{x}-\\bar{y} x_{i}+\\bar{y} \\bar{x}\\right) \\\\\n& =\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)\\left(x_{i}-\\bar{x}\\right) .\n\\end{aligned}\n\\end{equation}\\] In der Fortsetzung von (1.16) ergibt sich dann \\[\\begin{equation}\n\\begin{aligned}\n\\hat{\\beta}_{1}\\left(\\sum_{i=1}^{n} x_{i}^{2}-\\bar{x} \\sum_{i=1}^{n} x_{i}\\right) & =\\sum_{i=1}^{n} y_{i} x_{i}-\\bar{y} \\sum_{i=1}^{n} x_{i} \\\\\n\\Leftrightarrow \\hat{\\beta}_{1}\\left(\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}\\right) & =\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)\\left(x_{i}-\\bar{x}\\right) \\\\\n\\Leftrightarrow \\hat{\\beta}_{1} & =\\frac{\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)\\left(x_{i}-\\bar{x}\\right)}{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}} \\\\\n\\Leftrightarrow \\hat{\\beta}_{1} & =\\frac{c_{x y}}{s_{x}^{2}} .\n\\end{aligned}\n\\end{equation}\\]\n\n\n\n\n\n\n\n\n\n\n\n\nAbbildung 25.4: Ausgleichsgerade für den Beispieldatensatz.\n\n\n\n\nDefinition 25.2 (Ausgleichspolynom) Für \\(\\beta:=\\left(\\beta_{0}, \\ldots, \\beta_{k}\\right)^{T} \\in \\mathbb{R}^{k+1}\\) heißt die Polynomfunktion \\(k\\) ten Grades \\[\\begin{equation}\nf_{\\beta}: \\mathbb{R} \\rightarrow \\mathbb{R}, x \\mapsto f_{\\beta}(x):=\\sum_{i=0}^{k} \\beta_{i} x^{i}\n\\end{equation}\\] für die für einen Datensatz \\(\\left\\{\\left(x_{1}, y_{1}\\right), \\ldots,\\left(x_{n}, y_{n}\\right)\\right\\} \\subset \\mathbb{R}^{2}\\) die Funktion \\[\\begin{equation}\nq: \\mathbb{R}^{k+1} \\rightarrow \\mathbb{R}_{\\geq 0}, \\beta \\mapsto q(\\beta):=\\sum_{i=1}^{n}\\left(y_{i}-f_{\\beta}\\left(x_{i}\\right)\\right)^{2}=\\sum_{i=1}^{n}\\left(y_{i}-\\sum_{i=0}^{k} \\beta_{i} x^{i}\\right)^{2}\n\\end{equation}\\] der quadrierten vertikalen Abweichungen der \\(y_{i}\\) von den Funktionswerten \\(f_{\\beta}\\left(x_{i}\\right)\\) ihr Minimum annimt, das Ausgleichspolynom kten Grades für den Datensatz \\(\\left\\{\\left(x_{1}, y_{1}\\right), \\ldots,\\left(x_{n}, y_{n}\\right)\\right\\}\\).\n\n\n\n\n\n\n\n\nAbbildung 25.5: Ausgleichspolynome ersten bis vierten Grades für den Beispieldatensatz.",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "401-Regression.html#einfache-lineare-regression",
    "href": "401-Regression.html#einfache-lineare-regression",
    "title": "25  Regression",
    "section": "25.2 Einfache lineare Regression",
    "text": "25.2 Einfache lineare Regression\nEine Ausgleichsgerade erlaubt Aussagen über unbeobachtete der abhängigen Variable. Allerdings erlaubt eine Ausgleichsgerade nur implizite Aussagen über die mit der Anpassung einer linear-affinen Funktion an einen Datensatz verbundene Unsicherheit. In der einfachen linearen Regression wird die Idee einer Ausgleichsgerade um eine probabilistische Komponente erweitert. Die einfache lineare Regression erlaubt damit insbesondere im Sinne der Frequentistischen Inferenz Konfidenzintervalle für die Ausgleichsgeradenparameter anzugeben und Hypothesentests bezüglich der Ausgleichsgeradenparameter durchzuführen. Wir wollen hier zunächst nur das Modell der einfachen linearen Regression und die auf ihm basierende Maximum-Likelihood Schätzung der Ausgleichsgeradenparameter betrachten.\nDie Bewertung der mit dieser Schätzung verbundenen Unsicherheit sowie parameterzentrierte Hypothesentests behandeln wir dann an späterer Stelle zunächst im Kontext des Allgemeinen Linearen Modells. Wir beginnen mit folgender Definition.\n\nDefinition 25.3 (Modell der einfachen linearen Regression) Es sei \\[\n\\upsilon_{i}=\\beta_{0}+\\beta_{1} x_{i}+\\varepsilon_{i} \\text { mit } \\varepsilon_{i} \\sim N\\left(0, \\sigma^{2}\\right) \\text { u.i.v. für } i=1, \\ldots, n\n\\tag{25.1}\\] wobei\n\n\\(\\upsilon_{i}\\) beobachtbare Zufallsvariablen sind, die Werte einer abhängigen Variable modellieren,\n\\(x_{i} \\in \\mathbb{R}\\) fest vorgegebene Prädiktorwerte oder Regressorwerte sind, die Werte einer unabhängigen Variable modellieren,\n\\(\\beta_{0}, \\beta_{1} \\in \\mathbb{R}\\) wahre, aber unbekannte, Offset- und Steigungsparameterwerte sind und\n\\(\\varepsilon_{i}\\) unabhängig und identisch normalverteilte nicht-beobachtbare Zufallsvariablen mit wahrem, aber unbekanntem, Varianzparameter \\(\\sigma^{2}&gt;0\\) sind, die Fehler-oder Störvariablen modellieren.\n\nDann heißt Gleichung 25.1 Modell der einfachen linearen Regression.\n\nIm Gegensatz zur Ausgleichsgerade treten im Modell der einfachen linearen Regression also explizit Zufallsvariablen auf. Speziell definiert das Modell der einfachen linearen Regression wie \\(n\\) beobachtbare (abhängige) Zufallsvariablen \\(\\upsilon_{i}\\) anhand der Werte \\(x_{i}\\) einer unabhängigen Variable, der Parameterwerte \\(\\beta_{0}\\) und \\(\\beta_{1}\\) sowie durch Addition der normalverteilten Fehlervariablen \\(\\varepsilon_{i}\\) generiert wird. Das Modell hat dabei drei Parameter, den Offsetparameter \\(\\beta_{0}\\), den Steigungsparameter \\(\\beta_{1}\\) und den Varianzparameter \\(\\sigma^{2}\\) der normalverteilten Fehlervariablen. Addition der festen Werte \\(\\beta_{0}\\) und \\(\\beta_{1} x_{i}\\) zu der normalverteilten Zufallsvariable \\(\\varepsilon_{i}\\) impliziert dabei eine Normalverteilung von \\(\\upsilon_{i}\\). Dies ist die Aussage folgenden Theorems.\n\nTheorem 25.2 (Datenverteilung der einfachen linearen Regression) Das Modell der einfachen linearen Regression \\[\\begin{equation}\n\\upsilon_{i}=\\beta_{0}+\\beta_{1} x_{i}+\\varepsilon_{i} \\text { mit } \\varepsilon_{i} \\sim N\\left(0, \\sigma^{2}\\right) \\text { u.i.v. für } i=1, \\ldots, n\n\\end{equation}\\] lässt sich mit \\(\\mu_{i}:=\\beta_{0}+\\beta_{1} x_{i}\\) äquivalent in der Form \\[\\begin{equation}\n\\upsilon_{i} \\sim N\\left(\\mu_{i}, \\sigma^{2}\\right) \\text { u.v. für } i=1, \\ldots, n\n\\end{equation}\\] schreiben.\n\n\nBeweis. Wir zeigen die Äquivalenz für ein \\(i\\), die Unabhängigkeit der \\(\\upsilon_{i}\\) zeigen wir an späterer Stelle im Rahmen des Allgemeinen Linearen Modells. Die Äquivalenz beider Modellformen für ein \\(i\\) folgt direkt aus der Transformation normalverteilter Zufallsvariablen durch linear-affine Funktionen. Speziell gilt im vorliegenden Fall für \\(\\varepsilon_{i} \\sim N\\left(0, \\sigma^{2}\\right)\\), dass \\[\\begin{equation}\n\\upsilon_{i}=f\\left(\\varepsilon_{i}\\right) \\text { mit } f: \\mathbb{R} \\rightarrow \\mathbb{R}, \\varepsilon_{i} \\mapsto f\\left(\\varepsilon_{i}\\right):=\\varepsilon_{i}+\\left(\\beta_{0}+\\beta_{1} x_{i}\\right)\n\\end{equation}\\] Mit dem WDF Transformationstheorem bei linear-affinen Abbildungen folgt dann \\[\\begin{equation}\n\\begin{aligned}\np_{\\upsilon_{i}}\\left(y_{i}\\right) & =\\frac{1}{|1|} p_{\\varepsilon_{i}}\\left(\\frac{y_{i}-\\beta_{0}-\\beta_{1} x_{i}}{1}\\right) \\\\\n& =N\\left(y_{i}-\\beta_{0}-\\beta_{1} x_{i} ; 0, \\sigma^{2}\\right) \\\\\n& =\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} \\exp \\left(-\\frac{1}{2 \\sigma^{2}}\\left(y_{i}-\\beta_{0}-\\beta_{1} x_{i}-0\\right)^{2}\\right) \\\\\n& =\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} \\exp \\left(-\\frac{1}{2 \\sigma^{2}}\\left(y_{i}-\\left(\\beta_{0}+\\beta_{1} x_{i}\\right)^{2}\\right)\\right. \\\\\n& =N\\left(y_{i} ; \\beta_{0}+\\beta_{1} x_{i}, \\sigma^{2}\\right) .\n\\end{aligned}\n\\end{equation}\\] Definition von \\(\\mu_{i}:=\\beta_{0}+\\beta_{1} x_{i}\\) ergibt dann die Aussage des Theorems.\n\nTheorem 25.2 besagt insbesondere, dass die Datenvariablen \\(\\upsilon_{i}\\) univariat normalverteilte Zufallsvariablen sind, deren Erwartungswertparameter jeweils vom Wert der unabhängigen Variable \\(x_{i}\\) abhängen. Abbildung 1.6 visualisiert das Modell und eine Realisation der einfachen linearen Regression für wahre, aber unbekannte, Parameterwerte \\(\\beta_{0}:=0, \\beta_{1}:=1\\) und \\(\\sigma^{2}:=1\\).\n\n\n\n\n\n\nAbbildung 25.6: Modell der einfachen linearen Regression für \\(\\beta_{0}:=0, \\beta_{1}:=1\\) und \\(\\sigma^{2}:=1\\).\n\n\n\nDa es sich bei dem Modell der einfachen linearen Regression um ein parametrisches Frequentistisches Modell handelt, können Schätzer für die Modellparameter mithilfe des MaximumLikelihood Prinzips gewonnen werden. Insbesondere stellt sich dabei heraus, dass die Maximum-Likelihood Schätzer des Offset- und des Steigungsparameter mit den Werten der Ausgleichsgeradenparameter identisch sind. Dies ist eine der Aussagen folgenden Theorems. Wir verzichten hier bei den Schätzern aus Gründen der notationstechnischen Übersichtlichkeit auf \\({ }^{\\mathrm{ML}}\\) Superskripte.\n\nTheorem 25.3 (Maximum-Likelihood Schätzung der einfachen linearen Regression) \\[\\begin{equation}\n\\upsilon_{i}=\\beta_{0}+\\beta_{1} x_{i}+\\varepsilon_{i} \\text { mit } \\varepsilon_{i} \\sim N\\left(0, \\sigma^{2}\\right) \\text { u.i.v. für } i=1, \\ldots, n\n\\end{equation}\\] das Modell der einfachen linearen Regression. Dann sind Maximum Likelihood Schätzer der Modellparameter \\(\\beta_{0}, \\beta_{1}\\) und \\(\\sigma^{2}\\) gegeben durch \\[\\begin{equation}\n\\hat{\\beta}_{1}:=\\frac{c_{x y}}{s_{x}^{2}}, \\quad \\hat{\\beta}_{0}:=\\bar{y}-\\hat{\\beta}_{1} \\bar{x} \\quad \\text { und } \\hat{\\sigma}^{2}:=\\frac{1}{n} \\sum_{i=1}^{n}\\left(y_{i}-\\left(\\hat{\\beta}_{0}+\\hat{\\beta}_{1} x_{i}\\right)\\right)^{2}\n\\end{equation}\\]\n\n\nBeweis. Wir zeigen zunächst, dass die Ausgleichsgerardenparameter \\(\\hat{\\beta}_{0}\\) und \\(\\hat{\\beta}_{1}\\) den entsprechenden Maximum-Likelihood Schätzern gleichen. Dazu halten wir zunächst fest, dass aufgrund der Unabhängigkeit der \\(\\upsilon_{1}, \\ldots,\\upsilon_{n}\\) die Likelihood-Funktion des Modells der einfachen linearen Regression bezüglich \\(\\beta_{0}\\) und \\(\\beta_{1}\\) die Form \\[\\begin{equation}\n\\begin{aligned}\nL: \\mathbb{R}^{2} \\rightarrow \\mathbb{R}_{&gt;0},\\left(\\beta_{0}, \\beta_{1}\\right) \\mapsto L\\left(\\beta_{0}, \\beta_{1}\\right): & =\\prod_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} \\exp \\left(-\\frac{1}{2 \\sigma^{2}}\\left(y_{i}-\\left(\\beta_{0}+\\beta_{1} x_{i}\\right)\\right)^{2}\\right) \\\\\n& =\\left(2 \\pi \\sigma^{2}\\right)^{-\\frac{n}{2}} \\exp \\left(-\\frac{1}{2 \\sigma^{2}} \\sum_{i=1}^{n}\\left(y_{i}-\\left(\\beta_{0}+\\beta_{1} x_{i}\\right)\\right)^{2}\\right)\n\\end{aligned}\n\\end{equation}\\] hat. Weil für die Exponentialfunktion für \\(a&lt;b \\leq 0\\) gilt, dass \\(\\exp (a)&lt;\\exp (b)\\), wird der Exponentialterm dieser Likelihood-Funktion maximal, wenn der nicht-negative Term \\[\\begin{equation}\nq:=\\sum_{i=1}^{n}\\left(y_{i}-\\left(\\beta_{0}+\\beta_{1} x_{i}\\right)\\right)^{2}\n\\end{equation}\\] minimal und damit \\(-q\\) maximal wird. Im Rahmen des Beweises der Ausgleichsgeradenform haben wir aber schon gezeigt, dass der Term (1.30) für \\[\\begin{equation}\n\\hat{\\beta}_{1}:=\\frac{c_{x y}}{s_{x}^{2}} \\text { und } \\hat{\\beta}_{0}:=\\bar{y}-\\hat{\\beta}_{1} \\bar{x}\n\\end{equation}\\] minimal wird und damit \\(\\hat{\\beta}_{1}\\) und \\(\\hat{\\beta}_{0}\\) die Likelihood-Funktion maximieren.\nIn einem zweiten Schritt betrachten wir nun die Likelihood-Funktion des Modells der einfachen linearen Regression bezüglich \\(\\sigma^{2}\\) an der Stelle von \\(\\hat{\\beta}_{0}\\) und \\(\\hat{\\beta}_{1}\\). Wir erhalten die Likelihood-Funktion \\[\\begin{equation}\nL: \\mathbb{R}_{&gt;0} \\rightarrow \\mathbb{R}_{&gt;0}, \\sigma^{2} \\mapsto L\\left(\\sigma^{2}\\right)=\\left(2 \\pi \\sigma^{2}\\right)^{-\\frac{n}{2}} \\exp \\left(-\\frac{1}{2 \\sigma^{2}} \\sum_{i=1}^{n}\\left(y_{i}-\\left(\\hat{\\beta}_{0}+\\hat{\\beta}_{1} x_{i}\\right)\\right)^{2}\\right)\n\\end{equation}\\] und die entsprechende Log-Likelihood-Funktion \\[\\begin{equation}\n\\ell: \\mathbb{R}_{&gt;0} \\rightarrow \\mathbb{R}, \\sigma^{2} \\mapsto \\ell\\left(\\sigma^{2}\\right)=-\\frac{n}{2} \\ln 2 \\pi-\\frac{n}{2} \\ln \\sigma^{2}-\\frac{1}{2 \\sigma^{2}} \\sum_{i=1}^{n}\\left(y_{i}-\\left(\\hat{\\beta}_{0}+\\hat{\\beta}_{1} x_{i}\\right)\\right)^{2}\n\\end{equation}\\] In Analogie zu der Herleitung des Maximum-Likelihood Schätzers für \\(\\sigma^{2}\\) im Normalverteilungsmodell ergibt sich unter Beachtung von \\[\\begin{equation}\n\\hat{\\mu}=\\hat{\\beta}_{0}+\\hat{\\beta}_{1} x_{i}\n\\end{equation}\\] dann hier \\[\\begin{equation}\n\\hat{\\sigma}^{2}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(y_{i}-\\left(\\hat{\\beta}_{0}+\\hat{\\beta}_{1} x_{i}\\right)\\right)^{2}\n\\end{equation}\\]\n\nIn der Anwendung ist die Maximum-Likelihood Schätzung der Parameter der einfachen linearen Regression also im Wesentlichen mit der Bestimmung der Ausgleichsgeradenparameter identisch, wie folgender R Code zur Schätzung der Parameter für den Beispieldatensatz demonstriert.\n\n# Einlesen des Beispieldatensatzes\nfname       = \"./_data/401-regression.csv\" \nD           = read.table(fname, sep = \",\", header = TRUE)\n\n# Stichprobenstatistiken\nn           = length(D$y_i)                                                     # Anzahl Datenpunkte\nx_bar       = mean(D$x_i)                                                       # Stichprobenmittel der x_i-Werte\ny_bar       = mean(D$y_i)                                                       # Stichprobenmittel der y_i-Werte\ns2x         = var(D$x_i)                                                        # Stichprobenvarianz der  x_i-Werte\ncxy         = cov(D$x_i, D$y_i)                                                 # Stichprobenkovarianz der (x_i,y_i)-Werte\n\n# Parameteterschätzer\nbeta_1_hat  = cxy/s2x                                                           # \\hat{\\beta}_1, Steigungsparameter\nbeta_0_hat  = y_bar - beta_1_hat*x_bar                                          # \\hat{\\beta}_0, Offset Parameter\nsigsqr_hat  = (1/n)*sum((D$y_i-(beta_0_hat+beta_1_hat*D$x_i))^2)                # Varianzparameter\n\n\n# Ausgabe\ncat(\"beta_0_hat:\"  , beta_0_hat,\n    \"\\nbeta_1_hat:\", beta_1_hat,\n    \"\\nsigsqr_hat:\", sqrt(sigsqr_hat))\n\nbeta_0_hat: -6.194704 \nbeta_1_hat: 1.657055 \nsigsqr_hat: 3.536795",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "401-Regression.html#literaturhinweise",
    "href": "401-Regression.html#literaturhinweise",
    "title": "25  Regression",
    "section": "25.3 Literaturhinweise",
    "text": "25.3 Literaturhinweise\nDie Idee der Minimierung einer Summe von quadrierten Abweichungen bei der Anpassung einer Polynomfunktion an beobachtete Werte geht auf die Arbeiten von Legendre (1805) und Gauss (1809) im Kontext der Bestimmung von Planetenbahnen zurück. Eine historische Einordnung dazu gibt Stigler (1981). Der Begriff der Regression geht zurück auf Galton (1886). Stigler (1986) gibt dazu einen ausführlichen historischen Überblick.",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "401-Regression.html#selbstkontrollfragen",
    "href": "401-Regression.html#selbstkontrollfragen",
    "title": "25  Regression",
    "section": "25.4 Selbstkontrollfragen",
    "text": "25.4 Selbstkontrollfragen\n\nGeben Sie die Definition einer linear-affinen Funktion wieder.\nErläutern Sie die Bedeutung der Parameter einer linear-affinen Funktion.\nGeben Sie die Definition des Begriffs der Ausgleichsgerade wieder.\nErläutern Sie die Bedeutung der Funktion der quadrierten vertikalen Abweichungen.\nGeben Sie das Theorem zur Ausgleichsgerade wieder.\nSkizzieren Sie den Beweis des Theorems zur Ausgleichsgeraden.\nGeben Sie die Definition des Begriffs des Ausgleichspolynoms wieder.\nErläutern Sie die Motivation des einfachen linearen Regressionsmodells.\nGeben Sie die Definition des Modell der einfachen linearen Regression wieder.\nGeben Sie das Theorem zur Datenverteilung der einfachen linearen Regression wieder.\nGeben Sie das Theorem zur Maximum-Likelihood Schätzung der einfachen linearen Regression an.\n\n\n\n\n\nGalton, F. (1886). Regression Towards Mediocrity in Hereditary Stature. The Journal of the Anthropological Institute of Great Britain and Ireland, 15, 246. https://doi.org/10.2307/2841583\n\n\nGauss, C. F. (1809). Theoria Motus Corporum Coelestium in Sectionibus Conicis Solem Ambientium. Cambridge University Press.\n\n\nLegendre, A. M. (1805). Nouvelles Methodes Pour La Determination Des Orbites Des Cometes. Didot Paris.\n\n\nStigler, S. M. (1981). Gauss and the Invention of Least Squares. The Annals of Statistics, 9(3). https://doi.org/10.1214/aos/1176345451\n\n\nStigler, S. M. (1986). The History of Statistics: The Measurement of Uncertainty before 1900. Belknap Press of Harvard University Press.",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "402-Korrelation.html",
    "href": "402-Korrelation.html",
    "title": "26  Korrelation",
    "section": "",
    "text": "26.1 Grundlagen\nWir erinnern zunächst an den Begriff der Korrelation zweier Zufallsvariablen.\nDie Zahl \\(\\rho(\\xi, \\upsilon)\\) wird auch Korrelationskoeffizient von \\(\\xi\\) und \\(\\upsilon\\) genannt. Wir haben bereits gesehen, dass \\(-1 \\leq \\rho(\\xi, \\upsilon) \\leq 1\\) gilt und dass \\(\\xi\\) und \\(\\upsilon\\) unkorreliert genannt werden, wenn \\(\\rho(\\xi, \\upsilon)=0\\) ist. Weiterhin haben wir bereits gesehen, dass aus der Unabhängigkeit der Zufallsvariablen \\(\\xi\\) und \\(\\upsilon\\) immer die Unkorreliertheit von \\(\\xi\\) und \\(\\upsilon\\) folgt, dass aber im Allgemeinen aus der Unkorreliertheit von \\(\\xi\\) und \\(\\upsilon\\) nicht die Unabhängigkeit von \\(\\xi\\) und \\(\\upsilon\\) folgt.\nLiegen von zwei Zufallsvariablen Realisationen als ein bivariater Datensatz vor, so kann man anhand folgender Definition die realisationsspezifische Stichprobenkorrelation bestimmen.\nFolgender R Code wertet die Stichprobenkorrelation des Beispieldatensatzes aus.\n# Laden des Beispieldatensatzes\nfname = \"./_data/402-korrelation.csv\"                                           # Dateipfad\nD     = read.table(fname, sep = \",\", header = TRUE)                             # Laden als Dataframe\nx_i   = D$x_i                                                                   # x_i Werte\ny_i   = D$y_i                                                                   # y_i Werte\nn     = length(x_i)                                                             # n\n\n# \"Manuelle\" Berechnung der Stichprobenkorrelation\nx_bar = (1/n)*sum(x_i)                                                          # \\bar{x}\ny_bar = (1/n)*sum(y_i)                                                          # \\bar{y}\ns_x   = sqrt(1/(n-1)*sum((x_i - x_bar)^2))                                      # s_x\ns_y   = sqrt(1/(n-1)*sum((y_i - y_bar)^2))                                      # s_y\nc_xy  = 1/(n-1) * sum((x_i - x_bar) * (y_i - y_bar))                            # c_{xy}\nr_xy  = c_xy/(s_x * s_y)                                                        # r_{xy}\nprint(r_xy)                                                                     # Ausgabe\n\n[1] 0.9378162\n\n# Automatische Berechnung mit cor()\nr_xy  = cor(x_i,y_i)                                                             # r_{xy}\nprint(r_xy)                                                                      # Ausgabe\n\n[1] 0.9378162\nIm Beispieldatensatz sind die Anzahl der Therapiestunden und die Symptomreduktion also mit \\(r_{x y}=0.93\\) hoch korreliert. Allgemein spricht man bei absoluten Werten von \\(r_{x y}\\) größer als etwa 0.70 von hoher Korrelation, bei absoluten Werten von \\(r_{x y}\\) zwischen etwa 0.30 und 0.70 von mittlerer Korrelation und bei absoluten Werten von \\(r_{x y}\\) zwischen 0.00 und 0.30 von niedriger Korrelation. Eine niedrige Korrelation zweier Variablen bedeutet aber nicht zwangsläufig, dass diese Korrelation irrelevant ist (man denke an Gesundheitsrisikofaktoren), genauso wenig wie eine hohe Korrelation zweier Variablen trivial sein kann (man denke an die Korrelation von Körpergröße und Schuhgröße).\nDa die Stichprobenkorrelation lediglich die auf das Intervall \\([-1,1]\\) normalisierte Stichprobenkovarianz \\(c_{x y}\\) ist, wird die Höhe der Stichprobenkorrelation und insbesondere ihr Vorzeichen entscheidend durch die Werte Stichprobenkovarianzsummenterme \\(\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)\\) bestimmt. Dabei ist es entscheidend, wie häufig über die Datenpaare \\(\\left(x_{i}, y_{i}\\right)\\) hinweg die \\(x_{i}\\) und \\(y_{i}\\) gleichartig oder entgegengesetzt von ihren jeweiligen Stichprobenmitteln abweichen. Dies ist schematisch in Abbildung 26.1 dargestellt. Bei häufiger richtungsgleicher Abweichung von ihren jeweiligen Mittelwerten, sowohl in positiver als auch in negativer Richtung, ergibt das Produkt \\(\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)\\) eine positive Zahl, trägt also zu einer positiven Stichprobenkovarianz bei. Bei häufiger entgegengesetzter Abweichung von ihren jeweiligen Mittelwerten ergibt das Produkt \\(\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)\\) häufig eine negative Zahl, trägt also zu einer negativen Stichprobenkovarianz bei. Kommen sowohl richtungsgleiche als auch entgegengesetzte Abweichungen der \\(x_{i}\\) und \\(y_{i}\\) häufig vor, so gleichen sich positive und negative Beiträge zur Stichprobenkovarianzsumme eher aus und es resultiert eine geringe Stichprobenkovarianz bzw. ein Stichprobenkorrelationskoeffizient nahe Null. Abbildung 2.2 zeigt bivariate Datensätze von jeweils \\(n=30\\) Datenpunkten zusammen mit ihren jeweiligen Stichprobenkorrelationskoeffizienten.\nDer Vorteil des Stichprobenkorrelationskoeffizienten gegenüber der Stichprobenkovarianz als Zusammenhangsmaß ist es, dass der absolute Wert des Stichprobenkorrelationskoeffizienten bei linear-affiner Transformation der zugrundeliegende Wertemenge gleich bleibt, wohingegen die Stichprobenkovarianz ihren Wert je nach gewähltem Maßstab ändert. Man sagt deshalb auch, dass der Stichprobenkorrelationkoeffizient maßstabsunabhängig ist. Dies ist die zentrale Aussage folgenden Theorems.",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Korrelation</span>"
    ]
  },
  {
    "objectID": "402-Korrelation.html#sec-grundlagen",
    "href": "402-Korrelation.html#sec-grundlagen",
    "title": "26  Korrelation",
    "section": "",
    "text": "Definition 26.1 (Korrelation) Die Korrelation zweier Zufallsvariablen \\(\\xi\\) und \\(\\upsilon\\) ist definiert als \\[\\begin{equation}\n\\rho(\\xi, \\upsilon):=\\frac{\\mathbb{C}(\\xi, \\upsilon)}{\\mathbb{S}(\\xi) \\mathbb{S}(\\upsilon)}\n\\end{equation}\\] wobei \\(\\mathbb{C}(\\xi, \\upsilon)\\) die Kovarianz von \\(\\xi\\) und \\(\\upsilon\\) und \\(\\mathbb{S}(\\xi)\\) und \\(\\mathbb{S}(\\upsilon)\\) die Standardabweichungen von \\(\\xi\\) und \\(\\upsilon\\), respektive, bezeichnen.\n\n\n\n\nDefinition 26.2 (Stichprobenkorrelation) \\(\\left\\{\\left(x_{1}, y_{1}\\right), \\ldots,\\left(x_{n}, y_{n}\\right)\\right\\} \\subset \\mathbb{R}^{2}\\) sei ein Datensatz. Weiterhin seien:\n\nDie Stichprobenmittel der \\(x_{i}\\) und \\(y_{i}\\) definiert als \\[\\begin{equation}\n\\bar{x}:=\\frac{1}{n} \\sum_{i=1}^{n} x_{i} \\mbox{ und } \\bar{y}:=\\frac{1}{n} \\sum_{i=1}^{n} y_{i}\n\\end{equation}\\]\nDie Stichprobenstandardabweichungen \\(x_{i}\\) und \\(y_{i}\\) definiert als \\[\\begin{equation}\ns_{x}:=\\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}} \\mbox{ und } s_{y}:=\\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2}} .\n\\end{equation}\\]\nDie Stichprobenkovarianz der \\(\\left(x_{1}, y_{1}\\right), \\ldots,\\left(x_{n}, y_{n}\\right)\\) definiert als \\[\\begin{equation}\nc_{x y}:=\\frac{1}{n-1} \\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)\n\\end{equation}\\]\n\nDann ist die Stichprobenkorrelation der \\(\\left(x_{1}, y_{1}\\right), \\ldots,\\left(x_{n}, y_{n}\\right)\\) definiert als \\[\\begin{equation}\nr_{x y}:=\\frac{c_{x y}}{s_{x} s_{y}}\n\\end{equation}\\] und wird auch Pearson’s Stichprobenkorrelationskoeffizient genannt.\n\n\n\n\n\n\n\n\n\n\nAbbildung 26.1: Mechanik der Stichprobenkorrelationssummenterme.\n\n\n\n\n\n\nTheorem 26.1 (Stichprobenkorrelation bei linear affiner Transformation) Für einen Datensatz \\(\\left\\{\\left(x_{i}, y_{i}\\right)\\right\\}_{i=1, \\ldots n} \\subset \\mathbb{R}^{2}\\) sei \\(\\left\\{\\left(\\tilde{x}_{i}, \\tilde{y}_{i}\\right)\\right\\}_{i=1, \\ldots n} \\subset \\mathbb{R}^{2}\\) der linear-affin transformierte Datensatz mit \\[\\begin{equation}\n\\left(\\tilde{x}_{i}, \\tilde{y}_{i}\\right)=\\left(a_{x} x_{i}+b_{x}, a_{y} y_{i}+b_{y}\\right), a_{x}, a_{y} \\neq 0 .\n\\end{equation}\\] gegeben. Dann gilt \\[\\begin{equation}\n\\left|r_{\\tilde{x} \\tilde{y}}\\right|=\\left|r_{x y}\\right| .\n\\end{equation}\\]\n\n\n\n\n\n\n\nAbbildung 26.2: Beispiele bivariater Datensätze und ihrer Stichprobenkorrelationen.\n\n\n\n\nBeweis. \\[\\begin{equation}\n\\begin{aligned}\nr_{\\tilde{x} \\tilde{y}}\n& :=\\frac{\\frac{1}{n-1} \\sum_{i=1}^{n}\\left(\\tilde{x}_{i}-\\bar{\\tilde{x}}\\right)\\left(\\tilde{y}_{i}-\\bar{\\tilde{y}}\\right)}{\\sqrt{\\frac{1}{n-1}\\left(\\sum_{i=1}^{n} \\tilde{x}_{i}-\\bar{\\tilde{x}}\\right)^{2}} \\sqrt{\\frac{1}{n-1}\\left(\\sum_{i=1}^{n} \\tilde{y}_{i}-\\bar{\\tilde{y}}\\right)^{2}}} \\\\\n& =\\frac{\\sum_{i=1}^{n}\\left(a_{x} x_{i}+b_{x}-\\left(a_{x} \\bar{x}+b_{x}\\right)\\right)\\left(a_{y} y_{i}+b_{y}-\\left(a_{y} \\bar{y}+b_{y}\\right)\\right)}{\\sqrt{\\sum_{i=1}^{n}\\left(a_{x} x_{i}+b_{x}-\\left(a_{x} \\bar{x}+b_{x}\\right)\\right)^{2}} \\sqrt{\\sum_{i=1}^{n}\\left(a_{y} y_{i}+b_{y}-\\left(a_{y} \\bar{y}+b_{y}\\right)\\right)^{2}}} \\\\\n& =\\frac{a_{x} a_{y} \\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}{\\sqrt{a_{x}^{2} \\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}} \\sqrt{a_{y}^{2} \\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2}}} \\\\\n& =\\frac{a_{x} a_{y}}{\\left|a_{x}\\right|\\left|a_{y}\\right|} \\frac{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}{\\sqrt{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}} \\sqrt{\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2}}} \\\\\n& =\\frac{a_{x} a_{y}}{\\left|a_{x}\\right|\\left|a_{y}\\right|} \\frac{c_{x y}}{s_{x} s_{y}} \\\\\n& =\\frac{a_{x} a_{y}}{\\left|a_{x}\\right|\\left|a_{y}\\right|} r_{x y} .\n\\end{aligned}\n\\end{equation}\\] Also folgt, durch Durchspielen aller möglichen Vorzeichenfälle, dass \\[\\begin{equation}\n\\left|r_{\\tilde{x} \\tilde{y}}\\right|=\\left|r_{x y}\\right| .\n\\end{equation}\\]",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Korrelation</span>"
    ]
  },
  {
    "objectID": "402-Korrelation.html#sec-korrelation-und-bestimmtheitsmass",
    "href": "402-Korrelation.html#sec-korrelation-und-bestimmtheitsmass",
    "title": "26  Korrelation",
    "section": "26.2 Korrelation und Bestimmheitsmaß",
    "text": "26.2 Korrelation und Bestimmheitsmaß\nDas sogenannte Bestimmtheitsmaß \\(\\mathrm{R}^{2}\\) ist eine beliebte, häufig reportierte Statistik zur Beschreibung der Zusammenhangsstärke der Werte einer unabhängigen und einer abhängigen Variable. Numerisch handelt es sich bei \\(\\mathrm{R}^{2}\\) lediglich um den quadrierten Stichprobenkorrelationskoeffizienten. Ist die seStichprobenkorrelation zum Beispiel \\(r_{x y}=0.5\\), dann ist \\(\\mathrm{R}^{2}=0.5^{2}=0.25\\), ist die Stichprobenkorrelation dagegen \\(r_{x y}=-0.5\\), dann gilt analog \\(\\mathrm{R}^{2}=(-0.5)^{2}=0.25\\). An diesen Beispielen erkennt man, dass \\(\\mathrm{R}^{2}\\) weniger Information über die Rohdaten enthält als \\(r_{x y}\\), da das Vorzeichen und damit die Richtung des Zusammenhangs wegfällt. Per se ist die Angabe von \\(\\mathrm{R}^{2}\\) anstelle von \\(r_{x y}\\) als Deskriptivstatistik zur Beschreibung der Zusammenhangsstärke der Werte von unabhängiger und abhängiger Variable ohne Vorteil. Wir wollen hier trotzdem etwas genauer auf \\(\\mathrm{R}^{2}\\) eingehen, da ein tieferes Verständnis von \\(\\mathrm{R}^{2}\\) einerseits den Einstieg in das Konzept der Varianzzerlegungen erlaubt und andererseits die Zusammenhänge zwischen den Konzepten der Ausgleichsgerade und der Stichprobenkorrelation weiter verdeutlicht. Wir erweitern dazu zunächst die Beschreibung der Ausgleichsgerade aus Kapitel 25.1 durch die Begriffe der erklärten Werte und der Residuen.\n\nDefinition 26.3 (Erklärte Werte und Residuen einer Ausgleichsgerade.) Gegeben sei ein Datensatz \\(\\left\\{\\left(x_{1}, y_{1}\\right), \\ldots,\\left(x_{n}, y_{n}\\right)\\right\\} \\subset \\mathbb{R}^{2}\\) und die zu diesem Datensatz gehörende Ausgleichsgerade \\[\\begin{equation}\nf_{\\hat{\\beta}}: \\mathbb{R} \\rightarrow \\mathbb{R}, x \\mapsto f_{\\hat{\\beta}}(x):=\\hat{\\beta}_{0}+\\hat{\\beta}_{1} x\n\\end{equation}\\] Dann werden für \\(i=1, \\ldots, n\\) \\[\\begin{equation}\n\\widehat{y}_{i}:=\\hat{\\beta}_{0}+\\hat{\\beta}_{1} x_{i}\n\\end{equation}\\] die durch die Ausgleichsgerade erklärten Werte genannt und \\[\\begin{equation}\n\\hat{\\varepsilon}_{i}:=y_{i}-\\hat{y}_{i}\n\\end{equation}\\] werden die Residuen der Ausgleichsgerade genannt.\n\nEtwas allgemeiner formuliert sind die erklärten Werte damit die Datenvorhersage des Modells basierend auf den geschätzten Parameterwerten, während die Residuen die Differenzen zwischen den geschätzten Datenvorhersagen und den beobachteten Datenwerten bezeichnen. Abbildung 26.3 verdeutlicht diese Begriffe am Beispiel der Ausgleichsgerade des Beispieldatensatzes.\n\n\n\n\n\n\nAbbildung 26.3: Ausgleichsgerade, erklärte Werte und Residuen für den Beispieldatensatz..\n\n\n\nMithilfe der Begriffe der erklärten Werte und Residuen lässt sich nun folgende Quadratsummenzerlegung beim Vorliegen einer Ausgleichsgerade eines Datensatzes angeben.\n\nTheorem 26.2 (Quadratsummenzerlegung bei Ausgleichsgerade) Für einen Datensatz \\(\\left\\{\\left(x_{1}, y_{1}\\right), \\ldots,\\left(x_{n}, y_{n}\\right)\\right\\} \\subset \\mathbb{R}^{2}\\) und seine zugehörige Ausgleichsgerade \\(f_{\\hat{\\beta}}\\) seien für \\[\\begin{equation}\n\\bar{y}:=\\frac{1}{n} \\sum_{i=1}^{n} y_{i} \\mbox{ und } \\hat{y}_{i}:=\\hat{\\beta}_{0}+\\hat{\\beta}_{1} x_{i} \\text {, für } i=1, \\ldots, n\n\\end{equation}\\] das Stichprobenmittel der \\(y_{i}\\)-Werte und die durch die Ausgleichsgerade erklärten Werte, respektive. Weiterhin seien\n\ndie Total Sum of Squares definiert als \\[\\begin{equation}\n\\mbox{SQT} := \\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2}\n\\end{equation}\\]\ndie Explained Sum of Squares definiert als \\[\\begin{equation}\n\\mbox{SQE} := \\sum_{i=1}^{n}\\left(\\widehat{y}_{i}-\\bar{y}\\right)^{2}\n\\end{equation}\\]\ndie Residual Sum of Squares definiert als \\[\\begin{equation}\n\\mbox{SQR}  := \\sum_{i=1}^{n}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}\n\\end{equation}\\]\n\nDann gilt \\[\\begin{equation}\n\\mbox{SQT} = \\mbox{SQE} + \\mbox{SQR}\n\\end{equation}\\]\n\n\nBeweis. \\[\\begin{equation}\n\\begin{aligned}\n\\mathrm{SQT} & =\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2} \\\\\n& =\\sum_{i=1}^{n}\\left(y_{i}-\\hat{y}_{i}+\\hat{y}_{i}-\\bar{y}\\right)^{2} \\\\\n& =\\sum_{i=1}^{n}\\left(\\left(y_{i}-\\hat{y}_{i}\\right)+\\left(\\hat{y}_{i}-\\bar{y}\\right)\\right)^{2} \\\\\n& =\\sum_{i=1}^{n}\\left(\\left(y_{i}-\\hat{y}_{i}\\right)^{2}+2\\left(y_{i}-\\hat{y}_{i}\\right)\\left(\\hat{y}_{i}-\\bar{y}\\right)+\\left(\\hat{y}_{i}-\\bar{y}\\right)^{2}\\right) \\\\\n& =\\sum_{i=1}^{n}\\left(\\hat{y}_{i}-\\bar{y}\\right)^{2}+2 \\sum_{i=1}^{n}\\left(y_{i}-\\hat{y}_{i}\\right)\\left(\\hat{y}_{i}-\\bar{y}\\right)+\\sum_{i=1}^{n}\\left(y_{i}-\\hat{y}_{i}\\right)^{2} \\\\\n& =\\mathrm{SQE}+2 \\sum_{i=1}^{n}\\left(y_{i}-\\hat{y}_{i}\\right)\\left(\\hat{y}_{i}-\\bar{y}\\right)+\\mathrm{SQR} \\\\\n& =\\mathrm{SQE}+\\mathrm{SQR}\n\\end{aligned}\n\\end{equation}\\] Dabei ergibt sich die letzte Gleichung mit \\[\\begin{equation}\n\\bar{\\hat{y}}:=\\frac{1}{n} \\sum_{i=1}^{n} \\hat{y}_{i}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(\\hat{\\beta}_{0}+\\hat{\\beta}_{1} x_{i}\\right)=\\hat{\\beta}_{0}+\\hat{\\beta}_{1} \\bar{x}=\\bar{y}-\\hat{\\beta}_{1} \\bar{x}+\\hat{\\beta}_{1} \\bar{x}=\\bar{y}\n\\end{equation}\\] und damit auch \\[\\begin{equation}\n\\bar{\\hat{y}}=\\bar{y} \\Leftrightarrow \\frac{1}{n} \\sum_{i=1}^{n} \\hat{y}_{i}=\\frac{1}{n} \\sum_{i=1}^{n} y_{i} \\Leftrightarrow \\sum_{i=1}^{n} \\hat{y}_{i}=\\sum_{i=1}^{n} y_{i} \\Leftrightarrow \\bar{y} \\sum_{i=1}^{n} \\hat{y}_{i}=\\bar{y} \\sum_{i=1}^{n} y_{i}\n\\end{equation}\\] sowie \\[\\begin{equation}\n\\bar{\\hat{y}}=\\bar{y} \\Leftrightarrow \\frac{1}{n} \\sum_{i=1}^{n} \\hat{y}_{i}=\\frac{1}{n} \\sum_{i=1}^{n} y_{i} \\Leftrightarrow \\sum_{i=1}^{n} y_{i}=\\sum_{i=1}^{n} \\hat{y}_{i} \\Leftrightarrow \\sum_{i=1}^{n} y_{i} \\hat{y}_{i}=\\sum_{i=1}^{n} \\hat{y}_{i} \\hat{y}_{i}\n\\end{equation}\\] aus \\[\\begin{equation}\n\\begin{aligned}\n\\sum_{i=1}^{n}\\left(y_{i}-\\hat{y}_{i}\\right)\\left(\\hat{y}_{i}-\\bar{y}\\right) & =\\sum_{i=1}^{n}\\left(y_{i} \\hat{y}_{i}-y_{i} \\bar{y}-\\hat{y}_{i} \\hat{y}_{i}+\\hat{y}_{i} \\bar{y}\\right) \\\\\n& =\\sum_{i=1}^{n} y_{i} \\hat{y}_{i}-\\sum_{i=1}^{n} y_{i} \\bar{y}-\\sum_{i=1}^{n} \\hat{y}_{i} \\hat{y}_{i}+\\sum_{i=1}^{n} \\hat{y}_{i} \\bar{y} \\\\\n& =\\sum_{i=1}^{n} y_{i} \\hat{y}_{i}-\\sum_{i=1}^{n} \\hat{y}_{i} \\hat{y}_{i}+\\bar{y} \\sum_{i=1}^{n} \\hat{y}_{i}-\\bar{y} \\sum_{i=1}^{n} y_{i} \\\\\n& =0+0 \\\\\n& =0\n\\end{aligned}\n\\end{equation}\\]\n\nDie Begriffsbildungen von Theorem 26.2 erklären sich intuitiv wie folgt:\n\nSQT repräsentiert die Gesamtvariabilität der \\(y_{i}\\)-Werte um ihren Mittelwert \\(\\bar{y}\\).\nSQE repräsentiert die Variabilität der erklärten Werte \\(\\hat{y}_{i}\\) um ihren Mittelwert. Große Werte von SQE repräsentieren damit eine große absolute Steigung der \\(y_{i}\\) mit den \\(x_{i}\\) und kleine Werte von SQE repräsentieren eine kleine absolute Steigung der \\(y_{i}\\) mit den \\(x_{i}\\). SQE ist somit ein Maß für die Stärke des linearen Zusammenhangs der \\(x_{i^{-}}\\) und \\(y_{i}\\)-Werte\nSQR ist die Summe der quadrierten Residuen, denn es gilt \\[\\begin{equation}\n\\mathrm{SQR}:=\\sum_{i=1}^{n}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}:=\\sum_{i=1}^{n} \\hat{\\varepsilon}_{i}^{2}\n\\end{equation}\\]\n\nGroße Werte von SQR repräsentieren damit große Abweichungen der erklärten von den beobachteten \\(y_{i}\\)-Werten und kleine Werte von SQR repräsentieren geringe Abweichungen der erklärten von den beobachteten \\(y_{i}\\)-Werten. SQR ist also ein Maß für die Güte der Beschreibung der Datenmenge durch die Ausgleichsgerade.\nDie zentrale Aussage des Theorem 2.1 ist nun, dass sich die Gesamtstreung der \\(y_{i}\\)-Werte um ihren Mittelwert \\(\\bar{y}\\) gerade aus der Summe der Stärke des linearen Zusammenhangs der \\(x_{i^{-}}\\)und \\(y_{i}\\)-Werte (also des “deterministischen Einflusses” der \\(x_{i}\\) auf die \\(y_{i}\\) ) sowie den den Abweichungen von diesem linearen Zusammenhang (also dem “Rauschen”) zusammensetzt. Obwohl es sich bei SQT formal nicht um ein Varianzmaß handelt, spricht man in diesem Zusammenhang auch oft von einer Varianzzerlegung in erklärte Varianz und Residualvarianz. Dieses Motiv ist ein zentraler Aspekt des Allgemeinen Linearen Modells und wird in späteren Kapiteln erneut aufgegriffen werden. Für den Moment erlaubt Theorem 2.1 nun folgende Defintion des Bestimmtheitsmaßes \\(\\mathrm{R}^{2}\\).\n\nDefinition 26.4 (Bestimmtheitsmaß \\(\\mathrm{R}^{2}\\)) Für einen Datensatz \\(\\left\\{\\left(x_{1}, y_{1}\\right), \\ldots,\\left(x_{n}, y_{n}\\right)\\right\\} \\subset \\mathbb{R}^{2}\\) und seine zugehörige Ausgleichsgerade \\(f_{\\hat{\\beta}}\\) sowie die zugehörigen Explained Sum of Squares SQE und Total Sum of Squares SQT heißt \\[\\begin{equation}\n\\mathrm{R}^{2}:=\\frac{\\mathrm{SQE}}{\\mathrm{SQT}}\n\\end{equation}\\] das Bestimmtheitsmaß oder der Determinationskoeffizient.\n\nFolgendes Theorem liefert nun den oben erwähnten Zusammenhang zwischen dem Bestimmtheitsmaß und der Stichprobenkorrelation.\n\nTheorem 26.3 (Stichprobenkorrelation und Bestimmtheitsmaß) Für einen Datensatz \\(\\left\\{\\left(x_{1}, y_{1}\\right), \\ldots,\\left(x_{n}, y_{n}\\right)\\right\\} \\subset \\mathbb{R}^{2}\\) sei \\(\\mathrm{R}^{2}\\) das Bestimmtheitsmaß und \\(r_{x y}\\) sei die Stichprobenkorrelation. Dann gilt \\[\\begin{equation}\n\\mathrm{R}^{2} = r_{x y}^{2}\n\\end{equation}\\]\n\n\nBeweis. Wir halten zunächst fest, dass mit \\[\\begin{equation}\n\\bar{\\hat{y}}\n:=\\frac{1}{n} \\sum_{i=1}^{n} \\hat{y}_{i}\n=\\frac{1}{n} \\sum_{i=1}^{n}\\left(\\hat{\\beta}_{0}+\\hat{\\beta}_{1} x_{i}\\right)\n=\\hat{\\beta}_{0}+\\hat{\\beta}_{1} \\bar{x}\n=\\bar{y}-\\hat{\\beta}_{1} \\bar{x}+\\hat{\\beta}_{1} \\bar{x}=\\bar{y}\n\\end{equation}\\] folgt, dass \\[\\begin{equation}\n\\begin{aligned}\n\\mathrm{SQE} & =\\sum_{i=1}^{n}\\left(\\hat{y}_{i}-\\bar{y}\\right)^{2} \\\\\n& =\\sum_{i=1}^{n}\\left(\\hat{y}_{i}-\\bar{\\bar{y}}\\right)^{2} \\\\\n& =\\sum_{i=1}^{n}\\left(\\hat{\\beta}_{0}+\\hat{\\beta}_{1} x_{i}-\\hat{\\beta}_{0}-\\hat{\\beta}_{1} \\bar{x}\\right)^{2} \\\\\n& =\\sum_{i=1}^{n}\\left(\\hat{\\beta}_{1}\\left(x_{i}-\\bar{x}\\right)\\right)^{2} \\\\\n& =\\hat{\\beta}_{1}^{2} \\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}\n\\end{aligned}\n\\end{equation}\\] Damit ergibt sich dann \\[\\begin{equation}\n\\begin{aligned}\n\\mathrm{R}^{2} & =\\frac{\\mathrm{SQE}}{\\mathrm{SQT}} \\\\\n& =\\frac{\\sum_{i=1}^{n}\\left(\\hat{y}_{i}-\\bar{y}\\right)^{2}}{\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2}} \\\\\n& =\\hat{\\beta}_{1}^{2} \\frac{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}}{\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2}} \\\\\n& =\\frac{c_{x y}^{2}}{s_{x}^{4}} \\frac{\\frac{1}{n-1} \\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}}{\\frac{1}{n-1} \\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2}} \\\\\n& =\\frac{c_{x y}^{2}}{s_{x}^{4}} \\frac{s_{x}^{2}}{s_{y}^{2}} \\\\\n& =\\frac{c_{x y}^{2}}{s_{x}^{2} s_{y}^{2}} \\\\\n& =\\left(\\frac{c_{x y}}{s_{x} s_{y}}\\right)^{2} \\\\\n& =r_{x y}^{2} .\n\\end{aligned}\n\\end{equation}\\]\n\nMan beachte, dass mit \\(-1 \\leq r_{x y} \\leq 1\\) aus Theorem 26.3 direkt folgt, dass \\(0 \\leq \\mathrm{R}^{2} \\leq 1\\). Nach Definition 26.4 gilt \\(\\mathrm{R}^{2}=0\\) genau dann, wenn \\(\\mathrm{SQE}=0\\) ist. \\(\\mathrm{R}^{2}=0\\) bedeutet also, dass die erklärte Datenvariabilität durch die Ausgleichsgerade gleich Null ist und beschreibt damit den Fall einer denkbar schlechten Datenerklärung durch die Ausgleichsgerade. Andererseits gilt \\(\\mathrm{R}^{2}=1\\) genau dann, wenn \\(\\mathrm{SQE}=\\mathrm{SQT}\\) ist. \\(\\mathrm{R}^{2}=1\\) bedeutet also, dass die Gesamtstreuung gleich der durch die Ausgleichsgerade erklärten Streuung ist und beschreibt den Fall, dass sämtliche Datenvariabilität durch die Ausgleichsgerade erklärt werden kann. Man sagt deshalb auch oft etwas ungenau, dass \\(\\mathrm{R}^{2}\\) die durch die Ausgleichsgerade erklärte Varianz an der Gesamtvarianz der Daten repräsentiert. Neben der jeweiligen Stichprobenkorrelation ist in Abbildung 26.2 auch jeweils das Bestimmtheitsmaß für die bivariaten Beispieldatensätze aufgeführt.",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Korrelation</span>"
    ]
  },
  {
    "objectID": "402-Korrelation.html#sec-korrelation-und-linear-affine-abhaengigkeit",
    "href": "402-Korrelation.html#sec-korrelation-und-linear-affine-abhaengigkeit",
    "title": "26  Korrelation",
    "section": "26.3 Korrelation und linear-affine Abhängigkeit",
    "text": "26.3 Korrelation und linear-affine Abhängigkeit\nDie Tatsache, dass stochastische Unabhängigkeit zwar Unkorreliertheit impliziert, dass umgekehrt die Unkorreliertheit zweier Zufallsvariablen aber nicht ihre stochatische Unabhängigkeit impliziert, deutet daraufhin, dass die Korrelation zweier Zufallsvariablen nur bestimmte Formen der Abhängigkeit zwischen Variablen misst. Abbildung 2.4 verdeutlicht dies anhand dreier Simulationsbeispiele.\n\n\n\n\n\n\nAbbildung 26.4: Korrelation und Abhängigkeit.\n\n\n\nAbbildung 26.4 A zeigt eine Realisation des Modells \\[\\begin{equation}\n\\upsilon_i=x_{i}+\\varepsilon_{i} \\mbox{ mit } \\varepsilon_{i} \\sim N(0,1) \\mbox{ für } i=1,...,n.\n\\end{equation}\\] Die Stichprobenkorrelationen des Datensatzes \\(\\left\\{\\left(x_{i}, y_{i}\\right)\\right\\}_{i=1}^{n}\\) ergibt sich hier zu \\(r_{x y}=0.90\\). Es besteht eine klare Abhängigkeit des Wertes der \\(\\upsilon_i\\) Realisation \\(y_{i}\\) vom Wert \\(x_{i}\\): je höher der Wert von \\(x_{i}\\), desto höher der Erwartungswert für den Wert von \\(\\upsilon_i\\). Abbildung 26.4 B zeigt eine Realisation des Modells \\[\\begin{equation}\n\\upsilon_i=x_{i}^{2}+\\varepsilon_{i} \\mbox{ mit } \\varepsilon_{i} \\sim N(0,1) \\mbox{ für } i=1,...,n.\n\\end{equation}\\] Die Stichprobenkorrelationen des Datensatzes \\(\\left\\{\\left(x_{i}, y_{i}\\right)\\right\\}_{i=1}^{n}\\) ergibt sich hier zu \\(r_{x y} = -0.01\\). Die Stichprobenkorrelation ist also minimal. Es besteht aber auch hier eine klare Abhängigkeit des Wertes der \\(\\upsilon_i\\) Realisation \\(y_{i}\\) vom Wert \\(x_{i}\\) : je höher oder je niedriger der Wert von \\(x_{i}\\), desto höher der Erwartungswert für den Wert von \\(\\upsilon_i\\), es besteht ein quadratischer Zusammenhang. Ein ähnliches Bild ergibt sich beim Betrachten von Abbildung 26.4 C einer Realisation des Modells \\[\\begin{equation}\n\\upsilon_i\n= 8 \\cos \\left(2 x_{i}\\right)+\\varepsilon_{i} \\mbox{ mit } \\varepsilon_{i} \\sim N(0,1) \\mbox{ für } i=1,...,n.\n\\end{equation}\\] Auch hier ergibt sich das Bild einer Abhängigkeit des Wertes der \\(\\upsilon_i\\) Realisation \\(y_{i}\\) vom Wert \\(x_{i}\\), in diesem Fall im Sinne einer zyklischen Abhängigkeit, die Stichprobenkorrelation ist aber mit \\(r_{x y}=-0.01\\) wiederum minimal. Diese Simulationsbeispiele belegen also intuitiv, dass die Stichprobenkorrelation verschwindend gering sein kann, auch wenn klare Abhängigkeiten zwischen zwei Variablen bestehen. Als allgemeines Maß für die Abhängigkeit zweier Variablen ist die Korrelation also ungeeignet. Diesen Umstand zugrunde liegt die Tatsache, dass die Korrelation lediglich ein Maß für den linear-affinen Zusammenhang zweier Zufallsvariablen, nicht aber für ihre stochastische Abhängigkeit ist. Wir formalisieren und präzisieren diese Aussage in folgenden Theorem.\n\nTheorem 26.4 (Korrelation und linear-affine Abhängigkeit) \\(\\xi\\) und \\(\\upsilon\\) seien zwei Zufallsvariablen mit positiver Varianz. Dann besteht genau dann eine lineare-affine Abhängigkeit der Form \\[\\begin{equation}\n\\upsilon=\\beta_{0}+\\beta_{1} \\xi \\mbox{ mit } \\beta_{0}, \\beta_{1} \\in \\mathbb{R}\n\\end{equation}\\] zwischen \\(\\xi\\) und \\(\\upsilon\\), wenn \\[\\begin{equation}\n\\rho(\\xi, \\upsilon)=1 \\mbox{ oder } \\rho(\\xi, \\upsilon)=-1\n\\end{equation}\\] gilt.\n\n\nBeweis. Wir beschränken uns auf den Beweis der Aussage, dass aus \\(\\upsilon=\\beta_{0}+\\beta_{1} \\xi\\) folgt, dass \\(\\rho(\\xi, \\upsilon)= \\pm 1\\) ist. Dazu halten wir zunächst fest, dass mit den Theoremen zu den Eigenschaften von Erwartungswert und Varianz gilt, dass \\[\\begin{equation}\n\\mathbb{E}(\\upsilon) = \\beta_{0}+\\beta_{1} \\mathbb{E}(\\xi)\n\\mbox{ und }\n\\mathbb{V}(\\upsilon) = \\beta_{1}^{2} \\mathbb{V}(\\xi)\n\\end{equation}\\] Wegen \\(\\mathbb{V}(\\xi)&gt;0\\) und \\(\\mathbb{V}(\\upsilon)&gt;0\\) gilt damit \\(\\beta_{1} \\neq 0\\). Es folgt dann \\[\\begin{equation}\n\\beta_{1} &gt; 0\n\\Rightarrow\n\\mathbb{S}(\\upsilon)=\\beta_{1} \\mathbb{S}(\\xi) &gt; 0\n\\mbox{ und }\n\\beta_{1} &lt; 0\n\\Rightarrow\n\\mathbb{S}(\\upsilon) = -\\beta_{1} \\mathbb{S}(\\xi) &gt; 0.\n\\end{equation}\\] Weiterhin gilt \\[\\begin{equation}\n\\begin{aligned}\n\\upsilon -\\mathbb{E}(\\upsilon)\n& = \\beta_{0}+\\beta_{1} \\xi-\\mathbb{E}(\\upsilon) \\\\\n& = \\beta_{0}+\\beta_{1} \\xi-\\beta_{0}-\\beta_{1} \\mathbb{E}(\\xi) \\\\\n& = \\beta_{1} \\xi-\\beta_{1} \\mathbb{E}(\\xi) \\\\\n& = \\beta_{1}(\\xi-\\mathbb{E}(\\xi)) .\n\\end{aligned}\n\\end{equation}\\] Für die Kovarianz von \\(\\xi\\) und \\(\\upsilon\\) ergibt sich also \\[\\begin{equation}\n\\begin{aligned}\n\\mathbb{C}(\\xi,\\upsilon)\n& = \\mathbb{E}((\\upsilon-\\mathbb{E}(\\upsilon))(\\xi-\\mathbb{E}(\\xi))) \\\\\n& = \\mathbb{E}\\left(\\beta_{1}(\\xi-\\mathbb{E}(\\xi))(\\xi-\\mathbb{E}(\\xi))\\right) \\\\\n& = \\beta_{1} \\mathbb{E}\\left((\\xi-\\mathbb{E}(\\xi))^{2}\\right) \\\\\n& = \\beta_{1} \\mathbb{V}(\\xi) .\n\\end{aligned}\n\\end{equation}\\] Damit ergibt für die Korrelation von \\(\\xi\\) und \\(\\upsilon\\) \\[\\begin{equation}\n\\rho(\\xi, \\upsilon)\n= \\frac{\\mathbb{C}(\\xi,\\upsilon)}{\\mathbb{S}(\\xi) \\mathbb{S}(\\upsilon)}\n= \\pm \\frac{\\beta_{1} \\mathbb{V}(\\xi)}{\\mathbb{S}(\\xi) \\beta_{1} \\mathbb{S}(\\xi)}\n= \\pm \\frac{\\beta_{1} \\mathbb{V}(\\xi)}{\\beta_{1} \\mathbb{V}(\\xi)}\n= \\pm 1\n\\end{equation}\\]\n\nDie Korrelation zweier Zufallsvariablen wird also genau dann maximal, wenn zwischen den beiden Zufallsvariablen ein linear-affiner Zusammenhang besteht. Dabei impliziert die linear-affine Abhängigkeit von \\(\\upsilon\\) von \\(\\xi\\) auch immer die linear-affine Abhängigkeit von \\(\\xi\\) von \\(\\upsilon\\), denn \\[\\begin{equation}\n\\upsilon = \\beta_{0} + \\beta_{1} \\xi\n\\Leftrightarrow\n-\\beta_{0} + \\upsilon = \\beta_{1} \\xi\n\\Leftrightarrow\n\\xi = -\\frac{\\beta_{0}}{\\beta_{1}}+\\frac{1}{\\beta_{1}} \\upsilon\n\\Leftrightarrow\n\\xi = \\tilde{\\beta}_{0}+\\tilde{\\beta}_{1} \\upsilon\n\\end{equation}\\] mit \\[\\begin{equation}\n\\tilde{\\beta}_{0} = -\\frac{\\beta_{0}}{\\beta_{1}} \\mbox{ und } \\tilde{\\beta}_{1} = \\frac{1}{\\beta_{1}}.\n\\end{equation}\\]",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Korrelation</span>"
    ]
  },
  {
    "objectID": "402-Korrelation.html#literaturhinweise",
    "href": "402-Korrelation.html#literaturhinweise",
    "title": "26  Korrelation",
    "section": "26.4 Literaturhinweise",
    "text": "26.4 Literaturhinweise\nDer Begriff der Korrelation erscheint, allerdings basierend auf früheren Arbeiten zum Beispiel von Bravais (1844), zunächst bei Galton (1890) (vgl. Stigler (1986)) und wird unter anderem durch die Arbeiten von Pearson (1895), Pearson (1896), Pearson (1900), Pearson (1901) im Kontext multivariater Normalverteilungen weiter ausgearbeitet. Eine frühe Studie zum Verhältnis von Korrelation und Kausalität ist Wright (1921).",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Korrelation</span>"
    ]
  },
  {
    "objectID": "402-Korrelation.html#selbstkontrollfragen",
    "href": "402-Korrelation.html#selbstkontrollfragen",
    "title": "26  Korrelation",
    "section": "26.5 Selbstkontrollfragen",
    "text": "26.5 Selbstkontrollfragen\n\nGeben Sie die Definition der Korrelation zweier Zufallsvariablen wieder.\nGeben Sie die Definitionen von Stichprobenmittel, Stichprobenstandardabweichung, Stichprobenkovarianz und Stichprobenkorrelation wieder.\nErläutern Sie anhand der Mechanik der Kovariationsterme, wann eine Stichprobenkorrelation einen hohen absoluten Wert annimmt, einen hohen positiven Wert annimmt, einen hohen negativen Wert annimmt und einen niedrigen Wert annimmt.\nGeben Sie das Theorem zur Stichprobenkorrelation bei linear-affinen Transformationen wieder.\nErläutern Sie das Theorem zur Stichprobenkorrelation bei linear-affinen Transformationen.\nGeben Sie die Definitionen von erklärten Werten und Residuen einer Ausgleichsgerade wieder.\nGeben Sie das Theorem zur Quadratsummenzerlegung bei einer Ausgleichsgerade wieder.\nErläutern Sie die intuitiven Bedeutungen von SQT, SQE und SQR.\nGeben Sie die Definition des Bestimmtheitsmaßes \\(\\mathrm{R}^{2}\\) wieder.\nGeben Sie das Theorem zum Zusammenhang von Stichprobenkorrelation und Bestimmtheitsmaß wieder.\nErläutern Sie die Bedeutung von hohen und niedrigen \\(\\mathrm{R}^{2}\\) Werten im Lichte der Ausgleichsgerade.\nGeben Sie das Theorem zum Zusammenhang von Korrelation und linear-affiner Abhängigkeit wieder.\n\n\n\n\n\nBravais, A. (1844). Analyse Mathématique : Sur Les Probabilités Des Erreurs de Situation d’un Point.\n\n\nGalton, F. (1890). Kinship and Correlation. Statistical Science, 4(2). https://doi.org/10.1214/ss/1177012581\n\n\nPearson, K. (1895). Note on Regression and Inheritance in the Case of Two Parents. Proceedings of the Royal Society of London, 5, 240–242. https://www.jstor.org/stable/115794\n\n\nPearson, K. (1896). Mathematical Contributions to the Theory of Evolution. III. Regression, Heredity, and Panmixia. Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character, 18, 253–318. https://www.jstor.org/stable/90707\n\n\nPearson, K. (1900). On the Criterion That a given System of Deviations from the Probable in the Case of a Correlated System of Variables Is Such That It Can Be Reasonably Supposed to Have Arisen from Random Sampling. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 50(302), 157–175. https://doi.org/10.1080/14786440009463897\n\n\nPearson, K. (1901). On Lines and Planes of Closest Fit to Systems of Points in Space. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 2(11), 559–572. https://doi.org/10.1080/14786440109462720\n\n\nStigler, S. M. (1986). The History of Statistics: The Measurement of Uncertainty before 1900. Belknap Press of Harvard University Press.\n\n\nWright, S. (1921). Correlation and Causation. Journal of Agriculture Research, 20(7), 557–585.",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Korrelation</span>"
    ]
  },
  {
    "objectID": "403-Modellformulierung.html",
    "href": "403-Modellformulierung.html",
    "title": "27  Modellformulierung",
    "section": "",
    "text": "27.1 Allgemeine Theorie\nWir definieren das Allgemeine Lineare Modell (ALM) wie folgt.\nIn Gleichung 27.1 bezeichnen wir \\(X\\beta \\in \\mathbb{R}^{n}\\) als den deterministischen Aspekt des ALMs und \\(\\varepsilon\\) als den probabilistischen Aspekt des ALMs. Das ALM postuliert also, dass Daten aus der Addition eines deterministischen Aspektes \\(X\\beta \\in \\mathbb{R}^{n}\\) unter der Addition eines multivariat normalverteilten probabilistischen Aspektes \\(\\varepsilon\\) zustande kommen. Man beachte, dass \\(X\\beta \\in \\mathbb{R}^{n}\\) ein \\(n\\)-dimensionaler Vektor und \\(\\varepsilon\\) ein \\(n\\)-dimensionaler Zufallsvektor ist. Der resultierende Vektor \\(\\upsilon\\) ist ein Zufallsvektor, weil er aus der Addition des Zufallsvektors \\(\\varepsilon\\) zu dem Vektor \\(X\\beta \\in \\mathbb{R}^{n}\\) resultiert. Das ALM ist also ein probabilitisches Modell bei dem durch \\(\\upsilon\\) vorliegende Datensätze modelliert werden. Generativ betrachtet entsteht im ALM ein Datensatz \\(y \\in \\mathbb{R}^{n}\\) als Realisierung von \\(\\upsilon\\) dann durch Addition des deterministischen Modellaspekts und einer nicht direkt beobachtbaren Realisierung \\(e \\in \\mathbb{R}^{n}\\) von \\(\\varepsilon\\), \\[\\begin{equation}\ny=X\\beta+e\n\\end{equation}\\] mit \\(y \\in \\mathbb{R}^{n}, X\\beta \\in \\mathbb{R}^{n}\\) und \\(e \\in \\mathbb{R}^{n}\\).\nDie Gesamtzahl an Parametern des ALMs beträt \\(p+1\\), bestehend aus \\(p\\) skalaren Betaparametern und einem Varianzparameter \\(\\sigma^{2}\\). Der Betaparametervektor \\(\\beta \\in \\mathbb{R}^{p}\\) wird dabei auch Gewichtsvektor oder Effektvektor genannt. Seine Einträge wichten die Einträge der Spalten der Designmatrix \\(X \\in \\mathbb{R}^{n \\times p}\\) in der Erzeugung des deterministischen Modellaspekts \\(X\\beta \\in \\mathbb{R}^{n}\\). Die Spalten der Designmatrix werden in unterschiedlichen Kontexten unterschiedlich bezeichnet, gebräuchliche Bezeichungen sind zum Beispiel Prädiktoren, Regressoren oder Kovariaten. Allgemein betrachtet modellieren die Spalten der Designmatrix unabhängige Variablen und der Datenvektor abhängige Variablen.\nMan beachte, dass der Kovarianzmatrixparameter von \\(\\varepsilon\\) als sphärisch angenommen wird. Damit folgt direkt, dass die \\(\\varepsilon_{1}, \\ldots, \\varepsilon_{n}\\) unabhängige normalverteilte Zufallsvariablen mit identischem Varianzparameter sind. Weil für \\(\\varepsilon\\) zusätzlich der Erwartungswertparameter als \\(0_{n} \\in \\mathbb{R}^{n}\\) angenommen wird, sind die \\(\\varepsilon_{1}, \\ldots, \\varepsilon_{n}\\) auch identisch normalverteilte Zufallsvariablen. Wenn \\(x_{ij} \\in \\mathbb{R}\\) das \\(ij\\)te Element der Designmatrix \\(X \\in \\mathbb{R}^{n \\times p}\\) bezeichnet, dann gilt damit für jede Komponente \\(\\upsilon_{i}, i=1, \\ldots, n\\) von \\(\\upsilon\\) nach Gleichung 27.1, dass \\[\\begin{equation}\n\\upsilon_{i}\n= x_{i 1}\\beta_{1} + x_{i2}\\beta_{2} + \\cdots + x_{ip}\\beta_{p} + \\varepsilon_{i}\n\\mbox{ mit }\n\\varepsilon_{1},\\ldots,\\varepsilon_{n} \\sim N\\left(0, \\sigma^{2}\\right).\n\\end{equation}\\] Die in Gleichung 27.1 implizite Verteilung des Datenvektors \\(\\upsilon\\) halten wir in folgendem Theorem fest.\nIm ALM sind die Daten \\(\\upsilon\\) also ein \\(n\\)-dimensionaler normalverteilter Zufallsvektor mit Erwartungswertparameter \\(\\mu=X\\beta \\in \\mathbb{R}^{n}\\) und Kovarianzmatrixparameter \\(\\sigma^{2} I_{n} \\in \\mathbb{R}^{n \\times n}\\). Das ALM ist also eine multivariate Normalverteilung deren Erwartungswertparameter mithilfe einer Designmatrix und eines Betaparametervektors parameterisiert ist. Weiterhin sind die Komponenten \\(\\upsilon_{1}, \\ldots, \\upsilon_{n}\\) von \\(\\upsilon\\), also die Zufallsvariablen, die skalare Datenpunkte modellieren, damit unabhängige normalverteilte Zufallsvariablen der Form \\[\\begin{equation}\n\\upsilon_{i} \\sim N\\left((X\\beta)_{i}, \\sigma^{2}\\right) \\mbox{ für } i=1, \\ldots, n.\n\\end{equation}\\] Da im Allgemeinen aber \\((X\\beta)_{i} \\neq(X\\beta)_{j}\\) für \\(i \\neq j\\) gilt, sind die \\(\\upsilon_{i}, i=1, \\ldots, n\\) im Allgemeinen aber nicht identisch verteilt. Das Szenario unabhängig und identisch normalverteilter Zufallsvariablen kann aber natürlich als Spezialfall des ALMs formuliert werden.",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Modellformulierung</span>"
    ]
  },
  {
    "objectID": "403-Modellformulierung.html#allgemeine-theorie",
    "href": "403-Modellformulierung.html#allgemeine-theorie",
    "title": "27  Modellformulierung",
    "section": "",
    "text": "Definition 27.1 (Allgemeines Lineares Modell) Es sei \\[\n\\upsilon = X\\beta + \\varepsilon\n\\tag{27.1}\\] wobei\n\n\\(\\upsilon\\) ein \\(n\\)-dimensionaler beobachtbarer Zufallsvektor ist, der Daten genannt wird,\n\\(X \\in \\mathbb{R}^{n \\times p}\\) für \\(n&gt;p\\) und \\(\\mbox{rg}(X)=p\\) eine Matrix ist, die Designmatrix genannt wird,\n\\(\\beta \\in \\mathbb{R}^{p}\\) ein unbekannter Parametervektor ist, der Betaparametervektor genannt wird,\n\\(\\varepsilon\\) ein \\(n\\)-dimensionaler nicht-beobachtbarer Zufallsvektor ist, der Zufallsfehler genannt wird und für den angenommen wird, dass mit einem unbekannten Varianzparameter \\(\\sigma^{2}&gt;0\\) gilt, dass \\[\\begin{equation}\n\\varepsilon \\sim N\\left(0_{n}, \\sigma^{2} I_{n}\\right)\n\\end{equation}\\]\n\nDann heißt Gleichung 27.1 Allgemeines Lineares Modell (ALM).\n\n\n\n\n\nTheorem 27.1 (Datenverteilung des Allgemeinen Linearen Modells) Es sei \\[\\begin{equation}\n\\upsilon=X\\beta+\\varepsilon \\mbox{ mit } \\varepsilon \\sim N\\left(0_{n},\\sigma^{2} I_{n}\\right)\n\\end{equation}\\] das ALM. Dann gilt \\[\\begin{equation}\n\\upsilon \\sim N\\left(\\mu, \\sigma^{2} I_{n}\\right) \\mbox{ mit } \\mu:=X\\beta \\in \\mathbb{R}^{n}\n\\end{equation}\\]\n\n\nBeweis. Mit dem Theorem zur linear-affinen Transformation von multivariaten Normalverteilungen gilt für \\[\\begin{equation}\n\\varepsilon \\sim N\\left(0_{n}, \\sigma^{2} I_{n}\\right) \\mbox{ und } \\upsilon  := I_{n} \\varepsilon + X\\beta,\n\\end{equation}\\] dass \\[\\begin{equation}\n\\upsilon\n\\sim N\\left(I_{n} 0_{n}+X\\beta, I_{n}\\left(\\sigma^{2} I_{n}\\right) I_{n}^{T}\\right)\n= N\\left(X\\beta, \\sigma^{2} I_{n}\\right).\n\\end{equation}\\] Mit der Definition \\(\\mu:=X\\beta \\in \\mathbb{R}^{n}\\) folgt die Aussage des Theorems dann direkt.\n\n\n\nBeispiel (1) Unabhängige und identisch normalverteilte Zufallsvariablen\nWir betrachten das Szenario von \\(n\\) unabhängigen und identisch normalverteilten Zufallsvariablen mit Erwartungswertparameter \\(\\mu \\in \\mathbb{R}\\) und Varianzparameter \\(\\sigma^{2}\\), \\[\n\\upsilon_{1}, \\ldots, \\upsilon_{n} \\sim N\\left(\\mu, \\sigma^{2}\\right).\n\\tag{27.2}\\] Dann gilt, dass Gleichung 27.2 äquivalent ist zu \\[\\begin{equation}\n\\upsilon_{i} = \\mu+\\varepsilon_{i},\n\\varepsilon_{i} \\sim N\\left(0, \\sigma^{2}\\right)\n\\mbox{ für } i=1, \\ldots, n\n\\mbox{ mit unabhängigen } \\varepsilon_{i}.\n\\end{equation}\\] In Matrixschreibweise ist dies wiederum äquivalent zu dem ALM Spezialfall \\[\\begin{equation}\n\\upsilon \\sim N\\left(X\\beta, \\sigma^{2} I_{n}\\right)\n\\mbox{ mit } X:=1_{n} \\in \\mathbb{R}^{n \\times 1}, \\beta:=\\mu \\in \\mathbb{R}^{1}, \\sigma^{2}&gt;0.\n\\end{equation}\\] In R können Realisierungen des ALMs leicht mithilfe eines Zufallszahlengenerators für multivariate Normalverteilungen durch Spezifikation der entsprechenden Erwartungs- und Kovarianzparameter gewonnen werden. Folgender R Code zeigt, wie \\(n\\) unabhängig und identisch normalverteilte skalare Datenpunkte im Sinne des ALMs realisiert werden können. Man beachte, dass \\(n\\) skalare Datenpunkte dabei einer Realisierung des ALMs entsprechen.\n\n# Modellformulierung\nlibrary(MASS)                                                                   # Multivariate Normalverteilung\nset.seed(0)                                                                     # Zufallzahlengeneratorseed\nn      = 12                                                                     # Anzahl von Datenpunkten\np      = 1                                                                      # Anzahl von Betaparametern\nX      = matrix(rep(1,n), nrow = n)                                             # Designmatrix\nI_n    = diag(n)                                                                # n x n Einheitsmatrix\nbeta   = 2                                                                      # wahrer, aber unbekannter, Betaparameter\nsigsqr = 1                                                                      # wahrer, aber unbekannter, Varianzparameter\n\n# Datenrealisierung\ny      = mvrnorm(1, X %*% beta, sigsqr*I_n)                                     # eine Realisierung eines n-dimensionalen ZVs\n\n\n\nRealisierungen:  1.2 2.76 4.4 1.99 1.71 1.07 0.46 2.41 3.27 3.33 1.67 3.26\n\n\n\n\nBeispiel (2) Einfache lineare Regression\nWir betrachten das generative Modell der einfachen linearen Regression \\[\\begin{equation}\ny_{i}=\\beta_{0}+\\beta_{1} x_{i}+\\varepsilon_{i}, \\varepsilon_{i} \\sim N\\left(0, \\sigma^{2}\\right) \\mbox{ für } i=1, \\ldots, n \\text {, }\n\\end{equation}\\] Wir haben bereits gesehen, dass dieses Modell äquivalent ist zu dem Normalverteilungsmodell der Regression \\[\\begin{equation}\n\\upsilon_{i} \\sim N\\left(\\mu_{i}, \\sigma^{2}\\right) \\mbox{ mit } \\mu_{i}:=\\beta_{0}+\\beta_{1} x_{i} \\mbox{ für } i=1, \\ldots, n.\n\\end{equation}\\] In Matrixschreibweise ist dies wiederum äquivalent zu dem ALM Spezialfall \\[\n\\upsilon \\sim N\\left(X\\beta, \\sigma^{2} I_{n}\\right) \\mbox{ mit } X:=\\left(\\begin{array}{cc}\n1 & x_{1} \\\\\n1 & x_{2} \\\\\n\\vdots & \\vdots \\\\\n1 & x_{n}\n\\end{array}\\right) \\in \\mathbb{R}^{n \\times 2}, \\beta:=\\left(\\begin{array}{c}\n\\beta_{0} \\\\\n\\beta_{1}\n\\end{array}\\right) \\in \\mathbb{R}^{2}, \\sigma^{2}&gt;0.\n\\tag{27.3}\\] R Code zur Simulation von Realisierungen einer einfachen linearen Regression hat dementsprechend eine sehr ähnliche Struktur wie obiger R Code zur Simulation von Realisierungen von \\(n\\) unabhängig und identisch normalverteilten Zufallsvariablen.\n\n# Modellformulierung\nlibrary(MASS)                                                                   # Multivariate Normalverteilung\nset.seed(0)                                                                     # Zufallzahlengeneratorseed\nn      = 10                                                                     # Anzahl von Datenpunkten\np      = 2                                                                      # Anzahl von Betaparametern\nx      = 1:n                                                                    # Prädiktorwerte\nX      = matrix(c(rep(1,n),x), nrow = n)                                        # Designmatrix\nI_n    = diag(n)                                                                # n x n Einheitsmatrix\nbeta   = matrix(c(0,1), nrow = p)                                               # wahrer, aber unbekannter, Betaparameter\nsigsqr = 1                                                                      # wahrer, aber unbekannter, Varianzparameter\n\n# Datenrealisierung\ny      = mvrnorm(1, X %*% beta, sigsqr*I_n)                                     # eine Realisierung eines n-dimensionalen ZVs\n\n\n\nRealisierungen:  3.4 1.99 2.71 3.07 3.46 6.41 8.27 9.33 8.67 11.26\n\n\nWir visualisieren obige Realisierung in Abbildung 27.1.\n\n\n\n\n\n\nAbbildung 27.1: Realisierung einer einfachen linearen Regression.",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Modellformulierung</span>"
    ]
  },
  {
    "objectID": "403-Modellformulierung.html#identifizierbarkeit-und-schätzbarkeit",
    "href": "403-Modellformulierung.html#identifizierbarkeit-und-schätzbarkeit",
    "title": "27  Modellformulierung",
    "section": "27.2 Identifizierbarkeit und Schätzbarkeit",
    "text": "27.2 Identifizierbarkeit und Schätzbarkeit\nWir haben oben gesehen, dass die Datenverteilung des ALM durch \\[\\begin{equation}\n\\upsilon \\sim N\\left(\\mu, \\sigma^{2} I_{n}\\right)\n\\mbox{ mit } \\mu:=X\\beta \\in \\mathbb{R}^{n} \\mbox{ für } X \\in \\mathbb{R}^{n \\times p}, \\beta \\in \\mathbb{R}^{p}\n\\end{equation}\\] gegeben ist. Das ALM ist also eine multivariate Normalverteilung mit einer speziellen Erwartungswertparameterisierung. Um die Begriffe der Identifizierbarkeit und Schätzbarkeit im Kontext von ALMs einzuführen ist es hilfreich den Begriff der Parametrisierung einer multivariaten Normalverteilung zunächst etwas zu verallgemeinern.\n\nDefinition 27.2 (Betaparameterisieruhg) \\(N\\left(\\mu, \\sigma^{2} I_{n}\\right)\\) sei eine multivariate Normalverteilung mit sphärischem Kovarianzmatrixparameter. Dann bezeichnen wir eine multivariate, vektorwertige Funktion der Form \\[\\begin{equation}\nf: \\mathbb{R}^{p} \\rightarrow \\mathbb{R}^{n}, \\beta \\mapsto f(\\beta)=: \\mu\n\\end{equation}\\] als eine Betaparametrisierung von \\(\\mu\\).\n\nDas ALM beruht offenbar auf der Designmatrix-abhängigen Betaparametrisierung \\[\\begin{equation}\nf: \\mathbb{R}^{p} \\rightarrow \\mathbb{R}^{n}, \\beta \\mapsto f(\\beta):=X\\beta.\n\\end{equation}\\] Mit Hilfe des Begriffs der Betaparametrisierung können wir nun den Begriff der Betaparameteridentifizierbarkeit formulieren:\n\nDefinition 27.3 (Betaparameteridentifizierbarkeit) \\(N\\left(\\mu, \\sigma^{2} I_{n}\\right)\\) sei eine multivariate Normalverteilung mit sphärischem Kovarianzmatrixparameter und \\(f\\) sei eine Betaparametrisierung von \\(\\mu\\). \\(\\beta\\) heißt dann und nur dann identifizierbar, wenn für beliebige \\(\\beta_{1}, \\beta_{2} \\in \\mathbb{R}^{p}\\) gilt, dass aus \\(f\\left(\\beta_{1}\\right)=f\\left(\\beta_{2}\\right)\\) folgt, dass \\(\\beta_{1}=\\beta_{2}\\) gilt.\n\nDie Betaparameter allgemeiner linearer Modelle, deren Designmatrix vollen Rang hat, sind identifizierbar. Dies ist die Aussage folgenden Theorems:\n\nTheorem 27.2 (Betaparameteridentifizierbarkeit bei vollem Designmatrixrang) \\(\\upsilon \\sim N\\left(X\\beta, \\sigma^{2} I_{n}\\right)\\) sei die Datenverteilung eines ALMs mit \\(\\mbox{rg}(X)=p\\). Dann ist \\(\\beta\\) identifizierbar.\n\n\nBeweis. Für \\(X \\in \\mathbb{R}^{n \\times p}\\) impliziert \\(\\mbox{rg}(X)=p\\), dass \\(\\left(X^{T} X\\right) \\in \\mathbb{R}^{p \\times p}\\) eine invertierbare Matrix ist. Dann aber gilt für beliebige \\(\\beta_{1}, \\beta_{2} \\in \\mathbb{R}^{p}\\) \\[\\begin{equation}\n\\begin{aligned}\nf\\left(\\beta_{1}\\right) = f\\left(\\beta_{2}\\right)\n& \\Leftrightarrow X\\beta_{1}=X\\beta_{2} \\\\\n& \\Leftrightarrow X^{T} X\\beta_{1}=X^{T} X\\beta_{2} \\\\\n& \\Leftrightarrow\\left(X^{T} X\\right)^{-1} X^{T} X\\beta_{1}=\\left(X^{T} X\\right)^{-1} X^{T} X\\beta_{2} \\\\\n& \\Leftrightarrow \\beta_{1}=\\beta_{2}.\n\\end{aligned}\n\\end{equation}\\]\n\nIm Rahmen der Analyse schätzbarer Funktionen benötigen wir weiterhin den Begriff der Identifizierbarkeit von vektorwertigen Funktionen der Betaparameter. Wir definieren:\n\nDefinition 27.4 (Identifizierbarkeit von Funktionenen der Betaparameter) \\(N\\left(\\mu, \\sigma^{2} I_{n}\\right)\\) sei eine multivariate Normalverteilung mit sphärischem Kovarianzmatrixparameter und \\(f\\) sei eine Betaparametrisierung von \\(\\mu\\). Weiterhin sei \\[\\begin{equation}\ng: \\mathbb{R}^{p} \\rightarrow \\mathbb{R}^{k}, \\beta \\mapsto g(\\beta)\n\\end{equation}\\] eine Funktion des Betaparametervektors. \\(g\\) heißt dann und nur dann identifizierbar, wenn für beliebige \\(f\\left(\\beta_{1}\\right), f\\left(\\beta_{2}\\right) \\in \\mathbb{R}^{n}\\) gilt, dass aus \\(f\\left(\\beta_{1}\\right)=f\\left(\\beta_{2}\\right)\\) folgt, dass \\(g\\left(\\beta_{1}\\right)=g\\left(\\beta_{2}\\right)\\).\n\nSchätzbare Funktionen sind lineare Funktionen von \\(\\beta\\), die identifizierbar sind. Allgemein gilt folgendes Theorem:\n\nTheorem 27.3 (Identifizierbare Betaparameterfunktionen) \\(N\\left(\\mu, \\sigma^{2} I_{n}\\right)\\) sei eine multivariate Normalverteilung mit sphärischem Kovarianzmatrixparameter und \\(f\\) sei eine Betaparametrisierung von \\(\\mu\\). Weiterhin sei \\[\\begin{equation}\ng: \\mathbb{R}^{p} \\rightarrow \\mathbb{R}^{k}, \\beta \\mapsto g(\\beta)\n\\end{equation}\\] eine Funktion des Betaparametervektors. Die Funktion \\(g\\) ist dann und nur dann identifizierbar, wenn \\(g\\) eine Funktion von \\(f\\) ist, wenn also eine Funktion \\(\\phi\\) existiert, so dass \\[\\begin{equation}\ng = \\phi \\circ f\n\\end{equation}\\]\n\n\nBeweis. Wir zeigen die Aussage lediglich in eine Richtung. \\(\\Rightarrow\\) Wir nehmen an, es existiert eine Funktion \\(\\phi\\), so dass \\[\\begin{equation}\ng=\\phi \\circ f .\n\\end{equation}\\] Dann impliziert die Tatsache, dass eine Funktion einem Argument genau einen Funktionswert zuordnet, dass gilt \\[\\begin{equation}\nf\\left(\\beta_{1}\\right)=f\\left(\\beta_{2}\\right)\n\\Leftrightarrow\n\\phi\\left(f\\left(\\beta_{1}\\right)\\right)=\\phi\\left(f\\left(\\beta_{2}\\right)\\right)\n\\Leftrightarrow g\\left(\\beta_{1}\\right)=g\\left(\\beta_{2}\\right)\n\\end{equation}\\] Also ist \\(g\\) identifizierbar, denn aus \\(f\\left(\\beta_{1}\\right)=f\\left(\\beta_{2}\\right)\\) folgt, dass \\(g\\left(\\beta_{1}\\right)=g\\left(\\beta_{2}\\right)\\).\n\nWie oben bereits erwähnt sind schätzbare Funktionen lineare Funktionen von \\(\\beta\\), die identifizierbar sind. Die klassische Definition einer schätzbaren Funktion ist folgende.\n\nDefinition 27.5 (Schätzbare Funktion) \\(N\\left(X\\beta, \\sigma^{2} I_{n}\\right)\\) sei ein ALM. Dann heißt eine lineare Funktion \\[\\begin{equation}\ng: \\mathbb{R}^{p} \\rightarrow \\mathbb{R}^{k}, \\beta \\mapsto g(\\beta):=C^{T} \\beta\n\\end{equation}\\] mit \\(C \\in \\mathbb{R}^{p \\times k}\\) schätzbar, wenn eine Matrix \\(P \\in \\mathbb{R}^{n \\times k}\\) existiert, so dass \\[\\begin{equation}\nC^{T} \\beta=P^{T} X\\beta.\n\\end{equation}\\]\n\nDiese Definition erschließt sich wie folgt: Nach dem Theorem zu identifizierbaren Funktionen muss eine identifizierbare lineare Funktion von \\(\\beta\\) eine Funktion der Form \\[\\begin{equation}\ng(\\beta)=(\\phi \\circ f)(\\beta)\n\\end{equation}\\] sein. Da weiterhin gilt, dass für ein ALM \\(f(\\beta)=X\\beta\\) und dass \\(g\\) eine lineare Funktion ist, also mithilfe einer Matrix \\(C^{T}\\) geschrieben werden kann, muss auch \\(\\phi\\) eine lineare Funktion sein. Damit kann aber auch \\(\\phi\\) geschrieben werden als \\[\\begin{equation}\n\\phi(f(\\beta))=\\phi(X\\beta)=P^{T} X\\beta\n\\end{equation}\\] mit einer geeigneten Matrix \\(P \\in \\mathbb{R}^{n \\times k}\\).",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Modellformulierung</span>"
    ]
  },
  {
    "objectID": "403-Modellformulierung.html#designspektrum",
    "href": "403-Modellformulierung.html#designspektrum",
    "title": "27  Modellformulierung",
    "section": "27.3 Designspektrum",
    "text": "27.3 Designspektrum\nDie Wahl von Designmatrix und Betaparameter öffnet eine große Freiheit zur Implementation verschiedenster Erwartungswertparameterszenarien der ALM Datenverteilung. Prinzipiell liegen dabei alle speziellen Designs in einem Kontinuum zwischen folgenden beiden Extrema:\n\nDie Erwartungswerte aller Datenvariablen sind identisch, d.h. \\[\\begin{equation}\n\\upsilon_{i} \\sim N\\left(\\mu, \\sigma^{2}\\right) \\text { u.i.v. für } i=1, \\ldots, n\n\\end{equation}\\] also \\[\\begin{equation}\n\\upsilon=X\\beta+\\varepsilon \\operatorname{mit} X:=1_{n} \\in \\mathbb{R}^{n \\times 1}, \\beta:=\\mu \\in \\mathbb{R}, \\varepsilon \\sim N\\left(0_{n}, \\sigma^{2} I_{n}\\right) .\n\\end{equation}\\]\nDie Erwartungswerte aller Datenvariablen sind paarweise verschieden, d.h. \\[\\begin{equation}\n\\upsilon_{i} \\sim N\\left(\\mu_{i}, \\sigma^{2}\\right) \\text { u.v. für } i=1, \\ldots, n \\text {, }\n\\end{equation}\\] also \\[\\begin{equation}\n\\upsilon=X\\beta+\\varepsilon \\operatorname{mit} X:=I_{n} \\in \\mathbb{R}^{n \\times n}, \\beta:=\\left(\\mu_{1}, \\ldots, \\mu_{n}\\right)^{T} \\in \\mathbb{R}^{n}, \\varepsilon \\sim N\\left(0_{n}, \\sigma^{2} I_{n}\\right).\n\\end{equation}\\]\n\nIn Szenario (1) wird jegliche Datenvariabilität dem Zufallsfehlerterm zugeschrieben, in Szenario (2) wird dagegen jegliche Datenvariabilität dem Erwartungswertparameter zugeschrieben. Beide Extremszenarien sind wissenschaftlich nicht ergiebig, da sie keine theoriegeleitete systematische Abhängigkeit zwischen unabhängigen und abhängigen Variablen repräsentieren. Alle im weiteren Verlauf betrachteten ALM Designs liegen damit zwischen den beiden Extremszenarien und repräsentieren verschiedene Formen der systematischen Abhängigkeit zwischen unabhängigen und abhängigen Variablen. Insbesondere unterscheidet man\n\nFaktorielle Designs, bei denen die Designmatrix im Wesentlichen nur 1en und 0en und manchmal -1en enthält. In diesem Fall repräsentieren die Betaparameter Gruppenerwartungswerte und wir werden sehen, dass die Betaparameterschätzer in der Repräsentationen von Gruppenstichprobenmitteln resultieren. Man sagt manchmal, dass faktorielle Designs der Untersuchung von Unterschiedshypothesen dienen. Beispiele für faktorielle Designs sind verschiedene Designs zur Implementation von \\(T\\)-Tests und Varianzanalysen.\nParameterische Designs, bei denen die Designmatrix aus Spalten mit kontinuierlichen reellen Werten besteht. Vor allem in diesem Kontext werden die Spalten der Designmatrix oft als Regressoren oder Prädiktoren bezeichnet. In diesem Fall repräsentieren die Betaparameter partielle Steigungsparameter und wir werden sehen, dass sich die entsprechenden Betaparameterschätzer als normalisierte Regressor-Daten Kovarianzen ergeben. Man sagt manchmal, dass parametrische Designs zur Untersuchung von Zusammenhangshypothesen dienen. Beispiele für parametrische Designs sind verschiedene Designs zur Implementation von einfacher linearer Regression und insbesondere multipler linearer Regression.\nFaktoriell-parametrische Designs, bei denen die Spalten der Designmatrix sowohl faktorielle als auch parametrische Prädiktoren repräsentieren. In diesem Kontext werden die parametrischen Regressoren oft als Kovariaten betrachtet. Gemischt faktoriell-parametrische Designs sind das zentrale Charakteristikum der Kovarianzanalyse die auf eine kontrollierte Untersuchung von Unterschiedshypothesen bei Vorliegen weiterer möglicher Abhängigkeiten zwischen unabhängigen und abhängigen Variablen bzw. auf die kontrollierte Untersuchung von Zusammenhangshypothesen bei Vorliegen weiterer möglicher Gruppenunterschiede abzielt.",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Modellformulierung</span>"
    ]
  },
  {
    "objectID": "403-Modellformulierung.html#literaturhinweise",
    "href": "403-Modellformulierung.html#literaturhinweise",
    "title": "27  Modellformulierung",
    "section": "27.4 Literaturhinweise",
    "text": "27.4 Literaturhinweise\nDas Allgemeine Lineare Modell hat eine lange Geschichte, deren moderne Inkarnation üblicherweise auf die Arbeiten von Legendre (1805) und Gauss (1809) zurückgeführt wird. Matrixbasierte Formulierungen der multiplen Regression finden sich spätestens bei Aitken (1936) und Scheffé (1959). Eingang in den psychologischen Methodenkanon findet das Allgemeine Lineare Modell spätestens mit Cohen (1968). Seal (1967) gibt einen ausführlichen Überblick zur Geschichte des Allgemeinen Linearen Modells im 19. und der ersten Hälfte des 20. Jahrhunderts. Die in diesem Abschnitt gegebene Diskussion von Identifizierbarkeit und Schätzbarkeit beruht auf der Darstellung in Christensen (2011).",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Modellformulierung</span>"
    ]
  },
  {
    "objectID": "403-Modellformulierung.html#selbstkontrollfragen",
    "href": "403-Modellformulierung.html#selbstkontrollfragen",
    "title": "27  Modellformulierung",
    "section": "27.5 Selbstkontrollfragen",
    "text": "27.5 Selbstkontrollfragen\n\nGeben Sie die Definition des Allgemeinen Linearen Modells wieder.\nErläutern Sie die deterministischen und probabilistischen Aspekte des ALMs.\nWie viele skalare Parameter hat das ALM mit sphärischer Kovarianzmatrix?\nWarum sind die Komponenten des ALM Zufallsfehlers unabhängig und identisch verteilt?\nGeben Sie das Theorem zur Datenverteilung des Allgemeinen Linearen Modells wieder.\nSind die Komponenten des ALM Datenvektors immer unabhängig und identisch verteilt?\nSchreiben Sie das Szenario von \\(n\\) unabhängig und identisch normalverteilten Zufallsvariablen in ALM Form.\nSchreiben Sie das Szenario der einfachen linearen Regression in ALM Form.\n\n\n\n\n\nAitken, A. C. (1936). IV.—On Least Squares and Linear Combination of Observations. Proceedings of the Royal Society of Edinburgh, 55, 42–48. https://doi.org/10.1017/S0370164600014346\n\n\nChristensen, R. (2011). Plane Answers to Complex Questions. Springer New York. https://doi.org/10.1007/978-1-4419-9816-3\n\n\nCohen, J. (1968). Multiple Regression as a General Data-Analytic System. Psychological Bulletin, 70(6, Pt.1), 426–443. https://doi.org/10.1037/h0026714\n\n\nGauss, C. F. (1809). Theoria Motus Corporum Coelestium in Sectionibus Conicis Solem Ambientium. Cambridge University Press.\n\n\nLegendre, A. M. (1805). Nouvelles Methodes Pour La Determination Des Orbites Des Cometes. Didot Paris.\n\n\nScheffé, H. (1959). The Analysis of Variance (Wiley classics library ed). Wiley-Interscience Publication.\n\n\nSeal, H. L. (1967). Studies in the History of Probability and Statistics. XV: The Historical Development of the Gauss Linear Model. Biometrika, 54(1/2), 1. https://doi.org/10.2307/2333849",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Modellformulierung</span>"
    ]
  },
  {
    "objectID": "404-Parameterschätzung.html",
    "href": "404-Parameterschätzung.html",
    "title": "28  Parameterschätzung",
    "section": "",
    "text": "28.1 Betaparameterschätzung\nWir fassen die Frequentistische Punktschätzung des Betaparametervektors in folgendem Theorem zusammen.\nTheorem 28.1 gibt mit \\[\\begin{equation}\n\\hat{\\beta}=\\left(X^{T}X\\right)^{-1}X^{T}y\n\\end{equation}\\] eine Formel an, um \\(\\beta\\) anhand der Designmatrix und einer Realisierung \\(y \\in \\mathbb{R}^{n}\\) von \\(\\upsilon\\) konkret zu schätzen. Als Zufallsvektor ist \\(\\hat{\\beta}\\) ist ein unverzerrter Schätzer von \\(\\beta\\) und als Maximum-Likelihood-Schätzer insbesondere auch konsistent, asymptotisch normalverteilt und asymptotisch effizient. Wir sehen an späterer Stelle dass \\(\\hat{\\beta}\\) sogar normalverteilt ist. Neben den genannten Eigenschaften hat \\(\\hat{\\beta}\\) noch weitere gute Eigenschaften. Zum Beispiel besitzt \\(\\hat{\\beta}\\) die kleinste Varianz in der Klasse der linearen unverzerrten Schätzer von \\(\\beta\\). Diese Eigenschaft ist Kernaussage des Gauss-Markov Theorems, auf das wir hier aber nicht näher eingehen wollen.\nMithilfe des Betaparameterschätzers können wir die Begriffe der erklärten Daten, des Residuenvektors und der Residuen definieren, die wir an vielen Stellen benötigen werden.",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Parameterschätzung</span>"
    ]
  },
  {
    "objectID": "404-Parameterschätzung.html#betaparameterschätzung",
    "href": "404-Parameterschätzung.html#betaparameterschätzung",
    "title": "28  Parameterschätzung",
    "section": "",
    "text": "Theorem 28.1 (Betaparameterschätzer) Es sei \\[\\begin{equation}\n\\upsilon = X\\beta+\\varepsilon \\mbox{ mit } \\varepsilon \\sim N\\left(0_{n}, \\sigma^{2} I_{n}\\right)\n\\end{equation}\\] das Allgemeine Lineare Modell und es sei \\[\\begin{equation}\n\\hat{\\beta}:=\\left(X^{T}X\\right)^{-1}X^{T}\\upsilon .\n\\end{equation}\\] Dann gelten:\n\n\\(\\hat{\\beta}\\) ist der KQ-Schätzer von \\(\\beta \\in \\mathbb{R}^{p}\\), für einen beliebigen festen Wert \\(y \\in \\mathbb{R}^n\\) von \\(\\upsilon\\) gilt also \\[\\begin{equation}\n\\hat{\\beta}=\\operatorname{argmin}_{\\tilde{\\beta}}(y - X\\tilde{\\beta})^{T}(y - X\\tilde{\\beta}) .\n\\end{equation}\\]\n\\(\\hat{\\beta}\\) ist ein unverzerrter Maximum-Likelihood-Schätzer von \\(\\beta \\in \\mathbb{R}^{p}\\).\n\n\n\nBeweis. (1) Wir zeigen in einem ersten Schritt, dass \\(\\hat{\\beta}\\) ein KQ-Schätzer ist, dass also \\(\\hat{\\beta}\\) für einen beliebigen festen Wert \\(y \\in \\mathbb{R}^n\\) von \\(\\upsilon\\) die Summe der Abweichungsquadrate \\[\\begin{equation}\n(y-X\\tilde{\\beta})^{T}(y-X\\tilde{\\beta})=\\sum_{i=1}^{n}\\left(\\upsilon_{i}-(X \\tilde{\\beta})_{i}\\right)^{2}\n\\end{equation}\\] minimiert (die Notation \\(\\tilde{\\beta}\\) für das Minimierungsargument dient hier lediglich dazu, es vom wahrem, aber unbekannten, Parameterwert \\(\\beta \\in \\mathbb{R}^{p}\\) abzugrenzen und ist ansonsten ohne Bedeutung). Dazu halten wir zunächst fest, dass \\[\\begin{equation}\n\\hat{\\beta} = \\left(X^{T}X\\right)^{-1}X^{T}y\n\\Leftrightarrow\nX^{T}X\\hat{\\beta} = X^{T}y\n\\Leftrightarrow\nX^{T}y-X^{T}X\\hat{\\beta} = 0_{p}\n\\Leftrightarrow X^{T}(y-X\\hat{\\beta})=0_{p}.\n\\end{equation}\\] Weiterhin gilt dann auch, dass \\[\\begin{equation}\nX^{T}(y-X\\hat{\\beta})=0_{p}\n\\Leftrightarrow\n\\left(X^{T}(y-X\\hat{\\beta})\\right)^{T}=0_{p}^{T}\n\\Leftrightarrow\n(y-X\\hat{\\beta})^{T} X=0_{p}^{T}\n\\end{equation}\\] Weiterhin halten wir ohne Beweis fest, dass für jede Matrix \\(X \\in \\mathbb{R}^{n \\times p}\\) gilt, dass \\[\\begin{equation}\nz^{T}X^{T}Xz \\geq 0 \\mbox{ für alle } z \\in \\mathbb{R}^{p}.\n\\end{equation}\\] Wir betrachten nun für festes \\(y\\) und ein beliebiges \\(\\tilde{\\beta}\\) die Summe der Abweichungsquadrate \\[\\begin{equation}\n(y-X\\tilde{\\beta})^{T}(y-X\\tilde{\\beta})\n\\end{equation}\\] Es ergibt sich \\[\\begin{equation}\n\\begin{aligned}\n& (y-X\\tilde{\\beta})^{T}(y-X\\tilde{\\beta}) \\\\\n& =(y-X\\hat{\\beta}+X\\hat{\\beta}- X\\tilde{\\beta})^{T}(y-X\\hat{\\beta}+X\\hat{\\beta}- X\\tilde{\\beta}) \\\\\n& =((y-X\\hat{\\beta})+X(\\hat{\\beta}-\\tilde{\\beta}))^{T}((y-X\\hat{\\beta})+X(\\hat{\\beta}-\\tilde{\\beta})) \\\\\n& =(y-X\\hat{\\beta})^{T}(y-X\\hat{\\beta})+(y-X\\hat{\\beta})^{T} X(\\hat{\\beta}-\\tilde{\\beta})+(\\hat{\\beta}-\\tilde{\\beta})^{T} X^{T}(y-X\\hat{\\beta})+(\\hat{\\beta}-\\tilde{\\beta})^{T} X^{T}X(\\hat{\\beta}-\\tilde{\\beta}) \\\\\n& =(y-X\\hat{\\beta})^{T}(y-X\\hat{\\beta}) 0_{p}^{T}(\\hat{\\beta}-\\tilde{\\beta})+(\\hat{\\beta}-\\tilde{\\beta})^{T} 0_{p}+(\\hat{\\beta}-\\tilde{\\beta})^{T} X^{T}X(\\hat{\\beta}-\\tilde{\\beta}) \\\\\n& =(y-X\\hat{\\beta})^{T}(y-X\\hat{\\beta})+(\\hat{\\beta}-\\tilde{\\beta})^{T} X^{T}X(\\hat{\\beta}-\\tilde{\\beta})\n\\end{aligned}\n\\end{equation}\\] Auf der rechten Seite obiger Gleichung ist nur der zweite Term von \\(\\tilde{\\beta}\\) abhängig. Da für diesen Term gilt, dass \\[\\begin{equation}\n(\\hat{\\beta}-\\tilde{\\beta})^{T} X^{T}X(\\hat{\\beta}-\\tilde{\\beta}) \\geq 0\n\\end{equation}\\] nimmt dieser Term genau dann seinen Minimalwert 0 an, wenn \\[\\begin{equation}\n(\\hat{\\beta}-\\tilde{\\beta})=0_{p} \\Leftrightarrow \\tilde{\\beta}=\\hat{\\beta}\n\\end{equation}\\] Also gilt \\[\\begin{equation}\n\\hat{\\beta}=\\operatorname{argmin}_{\\tilde{\\beta}}(y-X\\tilde{\\beta})^{T}(y-X\\tilde{\\beta}) .\n\\end{equation}\\] (2) Um zu zeigen, dass \\(\\hat{\\beta}\\) ein Maximum Likelihood Schätzer ist, betrachten wir für einen beliebigen Wert \\(y \\in \\mathbb{R}^{n}\\) von \\(\\upsilon\\) und festes \\(\\sigma^{2}&gt;0\\) die Log-Likelihood Funktion \\[\\begin{equation}\n\\ell: \\mathbb{R}^{p} \\rightarrow \\mathbb{R}, \\tilde{\\beta} \\mapsto \\ln p_{\\tilde{\\beta}}(v)=\\ln N\\left(y ; X \\tilde{\\beta}, \\sigma^{2} I_{n}\\right)\n\\end{equation}\\] wobei gilt, dass \\[\\begin{equation}\n\\begin{aligned}\n\\ln N\\left(y ; X \\tilde{\\beta}, \\sigma^{2} I_{n}\\right)\n& =\\ln \\left((2 \\pi)^{-\\frac{n}{2}}\\left|\\sigma^{2} I_{n}\\right|^{-\\frac{1}{2}} \\exp \\left(-\\frac{1}{2 \\sigma^{2}}(y-X\\tilde{\\beta})^{T}(y-X\\tilde{\\beta})\\right)\\right) \\\\\n& =-\\frac{n}{2} \\ln 2 \\pi-\\frac{1}{2} \\ln \\left|\\sigma^{2} I_{n}\\right|-\\frac{1}{2 \\sigma^{2}}(y-X\\tilde{\\beta})^{T}(y-X\\tilde{\\beta})\n\\end{aligned}\n\\end{equation}\\] Dabei hängt allein der Term \\(-\\frac{1}{2 \\sigma^{2}}(y-X\\tilde{\\beta})^{T}(y-X\\tilde{\\beta})\\) von \\(\\tilde{\\beta}\\) ab. Weil aber \\((y-X\\tilde{\\beta})^{T}(y-X\\tilde{\\beta}) \\geq 0\\) gilt, wird dieser Term aufgrund des negativen Vorzeichens maximal, wenn \\((y-X\\tilde{\\beta})^{T}(y-X\\tilde{\\beta})\\) minimal wird. Dies ist aber wie oben gezeigt genau für \\(\\tilde{\\beta}=\\hat{\\beta}\\) der Fall. Die Unverzerrtheit von \\(\\hat{\\beta}\\) schließlich ergibt sich aus \\[\\begin{equation}\n\\mathbb{E}(\\hat{\\beta})\n= \\mathbb{E}\\left(\\left(X^{T}X\\right)^{-1}X^{T}\\upsilon\\right)\n= \\left(X^{T}X\\right)^{-1}X^{T} \\mathbb{E}(v)\n= \\left(X^{T}X\\right)^{-1}X^{T}X \\beta\n= \\beta .\n\\end{equation}\\]\n\n\n\n\nDefinition 28.1 (Erklärte Daten, Residuenvektor und Residuen) Es sei \\[\\begin{equation}\n\\upsilon = X\\beta+\\varepsilon \\operatorname{mit} \\varepsilon \\sim N\\left(0_{n}, \\sigma^{2} I_{n}\\right)\n\\end{equation}\\] das Allgemeine Lineare Modell und es sei \\[\\begin{equation}\n\\hat{\\beta}:=\\left(X^{T}X\\right)^{-1}X^{T}\\upsilon .\n\\end{equation}\\] der Betaparameterschätzer. Dann heißt der Zufallsvektor \\[\\begin{equation}\n\\hat{\\upsilon}:=X\\hat{\\beta}=X\\left(X^{T}X\\right)^{-1}X^{T}\\upsilon\n\\end{equation}\\] die erklärten Daten, der Zufallsvektor \\[\\begin{equation}\n\\hat{\\varepsilon}:=\\upsilon-\\hat{\\upsilon}=\\upsilon-X\\hat{\\beta}\n\\end{equation}\\] heißt Residuenvektor und für \\(i=1, \\ldots, n\\) heißen die Komponenten dieses Zufallsvektors \\[\\begin{equation}\n\\hat{\\varepsilon}_{i}:=\\upsilon_{i}-\\hat{v}_{i}=\\upsilon_{i}-(X\\hat{\\beta})_{i}\n\\end{equation}\\] die Residuen.",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Parameterschätzung</span>"
    ]
  },
  {
    "objectID": "404-Parameterschätzung.html#varianzparameterschätzung",
    "href": "404-Parameterschätzung.html#varianzparameterschätzung",
    "title": "28  Parameterschätzung",
    "section": "28.2 Varianzparameterschätzung",
    "text": "28.2 Varianzparameterschätzung\nWir fassen die Frequentistische Punktschätzung des Varianzparameters in folgendem Theorem zusammen, das wir an dieser Stelle nicht beweisen wollen.\n\nTheorem 28.2 (Varianzparameterschätzer) Es sei \\[\\begin{equation}\n\\upsilon = X\\beta+\\varepsilon \\mbox{ mit } \\varepsilon \\sim N\\left(0_{n}, \\sigma^{2} I_{n}\\right)\n\\end{equation}\\] das Allgemeine Lineare Modell. Dann ist \\[\\begin{equation}\n\\hat{\\sigma}^{2}:=\\frac{\\hat{\\varepsilon}^{T} \\hat{\\varepsilon}}{n-p}\n\\end{equation}\\] ein unverzerrter Schätzer von \\(\\sigma^{2}&gt;0\\).\n\nTheorem 28.2 gibt mit \\[\\begin{equation}\n\\hat{\\sigma}^{2}\n= \\frac{(y - X\\hat{\\beta})^{T}(y - X\\hat{\\beta})}{n-p}\n\\end{equation}\\] eine Formel an, um \\(\\sigma^{2}\\) anhand der Designmatrix, des Betaparameterschätzers und einer Realisierung \\(y \\in \\mathbb{R}^n\\) von \\(\\upsilon\\) zu schätzen. Offenbar gilt mit Theorem 28.2, dass \\[\\begin{equation}\n\\hat{\\sigma}^{2}=\\frac{1}{n-p} \\sum_{i=1}^{n}\\left(y_{i}-(X\\hat{\\beta})_{i}\\right)^{2}\n\\end{equation}\\] \\(\\hat{\\sigma}^{2}\\) wird also durch die Summe der quadrierten Residuen, also als eine Summe von Abweichungsquadraten geschätzt. Für einen Beweis von Theorem 28.2 verweisen wir zum Beispiel auf Searle (1971), Searle & Gruber (2017) oder Rencher & Schaalje (2008). Aus probabilistischer Perspektive handelt es sich bei \\(\\hat{\\sigma}^{2}\\) nicht um einen Maximum-Likelihood-Schätzer, sondern um einen Restricted Maximum-Likelihood-Schätzer von \\(\\sigma^{2}\\) (vgl. Harville (1977), Foulley (1993), Starke & Ostwald (2017)). Aus geometrischer Perspektive handelt es sich bei \\(\\hat{\\sigma}^{2}\\) um einen KQ-Schätzer (vgl. Christensen (2011)).",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Parameterschätzung</span>"
    ]
  },
  {
    "objectID": "404-Parameterschätzung.html#unabhängig-identisch-normalverteilte-zufallsvariablen",
    "href": "404-Parameterschätzung.html#unabhängig-identisch-normalverteilte-zufallsvariablen",
    "title": "28  Parameterschätzung",
    "section": "28.3 Unabhängig identisch normalverteilte Zufallsvariablen",
    "text": "28.3 Unabhängig identisch normalverteilte Zufallsvariablen\nAls erste Anwendung von Theorem 28.1 und Theorem 28.2 analysieren wir das Szenario von \\(n\\) unabhängigen und identisch normalverteilten Zufallsvariablen mit Erwartungswertparameter \\(\\mu \\in \\mathbb{R}\\) und Varianzparameter \\(\\sigma^{2}\\), \\[\\begin{equation}\n\\upsilon_{i} \\sim N\\left(\\mu, \\sigma^{2}\\right) \\mbox{ für } i=1, \\ldots, n.\n\\end{equation}\\] Schreibt man dieses Modell in seiner Designmatrixform (vgl. Gleichung 27.2) dann gilt, wie unten gezeigt, \\[\n\\hat{\\beta} =\n\\frac{1}{n} \\sum_{i=1}^{n} \\upsilon_{i} =: \\bar{\\upsilon}\n\\mbox{ und }\n\\hat{\\sigma}^{2} = \\frac{1}{n-1} \\sum_{i=1}^{n}\\left(\\upsilon_{i}-\\bar{\\upsilon}\\right)^{2} =: s_{\\upsilon}^{2}\n\\tag{28.1}\\] In diesem Fall ist also der Betaparameterschätzer mit dem Stichprobenmittel \\(\\bar{\\upsilon}\\) der \\(\\upsilon_{1}, \\ldots, \\upsilon_{n}\\) und der Varianzparameterschätzer mit der Stichprobenvarianz \\(s_{\\upsilon}^{2}\\) der \\(\\upsilon_{1}, \\ldots, \\upsilon_{n}\\) identisch.\nGleichung 28.1 ergibt sich wie folgt. Zum einen gilt \\[\\begin{equation}\n\\begin{aligned}\n\\hat{\\beta}\n& = \\left(X^{T}X\\right)^{-1}X^{T}\\upsilon \\\\\n& = \\left(1_{n}^{T} 1_{n}\\right)^{-1} 1_{n}^{T} v \\\\\n&  \n\\begin{pmatrix}\n1 & \\cdots & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n1        \\\\\n\\vdots   \\\\\n1\n\\end{pmatrix}^{-1}\n\\begin{pmatrix}\n1 & \\cdots & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n\\upsilon_{1}    \\\\\n\\vdots          \\\\\n\\upsilon_{n}\n\\end{pmatrix}   \\\\\n& = n^{-1} \\sum_{i=1}^{n} \\upsilon_{i} \\\\\n& =\\frac{1}{n} \\sum_{i=1}^{n} \\upsilon_{i} \\\\\n& =: \\bar{\\upsilon} .\n\\end{aligned}\n\\end{equation}\\] Zum anderen gilt \\[\\begin{equation}\n\\begin{aligned}\n\\hat{\\sigma}^{2}\n& = \\frac{1}{n-1}(\\upsilon-X\\hat{\\beta})^{T}(\\upsilon-X\\hat{\\beta}) \\\\\n& = \\frac{1}{n-1}\\left(v-1_{n} \\bar{\\upsilon}\\right)^{T}\\left(v-1_{n} \\bar{\\upsilon}\\right) \\\\\n& = \\frac{1}{n-1}\n\\left(\n\\begin{pmatrix}\n\\upsilon_{1} \\\\\n\\vdots \\\\\n\\upsilon_{n}\n\\end{pmatrix} -\n\\begin{pmatrix}\n1 \\\\\n\\vdots \\\\\n1\n\\end{pmatrix}\n\\bar{\\upsilon}^{T}\n\\right)\n\\left(\n\\begin{pmatrix}\n\\upsilon_{1} \\\\\n\\vdots \\\\\n\\upsilon_{n}\n\\end{pmatrix}\n-\\begin{pmatrix}\n1 \\\\\n\\vdots \\\\\n1\n\\end{pmatrix}\n\\bar{\\upsilon}\n\\right)\n\\\\\n& =\n\\frac{1}{n-1}\n\\begin{pmatrix} \\upsilon_{1} - \\bar{\\upsilon} & \\cdots & \\upsilon_{n}-\\bar{\\upsilon}\\end{pmatrix}\n\\begin{pmatrix}\n\\upsilon_{1}-\\bar{\\upsilon}      \\\\\n\\vdots                          \\\\\n\\upsilon_{n}-\\bar{\\upsilon}\n\\end{pmatrix}                       \\\\\n& =\\frac{1}{n-1} \\sum_{i=1}^{n}\\left(\\upsilon_{i}-\\bar{\\upsilon}\\right)^{2} \\\\\n& =s_{\\upsilon}^{2}.\n\\end{aligned}\n\\end{equation}\\] Wir demonstrieren die Parameterschätzung in diesem Szenario in folgendem R Code.\n\n# Modellformulierung\nlibrary(MASS)                                                                   # Multivariate Normalverteilung\nn          = 12                                                                 # Anzahl Datenpunkte\np          = 1                                                                  # Anzahl Betaparameter\nX          = matrix(rep(1,n), nrow = n)                                         # Designmatrix\nI_n        = diag(n)                                                            # n x n Einheitsmatrix\nbeta       = 2                                                                  # wahrer, aber unbekannter, Betaparameter\nsigsqr     = 1                                                                  # wahrer, aber unbekannter, Varianzparameter\n\n# Datenrealisierung\ny          =  mvrnorm(1, X %*% beta, sigsqr*I_n)                                # eine Realisierung eines n-dimensionalen ZVs\n\n# Parameterschätzung\nbeta_hat   = solve(t(X) %*% X) %*% t(X) %*% y                                   # Betaparameterschätzer\neps_hat    = y - X %*% beta_hat                                                 # Residuenvektor\nsigsqr_hat = (t(eps_hat) %*% eps_hat) /(n-p)                                    # Varianzparameterschätzer\n\n\n\nbeta       :  2 \nhat{beta}  :  1.784511 \nsigsqr     :  1 \nhat{sigsqr}:  0.6243704\n\n\nDie Frequentistische Bedeutung der Schätzerunverzerrtheit in diesem Szenario simuliert folgender R Code.\n\n# Modellformulierung\nlibrary(MASS)                                                                   # Multivariate Normalverteilung\nn         = 12                                                                  # Anzahl Datenpunkte\np          = 1                                                                  # Anzahl Betaparameter\nX          = matrix(rep(1,n), nrow = n)                                         # Designmatrix\nI_n        = diag(n)                                                            # n x n Einheitsmatrix\nbeta       = 2                                                                  # wahrer, aber unbekannter, Betaparameter\nsigsqr     = 1                                                                  # wahrer, aber unbekannter, Varianzparameter\n\n# Frequentistische Simulation\nnsim       = 1e4                                                                # Anzahl Datenrealisierungen\nbeta_hat   = rep(NaN,nsim)                                                      # \\hat{\\beta} Realisierungsarray\nsigsqr_hat = rep(NaN,nsim)                                                      # \\hat{sigsqr} Realisierungsarray\nfor(i in 1:nsim){                                                               # Simulationsiterationen\n  y             = mvrnorm(1, X %*% beta, sigsqr*I_n)                            # Datenrealisierung\n  beta_hat[i]   = solve(t(X) %*% X) %*% t(X) %*% y                              # Betaparameterschätzer\n  eps_hat       = y - X %*% beta_hat[i]                                         # Residuenvektor\n  sigsqr_hat[i] = (t(eps_hat) %*% eps_hat) /(n-p)                               # Varianzparameterschätzer\n}\n\n\n\nWahrer, aber unbekannter, Betaparameter                  :  2 \nGeschätzter Erwartungswert des Betaparameterschätzers    :  2.002293 \nWahrer, aber unbekannter, Varianzparameter               :  1 \nGeschätzter Erwartungswert des Varianzparameterschätzers :  1.008244",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Parameterschätzung</span>"
    ]
  },
  {
    "objectID": "404-Parameterschätzung.html#einfache-lineare-regression",
    "href": "404-Parameterschätzung.html#einfache-lineare-regression",
    "title": "28  Parameterschätzung",
    "section": "28.4 Einfache lineare Regression",
    "text": "28.4 Einfache lineare Regression\nAls zweite Anwendung von Theorem 28.1 und Theorem 28.2 analysieren wir das Szenario der einfachen linearen Regression\n\\[\\begin{equation}\n\\upsilon_{i}=\\beta_{0}+\\beta_{1} x_{i}+\\varepsilon_{i}\n\\mbox{ mit } \\varepsilon_{i} \\sim N\\left(0, \\sigma^{2}\\right) \\mbox{ für } i=1, \\ldots, n.\n\\end{equation}\\] Basierend auf der Designmatrixform Gleichung 27.3 dieses Modells ergibt sich, wie unten gezeigt, \\[\n\\hat{\\beta}\n=\\begin{pmatrix}\n\\hat{\\beta}_{0} \\\\\n\\hat{\\beta}_{1}\n\\end{pmatrix}\n=\\begin{pmatrix}\n\\bar{\\upsilon}-\\frac{c_{x v}}{s_{x}^{2}} \\bar{x} \\\\\n\\frac{c_{x v}}{s_{x}^{2}}\n\\end{pmatrix}\n\\mbox{ und }\n\\hat{\\sigma}^{2} = \\frac{1}{n-2}\\sum_{i=1}^{n}\\left(\\upsilon_{i}-\\left(\\hat{\\beta}_{0}+\\hat{\\beta}_{1} x_{i}\\right)\\right)^{2}\n\\tag{28.2}\\]\nwobei \\(\\bar{x}\\) und \\(\\bar{\\upsilon}\\) die Stichprobenmittel der \\(x_{1}, \\ldots, x_{n}\\) und \\(\\upsilon_{1}, \\ldots, \\upsilon_{n}, c_{x v}\\) die Stichprobenkovarianz der \\(x_{1},\n\\ldots, x_{n}\\) und \\(\\upsilon_{1}, \\ldots, \\upsilon_{n}\\) und \\(s_{x}^{2}\\) die Stichprobenvarianz der \\(x_{1}, \\ldots, x_{n}\\) bezeichnen. Wie in Kapitel 1 sind die Bezeichnungen Stichprobenkovarianz und Stichprobenvarianz bezüglich der \\(x_{1}, \\ldots, x_{n}\\) hier lediglich formal gemeint, da keine Annahme zugrundeliegt, dass die \\(x_{1}, \\ldots, x_{n}\\) Realisierungen von Zufallsvariablen sind.\nWir halten also fest, dass für eine parametrische Designmatrixspalte sich der entsprechende Betaparameterschätzer aus der Stichprobenkovarianz der respektiven Spalte mit den Daten geteilt durch die Stichprobenvarianz der entsprechenden Spalte ergibt und somit einer standardisierten Stichprobenkovarianz entspricht. Ein Vergleich mit den Parametern der Ausgleichsgerade in Kapitel 25 zeigt weiterhin die Identität der Betaparameterschätzerkomponenten \\(\\hat{\\beta}_{0}\\) und \\(\\hat{\\beta}_{1}\\) mit den dort unter dem Kriterium der Minimierung der quadrierten vertikalen Abweichungen hergeleiteten Parametern. Dies überrascht nicht, da sowohl \\(\\hat{\\beta}\\) als auch die Parameter der Ausgleichsgerade bei festem Wert \\(y \\in \\mathbb{R}^n\\) von \\(\\upsilon\\) den Wert \\[\\begin{equation}\nq(\\tilde{\\beta})\n=\\sum_{i=1}^{n}\\left(y_{i}-\\left(\\tilde{\\beta}_{0}+\\tilde{\\beta}_{1} x_{i}\\right)\\right)^{2}=(y- X\\tilde{\\beta})^{T}(y- X\\tilde{\\beta})\n\\end{equation}\\] hinsichtlich \\(\\tilde{\\beta}\\) minimieren.\nUm die Form des Betaparameterschätzers in Gleichung 28.2 herzuleiten, halten wir zunächst fest, dass \\[\\begin{equation}\n\\begin{aligned}\n\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)\\left(\\upsilon_{i}-\\bar{\\upsilon}\\right)\n& =\\sum_{i=1}^{n}\\left(x_{i} \\upsilon_{i}-x_{i} \\bar{\\upsilon}-\\bar{x} \\upsilon_{i}+\\bar{x} \\bar{\\upsilon}\\right) \\\\\n& =\\sum_{i=1}^{n} x_{i} \\upsilon_{i}-\\sum_{i=1}^{n} x_{i} \\bar{\\upsilon}-\\sum_{i=1}^{n} \\bar{x} \\upsilon_{i}+\\sum_{i=1}^{n} \\bar{x} \\bar{\\upsilon} \\\\\n& =\\sum_{i=1}^{n} x_{i} \\upsilon_{i}-\\bar{\\upsilon} \\sum_{i=1}^{n} x_{i}-\\bar{x} \\sum_{i=1}^{n} \\upsilon_{i}+n \\bar{x} \\bar{\\upsilon} \\\\\n& =\\sum_{i=1}^{n} x_{i} \\upsilon_{i}-\\bar{\\upsilon} n \\bar{x}-\\bar{x} n \\bar{\\upsilon}+n \\bar{x} \\bar{\\upsilon} \\\\\n& =\\sum_{i=1}^{n} x_{i} \\upsilon_{i}-n \\bar{x} \\bar{\\upsilon}-n \\bar{x} \\bar{\\upsilon}+n \\bar{x} \\bar{\\upsilon} \\\\\n& =\\sum_{i=1}^{n} x_{i} \\upsilon_{i}-n \\bar{x} \\bar{\\upsilon}\n\\end{aligned}\n\\end{equation}\\] Weiterhin halten wir fest, dass \\[\\begin{equation}\n\\begin{aligned}\n\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}\n& =\\sum_{i=1}^{n}\\left(x_{i}^{2}-2 x_{i} \\bar{x}+\\bar{x}^{2}\\right) \\\\\n& =\\sum_{i=1}^{n} x_{i}^{2}-\\sum_{i=1}^{n} 2 x_{i} \\bar{x}+\\sum_{i=1}^{n} \\bar{x}^{2} \\\\\n& =\\sum_{i=1}^{n} x_{i}^{2}-2 \\bar{x} \\sum_{i=1}^{n} x_{i}+n \\bar{x}^{2} \\\\\n& =\\sum_{i=1}^{n} x_{i}^{2}-2 \\bar{x} n \\bar{x}+n \\bar{x}^{2} \\\\\n& =\\sum_{i=1}^{n} x_{i}^{2}-2 n \\bar{x}^{2}+n \\bar{x}^{2} \\\\\n& =\\sum_{i=1}^{n} x_{i}^{2}-n \\bar{x}^{2} .\n\\end{aligned}\n\\end{equation}\\] Aus der Definition von \\(\\hat{\\beta}\\) ergibt sich \\[\\begin{equation}\n\\begin{aligned}\n\\hat{\\beta}\n& =\\left(X^{T}X\\right)^{-1}X^{T}\\upsilon \\\\\n& =\\left(\n\\begin{pmatrix}\n1 & \\cdots & 1 \\\\\nx_{1} & \\cdots & x_{n}\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & x_{1} \\\\\n\\vdots & \\vdots \\\\\n1 & x_{n}\n\\end{pmatrix}\n\\right)^{-1}\n\\begin{pmatrix}\n1 & \\cdots & 1 \\\\\nx_{1} & \\cdots & x_{n}\n\\end{pmatrix}\n\\begin{pmatrix}\n\\upsilon_{1} \\\\\n\\vdots \\\\\n\\upsilon_{n}\n\\end{pmatrix} \\\\\n&\n=\\begin{pmatrix}\nn & \\sum_{i=1}^{n} x_{i} \\\\\n\\sum_{i=1}^{n} x_{i} & \\sum_{i=1}^{n} x_{i}^{2}\n\\end{pmatrix}\\begin{pmatrix}\n\\sum_{i=1}^{n} \\upsilon_{i} \\\\\n\\sum_{i=1}^{n} x_{i} \\upsilon_{i}\n\\end{pmatrix} \\\\\n& =\n\\begin{pmatrix}\nn & n \\bar{x} \\\\\nn \\bar{x} & \\sum_{i=1}^{n} x_{i}^{2}\n\\end{pmatrix}^{-1}\n\\begin{pmatrix}\nn \\bar{\\upsilon} \\\\\n\\sum_{i=1}^{n} x_{i} \\upsilon_{i}\n\\end{pmatrix}\n\\end{aligned}\n\\end{equation}\\] Die Inverse von \\(X^{T}X\\) ist gegeben durch \\[\\begin{equation}\n\\frac{1}{s_{x}^{2}}\\begin{pmatrix}\n\\frac{s_{x}^{2}}{n}+\\bar{x}^{2} & -\\bar{x} \\\\\n-\\bar{x} & 1\n\\end{pmatrix},\n\\end{equation}\\] weil \\[\\begin{equation}\n\\begin{aligned}\n& \\frac{1}{s_{x}^{2}}\\begin{pmatrix}\n\\frac{s_{x}^{2}}{n}+\\bar{x}^{2} & -\\bar{x} \\\\\n-\\bar{x} & 1\n\\end{pmatrix}\\begin{pmatrix}\nn & n \\bar{x} \\\\\nn \\bar{x} & \\sum_{i=1}^{n} x_{i}^{2}\n\\end{pmatrix} \\\\\n& =\\frac{1}{s_{x}^{2}}\\begin{pmatrix}\n\\frac{n s_{x}^{2}}{n}+n \\bar{x}^{2}-n \\bar{x}^{2} & \\frac{s_{x}^{2} n \\bar{x}}{n}+n \\bar{x}^{2} \\bar{x}-\\bar{x} \\sum_{i=1}^{n} x_{i}^{2} \\\\\n-\\bar{x} n+n \\bar{x} & -n \\bar{x}^{2}+\\sum_{i=1}^{n} x_{i}^{2}\n\\end{pmatrix} \\\\\n& =\\frac{1}{s_{x}^{2}}\\begin{pmatrix}\ns_{x}^{2} & s_{x}^{2} \\bar{x}-\\bar{x}\\left(\\sum_{i=1}^{n} x_{i}^{2}-n \\bar{x}^{2}\\right) \\\\\n0 & \\sum_{i=1}^{n} x_{i}^{2}-n \\bar{x}^{2}\n\\end{pmatrix} \\\\\n& =\\frac{1}{s_{x}^{2}}\\begin{pmatrix}\ns_{x}^{2} & s_{x}^{2} \\bar{x}-\\bar{x} s_{x}^{2} \\\\\n0 & s_{x}^{2}\n\\end{pmatrix} \\\\\n& =\\frac{1}{s_{x}^{2}}\\begin{pmatrix}\ns_{x}^{2} & 0 \\\\\n0 & s_{x}^{2}\n\\end{pmatrix} \\\\\n& =\\begin{pmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{pmatrix} .\n\\end{aligned}\n\\end{equation}\\] Es ergibt sich also \\[\\begin{equation}\n\\begin{aligned}\n\\hat{\\beta}=\\begin{pmatrix}\n\\frac{1}{n}+\\frac{\\bar{x}^{2}}{s_{x}^{2}} & -\\frac{\\bar{x}}{s_{x}^{2}} \\\\\n-\\frac{\\bar{x}}{s_{x}^{2}} & \\frac{1}{s_{x}^{2}}\n\\end{pmatrix}\\begin{pmatrix}\nn \\bar{\\upsilon} \\\\\n\\sum_{i=1}^{n} x_{i} \\upsilon_{i}\n\\end{pmatrix} & =\\begin{pmatrix}\n\\left(\\frac{1}{n}+\\frac{\\bar{x}^{2}}{s_{x}^{2}}\\right) n \\bar{\\upsilon}-\\frac{\\bar{x} \\sum_{i=1}^{n} x_{i} \\upsilon_{i}}{s_{x}^{2}} \\\\\n\\frac{\\sum_{i=1}^{n} x_{i} \\upsilon_{i}}{s_{x}^{2}}-\\frac{n \\bar{x} \\bar{\\upsilon}}{s_{x}^{2}}\n\\end{pmatrix} \\\\\n& =\\begin{pmatrix}\n\\frac{n \\bar{\\upsilon}}{n}+\\frac{\\bar{x}^{2} n \\bar{\\upsilon}}{s_{x}^{2}}-\\frac{\\bar{x} \\sum_{i=1}^{n} x_{i} \\upsilon_{i}}{s_{x}^{2}} \\\\\n\\frac{\\sum_{i=1}^{n} x_{i} \\upsilon_{i}-n \\bar{x} \\bar{\\upsilon}}{s_{x}^{2}}\n\\end{pmatrix} \\\\\n& =\\begin{pmatrix}\n\\bar{\\upsilon}+\\frac{\\bar{x} n \\bar{x} \\bar{\\upsilon}-\\bar{x} \\sum_{i=1}^{n} x_{i} \\upsilon_{i}}{s_{x}^{2}} \\\\\n\\frac{\\sum_{i=1}^{n} x_{i} \\upsilon_{i}-n \\bar{x} \\bar{\\upsilon}}{s_{x}^{2}}\n\\end{pmatrix} \\\\\n& =\\begin{pmatrix}\n\\bar{\\upsilon}-\\frac{\\sum_{i=1}^{n} x_{i} \\upsilon_{i}-n \\bar{x} \\bar{\\upsilon}}{s_{x}^{2}} \\bar{x} \\\\\n\\frac{\\sum_{i=1}^{n} x_{i} \\upsilon_{i}-n \\bar{x} \\bar{\\upsilon}}{s_{x}^{2}}\n\\end{pmatrix} \\\\\n& =\\begin{pmatrix}\n\\bar{\\upsilon}-\\frac{c_{x v}}{s_{x}^{2}} \\bar{x} \\\\\n\\frac{c_{x} v_{x}}{s_{x}^{2}}\n\\end{pmatrix} .\n\\end{aligned}\n\\end{equation}\\]\nWir demonstrieren die Parameterschätzung in diesem Szenario in folgendem R Code. Man beachte die weitgehende Übereinstimmung mit der Implementation der Parameterschätzung im Szenario der unabhängig und identisch normalverteilten Zufallsvariablen - lediglich die Designmatrix und die Dimension des Betaparameters ändern sich.\n\n# Modellformulierung\nlibrary(MASS)                                                                   # Multivariate Normalverteilung\nn          = 10                                                                 # Anzahl Datenpunkte\np          = 2                                                                  # Anzahl Betaparameter\nx          = 1:n                                                                # Prädiktorwerte\nX          = matrix(c(rep(1,n),x), nrow = n)                                    # Designmatrix\nI_n        = diag(n)                                                            # n x n Einheitsmatrix\nbeta       = matrix(c(0,1), nrow = p)                                           # wahrer, aber unbekannter, Betaparameter\nsigsqr     = 1                                                                  # wahrer, aber unbekannter, Varianzparameter\n\n# Datenrealisierung\ny          = mvrnorm(1, X %*% beta, sigsqr*I_n)                                 # eine Realisierung eines n-dimensionalen ZVs\n\n# Parameterschätzung\nbeta_hat   = solve(t(X) %*% X) %*% t(X) %*% y                                   # Betaparameterschätzer\neps_hat    = y - X %*% beta_hat                                                 # Residuenvektor\nsigsqr_hat = (t(eps_hat) %*% eps_hat) /(n-p)                                    # Varianzparameterschätzer\n\n\n\nbeta       :  0 1 \nhat{beta}  :  1.043493 0.8147543 \nsigsqr     :  1 \nhat{sigsqr}:  1.641318\n\n\nAnalog zum Szenario der unabhängig und identisch normalverteilten Zufallsvariablen kann auch hier die Frequentistische Bedeutung der Schätzerunverzerrtheit simuliert werden.\n\n# Modellformulierung\nlibrary(MASS)                                                                   # Multivariate Normalverteilung\nn          = 10                                                                 # Anzahl Datenpunkte\np          = 2                                                                  # Anzahl Betaparameter\nx          = 1:n                                                                # Prädiktorwerte\nX          = matrix(c(rep(1,n),x), nrow = n)                                    # Designmatrix\nI_n        = diag(n)                                                            # n x n Einheitsmatrix\nbeta       = matrix(c(0,1), nrow = p)                                           # wahrer, aber unbekannter, Betaparameter\nsigsqr     = 1                                                                  # wahrer, aber unbekannter, Varianzparameter\n\n# Frequentistische Simulation\nnsim       = 1e4                                                                # Anzahl Realisierungen des n-dimensionalen ZVs\nbeta_hat   = matrix(rep(NaN,p*nsim), nrow = p)                                  # \\hat{\\beta} Realisierungsarray\nsigsqr_hat = rep(NaN,nsim)                                                      # \\hat{sigsqr} Realisierungsarray\nfor(i in 1:nsim){                                                               # Simulationsiterationen\n  y             = mvrnorm(1, X %*% beta, sigsqr*I_n)                            # Datenrealisierung\n  beta_hat[,i]  = solve(t(X) %*% X) %*% t(X) %*% y                              # Betaparameterschätzer\n  eps_hat       = y - X %*% beta_hat[,i]                                        # Residuenvektor\n  sigsqr_hat[i] = (t(eps_hat) %*% eps_hat) /(n-p)                               # Varianzparameterschätzer\n}\n\n\n\nWahrer, aber unbekannter, Betaparameter                  :  0 1 \nGeschätzter Erwartungswert des Betaparameterschätzers    :  0.002571164 0.9996732 \nWahrer, aber unbekannter, Varianzparameter               :  1 \nGeschätzter Erwartungswert des Varianzparameterschätzers :  0.9899678",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Parameterschätzung</span>"
    ]
  },
  {
    "objectID": "404-Parameterschätzung.html#frequentistische-schätzerverteilungen",
    "href": "404-Parameterschätzung.html#frequentistische-schätzerverteilungen",
    "title": "28  Parameterschätzung",
    "section": "28.5 Frequentistische Schätzerverteilungen",
    "text": "28.5 Frequentistische Schätzerverteilungen\nWir dokumentieren die Frequentistische Verteilung des Betaparameterschätzers in folgendem Theorem.\n\nTheorem 28.3 (Frequentistische Verteilung des Betaparameterschätzers.) Es sei \\[\\begin{equation}\n\\upsilon = X\\beta+\\varepsilon \\mbox{ mit } \\varepsilon \\sim N\\left(0_{n}, \\sigma^{2} I_{n}\\right)\n\\end{equation}\\] das ALM. Weiterhin sei \\[\\begin{equation}\n\\hat{\\beta}:=\\left(X^{T}X\\right)^{-1}X^{T}\\upsilon\n\\end{equation}\\] der Betaparameterschätzer. Dann gilt \\[\\begin{equation}\n\\hat{\\beta} \\sim N\\left(\\beta, \\sigma^{2}\\left(X^{T}X\\right)^{-1}\\right) .\n\\end{equation}\\]\n\n\nBeweis. Das Theorem folgt direkt mit dem Theorem zur linearen Transformation von multivariaten Normalverteilungen. Speziell gilt hier: \\[\\begin{equation}\n\\hat{\\beta}\n\\sim N\\left(\\left(X^{T}X\\right)^{-1}X^{T}X \\beta,\\left(X^{T}X\\right)^{-1}X^{T}\\left(\\sigma^{2} I_{n}\\right)\\left(\\left(X^{T}X\\right)^{-1}X^{T}\\right)^{T}\\right) .\n\\end{equation}\\] Der Erwartungswertparameter vereinfacht sich dann zu \\[\\begin{equation}\n\\left(X^{T}X\\right)^{-1}X^{T}X \\beta=\\beta.\n\\end{equation}\\] Der Kovarianzmatrixparamter vereinfacht sich wie folgt: \\[\\begin{equation}\n\\begin{aligned}\n\\left(X^{T}X\\right)^{-1}X^{T}\\left(\\sigma^{2} I_{n}\\right)\\left(\\left(X^{T}X\\right)^{-1}X^{T}\\right)^{T} & =\\left(X^{T}X\\right)^{-1}X^{T}\\left(\\sigma^{2} I_{n}\\right) X\\left(X^{T}X\\right)^{-1} \\\\\n& =\\sigma^{2}\\left(X^{T}X\\right)^{-1}X^{T}X\\left(X^{T}X\\right)^{-1} \\\\\n& =\\sigma^{2}\\left(X^{T}X\\right)^{-1}\n\\end{aligned}\n\\end{equation}\\] Dabei hier die erste Gleichung aus der Tatsache, dass sowohl \\(X^{T}X\\) als auch ihre Inverse \\(\\left(X^{T}X\\right)^{-1}\\) symmetrische Matrizen sind. Damit folgt dann aber sofort \\[\\begin{equation}\n\\hat{\\beta} \\sim N\\left(\\beta, \\sigma^{2}\\left(X^{T}X\\right)^{-1}\\right) .\n\\end{equation}\\]\n\nMit Theorem 6.3 folgt also inbesondere auch für den Erwartungswert und die Kovarianzmatrix des Betaparameterschätzers, dass\n\\[\\begin{equation}\n\\mathbb{E}(\\widehat{\\beta})=\\beta \\mbox{ und } \\mathbb{C}(\\hat{\\beta})=\\sigma^{2}\\left(X^{T}X\\right)^{-1}\n\\end{equation}\\]\nAls Diagonalelemente von \\(\\mathbb{C}(\\hat{\\beta})\\) hängen die Varianzen der Betaparameterschätzerkomponenten also sowohl vom Varianzparameter der Fehlervariablen als auch von der Designmatrix ab. Insbesondere bei festem, wahren, aber unbekannten \\(\\sigma^{2}&gt;0\\) kann also die Designmatrix so gewält werden, dass die Varianz der Betaparameterschätzerkomponenten minimiert wird.\nDie Frequentistische Verteilung des Varianzparameterschätzers dokumentieren wir in folgendem Theorem, welches wir an dieser Stelle nicht beweisen wollen.\n\nTheorem 28.4 (Frequentistische Verteilung des Varianzparameterschätzers) Es sei \\[\\begin{equation}\n\\upsilon = X\\beta+\\varepsilon \\mbox{ mit } \\varepsilon \\sim N\\left(0_{n}, \\sigma^{2} I_{n}\\right)\n\\end{equation}\\] das ALM. Weiterhin sei \\[\\begin{equation}\n\\hat{\\sigma}^{2}=\\frac{\\hat{\\varepsilon}^{T} \\hat{\\varepsilon}}{n-p}\n\\end{equation}\\] der Varianzparameterschätzer. Dann gilt \\[\\begin{equation}\n\\frac{n-p}{\\sigma^{2}} \\hat{\\sigma}^{2} \\sim \\chi^{2}(n-p) .\n\\end{equation}\\]\n\nDa es sich bei \\((n-p) \\hat{\\sigma}^{2}\\) um eine Summe normalverteilter Zufallsvariablen handelt, liegt die \\(\\chi^{2}\\)-Verteilung im Lichte der \\(\\chi^{2}\\)-Transformation bei normalverteilten Zufallsvariablen zumindest nahe. Allerdings ist \\(\\hat{\\sigma}^{2}\\) selbst nicht \\(\\chi^{2}\\) verteilt, sondern lediglich seine durch Multiplikation mit \\(\\frac{n-p}{\\sigma^{2}}\\) skalierte Version. Wir wollen die Frequentistischen Schätzerverteilungen aus Theorem 28.3 und Theorem 28.4 noch an den beiden Standardbeispielen verdeutlichen.\n\nBeispiel (1) Unabhängige und identisch normalverteilte Zufallsvariablen\nEs sei\n\\[\\begin{equation}\n\\upsilon \\sim N\\left(X \\beta, \\sigma^{2} I_{n}\\right) \\mbox{ mit } X:=1_{n} \\in \\mathbb{R}^{n \\times 1}, \\beta:=\\mu \\in \\mathbb{R} \\mbox{ und } \\sigma^{2}&gt;0\n\\end{equation}\\] das ALM Szenario unabhängiger und identisch normalverteilter Zufallsvariablen bei bekannter Varianz. Wir haben bereits gesehen, dass in diesem Fall \\(\\hat{\\beta}\\) mit dem Stichprobenmittel \\(\\bar{\\upsilon}\\) identisch ist. Theorem 28.3 impliziert dann mit \\[\\begin{equation}\n\\left(X^{T}X\\right)^{-1}=\\left(1_{n}^{T} 1_{n}\\right)^{-1}=\\frac{1}{n},\n\\end{equation}\\] dass \\[\\begin{equation}\n\\bar{\\upsilon} \\sim N\\left(\\mu, \\frac{\\sigma^{2}}{n}\\right).\n\\end{equation}\\] Das Stichprobenmittel von \\(n\\) unabhängigen und identisch normalverteilten Zufallsvariablen mit Erwartungswertparameter \\(\\mu\\) und Varianzparameter \\(\\sigma^{2}\\) ist also normalverteilt mit Erwartungswertparameter \\(\\mu\\) und Varianzparameter \\(\\sigma^{2} / n\\). Wir haben diese Tatsache bereits im Kontext der Transformationen der Normalverteilungen unter dem Begriff der Mittelwertstransformation gesehen.\n\n\nBeispiel (2) Einfache lineare Regression\nEs sei \\[\\begin{equation}\n\\upsilon \\sim N\\left(X \\beta, \\sigma^{2} I_{n}\\right) \\mbox{ mit }\\begin{pmatrix}\n1 & x_{1} \\\\\n\\vdots & \\vdots \\\\\n1 & x_{n}\n\\end{pmatrix} \\in \\mathbb{R}^{n \\times 2}, \\beta \\in \\mathbb{R}^{2} \\mbox{ und } \\sigma^{2}&gt;0\n\\end{equation}\\] das Szenario der einfachen linearen Regression. Wir haben bereits gesehen, dass \\[\\begin{equation}\n\\sigma^{2}\\left(X^{T}X\\right)^{-1}=\\frac{\\sigma^{2}}{s_{x x}}\n\\begin{pmatrix}\n\\frac{s_{x x}}{n}+\\bar{x}^{2} & -\\bar{x} \\\\\n-\\bar{x} & 1\n\\end{pmatrix}\n\\mbox{ mit } s_{x x}:=\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}\n\\end{equation}\\]\nDie Varianz des Offsetparameterschätzers hängt damit sowohl von der Summe der quadrierten Differenzen der Werte der unabhängigen Variable von ihrem Stichprobenmittel und dem Stichprobenmittel der Werte der unabhängigen Variable selbst ab. Die Varianz des Steigungsparameterschätzers hängt dagegen nur von der Summe der quadrierten Differenzen der unabhängigen Variable von ihrem Stichprobenmittel ab. Die Kovarianz von Offset- und Steigungsparameterschätzern schließlich hängt vom Mittelwert der Werte der unabhängen Variable ab. Folgender R Code simuliert die frequentistischen Verteilungen von Beta- und Varianzparameterschätzern im Szenario der einfachen linearen Regression.\n\n# Modellformulierung\nlibrary(MASS)                                                                   # Multivariate Normalverteilung\nn        = 10                                                                   # Anzahl von Datenpunkten\np        = 2                                                                    # Anzahl von Betparametern\nx        = 1:n                                                                  # Prädiktorwerte\nX        = matrix(c(rep(1,n),x), nrow = n)                                      # Designmatrix\nI_n      = diag(n)                                                              # n x n Einheitsmatrix\nbeta     = matrix(c(0,1), nrow = p)                                             # wahrer,aber unbekannter,Betaparameter\nsigsqr   = .5                                                                   # wahrer,aber unbekannter,Varianzparameter\n\n# Frequentistische Simulation\nnsim     = 10                                                                   # Anzahl Realisierungen n-dimensionaler ZV\ny        = matrix(rep(NaN,n*nsim), nrow = n)                                    # y Realisierungsarray\nbeta_hat = matrix(rep(NaN,p*nsim), nrow = p)                                    # \\hat{\\beta} Realisierungsarray\nfor(i in 1:nsim){\n  y[,i]        = mvrnorm(1, X %*% beta, sigsqr*I_n)                             # eine Realisierung n-dimensionaler ZV\n  beta_hat[,i] = solve(t(X) %*% X) %*% t(X) %*% y[,i]                           # \\hat{\\beta} = (X^T)X^{-1}X^T\\upsilon\n}\n\n\n\n\n\n\n\nAbbildung 28.1: Frequentistische Betaparameterschätzerverteilung bei einfacher lineare Regression.\n\n\n\nAbbildung 28.1 zeigt 10 Realisationen des Modells einer einfachen linearen Regression und Abbildung 28.1 B zeigt die entsprechenden Betaparameterschätzerrealisationen sowie die analytische Verteilung des Betaparameterschätzers.",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Parameterschätzung</span>"
    ]
  },
  {
    "objectID": "404-Parameterschätzung.html#literaturhinweise",
    "href": "404-Parameterschätzung.html#literaturhinweise",
    "title": "28  Parameterschätzung",
    "section": "28.6 Literaturhinweise",
    "text": "28.6 Literaturhinweise\nPlackett (1949) gibt einen historischen Überblick zur Entwicklung der Betaparameterschätzung und insbesondere des Gauss-Markov Theorems. Das Problem der Varianzparameterschätzung ALM im Sinne der Restricted Maximum Likelihood Methode erscheint zunächst in Patterson & Thompson (1971) (vgl. Harville (1977)), Verbyla (1990)) und bleibt, in verallgemeinerten ALMs, Gegenstand aktueller Forschung (vgl. Lindholm & Wahl (2020)).",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Parameterschätzung</span>"
    ]
  },
  {
    "objectID": "404-Parameterschätzung.html#selbstkontrollfragen",
    "href": "404-Parameterschätzung.html#selbstkontrollfragen",
    "title": "28  Parameterschätzung",
    "section": "28.7 Selbstkontrollfragen",
    "text": "28.7 Selbstkontrollfragen\n\nGeben Sie das Theorem zum Betaparameterschätzer wieder.\nWarum ist der Betaparameterschätzer ein Maximum-Likelihood-Schätzer?\nGeben Sie das Theorem zum Varianzparameterschätzer wieder.\nGeben Sie die Parameterschätzer bei \\(n\\) u.i. normalverteilten Zufallsvariablen an.\nGeben Sie die Parameterschätzer bei einfacher linearer Regression an.\nGeben Sie das Theorem zur Verteilung des Betaparameterschätzers wieder.\nGeben Sie das Theorem zur Verteilung des Varianzparameterschätzers wieder. \n\n\n\n\n\nChristensen, R. (2011). Plane Answers to Complex Questions. Springer New York. https://doi.org/10.1007/978-1-4419-9816-3\n\n\nFoulley, J. (1993). A Simple Argument Showing How to Derive Restricted Maximum Likelihood.\n\n\nHarville, D. A. (1977). Maximum Likelihood Approaches to Variance Component Estimation and to Related Problems. Journal of the American Statistical Association, 72(358), 320. https://doi.org/10.2307/2286796\n\n\nLindholm, M., & Wahl, F. (2020). On the Variance Parameter Estimator in General Linear Models. Metrika, 83(2), 243–254. https://doi.org/10.1007/s00184-019-00751-4\n\n\nPatterson, H. D., & Thompson, R. (1971). Recovery of Inter-Block Information When Block Sizes Are Unequal. Biometrika, 58(3), 545–554. https://doi.org/10.1093/biomet/58.3.545\n\n\nPlackett, R. L. (1949). A Historical Note on the Method of Least Squares. Biometrika, 36(3/4), 458. https://doi.org/10.2307/2332682\n\n\nRencher, A. C., & Schaalje, G. B. (2008). Linear Models in Statistics (2nd ed). Wiley-Interscience.\n\n\nSearle, S. R. (1971). Linear Models. Wiley.\n\n\nSearle, S. R., & Gruber, M. H. J. (2017). Linear Models (Second edition). Wiley.\n\n\nStarke, L., & Ostwald, D. (2017). Variational Bayesian Parameter Estimation Techniques for the General Linear Model. Frontiers in Neuroscience, 11. https://doi.org/10.3389/fnins.2017.00504\n\n\nVerbyla, A. P. (1990). A Conditional Derivation of Residual Maximum Likelihood. Australian Journal of Statistics, 32(2), 227–230. https://doi.org/10.1111/j.1467-842X.1990.tb01015.x",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Parameterschätzung</span>"
    ]
  },
  {
    "objectID": "405-T-Statistiken.html",
    "href": "405-T-Statistiken.html",
    "title": "29  T-Statistiken",
    "section": "",
    "text": "29.1 Definition und Beispiele\nVor dem Hintergrund des ALMs und seiner Parameterpunktschätzer definieren wir die T-Statistik wie folgt.\nGeeignete Wahlen des Kontrastgewichtvektors \\(c \\in \\mathbb{R}^{p}\\) und des Parameters \\(\\beta_{0} \\in \\mathbb{R}^{p}\\) erlauben eine Vielzahl von Einsatzmöglichkeiten der T-Statistik. Betrachten wir zunächst den Kontrastgewichtsvektor. Offenbar dient der Konstrastgewichtsvektor dazu, den Zufallsvektor \\(\\hat{\\beta} \\in \\mathbb{R}^{p}\\) in die Zufallsvariable \\(c^{T} \\hat{\\beta}\\) zu transfomieren und sichert damit die Skalarität der T-Statistik. Weiterhin erlaubt die Wahl von \\(p\\)-dimensionalen Einheitsvektoren für den Kontrastgewichtsvektor die Auswahl einzelner Komponenten des Betaparameters zur Evaluation mithilfe der T-Statistik. Schließlich erlaubt eine generelle Wahl des Kontrastgewichtsvektors die Evaluation beliebiger Linearkombination der Betaparameterkomponenten, wie zum Beispiel Differenzen einzelner Komponenten. Beispielhaft seien für \\(\\hat{\\beta} \\in \\mathbb{R}^{2}\\) hier folgende Möglichkeiten für die Wahl von \\(c \\in \\mathbb{R}^{2}\\) hinsichtlich des Skalarproduktes \\(c^{T} \\hat{\\beta}\\) aufgeführt: \\[\\begin{equation}\nc:=\n\\begin{pmatrix}\n1 \\\\\n0\n\\end{pmatrix}\n\\Rightarrow\nc^{T}\\hat{\\beta} = \\hat{\\beta}_{1},\n\\quad\nc :=\n\\begin{pmatrix}\n0 \\\\\n1\n\\end{pmatrix}\n\\Rightarrow\nc^{T}\\hat{\\beta}\n=\n\\hat{\\beta}_{2}, \\quad\nc\n:=\\begin{pmatrix}\n1 \\\\\n-1\n\\end{pmatrix}\n\\Rightarrow\nc^{T}\\hat{\\beta} = \\hat{\\beta}_{1} - \\hat{\\beta}_{2}.\n\\end{equation}\\] Die Wahl des Parameters \\(\\beta_{0} \\in \\mathbb{R}^{p}\\) eröffnet die Möglichkeit, die T-Statistik unterschiedlich einzusetzen. Wählt man zum Beispiel \\(\\beta_{0}:=0_{p}\\), so erhält man mit der T-Statistik eine Deskriptivstatistik, die es erlaubt, geschätzte Regressoreffekte, also Komponenten oder Linearkombinationen von \\(\\hat{\\beta}\\), im Sinne eines Signal-zu-Rauschen Verhältnisses in Bezug zu der durch \\(\\hat{\\sigma}^{2}\\) quantifizierten Residualdatenvariabilität zu setzen. Der Nenner der TStatistik stellt dabei sicher, dass insbesondere die adäquate (Ko)Standardabweichung der entsprechenden Betaparameterkomponentenkombination als Bezugsgröße dient, da es sich bei \\(\\hat{\\sigma}^{2}\\left(X^{T} X\\right)^{-1}\\) bekanntlich um die Kovarianz des Betaparameterschätzers handelt (vgl. Theorem 28.3).\nWählt man für \\(\\beta_{0}\\) dagegen \\(\\beta\\), also den wahren, aber unbekannten, Betaparameterwert, so eröffnet die T-Statistik die Möglichkeit, für einzelne Komponenten des Betaparametervektors Konfidenzintervalle zu bestimmen. Wir vertiefen diesen Aspekt der T-Statistik in Kapitel 29.2. Deklariert man schließlich \\(\\beta_{0}\\) im Kontext eines Testszenarios als das Element einer Nullhypothese, so eröffnet die T-Statistik die hypothesentestbasierte Inferenz über Betaparameterkomponenten und ihrer Linearkombinationen. Anwendungsfälle dieser Art diskutieren wir ausführlich in Kapitel 31.\nDie Anwendung der T-Statistik zum Zwecke der Frequentistischen Inferenz im Sinne von Konfidenzintervallen und Hypothesentests basiert dabei natürlich auf der Frequentistischen Verteilung der T-Statistik vor dem Hintergrund des ALMs. Diese ist der zentrale Inhalt folgenden Theorems, auf dessen Beweis wir verzichten.\nIm Allgemeinen ist die T-Statistik also nichtzentral t-verteilt. Gilt dabei, wie bei der Bestimmung von Konfidenzintervallen (vgl. Kapitel 29.2) \\(\\beta_{0}:=\\beta\\) oder gilt in einem Testszenario bei Zutreffen der Nullhypothese \\(\\beta := \\beta_{0}\\) (vgl. Kapitel 31), so ist die T-Statistik sogar \\(t\\)-verteilt, jeweils mit Freiheitsgradparameter \\(n-p\\). Gilt in einem Testszenario dagegen, dass die Nullhypothese nicht zutrifft, so kann die Verteilung der T-Statistik aus Theorem 29.1 zur Herleitung der Testgütefunktion und damit zur Bestimmung der Power des Tests genutzt werden (vgl. Kapitel Kapitel 31). Wir werden diese Aspekte an gegebener Stelle vertiefen. An dieser Stelle wollen wir die T-Statistik und ihre Verteilung zunächst nur an den Beispielen der unabhängigen identisch normalverteilten Zufallsvariablen und der einfachen linearen Regression illustrieren.",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>T-Statistiken</span>"
    ]
  },
  {
    "objectID": "405-T-Statistiken.html#definition-und-beispiele",
    "href": "405-T-Statistiken.html#definition-und-beispiele",
    "title": "29  T-Statistiken",
    "section": "",
    "text": "Definition 29.1 (T-Statistik) Es sei \\[\\begin{equation}\n\\upsilon = X \\beta+\\varepsilon \\mbox{ mit } \\varepsilon \\sim N\\left(0_{n}, \\sigma^{2} I_{n}\\right)\n\\end{equation}\\] das ALM. Weiterhin seien \\[\\begin{equation}\n\\hat{\\beta}:=\\left(X^{T} X\\right)^{-1}X^{T}\\upsilon\n\\mbox{ und }\n\\hat{\\sigma}^{2}:=\\frac{(\\upsilon-X\\hat{\\beta})^{T}(\\upsilon-X\\hat{\\beta})}{n-p}\n\\end{equation}\\] die Betaparameter- und Varianzparameterschätzer, respektive. Dann ist für einen Kontrastgewichtsvektor \\(c \\in \\mathbb{R}^{p}\\) und einen Parameter \\(\\beta_{0} \\in \\mathbb{R}^{p}\\) die T-Statistik definiert als \\[\\begin{equation}\nT:=\\frac{c^{T} \\hat{\\beta}-c^{T} \\beta_{0}}{\\sqrt{\\hat{\\sigma}^{2} c^{T}\\left(X^{T} X\\right)^{-1} c}} .\n\\end{equation}\\]\n\n\n\n\n\nTheorem 29.1 (Frequentistische Verteilung der T-Statistik) Es sei \\[\\begin{equation}\n\\upsilon = X \\beta+\\varepsilon \\mbox{ mit } \\varepsilon \\sim N\\left(0_{n}, \\sigma^{2} I_{n}\\right)\n\\end{equation}\\] das ALM. Weiterhin seien \\[\\begin{equation}\n\\hat{\\beta} :=\n\\left(X^{T} X\\right)^{-1}X^{T}\\upsilon\n\\mbox{ und }\n\\hat{\\sigma}^{2}:=\\frac{(\\upsilon-X\\hat{\\beta})^{T}(\\upsilon-X\\hat{\\beta})}{n-p}\n\\end{equation}\\] die Betaparameter- und Varianzparameterschätzer, respektive. Schließlich sei für einen Kontrastgewichtsvektor \\(c \\in \\mathbb{R}^{p}\\) und einen Parameter \\(\\beta_{0} \\in \\mathbb{R}^{p}\\) \\[\\begin{equation}\nT := \\frac{c^{T}\\hat{\\beta} - c^{T}\\beta_{0}}{\\sqrt{\\hat{\\sigma}^{2} c^{T}\\left(X^{T} X\\right)^{-1} c}}\n\\end{equation}\\] die T-Statistik. Dann gilt \\[\\begin{equation}\nT \\sim t(\\delta, n-p)\n\\mbox{ mit }\n\\delta:=\\frac{c^{T} \\beta-c^{T} \\beta_{0}}{\\sqrt{\\sigma^{2} c^{T}\\left(X^{T} X\\right)^{-1} c}} .\n\\end{equation}\\]\n\n\n\nBeispiel (1) Unabhängige und identisch normalverteilte Zufallsvariablen\nEs sei \\[\\begin{equation}\n\\upsilon \\sim N\\left(X \\beta, \\sigma^{2} I_{n}\\right)\n\\mbox{ mit }\nX := 1_{n} \\in \\mathbb{R}^{n \\times 1}, \\beta:=\\mu \\in \\mathbb{R} \\mbox{ und } \\sigma^{2}&gt;0\n\\end{equation}\\] das ALM Szenario unabhängiger und identisch normalverteilter Zufallsvariablen und es seien \\(c:=1\\) und \\(\\beta_{0}:=\\mu_{0} \\in \\mathbb{R}\\). Dann gilt für die T-Statistik \\[\\begin{equation}\nT\n= \\frac{c^{T} \\hat{\\beta}-c^{T} \\mu_{0}}{\\sqrt{\\hat{\\sigma}^{2} c^{T}\\left(X^{T} X\\right)^{-1} c}}\n= \\frac{1^{T} \\bar{v}-1^{T} \\mu_{0}}{\\sqrt{s_{v}^{2} 1^{T}\\left(1_{n}^{T} 1_{n}\\right)^{-1} 1}}\n= \\sqrt{n}\\left(\\frac{\\bar{v}-\\mu_{0}}{s_{v}}\\right)\n\\end{equation}\\] Dies entspricht offenbar der bekannten Einstichproben-T-Teststatistik (vgl. Kapitel 24). Wie diese nimmt die hier betrachtete T-Statistik, bei Konstanz der jeweils komplementären Terme, große absolute Werte für eine große absolute Differenz von \\(\\bar{v}-\\mu_{0}\\) (oft als Effekt bezeichnet), sowie für kleine Werte von \\(s_{v}^{2}\\) (also eine geringe Datenvariabilität) und einen großen Wert von \\(n\\) (also einen großen Stichprobenumfang) an. Folgender R Code simuliert die Frequentistische Verteilung dieser T-Statistik für die Fälle \\(\\beta=\\beta_{0}\\) und \\(\\beta \\neq \\beta_{0}\\).\n\n# Libraries\nlibrary(MASS)                                                                   # multivariate Normalverteilung\n\n# Modellformulierung\nn          = 12                                                                 # Anzahl von Datenpunkten\np          = 1                                                                  # Anzahl von Betaparametern\nX          = matrix(c(rep(1,n)), nrow = n)                                      # Designmatrix\nI_n        = diag(n)                                                            # Einheitsmatrix\nbeta       = c(0,1)                                                             # wahre , aber unbekannte , Betaparameter\nnscn       = length(beta)                                                       # Anzahl wahrer, aber unbekannter, Hypothesenszenarien\nsigsqr     = 1                                                                  # wahrer, aber unbekannter, Varianzparameter\nc          = 1                                                                  # Kontrastvektor von Interessse\nbeta_0     = 0                                                                  # Nullhypothesenbetaparameter\n\n# Frequentistische Simulation\nnsim       = 1e4                                                                # Anzahl Simulationen\ndelta      = rep(NaN, nscn)                                                     # Anzahl Nichtzentralitätsparameter\nTee        = matrix(rep(NaN, nscn*nsim), ncol = nscn)                           # T-Teststatistik Realisierungsarray\nfor(s in 1:nscn){                                                               # Hypothesenszenarien\n  delta[s]    = ((t(c) %*% beta[s] - t(c) %*% beta_0)/                          # Nichtzentralitätsparameter\n                sqrt(sigsqr*t(c)%*%solve(t(X)%*%X)%*%c))\n  for(i in 1:nsim){                                                             # Simulationsiterationen\n    y          = mvrnorm(1, X %*% beta[s], sigsqr*I_n)                          # y\n    beta_hat   = solve(t(X) %*% X) %*% t(X) %*% y                               # \\hat{\\beta}\n    eps_hat    = y - X %*% beta_hat                                             # \\hat{\\eps}\n    sigsqr_hat = (t(eps_hat) %*% eps_hat)/(n-p)                                 # \\hat{\\sigma}^2\n    Tee[i,s]   = ((t(c) %*% beta_hat - t(c) %*% beta_0)/                        # T\n                  sqrt(sigsqr_hat*t(c)%*%solve(t(X)%*%X)%*%c))\n  }\n}\n\nAbbildung 29.1 A und B zeigen die resultierenden simulierten und analytischen Verteilungen der T-Statistik.\n\n\n\n\n\n\nAbbildung 29.1: Verteilungen der T-Statistik bei unabhängig und identisch normalverteilten Zufallsvariablen.\n\n\n\n\n\nBeispiel (2) Einfache lineare Regression\nIn diesem Beispiel wollen wir nicht auf die spezifische Form der T-Statistik eingehen, aber anhand einer Simulation demonstrieren, wie sich das Prinzip der T-Statistik im Kontext der einfachen linearen Regression darstellt. Dazu betrachten wir das bekannte Beispielmodell der einfachen linearen Regression (vgl. Kapitel 27), in diesem Fall mit den wahren, aber unbekannten, Parameterwerten \\(\\beta_{A}:=(1,0)\\) und \\(\\beta_{B}:=(1,1)\\). Weiterhin betrachten wir den Kontrastgewichtsvektor \\(c:=(0,1)\\), so dass die T-Statistik zu Evaluation des Steigungsparameters der einfachen linearen Regression genutzt werden kann. Schließlich betrachten wir in beiden Fällen den Parameter \\(\\beta_{0}:=(0,0)^{T}\\), so dass im Fall von \\(\\beta_{A}\\) gilt, dass \\(c^{T} \\beta=c^{T} \\beta_{0}\\) und im Fall von \\(\\beta_{B}\\) gilt, dass \\(c^{T} \\beta \\neq c^{T} \\beta_{0}\\). Folgender R Code implementiert die skizzierten Szenarien, Abbildung 29.2 A und B zeigen die resultierenden simulierten und analytischen Verteilungen der T-Statistik.\n\n# Modellformulierung\nlibrary(MASS)                                                                   # multivariate Normalverteilung\nn          = 10                                                                 # Anzahl von Datenpunkten\np          = 2                                                                  # Anzahl von Betaparametern\nx          = 1:n                                                                # Prädiktorwerte\nX          = matrix(c(rep(1,n),x), ncol = p)                                    # Designmatrix\nI_n        = diag(n)                                                            # Einheitsmatrix\nbeta       = matrix(c(1,0,1,1), nrow = 2)                                       # wahre, aber unbekannte, Betaparameter\nnscn       = ncol(beta)                                                         # Anzahl wahrer, aber unbekannter, Hypothesenszenarien\nsigsqr     = 1                                                                  # wahrer, aber unbekannter, Varianzparameter\nc          = matrix(c(0,1), nrow = 2)                                           # Kontrastvektor von Interessse\nbeta_0     = matrix(c(0,0), nrow = 2)                                           # Nullhypothesenbetaparameter\n\n# Frequentistische Simulation\nnsim       = 1e4                                                                # Anzahl Simulationen\ndelta      = rep(NaN, nscn)                                                     # Anzahl Nichtzentralitätsparameter\nTee        = matrix(rep(NaN, nscn*nsim), ncol = nscn)                           # T-Teststatistik Realisierungsarray\nfor(s in 1:nscn){                                                               # Hypothesenszenarien\n  delta[s]    = ((t(c) %*% beta[,s] - t(c) %*% beta_0)/                         # Nichtzentralitätsparameter\n                sqrt(sigsqr*t(c)%*%solve(t(X)%*%X)%*%c))\n  for(i in 1:nsim){                                                             # Simulationsiterationen\n    y          = mvrnorm(1, X %*% beta[,s], sigsqr*I_n)                         # y\n    beta_hat   = solve(t(X) %*% X) %*% t(X) %*% y                               # \\hat{\\beta}\n    eps_hat    = y - X %*% beta_hat                                             # \\hat{\\eps}\n    sigsqr_hat = (t(eps_hat) %*% eps_hat)/(n-p)                                 # \\hat{\\sigma}^2\n    Tee[i,s]   = ((t(c) %*% beta_hat - t(c) %*% beta_0)/                        # T\n                  sqrt(sigsqr_hat*t(c)%*%solve(t(X)%*%X)%*%c))\n  }\n}\n\n\n\n\n\n\n\nAbbildung 29.2: Verteilungen der T-Statistik bei einfacher linearer Regression.",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>T-Statistiken</span>"
    ]
  },
  {
    "objectID": "405-T-Statistiken.html#sec-konfidenzintervalle-fuer-betaparameterkomponenten",
    "href": "405-T-Statistiken.html#sec-konfidenzintervalle-fuer-betaparameterkomponenten",
    "title": "29  T-Statistiken",
    "section": "29.2 Konfidenzintervalle für Betaparameterkomponenten",
    "text": "29.2 Konfidenzintervalle für Betaparameterkomponenten\nMithilfe der T-Statistik können Konfidenzintervalle für die Komponenten des Betaparametervektors bestimmt werden. Das folgende Theorem ist die zentrale Aussage dieses Abschnitts.\n\nTheorem 29.2 (Konfidenzintervalle für Betaparameterkomponenten) Es sei \\[\\begin{equation}\n\\upsilon = X \\beta+\\varepsilon \\mbox{ mit } \\varepsilon \\sim N\\left(0_{n}, \\sigma^{2} I_{n}\\right)\n\\end{equation}\\] das ALM, \\[\\begin{equation}\n\\hat{\\beta}:=\\left(X^{T} X\\right)^{-1}X^{T}\\upsilon\n\\mbox{ und }\n\\hat{\\sigma}^{2}:=\\frac{(\\upsilon-X\\hat{\\beta})^{T}(\\upsilon-X\\hat{\\beta})}{n-p}\n\\end{equation}\\] seien die Betaparameter- und Varianzparameterschätzer, respektive und für ein \\(\\delta \\in] 0,1[\\) sei \\[\\begin{equation}\nt_{\\delta}:=\\Psi^{-1}\\left(\\frac{1+\\delta}{2}; n-p\\right).\n\\end{equation}\\] Schließlich sei für \\(j=1, \\ldots, p\\) \\[\\begin{equation}\n\\lambda_{j}:=\\left(\\left(X^{T} X\\right)^{-1}\\right)_{j j}\n\\mbox{ das $j$te Diagonalelement von }\n\\left(X^{T} X\\right)^{-1}\n\\end{equation}\\] Dann ist für \\(j=1, \\ldots, p\\) \\[\\begin{equation}\n\\kappa_{j} :=\n\\left[\n\\hat{\\beta}_{j}-\\hat{\\sigma} \\sqrt{\\lambda_{j}} t_{\\delta},\n\\hat{\\beta}_{j}+\\hat{\\sigma} \\sqrt{\\lambda_{j}} t_{\\delta}\n\\right]\n\\end{equation}\\] ein \\(\\delta\\)-Konfidenzintervall für die \\(j\\)te Komponente \\(\\beta_{j}\\) des Betaparameters \\(\\beta=\\left(\\beta_{1}, \\ldots, \\beta_{p}\\right)^{T}\\).\n\n\nBeweis. Wir müssen zeigen, dass \\[\\begin{equation}\n\\mathbb{P}\\left(\\kappa_{j} \\ni \\beta_{j}\\right) = \\delta.\n\\end{equation}\\] Dazu halten wir zunächst fest, dass für alle \\(j=1, \\ldots, p\\) bei Wahl von \\(\\beta_{0}=\\beta\\) und \\(c:=e_{j}\\) nach Theorem 29.1 für \\(T \\sim t(\\delta, n-p)\\) gilt, dass \\[\\begin{equation}\nT\n= \\frac{e_{j}^{T} \\hat{\\beta}-e_{j}^{T} \\beta}{\\sqrt{\\hat{\\sigma}^{2} e_{j}^{T}\\left(X^{T} X\\right)^{-1} e_{j}}}\n= \\frac{\\hat{\\beta}_{j}-\\beta_{j}}{\\sqrt{\\hat{\\sigma}^{2}\\left(\\left(X^{T} X\\right)^{-1}\\right)_{j j}}}\n= \\frac{\\hat{\\beta}_{j}-\\beta_{j}}{\\hat{\\sigma}\\sqrt{\\lambda_{j}}}\n=: T_{j}\n\\end{equation}\\] und \\[\\begin{equation}\n\\delta\n= \\frac{e_{j}^{T} \\beta-e_{j}^{T} \\beta}{\\sqrt{\\hat{\\sigma}^{2} e_{j}^{T}\\left(X^{T} X\\right)^{-1} e_{j}}}\n= 0\n\\end{equation}\\] Damit gilt dann auch sofort, dass \\(T_{j} \\sim t(n-p)\\). Weiterhin erinnern erinnern wir daran (vgl. Kapitel 23), dass per Definition von \\(t_{\\delta}\\) gilt, dass \\[\\begin{equation}\n\\mathbb{P}\\left(-t_{\\delta} \\leq T_{j} \\leq t_{\\delta}\\right).\n\\end{equation}\\] Aus der Definition eines \\(\\delta\\)-Konfidenzintervalls folgt dann \\[\\begin{equation}\n\\begin{aligned}\n\\delta\n& = \\mathbb{P}\\left(-t_{\\delta} \\leq T_{j} \\leq t_{\\delta}\\right) \\\\\n& = \\mathbb{P}\\left(-t_{\\delta} \\leq \\frac{\\hat{\\beta}_{j}-\\beta_{j}}{\\hat{\\sigma} \\sqrt{\\lambda_{j}}} \\leq t_{\\delta}\\right) \\\\\n& = \\mathbb{P}\\left(-t_{\\delta} \\hat{\\sigma} \\sqrt{\\lambda_{j}} \\leq \\hat{\\beta}_{j}-\\beta_{j} \\leq t_{\\delta} \\hat{\\sigma} \\sqrt{\\lambda_{j}}\\right) \\\\\n& = \\mathbb{P}\\left(-\\hat{\\beta}_{j}-t_{\\delta} \\hat{\\sigma} \\sqrt{\\lambda_{j}} \\leq-\\beta_{j} \\leq-\\hat{\\beta}_{j}+t_{\\delta} \\hat{\\sigma} \\sqrt{\\lambda_{j}}\\right) \\\\\n& =\\mathbb{P}\\left(\\hat{\\beta}_{j}+t_{\\delta} \\hat{\\sigma} \\sqrt{\\lambda_{j}} \\geq \\beta_{j} \\geq \\hat{\\beta}_{j}-t_{\\delta} \\hat{\\sigma} \\sqrt{\\lambda_{j}}\\right) \\\\\n& =\\mathbb{P}\\left(\\hat{\\beta}_{j}-t_{\\delta} \\hat{\\sigma} \\sqrt{\\lambda_{j}} \\leq \\beta_{j} \\leq \\hat{\\beta}_{j}+t_{\\delta} \\hat{\\sigma} \\sqrt{\\lambda_{j}}\\right) \\\\\n& =\\mathbb{P}\\left(\\left[\\hat{\\beta}_{j}-\\hat{\\sigma} \\sqrt{\\lambda_{j}} t_{\\delta}, \\hat{\\beta}_{j}+\\hat{\\sigma} \\sqrt{\\lambda_{j}} t_{\\delta}\\right]\\right) \\\\\n& =\\mathbb{P}\\left(\\kappa_{j} \\ni \\beta_{j}\\right)\n\\end{aligned}\n\\end{equation}\\] und damit ist alles gezeigt.\n\n\nBeispiel (1) Unabhängige und identisch normalverteilte Zufallsvariablen\nWie gewohnt betrachten wir als erstes Beispiel die ALM Form des Szenarios unabhängig und identisch normalverteilter Zufallsvariablen \\[\\begin{equation}\n\\upsilon \\sim N\\left(X \\beta, \\sigma^{2} I_{n}\\right) \\mbox{ mit }\nX := 1_{n} \\in \\mathbb{R}^{n}, \\beta:=\\mu \\in \\mathbb{R}, \\sigma^{2}&gt;0\n\\end{equation}\\] Dann gelten, wie bereits gesehen \\[\\begin{equation}\n\\hat{\\beta} = \\frac{1}{n} \\sum_{i=1}^{n} v_{i} =: \\bar{v},\n\\hat{\\sigma}^{2}=\\frac{1}{n-1} \\sum_{i=1}^{n}\\left(v_{i}-\\bar{v}\\right)^{2}=: s_{v}^{2}\n\\mbox{ und } \\lambda_{1}=\\left(1_{n}^{T} 1_{n}\\right)^{-1}=\\frac{1}{n}.\n\\end{equation}\\] Nach Theorem 29.2 gilt dann, dass \\[\\begin{equation}\n\\kappa:=\\left[\\bar{v}-\\frac{s}{\\sqrt{n}} t_{\\delta}, \\bar{v}+\\frac{s}{\\sqrt{n}} t_{\\delta}\\right]\n\\end{equation}\\] ein \\(\\delta\\)-Konfidenzintervall für \\(\\beta\\) ist und dieses ist offenbar identisch mit dem bekannten \\(\\delta\\)-Konfidenzintervall für den Erwartungsparameter der Normalverteilung.\n\n\nBeispiel (2) Einfache lineare Regression\nIn diesem Beispiel wollen wir nicht auf die spezifische Form der Konfidenzintervalle für den Offset- und Steigungsparameter eingehen, sondern lediglich anhand folgender Simulation an die Frequentistische Bedeutung eines \\(\\delta\\)-Konfidenzintervalls erinnern: Realisierungen von \\(\\delta\\)-Konfidenzintervallen überdecken den wahren, aber unbekannten, Parameterwert mit einer frequentistischen Wahrscheinlichkeit von \\(\\delta\\). ?fig-elr-konfidenzintervalle zeigt, basierend auf folgendem R Code, dass dies in der konkreten Simulation mit \\(\\delta=0.95\\) für den Offsetparameter in 94 von 100 und für den Steigungsparameter in 93 von 100 Fällen der Fall ist.\n\n# Modellformulierung\nlibrary(MASS)                                                                   # multivariate Normalverteilung\nset.seed(0)                                                                     # Random number generator seed\nns         = 1e2                                                                # Anzahl Simulationen\nn          = 10                                                                 # Anzahl von Datenpunkten\np          = 2                                                                  # Anzahl von Betaparametern\nx          = 1:n                                                                # Prädiktorwerte\nX          = matrix(c(rep(1,n),x), ncol = p)                                    # Designmatrix\nI_n        = diag(n)                                                            # Einheitsmatrix\nbeta       = matrix(c(1,2), nrow = 2)                                           # wahre, aber unbekannte, Betaparameter\nsigsqr     = 1                                                                  # wahrer, aber unbekannter, Varianzparameter\ndelta      = 0.95                                                               # Konfidenzbedingung  \nt_delta    = qt((1+delta)/2,n-1)                                                # \\Psi^{-1}((1+\\delta)/2,n-1)\nlambda     = diag(solve(t(X) %*% X))                                            # \\lambda_j Werte\n\n# Simulation\nkappa      = array(rep(NaN, ns*p*p), dim=c(ns,2,2))                             # Konfidenzintervallarray  \nbeta_hat   = matrix(rep(NaN,p*ns), nrow = p)                                    # Betaparameterschätzer\nfor(i in 1:ns){                                                                 # Iteration über Realisierungen\n  y              = mvrnorm(1, X %*% beta, sigsqr*I_n)                           # Datenrealisierung\n  beta_hat[,i]   = solve(t(X) %*% X) %*% t(X) %*% y                             # \\hat{\\beta}\n  eps_hat        = y - X %*% beta_hat[,i]                                       # \\hat{\\varepsilon}\n  sigsqr_hat     = (t(eps_hat) %*% eps_hat)/(n-p)                               # \\hat{\\sigma}^2\n  for(j in 1:p){                                                                # Iteration über Betaarraykomponenten\n    kappa[i,1,j] = beta_hat[j,i]-sqrt(sigsqr_hat*lambda[j])*t_delta             # untere KI Grenze\n    kappa[i,2,j] = beta_hat[j,i]+sqrt(sigsqr_hat*lambda[j])*t_delta             # obere  KI Grenze\n  }\n}\n\n\n\n\n\n\n\nAbbildung 29.3: Konfidenzintervalle für Betaparameterkomponenten bei einfacher linearer Regression.",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>T-Statistiken</span>"
    ]
  },
  {
    "objectID": "405-T-Statistiken.html#literaturhinweise",
    "href": "405-T-Statistiken.html#literaturhinweise",
    "title": "29  T-Statistiken",
    "section": "29.3 Literaturhinweise",
    "text": "29.3 Literaturhinweise\nBox (1981) und Zabell (2008) geben einen historischen Überblick zur Entwicklung der T-Statistik und ihrer Verteilung im Kontext der Arbeiten von Student (1908) und Fisher (1925c), Fisher (1925b) und Fisher (1925a). Die Theorie der Konfidenzintervalle geht auf Neyman (1935) und Neyman (1937) zurück.",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>T-Statistiken</span>"
    ]
  },
  {
    "objectID": "405-T-Statistiken.html#selbstkontrollfragen",
    "href": "405-T-Statistiken.html#selbstkontrollfragen",
    "title": "29  T-Statistiken",
    "section": "29.4 Selbstkontrollfragen",
    "text": "29.4 Selbstkontrollfragen\n\nGeben Sie die Definition der T-Statistik wieder.\nErläutern Sie für die T-Statistik die Bedeutung der Wahl von \\(c \\in \\mathbb{R}^{p}\\).\nErläutern Sie für die T-Statistik die Bedeutung der Wahl von \\(\\beta_{0} \\in \\mathbb{R}^{p}\\).\nWann kann die T-Statistik als Signal-zu-Rauschen Verhältnis interpretiert werden?\nGeben Sie das Theorem zur T-Statistik wieder.\nGeben Sie die Form der T-Statistik im Szenario von \\(n\\) u.i. normalverteilten Zufallsvariablen wieder.\nGeben Sie das Theorem zu Konfidenzintervallen für Betaparameterkomponenten wieder. \n\n\n\n\n\nBox, J. F. (1981). Gosset, Fisher, and the t Distribution. The American Statistician, 35(2), 61. https://doi.org/10.2307/2683142\n\n\nFisher, R. A. (1925a). Applications of \"Student’s\" Distribution. Metron, 5, 90–104.\n\n\nFisher, R. A. (1925b). Statistical Methods for Research Workers. Oliver & Boyd.\n\n\nFisher, R. A. (1925c). Theory of Statistical Estimation. Mathematical Proceedings of the Cambridge Philosophical Society, 22(5), 700–725. https://doi.org/10.1017/S0305004100009580\n\n\nNeyman, J. (1935). On the Problem of Confidence Intervals. The Annals of Mathematical Statistics, 6(3), 111–116. https://doi.org/10.1214/aoms/1177732585\n\n\nNeyman, J. (1937). Outline of a Theory of Statistical Estimation Based on the Classical Theory of Probability. Statistical Stimation.\n\n\nStudent. (1908). The Probable Error of a Mean. Biometrika, 6(1), 1–25.\n\n\nZabell, S. L. (2008). On Student’s 1908 Article „The Probable Error of a Mean“. Journal of the American Statistical Association, 103(481), 1–7. https://doi.org/10.1198/016214508000000030",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>T-Statistiken</span>"
    ]
  },
  {
    "objectID": "406-F-Statistiken.html",
    "href": "406-F-Statistiken.html",
    "title": "30  F-Statistiken",
    "section": "",
    "text": "30.1 Likelihood-Quotienten-Statistiken\nWir definieren den Begriff der Likelihood-Quotienten-Statistik wie folgt.\nEine Likelihood-Quotienten-Statistik setzt die Wahrscheinlichkeitsmasse/dichte eines beobachteten Datensatzes \\(y \\in \\mathcal{Y}\\) unter zwei Frequentistischen Inferenzmodellen nach Optimierung der jeweiligen Modellparameter ins Verhältnis. Ein hoher Wert der Likelihood-Quotienten-Statistik entspricht einer höhereren Wahrscheinlichkeitsmasse/dichte des beobachteten Datensatzes \\(y \\in \\mathcal{Y}\\) unter \\(\\mathcal{M}_{0}\\) als unter \\(\\mathcal{M}_{1}\\) und vice versa.\nDie Wahrscheinlichkeitsmassen/dichten beobachteter Daten nach Modellschätzung unter verschiedenen Modellen zu betrachten ist ein allgemeines Vorgehen zum Vergleich von Modellen. Letztlich erlaubt dieses Vorgehen, verschiedene wissenschaftliche Theorien über die Genese beobachtbarer Daten quantitativ zu vergleichen und die damit verbundene Unsicherheit zu quantifizieren. Modellvergleiche sind ein zentrales Thema in der Bayesianischen Inferenz die die Logik von Likelihood-Quotienten-Statistiken zum Beispiel unter den Begriffen der Bayes Factors oder der des Bayesian Information Criterions auf allgemeine probabilistische Modelle generalisiert. Allerdings sind, wie hier gesehen, Modellvergleiche auch im Rahmen der Frequentistischen Inferenz möglich und sinnvoll, Modellvergleiche sind also k ein Alleinstellungsmerkmal der Bayesianischen gegenüber der Frequentistischen Inferenz.\nMit dem reduzierten Modell und dem vollständigen Modell betrachten wir im Folgenden zwei spezielle Formen von \\(\\mathcal{M}_{0}\\) und \\(\\mathcal{M}_{1}\\), respektive, im Kontext des ALMs.\nMan sagt auch, dass das reduzierte Modell im vollständigen Modell geschachtelt (engl. nested) ist. Die Likelihood-Quotienten-Statistik beim Vergleich eines vollständigen und eines reduzierten Modells hat eine einfache Form. Diese ist der zentrale Aspekt folgenden Theorems.",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>F-Statistiken</span>"
    ]
  },
  {
    "objectID": "406-F-Statistiken.html#likelihood-quotienten-statistiken",
    "href": "406-F-Statistiken.html#likelihood-quotienten-statistiken",
    "title": "30  F-Statistiken",
    "section": "",
    "text": "Definition 30.1 (Likelihood-Quotienten-Statistik) Gegeben seien zwei Frequentistische Inferenzmodelle \\[\\begin{equation}\n\\mathcal{M}_{0}:=\\left(\\mathcal{Y}, \\mathcal{A},\\left\\{\\mathbb{P}_{\\theta_{0}}^{0} \\mid \\theta_{0} \\in \\Theta_{0}\\right\\}\\right) \\mbox{ und }\n\\mathcal{M}_{1}:=\\left(\\mathcal{Y}, \\mathcal{A},\\left\\{\\mathbb{P}_{\\theta_{1}}^{1} \\mid \\theta_{1} \\in \\Theta_{1}\\right\\}\\right)\n\\end{equation}\\] mit identischem Datenraum, identischer \\(\\sigma\\)-Algebra und potentiell distinkten Mengen von Wahrscheinlichkeitsmaßen und Parameterräumen. Sei weiterhin \\(v\\) ein Zufallsvektor mit Datenraum \\(\\mathcal{Y}\\). Seien schließlich \\(L_{0}^{\\upsilon}\\) und \\(L_{1}^{\\upsilon}\\) die Likelihood-Funktionen von \\(\\mathcal{M}_{0}\\) und \\(\\mathcal{M}_{1}\\), respektive, wobei das Superskript \\({ }^{\\upsilon}\\) jeweils an die Datenabhängigkeit der Likelihood Funktion erinnern soll. Dann wird \\[\\begin{equation}\n\\Lambda:=\\frac{\\max _{\\theta_{0} \\in \\Theta_{0}} L_{0}^{\\upsilon}\\left(\\theta_{0}\\right)}{\\max _{\\theta_{1} \\in \\Theta_{1}} L_{1}^{\\upsilon}\\left(\\theta_{1}\\right)}\n\\end{equation}\\] Likelihood-Quotienten-Statistik genannt.\n\n\n\n\n\nDefinition 30.2 (Vollständiges und reduziertes Modell.) Für \\(p&gt;1\\) mit \\(p=p_{0}+p_{1}\\) seien \\[\\begin{equation}\nX :=\n\\begin{pmatrix}\nX_{0} & X_{1}\n\\end{pmatrix}\n\\in \\mathbb{R}^{n \\times p}\n\\mbox{ mit } X_{0} \\in \\mathbb{R}^{n \\times p_{0}}\n\\mbox{ und } X_{1} \\in \\mathbb{R}^{n \\times p_{1}}\n\\end{equation}\\] sowie \\[\\begin{equation}\n\\beta :=\n\\begin{pmatrix}\n\\beta_{0} \\\\\n\\beta_{1}\n\\end{pmatrix} \\in \\mathbb{R}^{p}\n\\mbox{ mit } \\beta_{0} \\in \\mathbb{R}^{p_{0}}\n\\mbox{ und } \\beta_{1} \\in \\mathbb{R}^{p_{1}}\n\\end{equation}\\] Partitionierungen einer \\(n \\times p\\) Designmatrix und eines \\(p\\)-dimensionalen Betaparametervektors. Dann nennen wir \\[\\begin{equation}\n\\upsilon = X\\beta+\\varepsilon \\mbox{ mit } \\varepsilon \\sim N\\left(0_{n}, \\sigma^{2} I_{n}\\right)\n\\end{equation}\\] das vollständige Modell und \\[\\begin{equation}\n\\upsilon\n= X_{0}\\beta_{0} + \\varepsilon_{0} \\mbox{ mit } \\varepsilon_{0} \\sim N\\left(0_{n}, \\sigma^{2} I_{n}\\right)\n\\end{equation}\\] das reduzierte Modell und sprechen von einer Partitionierung eines (vollständigen) Modells.\n\n\n\nTheorem 30.1 (Likelihood-Quotienten-Statistik von vollständigem und reduziertem Modell.) Für \\(p=p_{0}+p_{1}, p&gt;1\\) sei eine Partitionierung eines vollständigen ALMs gegeben und es seien \\(\\hat{\\sigma}^{2}\\) und \\(\\hat{\\sigma}_{0}^{2}\\) die Maximum-Likelihood-Schätzer des Varianzparameters unter vollständigem und reduziertem Modell, respektive. Weiterhin seien die zwei parametrischen statistischen Modelle \\(\\mathcal{M}_{0}\\) und \\(\\mathcal{M}_{1}\\) in der Definition der Likelihood-Quotienten-Statistik durch das reduzierte Modell und das vollständige Modell gegeben. Dann gilt \\[\\begin{equation}\n\\Lambda=\\left(\\frac{\\hat{\\sigma}^{2}}{\\hat{\\sigma}_{0}^{2}}\\right)^{\\frac{n}{2}}\n\\end{equation}\\]\n\n\nBeweis. Wir erinnern zunächst daran, dass die Maximum-Likelihood-Schätzer des Varianzparameters durch \\[\\begin{equation}\n\\hat{\\sigma}^{2}=\\frac{1}{n}(\\upsilon-X \\hat{\\beta})^{T}(\\upsilon-X \\hat{\\beta}) \\mbox{ und }\n\\hat{\\sigma}_{0}^{2}=\\frac{1}{n}\\left(\\upsilon-X_{0} \\hat{\\beta}_{0}\\right)^{T}\\left(\\upsilon-X_{0} \\hat{\\beta}_{0}\\right)\n\\end{equation}\\] respektive, gegeben sind, wobei \\(\\hat{\\beta}\\) und \\(\\hat{\\beta}_{0}\\) die Maximum-Likelihood-Schätzer der Betaparameter unter vollständigem und reduziertem Modell, respektive, bezeichnen. Weiterhin halten wir fest, dass für die Likelihood-Funktion des vollständigen Modells an der Stelle der Maximum-Likelihood-Schätzer gilt, dass \\[\\begin{equation}\n\\begin{aligned}\nL_{1}^{y}\\left(\\hat{\\beta}, \\hat{\\sigma}^{2}\\right) & =(2 \\pi)^{-\\frac{n}{2}}\\left(\\hat{\\sigma}^{2}\\right)^{-\\frac{n}{2}} \\exp \\left(-\\frac{1}{2 \\hat{\\sigma}^{2}}(y-X \\hat{\\beta})^{T}(y-X \\hat{\\beta})\\right) \\\\\n& =(2 \\pi)^{-\\frac{n}{2}}\\left(\\hat{\\sigma}^{2}\\right)^{-\\frac{n}{2}} \\exp \\left(-\\frac{n}{2} \\frac{(y-X \\hat{\\beta})^{T}(y-X \\hat{\\beta})}{(y-X \\hat{\\beta})^{T}(y-X \\hat{\\beta})}\\right) \\\\\n& =(2 \\pi)^{-\\frac{n}{2}}\\left(\\hat{\\sigma}^{2}\\right)^{-\\frac{n}{2}} e^{-\\frac{n}{2}}\n\\end{aligned}\n\\end{equation}\\] und analog, dass für die Likelihood-Funktion des reduzierten Modells an der Stelle der Maximum-Likelihood-Schätzer gilt, dass \\[\\begin{equation}\nL_{0}^{y}\\left(\\hat{\\beta}_{0}, \\hat{\\sigma}_{0}^{2}\\right)=(2 \\pi)^{-\\frac{n}{2}}\\left(\\hat{\\sigma}_{0}^{2}\\right)^{-\\frac{n}{2}} e^{-\\frac{n}{2}}\n\\end{equation}\\] Damit ergibt sich dann aber \\[\\begin{equation}\n\\Lambda\n= \\frac{\\max _{\\theta_{0} \\in \\Theta_{0}} L_{0}^{\\upsilon}\\left(\\theta_{0}\\right)}{\\max _{\\theta_{1} \\in \\Theta_{1}} L_{1}^{\\upsilon}\\left(\\theta_{1}\\right)}\n= \\frac{L_{0}^{\\upsilon}\\left(\\hat{\\beta}_{0}, \\hat{\\sigma}_{0}^{2}\\right)}{L_{1}^{\\upsilon}\\left(\\hat{\\beta}, \\hat{\\sigma}^{2}\\right)}\n= \\frac{(2 \\pi)^{-\\frac{n}{2}}\\left(\\hat{\\sigma}_{0}^{2}\\right)^{-\\frac{n}{2}} e^{-\\frac{n}{2}}}{(2 \\pi)^{-\\frac{n}{2}}\\left(\\hat{\\sigma}^{2}\\right)^{-\\frac{n}{2}}e^{-\\frac{n}{2}}}\n=\\left(\\frac{\\hat{\\sigma}_{0}^{2}}{\\hat{\\sigma}^{2}}\\right)^{-\\frac{n}{2}}=\\left(\\frac{\\hat{\\sigma}^{2}}{\\hat{\\sigma}_{0}^{2}}\\right)^{\\frac{n}{2}}\n\\end{equation}\\]",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>F-Statistiken</span>"
    ]
  },
  {
    "objectID": "406-F-Statistiken.html#definition-und-verteilung",
    "href": "406-F-Statistiken.html#definition-und-verteilung",
    "title": "30  F-Statistiken",
    "section": "30.2 Definition und Verteilung",
    "text": "30.2 Definition und Verteilung\nWir definieren nun die F-Statistik vor dem Hintegrund eines vollständigen und eines reduzierten Modells.\n\nDefinition 30.3 (F-Statistik) Für \\(X \\in \\mathbb{R}^{n \\times p}, \\beta \\in \\mathbb{R}^{p}\\) und \\(\\sigma^{2}&gt;0\\) sei ein ALM der Form \\[\\begin{equation}\n\\upsilon = X\\beta+\\varepsilon \\mbox{ mit } \\varepsilon \\sim N\\left(0_{n}, \\sigma^{2} I_{n}\\right)\n\\end{equation}\\] mit der Partitionierung \\[\\begin{equation}\nX=\\begin{pmatrix}\nX_{0} & X_{1}\n\\end{pmatrix}, X_{0} \\in \\mathbb{R}^{n \\times p_{0}}, X_{1} \\in \\mathbb{R}^{n \\times p_{1}}, \\mbox{ und } \\beta:=\\begin{pmatrix}\n\\beta_{0} \\\\\n\\beta_{1}\n\\end{pmatrix}, \\beta_{0} \\in \\mathbb{R}^{p_{0}}, \\beta_{1} \\in \\mathbb{R}^{p_{1}}\n\\end{equation}\\] mit \\(p=p_{0}+p_{1}\\) gegeben. Weiterhin seien mit \\[\\begin{equation}\n\\hat{\\beta}_{0} :=\\left(X_{0}^{T} X_{0}\\right)^{-1} X_{0}^{T}\\upsilon \\mbox{ und }\n\\hat{\\beta}:=\\left(X^{T} X\\right)^{-1}X^{T}\\upsilon\n\\end{equation}\\] die Residuenvektoren \\[\\begin{equation}\n\\hat{\\varepsilon}_{0}:=\\upsilon-X_{0} \\hat{\\beta}_{0} \\mbox{ und } \\hat{\\varepsilon}:=\\upsilon-X \\hat{\\beta}\n\\end{equation}\\] definiert. Dann ist die F-Statistik definiert als \\[\\begin{equation}\nF:=\\frac{\\left(\\hat{\\varepsilon}_{0}^{T} \\hat{\\varepsilon}_{0}-\\hat{\\varepsilon}^{T} \\hat{\\varepsilon}\\right) / p_{1}}{\\hat{\\varepsilon}^{T} \\hat{\\varepsilon} /(n-p)}\n\\end{equation}\\]\n\nDer Zähler der F-Statistik \\[\\begin{equation}\n\\frac{\\hat{\\varepsilon}_{0}^{T} \\hat{\\varepsilon}_{0}-\\hat{\\varepsilon}^{T} \\hat{\\varepsilon}}{p_{1}}\n\\end{equation}\\] misst, inwieweit die \\(p_{1}\\) Regressoren in \\(X_{1}\\) die Residualquadratsumme reduzieren und zwar im Verhältnis zur Anzahl dieser Regressoren. Das heißt, dass bei gleicher Größe der Residualquadratsummenreduktion (und gleichem Nenner) ein größerer \\(F\\) Wert resultiert, wenn diese durch weniger zusätzliche Regressoren resultiert, also \\(p_{1}\\) klein ist (und vice versa). Im Sinne der Anzahl der Spalten von \\(X\\) und der entsprechenden Komponenten von \\(\\beta\\) favorisiert die \\(F\\)-Statistik also weniger “komplexe” Modelle.\nFür den Nenner der F-Statistik gilt \\[\\begin{equation}\n\\frac{\\hat{\\varepsilon}^{T} \\hat{\\varepsilon}}{n-p}=\\hat{\\sigma}^{2}\n\\end{equation}\\] wobei \\(\\hat{\\sigma}^{2}\\) hier der aufgrund des vollständigen Modells geschätzte Schätzer von \\(\\sigma^{2}\\) ist. Werden die Daten tatsächlich unter dem reduzierten Modell generiert, so kann das vollständige Modell dies durch \\(\\widehat{\\beta}_{2} \\approx 0_{p_{1}}\\) abbilden und erreicht eine ähnliche \\(\\sigma^{2}\\) Schätzung wie das reduzierte Modell. Werden die Daten de-facto unter dem vollständigem Modell generiert, so ist \\(\\hat{\\varepsilon}^{T} \\hat{\\varepsilon} /(n-p)\\) ein besserer Schätzer von \\(\\sigma^{2}\\) als \\(\\hat{\\varepsilon}_{0}^{T} \\hat{\\varepsilon}_{0} /(n-p)\\), da sich für diesen die Datenvariabilität, die nicht durch die \\(p_{0}\\) Regressoren in \\(X_{0}\\) erklärt wird, in der Schätzung von \\(\\sigma^{2}\\) widerspiegeln würde. Der Nenner der F-Statistik ist also in beiden Fällen der sinnvollere Schätzer von \\(\\sigma^{2}\\).\nZusammengenommen misst die F-Statistik also die Residualquadratsummenreduktion durch die \\(p_{1}\\) Regressoren in \\(X_{1}\\) gegenüber den \\(p_{0}\\) Regressoren in \\(X_{0}\\) pro Datenvariabilitäts \\(\\left(\\sigma^{2}\\right)\\) - und Regressor \\(\\left(p_{1}\\right)\\)-Einheit.\n\nBeispiel (1) Einfache lineare Regression\nExemplarisch evaluiert untenstehender R Code die F-Statistik im Kontext folgender Partitionierung des Modells der einfachen linearen Regression \\[\\begin{equation}\nX=\\begin{pmatrix}\nX_{0} & X_{1}\n\\end{pmatrix}, X_{0}:=1_{n}, X_{1}:=\\left(x_{1}, \\ldots, x_{n}\\right)^{T},\n\\beta:=\\begin{pmatrix}\n\\beta_{0} \\\\\n\\beta_{1}\n\\end{pmatrix}\n\\end{equation}\\] und zwar einmal für den Fall, dass \\(\\beta=(1,0)^{T}\\), also dass das reduzierte Modell das wahre, aber unbekannte, datenerzeugende Modell ist und einmal für den Fall, dass \\(\\beta=(1,0.5)^{T}\\), also dass das vollständige Modell das wahre, aber unbekannte, datenerzeugende Modell ist. Im Sinne obiger Diskussion ergibt sich im ersten Fall eine F-Statistik nahe Null, im zweiten Fall dagegen eine hohe F-Statistik.\n\n# Modellformulierung\nlibrary(MASS)                                                           # Multivariate Normalverteilung\nnmod   = 2                                                              # Anzahl Modelle\nn      = 10                                                             # Anzahl Datenpunkte\np      = 2                                                              # Anzahl Betaparameter\np_0    = 1                                                              # Anzahl Betaparameter reduziertes Modell\np_1    = 1                                                              # Anzahl zusätzlicher Betaparameter vollständiges Modell\np      = p_0 + p_1                                                      # Anzahl Betaparameter im vollständigem Modell\nx      = 1:n                                                            # Prädiktorwerte\nX      = matrix(c(rep(1,n),x), nrow = n)                                # Designmatrix des vollständigen Modells\nX_0    = X[,1]                                                          # Designmatrix des reduzierten Modells\nI_n    = diag(n)                                                        # n x n Einheitsmatrix\nbeta   = matrix(c(1,0,1,.5), nrow = 2)                                  # wahre , aber unbekannte , Betaparameter\nnscn   = ncol(beta)                                                     # Anzahl wahrer, aber unbekannter, Hypothesenszenarien\nsigsqr = 1                                                              # wahrer, aber unbekannter, Varianzparameter\n\n# Modellsimulation und Evaluierung\nEff    = matrix(rep(NaN, nscn), nrow = nscn)                            # F-Statistiik Realisierungsarray\nfor(s in 1:nscn){                                                       # Szenarieniterationen\n  y               = mvrnorm(1, X %*%beta[,s], sigsqr*I_n)               # Datenrealisierung\n  beta_hat_0      = solve(t(X_0)%*%X_0)%*%t(X_0)%*%y                    # Betaparameterschätzer reduziertes Modell\n  beta_hat        = solve(t(X)  %*%X  )%*%t(X)  %*%y                    # Betaparameterschätzer vollständiges Modell\n  eps_0_hat       = y-X_0%*%beta_hat_0                                  # Residuenvektor reduziertes Modell\n  eps_hat         = y-X%*%beta_hat                                      # Residuenvektor vollständiges Modell\n  eps_0_eps_0_hat = t(eps_0_hat) %*% eps_0_hat                          # RQS reduziertes Modell\n  eps_eps_hat     = t(eps_hat)   %*% eps_hat                            # RQS vollständiges Modell\n  Eff[s]          = (((eps_0_eps_0_hat-eps_eps_hat)/p_1)/               # F-Statistik\n                      (eps_eps_hat/(n-p)))}\n\n\n\nF-Statistik für beta_1  = 0_{p_1}: 1.148258 \nF-Statistik für beta_1 != 0_{p_1}: 4.036362\n\n\nDie Likelihood-Quotienten-Statistik und die F-Statistik von vollständigem und reduziertem Modell sind ineinander überführbar. Dies ist die zentrale Aussage folgenden Theorems.\n\nTheorem 30.2 (F-Statistik und Likelihood-Quotienten-Statistik.) Es sei die Partitionierung eines ALMs in ein vollständiges und ein reduziertes Modell gegeben und \\(F\\) und \\(\\Lambda\\) seien die entsprechenden F- und Likelihood-Quotienten-Statistiken. Dann gilt \\[\\begin{equation}\nF=\\frac{n-p}{p_{1}}\\left(\\Lambda^{-\\frac{2}{n}}-1\\right).\n\\end{equation}\\]\n\n\nBeweis. Wir erinnern zunächst daran, dass die Maximum-Likelihood Schätzer des Varianzparameters durch \\[\\begin{equation}\n\\hat{\\sigma}^{2}\n= \\frac{1}{n}(\\upsilon - X \\hat{\\beta})^{T}(\\upsilon-X \\hat{\\beta})\n= \\frac{\\hat{\\varepsilon}^{T} \\hat{\\varepsilon}}{n} \\text { und }  \\hat{\\sigma}_{0}^{2}\n= \\frac{1}{n}\\left(\\upsilon-X_{0} \\hat{\\beta}_{0}\\right)^{T}\\left(\\upsilon-X_{0} \\hat{\\beta}_{0}\\right)\n= \\frac{\\hat{\\varepsilon}_{0}^{T} \\hat{\\varepsilon}_{0}}{n}\n\\end{equation}\\] gegeben sind. Mit der Definition der F-Statistik und der Form der Likelihood-Quotienten-Statistik für den Vergleich von reduziertem und vollständigem Modell ergibt sich dann \\[\\begin{align}\n\\begin{split}\nF\n& =\\frac{\\left(\\hat{\\varepsilon}_{0}^{T} \\hat{\\varepsilon}_{0}-\\hat{\\varepsilon}^{T} \\hat{\\varepsilon}\\right) / p_{1}}{\\hat{\\varepsilon}^{T} \\hat{\\varepsilon} /(n-p)} \\\\\n& =\\frac{n\\left(\\hat{\\sigma}_{0}^{2}-\\hat{\\sigma}^{2}\\right) / p_{1}}{n \\hat{\\sigma}^{2} /(n-p)} \\\\\n& =\\frac{n-p}{p_{1}} \\frac{\\hat{\\sigma}_{0}^{2}-\\hat{\\sigma}^{2}}{\\hat{\\sigma}^{2}} \\\\\n& =\\frac{n-p}{p_{1}}\\left(\\frac{\\hat{\\sigma}_{0}^{2}}{\\hat{\\sigma}^{2}}-\\frac{\\hat{\\sigma}^{2}}{\\hat{\\sigma}^{2}}\\right) \\\\\n& =\\frac{n-p}{p_{1}}\\left(\\Lambda^{-\\frac{2}{n}}-1\\right).\n\\end{split}\n\\end{align}\\]\n\nZwischen der F-Statistik und der Likelihood-Quotienten-Statistik besteht also ein nichtlinearer, reziproker Zusammenhang, den wir für \\(n=12, p=2\\) und \\(p_{1}=1\\) in ?fig-f-lambda visualisieren. Man beachte, dass für \\(\\Lambda=1\\) gilt, dass \\(F=0\\). Ein Wert von \\(F=0\\) impliziert also, dass das reduzierte Modell gegenüber dem vollständigen Modell im Lichte eines beobachteten Datensatzes die gleiche Plausibilität besitzt.\n\n\n\n\n\n\nAbbildung 30.1: Zusammenhang von F- und Likelihood-Quotienten-Statistik.\n\n\n\nWir dokumentieren die Frequentistische Verteilung der F-Statistik in folgendem Theorem, auf dessen Beweis wir verzichten.\n\nTheorem 30.3 (F-Statistik) Für \\(X \\in \\mathbb{R}^{n \\times p}, \\beta \\in \\mathbb{R}^{p}\\) und \\(\\sigma^{2}&gt;0\\) sei ein ALM der Form \\[\\begin{equation}\n\\upsilon = X\\beta+\\varepsilon \\mbox{ mit } \\varepsilon \\sim N\\left(0_{n}, \\sigma^{2} I_{n}\\right)\n\\end{equation}\\] mit der Partitionierung \\[\\begin{equation}\nX\n=\n\\begin{pmatrix}\nX_{0} & X_{1}\n\\end{pmatrix}, X_{0} \\in \\mathbb{R}^{n \\times p_{0}}, X_{1} \\in \\mathbb{R}^{n \\times p_{1}}, \\mbox{ und }\n\\beta :=\n\\begin{pmatrix}\n\\beta_{0} \\\\\n\\beta_{1}\n\\end{pmatrix}, \\beta_{0} \\in \\mathbb{R}^{p_{0}}, \\beta_{1} \\in \\mathbb{R}^{p_{1}}\n\\end{equation}\\] mit \\(p=p_{0}+p_{1}\\) gegeben. Schließlich sei \\[\\begin{equation}\nc :=\n\\begin{pmatrix}\n0_{p_{0}} \\\\\n1_{p_{1}}\n\\end{pmatrix} \\in \\mathbb{R}^{p}\n\\end{equation}\\] ein Vektor. Dann gilt \\[\\begin{equation}\nF \\sim f\\left(\\delta, p_{1}, n-p\\right) \\mbox{ mit }\n\\delta:=\\frac{c^{T} \\beta\\left(c^{T}\\left(X^{T} X\\right)^{-1} c\\right)^{-1} c^{T} \\beta}{\\sigma^{2}}\n\\end{equation}\\]\n\nMan beachte, dass die \\(F\\)-Statistik eine Funktion des Parameterschätzers, \\(\\delta\\) dagegen eine Funktion der wahren, aber unbekannten, Parameter ist. Wie die Verteilung der \\(T\\)-Statistik kann die Verteilung der \\(F\\)-Statistik für die Evaluation von Frequentistischen Konfidenzintervallen und Hypothesentests genutzt werden. Insbesondere letzteren Aspekt verdeutlichen wir in Kapitel 32 und Kapitel 33.\n\n\nBeispiel (1) Einfache lineare Regression\nExemplarisch evaluieren wir mithilfe untenstehenden R Codes die Verteilung der F-Statistik im Kontext der Partitionierung des Modells der einfachen linearen Regression und zwar erneut einmal für den Fall, dass \\(\\beta=(1,0)^{T}\\), also dass das reduzierte Modell das wahre, aber unbekannte, datenerzeugende Modell ist und einmal für den Fall, dass \\(\\beta=(1,0.5)^{T}\\), also dass das vollständige Modell das wahre, aber unbekannte, datenerzeugende Modell ist. Abbildung 8.2 A und B visualisieren die resultierenden Verteilungen, respektive.\n\n# Modellformulierung\nlibrary(MASS)                                                           # Multivariate Normalverteilung\nnmod   = 2                                                              # Anzahl Modelle\nn      = 10                                                             # Anzahl Datenpunkte\np_0    = 1                                                              # Anzahl Betaparameter im reduzierten Modell\np_1    = 1                                                              # Anzahl additiver Betaparameter im vollständigen Modell\np      = p_0 + p_1                                                      # Anzahl Betaparameter im vollständigem Modell\nx      = 1:n                                                            # Prädiktorwerte\nX      = matrix(c(rep(1,n),x), nrow = n)                                # Designmatrix des vollständigen Modells\nX_0    = X[,1]                                                          # Designmatrix des reduzierten Modells\nI_n    = diag(n)                                                        # n x n Einheitsmatrix\nbeta   = matrix(c(1,0,1,.5), nrow = 2)                                  # wahre , aber unbekannte , Betaparameter\nnscn   = ncol(beta)                                                     # Anzahl wahrer, aber unbekannter, Hypothesenszenarien\nsigsqr = 1                                                              # wahrer, aber unbekannter, Varianzparameter\nc      = matrix((c(0,1)), nrow = 2)                                     # Vektor  \n\n# Frequentistische Simulation\nnsim   = 1e4                                                            # Anzahl Realisierungen des n-dimensionalen ZVs\ndelta  = rep(NaN,nscn)                                                  # Nichtzentralitätsparameterarray\nEff    = matrix(rep(NaN, nscn*nsim), nrow = nscn)                       # F-Statistiik Realisierungsarray\nfor(s in 1:nscn){                                                       # Szenarieniterationen\n  delta[s] = (t(t(c)%*%beta[,s])%*%                                     # Nichtzentralitätsparameter\n              solve(t(c)%*%solve(t(X)%*%X)%*%c) %*%\n              (t(c)%*%beta[,s])/sigsqr)\n  for(i in 1:nsim){                                                     # Simulationsiterationen\n    y               = mvrnorm(1, X %*%beta[,s], sigsqr*I_n)             # Datenrealisierung\n    beta_hat_0      = solve(t(X_0)%*%X_0)%*%t(X_0)%*%y                  # Betaparameterschätzer reduziertes Modell\n    beta_hat        = solve(t(X)  %*%X  )%*%t(X)  %*%y                  # Betaparameterschätzer vollständiges Modell\n    eps_0_hat       = y-X_0%*%beta_hat_0                                # Residuenvektor reduziertes Modell\n    eps_hat         = y-X%*%beta_hat                                    # Residuenvektor vollständiges Modell\n    eps_0_eps_0_hat = t(eps_0_hat) %*% eps_0_hat                        # RQS reduziertes Modell\n    eps_eps_hat     = t(eps_hat)   %*% eps_hat                          # RQS vollständiges Modell\n    Eff[s,i]        = (((eps_0_eps_0_hat-eps_eps_hat)/p_1)/             # F-Statistik\n                         (eps_eps_hat/(n-p)))}}\n\n\n\n\n\n\n\nAbbildung 30.2: Exemplarische F-Statistik Verteilungen unter reduziertem und vollständigem wahren, aber unbekanntem, datenerzeugenden Modell bei einfacher lineare Regression.",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>F-Statistiken</span>"
    ]
  },
  {
    "objectID": "406-F-Statistiken.html#literaturhinweise",
    "href": "406-F-Statistiken.html#literaturhinweise",
    "title": "30  F-Statistiken",
    "section": "30.3 Literaturhinweise",
    "text": "30.3 Literaturhinweise\nDie Popularität von F-Statistiken, insbesondere im Kontext der Varianzanalyse, wird allgemein auf Fisher (1925) zurückgeführt. Seal (1967) gibt einen historischen Überblick. Likelihood-Quotienten-Statistiken werden insbesondere von Neyman & Pearson (1928) und Wilks (1938) betrachtet. Lehmann (2011) gibt einen integrierten historischen Überblick zu beiden Ansätzen. Die hier diskutierte Äquivalenz von Likelihood-Quotienten-Statistik und F-Statistik basiert auf der Darstellung in Seber & Lee (2003).",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>F-Statistiken</span>"
    ]
  },
  {
    "objectID": "406-F-Statistiken.html#selbstkontrollfragen",
    "href": "406-F-Statistiken.html#selbstkontrollfragen",
    "title": "30  F-Statistiken",
    "section": "30.4 Selbstkontrollfragen",
    "text": "30.4 Selbstkontrollfragen\n\nGeben Sie die Definition der Likelihood-Quotienten-Statistik wieder.\nErläutern Sie die Definition der Likelihood-Quotienten-Statistik.\nGeben Sie die Definition eines vollständigem und eines reduziertem ALMs wieder.\nGeben Sie das Theorem zum Likelihood-Quotienten von vollständigem und reduzierten ALM wieder.\nDefinieren Sie die F-Statistik.\nErläutern Sie den Zähler der F-Statistik.\nErläutern Sie den Nenner der F-Statistik.\nErläutern Sie die F-Statistik.\nGeben Sie das Theorem zu F-Statistik und Likelihood-Quotienten-Statistik wieder.\nGeben Sie das Theorem zur Verteilung der F-Statistik wieder. \n\n\n\n\n\nFisher, R. A. (1925). Applications of \"Student’s\" Distribution. Metron, 5, 90–104.\n\n\nLehmann, E. L. (2011). Fisher, Neyman, and the Creation of Classical Statistics. Springer New York. https://doi.org/10.1007/978-1-4419-9500-1\n\n\nNeyman, J., & Pearson, E. S. (1928). On the Use and Interpretation of Certain Test Criteria for Purposes of Statistical Inference: Part I. Biometrika, 20A(1/2), 175. https://doi.org/10.2307/2331945\n\n\nSeal, H. L. (1967). Studies in the History of Probability and Statistics. XV: The Historical Development of the Gauss Linear Model. Biometrika, 54(1/2), 1. https://doi.org/10.2307/2333849\n\n\nSeber, G. A. F., & Lee, A. J. (2003). Linear Regression Analysis (2nd ed). Wiley-Interscience.\n\n\nWilks, S. S. (1938). The Large-Sample Distribution of the Likelihood Ratio for Testing Composite Hypotheses. The Annals of Mathematical Statistics, 9(1), 60–62. https://doi.org/10.1214/aoms/1177732360",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>F-Statistiken</span>"
    ]
  },
  {
    "objectID": "407-T-Tests.html",
    "href": "407-T-Tests.html",
    "title": "31  T-Tests",
    "section": "",
    "text": "31.1 Einstichproben-T-Tests",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>T-Tests</span>"
    ]
  },
  {
    "objectID": "407-T-Tests.html#einstichproben-t-tests",
    "href": "407-T-Tests.html#einstichproben-t-tests",
    "title": "31  T-Tests",
    "section": "",
    "text": "Anwendungsszenario\nDas Anwendungsszenario eines Einstichproben-T-Test ist bekanntlich dadurch gekennzeichnet, dass \\(n\\) univariate Datenpunkte einer Stichprobe (Gruppe) randomisierter experimenteller Einheiten betrachtet werden, von denen angenommen wird, dass sie Realisierungen von \\(n\\) unabhängigen und identisch normalverteilten Zufallsvariablen sind. Hinsichtlich der identischen univariaten Normalverteilungen \\(N\\left(\\mu, \\sigma^{2}\\right)\\) dieser Zufallsvariablen wird angenommen, dass sowohl der Erwartungswertparameter \\(\\mu\\) als auch der Varianzparameter \\(\\sigma^{2}\\) unbekannt sind. Schließlich wird vorausgesetzt, dass ein Interesse an einem inferentiellen Vergleich des unbekannten Erwartungswertparameters \\(\\mu\\) mit einen vorgebenenen Wert \\(\\mu_{0}\\) (z.B. \\(\\mu_{0}:=0\\) ) besteht.\n\n\nAnwendungsbeispiel\nFür ein konkretes Anwendungsbeispiel betrachten wir die Analyse von Pre-PostInterventions-BDI-Differenzwerten einer Gruppe von \\(n=12\\) Patient:innen wie in Tabelle 31.1 dargestellt. Die ersten beiden Spalten dieser Tabelle listen die patientenspezifische BDI Werte vor (PreBDI) und nach (PosBDI) der psychotherapeutischen Intervention, die dritte Spalte dBDI zeigt die entsprechenden PreBDI-PosBDI Differenzwerte. Ein positiver Wert entspricht hier einer Verbesserung der Depressionssymptomatik und ein negativer Wert einer Verschlechterung der Depressionssymptomatik\n\n\n\n\nTabelle 31.1: Pre- und Post-Intervention BDI Werte\n\n\n\n\n\n\nPreBDI\nPosBDI\ndBDI\n\n\n\n\n29\n29\n0\n\n\n32\n27\n5\n\n\n28\n27\n1\n\n\n36\n22\n14\n\n\n32\n25\n7\n\n\n28\n28\n0\n\n\n33\n31\n2\n\n\n33\n27\n6\n\n\n33\n28\n5\n\n\n30\n27\n3\n\n\n36\n23\n13\n\n\n32\n26\n6\n\n\n\n\n\n\n\n\nBei der Anwendung eines Einstichproben-T-Tests auf die dBDI Daten dieses Datensatzes nehmen wir also an, dass die dBDI Daten Realisierungen von \\(n=12\\) unabhängig normalverteilten Zufallsvariablen \\(\\upsilon_i \\sim N\\left(\\mu, \\sigma^{2}\\right)\\) sind. Wir nehmen weiterhin an, dass wir daran interessiert sind, unsere Unsicherheit beim inferentiellen Vergleich des wahren, aber unbekannten, Erwartungswertparameters \\(\\mu\\) mit einem Vergleichswert \\(\\mu_{0}\\) im Sinne eines Hypothesentests zu quantifizieren.\nUnabhängig von diesem inferenzstatistischen Vorgehen betrachten wir zunächst die deskriptiven Statistiken der dBDI Daten, wie in Tabelle 31.2 dargestellt. Es fällt insbesondere auf, dass das Stichprobenmittel im Vergleich zur Standardabweichung relativ klein ist. Im Gruppenmittel unterscheiden sich die PreBDI und PosBDI also zwar in positiver Richtung, was eine Verringerung der Depressionssymptomatik impliziert, allerdings streuen die Daten auch über Patient:innen deutlich, wie auch bereits aus Tabelle 31.1 ersichtlich.\n\n\n\n\nTabelle 31.2: Deskriptivstatistiken der Pre-Post BDI Differenzwerte\n\n\n\n\n\n\n\nn\nMax\nMin\nMedian\nMean\nVar\nStd\n\n\n\n\nF2F\n12\n14\n0\n5\n5.17\n20.88\n4.57\n\n\n\n\n\n\n\n\n\n\nModellformulierung\nWir definieren nun das Einstichproben-T-Test-Modell wie folgt.\n\nDefinition 31.1 (Einstichproben-T-Test-Modell) Für \\(i=1, \\ldots, n\\) seien \\(\\upsilon_i\\) Zufallsvariablen, die die \\(n\\) Datenpunkte eines Einstichproben-T-Test-Szenarios modellieren. Dann hat das Einstichproben-T-Test-Modell die strukturelle Form \\[\\begin{equation}\n\\upsilon_i\n= \\mu+\\varepsilon_{i}\n\\mbox{ mit } \\varepsilon_{i} \\sim N\\left(0, \\sigma^{2}\\right)\n\\mbox{ u.i.v für } i=1, \\ldots, n \\mbox{ mit } \\mu \\in \\mathbb{R} \\mbox{ und } \\sigma^{2}&gt;0,\n\\end{equation}\\] die Datenverteilungsform \\[\\begin{equation}\n\\upsilon_i \\sim N\\left(\\mu, \\sigma^{2}\\right)\n\\mbox{ u.i.v für } i=1, \\ldots, n \\mbox{ mit } \\mu \\in \\mathbb{R} \\mbox{ und } \\sigma^{2}&gt;0,\n\\end{equation}\\] und für den Datenvektor \\(v=\\left(\\upsilon_{1}, \\ldots, \\upsilon_{n}\\right)^{T}\\) die Designmatrixform \\[\\begin{equation}\n\\upsilon = X\\beta+\\varepsilon\n\\mbox{ mit } X:=1_{n} \\in \\mathbb{R}^{n \\times 1},\n\\beta:=\\mu \\in \\mathbb{R},\n\\varepsilon \\sim N\\left(0_{n}, \\sigma^{2} I_{n}\\right)\n\\mbox{ und } \\sigma^{2}&gt;0.\n\\end{equation}\\]\n\nDas Modell des Einstichproben-T-Tests ist offenbar mit dem dem Modell unabhängiger und identisch normalverteilter Zufallsvariablen identisch (vgl. Kapitel 27). Die Äquivalenz von struktureller, Datenverteilungs- und Designmatrixform des Einstichproben-T-TestModells wurde in Kapitel 27 bereits ausführlich diskutiert. Die Simulation von Daten basierend auf dem Einstichproben-T-Test-Modell hat dementsprechend die gleiche Form wie die Simulation unabhängig und identisch normalverteilter Zufallsvariablen. Unterer R Code demonstriert dies.\n\n# Modellformulierung\nlibrary(MASS)                                                         # Multivariate Normalverteilung\nn      = 12                                                           # Anzahl von Datenpunkten\np      = 1                                                            # Anzahl von Betaparameter\nX      = matrix(rep(1,n), nrow = n)                                   # Designmatrix\nI_n    = diag(n)                                                      # n x n Einheitsmatrix\nbeta   = 5                                                            # wahrer, aber unbekannter, Betaparameter\nsigsqr = 14                                                           # wahrer, aber unbekannter, Varianzparameter\n\n# Datenrealisierung\ny      = mvrnorm(1, X %*% beta, sigsqr*I_n)                           # eine Realisierung eines n-dimensionalen ZVs\n\n\n\nModellschätzung\nDa die Form des Einstichproben-T-Test-Modells mit dem Szenario unabhängig und identisch normalverteilter Zufallsvariablen identisch ist, trifft dies auch auf die entsprechenden Beta- und Varianzparameterschätzer zu. Es ergibt sich also folgendes Theorem, das bereits in Kapitel 28 bewiesen wurde.\n\nTheorem 31.1 (Parameterschätzer im Einstichproben-T-Test-Modell) Gegeben sei die Designmatrixform des Einstichproben-T-Test-Modells. Dann ergeben sich für den Betaparameterschätzer \\[\\begin{equation}\n\\hat{\\beta}=\\frac{1}{n} \\sum_{i=1}^{n} \\upsilon_i=: \\bar{\\upsilon}\n\\end{equation}\\] und für den Varianzparameterschätzer \\[\\begin{equation}\n\\hat{\\sigma}^{2}=\\frac{1}{n-1} \\sum_{i=1}^{n}\\left(\\upsilon_i-\\bar{\\upsilon}\\right)^{2}=: s_{\\upsilon}^{2}\n\\end{equation}\\] \\(\\bar{\\upsilon}\\) und \\(s_{\\upsilon}^{2}\\) bezeichnen hier also wiederum das Stichprobenmittel und die Stichprobenvarianz der Zufallsvariablen \\(\\upsilon_{1}, \\ldots, \\upsilon_{n}\\).",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>T-Tests</span>"
    ]
  },
  {
    "objectID": "407-T-Tests.html#modellevaluation",
    "href": "407-T-Tests.html#modellevaluation",
    "title": "31  T-Tests",
    "section": "31.2 Modellevaluation",
    "text": "31.2 Modellevaluation\nBasierend auf Definition 29.1 formulieren wir nun die T-Teststatistik für das Einstichproben-T-Test Szenario und geben ihre frequentistische Verteilung an.\n\nTheorem 31.2 (T-Teststatistik des Einstichproben-T-Tests) Gegeben sei die Designmatrixform des Einstichproben-T-Test-Modells. Dann ergibt sich für die T-Teststatistik mit \\[\\begin{equation}\nc:=1 \\mbox{ und } c^{T} \\beta_{0}=: \\mu_{0},\n\\end{equation}\\] dass \\[\\begin{equation}\nT = \\sqrt{n}\\left(\\frac{\\bar{\\upsilon}-\\mu_{0}}{s_{\\upsilon}}\\right)\n\\end{equation}\\] und es gilt, dass \\[\\begin{equation}\nT \\sim t(\\delta, n-1) \\mbox{ mit } \\delta=\\sqrt{n}\\left(\\frac{\\mu-\\mu_{0}}{\\sigma}\\right).\n\\end{equation}\\]\n\n\nBeweis. Mit Theorem 29.1 gilt \\[\\begin{equation}\nT\n=\\frac{c^{T} \\hat{\\beta}-c^{T} \\beta_{0}}{\\sqrt{\\hat{\\sigma}^{2} c^{T}\\left(X^{T} X\\right)^{-1} c}}\n=\\frac{1^{T} \\bar{\\upsilon}-1^{T} \\mu_{0}}{\\sqrt{s_{\\upsilon}^{2} 1^{T}\\left(1_{n}^{T} 1_{n}\\right)^{-1} 1}}\n=\\sqrt{n}\\left(\\frac{\\bar{\\upsilon}-\\mu_{0}}{s_{\\upsilon}}\\right) .\n\\end{equation}\\] Weiterhin gilt mit demselben Theorem \\[\\begin{equation}\n\\delta\n=\\frac{c^{T} \\beta-c^{T} \\beta_{0}}{\\sqrt{\\sigma^{2} c^{T}\\left(X^{T} X\\right)^{-1} c}}\n=\\frac{1^{T} \\mu-1^{T} \\mu_{0}}{\\sqrt{\\sigma^{2} 1^{T}\\left(1_{n}^{T} 1_{n}\\right)^{-1} 1}}=\\sqrt{n}\\left(\\frac{\\mu-\\mu_{0}}{\\sigma}\\right).\n\\end{equation}\\]\n\nDie Formen der T-Teststatistik und ihre Verteilung im Einstichproben-T-Test-Spezialfall des ALMs sind also natürlicherweise mit den entsprechenden Formen im ALM-freien Kontext identisch. Die enstprechende Theorie zu Konfidenzintervallen und der Kontrolle des Testumfangs bei Einstichproben-T-Tests sowie der Gebrauch der Testgütefunktion zur Evaluation der Testtrennschärfe (Power) folgt also analog.\n\nAnwendungsbeispiel\nFolgender R Code demonstriert die Evaluation eines 95%-Konfidenzintervalls für den Erwartungswertparameter \\(\\mu\\) sowie Durchführung eines zweiseitigen Einstichproben-T-Tests mit einfacher Nullhypothese \\(\\Theta_{0}:=\\{0\\}\\) und Signifikanzlevel \\(\\alpha_{0}:=0.05\\) für das oben skizzierte Anwendungsbeispiel.\n\n# Datenanalyse\nD           = read.csv(\"./_data/407-t-tests.csv\")                        # Laden des Datensatzes\ny           = D$dBDI[D$COND == \"F2F\"]                                    # Post-Pre Differenzwerte\nn           = length(y)                                                  # Anzahl Datenpunkte\np           = 1                                                          # Anzahl Betaparameter\nc           = 1                                                          # Kontrastgewichtsvektor\nmu_0        = 0                                                          # Nullhypothesenparameter       \ndelta       = 0.95                                                       # Konfidenzlevel\nalpha_0     = 0.05                                                       # Signifikanzlevel  \nX           = matrix(rep(1,n), nrow = n)                                 # Einstichproben-T-Test Designmatrix \nbeta_hat    = solve(t(X)%*%X)%*%t(X)%*%y                                 # Betaparameterschätzer\neps_hat     = y - X %*% beta_hat                                         # Residuenvektor \nsigsqr_hat  = (t(eps_hat) %*% eps_hat)/(n-p)                             # Varianzparameterschätzer\nt_delta     = qt((1+delta)/2,n-1)                                        # \\Psi^{-1}(1+\\delta)/2, n-1)\nlambda      = diag(solve(t(X) %*% X))                                    # \\lambda_j Werte\nkappa_u     = beta_hat - sqrt(sigsqr_hat*lambda)*t_delta                 # untere Konfidenzintervallgrenze\nkappa_o     = beta_hat + sqrt(sigsqr_hat*lambda)*t_delta                 # obere Konfidenzintervallgrenze\nt_num       = t(c) %*% beta_hat - mu_0                                   # Zähler der Einstichproben-T-Teststatistik\nt_den       = sqrt(sigsqr_hat %*% t(c) %*% solve(t(X) %*% X) %*% c)      # Nenner der Einstichproben-T-Teststatistik\nt           = t_num/t_den                                                # Wert der Einstichproben-T-Teststatistik\npval        = 2*(1 - pt(abs(t), n-1))                                    # p-Wert bei zweiseitigem Einstichproben-T-Test\nk_alpha_0   = qt(1-alpha_0/2, n-1)                                       # kritischer Wert \nif(abs(t) &gt; k_alpha_0){phi = 1} else {phi = 0}                           # Einstichproben-T-Test\n\n\n\nBetaparameterschätzer            :  5.17 \n95%-Konfidenzintervall          :  2.26 8.07 \nVarianzparameterschätzer        :  20.88 \nalpha_0                         :  0.05 \nKritischer Wert                 :  2.2 \nEinstichproben-T-Teststatistik  :  3.92 \nphi                             :  1 \np-Wert                          :  0\n\n\nDie Nullhypothese wird in diesem Fall bei einem einem kritischen Wert von \\(k_{0.05}=2.20\\) und einem Wert der T-Statistik von \\(T=3.91\\) abgelehnt. Das \\(95/\\%\\)-Konfidenzintervall für den wahren, aber unbekannten, Erwartungswertparameter ist \\([2.26, 8.07]\\), überdeckt also den Nullhypothesenparameterwert \\(\\mu_0=0\\) nicht.",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>T-Tests</span>"
    ]
  },
  {
    "objectID": "407-T-Tests.html#zweistichproben-t-tests",
    "href": "407-T-Tests.html#zweistichproben-t-tests",
    "title": "31  T-Tests",
    "section": "31.3 Zweistichproben-T-Tests",
    "text": "31.3 Zweistichproben-T-Tests\n\nAnwendungsszenario\nDas Anwendungsszenario eines Zweistichproben-T-Tests für unabhängige Stichproben ist bekanntlich dadurch gekennzeichnet, dass insgesamt \\(n\\) univariate Datenpunkte zweier Stichproben (Gruppen) randomisierter experimenteller Einheiten betrachtet werden. Es wird dabei insbesondere angenommen, dass die \\(n_{1}\\) univariaten Datenpunkte der ersten Gruppe Realisierungen von \\(n_{1}\\) unabhängigen und identisch normalverteilten Zufallsvariablen mit Erwartungswertparameter \\(\\mu_{1}\\) und Varianzparameter \\(\\sigma^{2}\\) sind, während weiterhin angenommen wird, dass die \\(n_{2}\\) univariaten Datenpunkte der zweiten Gruppe Realisierungen von \\(n_{2}\\) unabhängigen und identisch normalverteilten Zufallsvariablen mit Erwartungswertparameter \\(\\mu_{2}\\) und Varianzparameter \\(\\sigma^{2}\\) sind. Es wird also insbesondere angenommen, dass sich die wahren, aber unbekannten, Erwartungswertparameter beider Gruppen von Zufallsvariablen unterscheiden können, die Varianzparameter beider Gruppen dagegen werden als identisch angenommen. Schließlich wird voraussgesetzt, dass ein Interesse am inferentiellen Vergleich der unbekannten Erwartungswertparameter \\(\\mu_{1}\\) und \\(\\mu_{2}\\) besteht, so zum Beispiel ihrer Gleichheit \\(\\mu_{1}=\\mu_{2}\\) oder Verschiedenheit \\(\\mu_{1} \\neq \\mu_{2}\\).\n\n\nAnwendungsbeispiel\nFür ein konkretes Anwendungsbeispiel betrachten wird die Analyse von Pre-Post-Interventions-BDI-Differenzwerten zweier Gruppen von je 12 Patient:innen in unterschiedlichen Therapiesettings, wie in Tabelle 31.3 dargestellt. Die erste Spalte der Tabelle (COND) listet das patientenspezifische Therapiesetting (F2F: face-to-face, ONL: online) auf. Die zweite Spalte der Tabelle (dBDI) listet die entsprechenden patientenspezifischen Pre-Post-Interventions-BDI-Differenzwerte auf. Positive Werte entsprechen hier erneut einer Abnahme der Depressionssymptomatik, negative Werte einer Zunahme der Depressionssymptomatik.\n\n\n\n\nTabelle 31.3: Pre-Post-BDI Differenzwerte für zwei Stichproben\n\n\n\n\n\n\nCOND\ndBDI\n\n\n\n\nF2F\n0\n\n\nF2F\n5\n\n\nF2F\n1\n\n\nF2F\n14\n\n\nF2F\n7\n\n\nF2F\n0\n\n\nF2F\n2\n\n\nF2F\n6\n\n\nF2F\n5\n\n\nF2F\n3\n\n\nF2F\n13\n\n\nF2F\n6\n\n\nONL\n5\n\n\nONL\n-1\n\n\nONL\n7\n\n\nONL\n4\n\n\nONL\n7\n\n\nONL\n10\n\n\nONL\n7\n\n\nONL\n6\n\n\nONL\n11\n\n\nONL\n10\n\n\nONL\n5\n\n\nONL\n-2\n\n\n\n\n\n\n\n\nZu Anwendung eines Zweistichproben-T-Tests auf die dBDI Daten nehmen wir an, dass die 12 Datenpunkte der F2F Therapiegruppe Realisierungen von \\(n_{1}=12\\) unabhängig und identisch normalverteilten Zufallsvariablen \\(\\upsilon_{1j} \\sim N\\left(\\mu_{1}, \\sigma^{2}\\right)\\) mit \\(j=1, \\ldots, n_{1}\\) sind und dass die 12 Datenpunkte der ONL Therapiegruppe Realisierungen von \\(n_{2}=12\\) unabhängig und identisch normalverteilten Zufallsvariablen \\(\\upsilon_{2j} \\sim N\\left(\\mu_{2}, \\sigma^{2}\\right)\\) mit \\(j=1, \\ldots, n_{2}\\) sind.\nUnabhängig von dem unten beschriebenen inferenzstatistischen Vorgehen betrachten wir auch hier zunächst die deskriptiven Statistiken der Therapiesetting-spezifischen dBDI Werte. Diese sind in Tabelle 31.4 aufgeführt.\n\n\n\n\nTabelle 31.4: Deskriptivstatistiken der Pre-Post BDI Differenzwerte bei unterschiedlichen Therapiesettings\n\n\n\n\n\n\n\nn\nMax\nMin\nMedian\nMean\nVar\nStd\n\n\n\n\nF2F\n12\n14\n0\n5.0\n5.17\n20.88\n4.57\n\n\nONL\n12\n11\n-2\n6.5\n5.75\n16.20\n4.03\n\n\n\n\n\n\n\n\n\n\nModellformulierung\nMit dem Index \\(i\\) für die Gruppen und dem Index \\(j\\) für die experimentellen Einheiten in jeder Gruppe definieren wir das Zweistichproben-T-Test-Modell wie folgt.\n\nDefinition 31.2 (Zweistichproben-T-Test-Modell) Für \\(i=1,2\\) und \\(j=1,\\ldots,n_{i}\\) seien \\(\\upsilon_{ij}\\) Zufallsvariablen, die die \\(n=n_{1}+n_{2}\\) Datenpunkte eines Zweistichproben-T-Test Szenarios modellieren. Dann hat das Zweistichproben-T-Test-Modell die strukturelle Form \\[\\begin{equation}\n\\upsilon_{ij} = \\mu_{i}+\\varepsilon_{ij}\n\\mbox{ mit } \\varepsilon_{ij} \\sim N\\left(0, \\sigma^{2}\\right)\n\\mbox{ u.i.v. mit } \\mu_{i} \\in \\mathbb{R} \\mbox{ und } \\sigma^{2}&gt;0,\n\\end{equation}\\] die Datenverteilungsform \\[\\begin{equation}\n\\upsilon_{ij} \\sim N\\left(\\mu_{i}, \\sigma^{2}\\right)\n\\mbox{ u.i.v. mit } \\mu_{i} \\in \\mathbb{R} \\mbox{ und } \\sigma^{2}&gt;0,\n\\end{equation}\\] und für den \\(n\\)-dimensionalen Datenvektor definiert als \\[\\begin{equation}\n\\upsilon := \\left(\\upsilon_{11}, \\cdots, \\upsilon_{1n_{1}},\n                  \\upsilon_{21}, \\cdots, \\upsilon_{2n_{2}}\\right)^{T}\n\\end{equation}\\] die Designmatrixform \\[\\begin{equation}\n\\upsilon = X\\beta+\\varepsilon\n\\end{equation}\\] mit \\[\\begin{equation}\nX:=\n\\begin{pmatrix}\n1_{n_{1}} & 0_{n_{1}} \\\\\n0_{n_{2}} & 1_{n_{2}}\n\\end{pmatrix} \\in \\mathbb{R}^{n \\times 2},\n\\beta:=\n\\begin{pmatrix}\n\\mu_{1} \\\\\n\\mu_{2}\n\\end{pmatrix} \\in \\mathbb{R}^{2},\n\\varepsilon \\sim N\\left(0_{n}, \\sigma^{2} I_{n}\\right), \\sigma^{2}&gt;0 .\n\\end{equation}\\]\n\nDie hier gewählte Definition des Zweistichproben-T-Test-Modells in Designmatrixform ist nicht die einzig mögliche, jedoch diejenige, unter der sich am klarsten die Äquivalenz zum Zweistichproben-T-Test-Modell im ALM-freien Kontext erkennen lässt. In Kapitel 32 lernen wir eine alternative Parameterisierung auch des Zweistichproben-T-Test-Modells kennen. Wie schon beim Szenario des Einstichproben-T-Tests ergibt sich die Äquivalenz der in Definition 31.2 formulierten Modellformen mit den Ergebnissen in Kapitel 27. Die Simulation von Daten basierend auf dem Zweistichproben-T-Test-Modell ist, bis auf die Definition von Designmatrix und Betaparametervektor mit den bisher bekannten Simulationen von ALM Spezialfällen identisch, wie folgender R Code demonstriert.\n\n# Modellformulierung\nlibrary(MASS)                                # Multivariate Normalverteilung\nn_1    = 12                                  # Anzahl von Datenpunkten Gruppe 1\nn_2    = 12                                  # Anzahl von Datenpunkten Gruppe 2\nn      = n_1 + n_2                           # Gesamtanzahl Datenpunkte\np      = 2                                   # Anzahl von Betaparameter\nX      = matrix(c(rep(1,n_1), rep(0,n_1),    # Designmatrix\n                  rep(0,n_2), rep(1,n_2)),\n                  nrow  = n)\nI_n    = diag(n)                             # n x n Einheitsmatrix\nbeta   = matrix(c(1,2), nrow = p)            # wahrer, aber unbekannter, Betaparameter\nsigsqr = 10                                  # wahrer, aber unbekannter, Varianzparameter\n\n# Datenrealisierung\ny      = mvrnorm(1, X %*% beta, sigsqr*I_n)  # eine Realisierung eines n-dimensionalen ZVs\n\n\n\nModellschätzung\nDie beiden Betaparameterkomponenten des Zweistichproben-T-Test-Modells in Designmatrixform werden wenig überraschend durch die entsprechenden Gruppenstichprobenmittel geschätzt. Für den Varianzparameterschätzer ergibt sich die sogenannte gepoolte Stichprobenvarianz. Dies sind die beiden Kernaussagen folgenden Theorems.\n\nTheorem 31.3 (Parameterschätzung im Zweistichproben-T-Test-Modell) Gegeben sei die Designmatrixform des Zweistichproben-T-Test-Modells. Dann ergeben sich für den Betaparameterschätzer \\[\\begin{equation}\n\\hat{\\beta}\n=\\begin{pmatrix}\n\\frac{1}{n_{1}} \\sum_{j=1}^{n_{1}} \\upsilon_{1j} \\\\\n\\frac{1}{n_{2}} \\sum_{j=1}^{n_{2}} \\upsilon_{2j}\n\\end{pmatrix}=:\\begin{pmatrix}\n\\bar{\\upsilon}_{1} \\\\\n\\bar{\\upsilon}_{2}\n\\end{pmatrix}\n\\end{equation}\\] und für den Varianzparameterschätzer \\[\\begin{equation}\n\\hat{\\sigma}^{2}\n= \\frac{\\sum_{j=1}^{n_{1}}\\left(\\upsilon_{1j}-\\bar{\\upsilon}_{1}\\right)^{2}+\\sum_{j=1}^{n_{2}}\\left(\\upsilon_{2j}-\\bar{\\upsilon}_{2}\\right)^{2}}{n_{1}+n_{2}-2}\n=: s_{12}^{2}\n\\end{equation}\\]\n\n\nBeweis. Für \\(i=1,2\\) sei \\(\\upsilon_i:=\\left(\\upsilon_{i1}, \\ldots, \\upsilon_{i n_{i}}\\right)^{T}\\). Dann ergibt sich für den Betaparameterschätzer \\[\\begin{align}\n\\begin{split}\n\\hat{\\beta}\n& = \\left(X^{T} X\\right)^{-1} X^{T} y \\\\\n& =\n\\left(\\begin{pmatrix}\n1_{n_{1}} & 0_{n_{2}} \\\\\n0_{n_{1}} & 1_{n_{2}}\n\\end{pmatrix}\n\\begin{pmatrix}\n1_{n_{1}} & 0_{n_{1}} \\\\\n0_{n_{2}} & 1_{n_{2}}\n\\end{pmatrix}\\right)^{-1}\n\\begin{pmatrix}\n1_{n_{1}} & 0_{n_{2}} \\\\\n0_{n_{1}} & 1_{n_{2}}\n\\end{pmatrix}\n\\begin{pmatrix}\n\\upsilon_{1} \\\\\n\\upsilon_{2}\n\\end{pmatrix} \\\\\n& =\n\\begin{pmatrix}\nn_{1} & 0 \\\\\n0 & n_{2}\n\\end{pmatrix}^{-1}\n\\begin{pmatrix}\n\\sum_{j=1}^{n_{1}} & \\upsilon_{1j} \\\\\n\\sum_{j=1}^{n_{2}} \\upsilon_{2j}\n\\end{pmatrix} \\\\\n& =\n\\begin{pmatrix}\nn_{1}^{-1} & 0 \\\\\n0 & n_{2}^{-1}\n\\end{pmatrix}\n\\begin{pmatrix}\n\\sum_{j=1}^{n_{1}} & \\upsilon_{1j} \\\\\n\\sum_{j=1}^{n_{2}} & \\upsilon_{2j}\n\\end{pmatrix} \\\\\n& =\n\\begin{pmatrix}\n\\frac{1}{n_{1}} \\sum_{j=1}^{n_{1}} \\upsilon_{1j} \\\\\n\\frac{1}{n_{2}} \\sum_{j=1}^{n_{2}} \\upsilon_{2j}\n\\end{pmatrix} \\\\\n& =:\n\\begin{pmatrix}\n\\bar{\\upsilon}_{1} \\\\\n\\bar{\\upsilon}_{2}\n\\end{pmatrix}\n\\end{split}\n\\end{align}\\] Gleichsam ergibt sich für Varianzparameterschätzer mit \\(n=n_{1}+n_{2}\\) und \\(p=2\\) \\[\\begin{align}\n\\begin{split}\n\\hat{\\sigma}^{2}\n& =\\frac{(\\upsilon - X \\hat{\\beta})^{T}(\\upsilon - X \\hat{\\beta})}{n-p} \\\\\n& =\\frac{1}{n_{1}+n_{2}-2}\n\\left(\n\\begin{pmatrix}\n\\upsilon_{1} \\\\\n\\upsilon_{2}\n\\end{pmatrix}-\n\\begin{pmatrix}\n1_{n_{1}} & 0_{n_{1}} \\\\\n0_{n_{2}} & 1_{n_{2}}\n\\end{pmatrix}\n\\begin{pmatrix}\n\\bar{\\upsilon}_{1} \\\\\n\\bar{\\upsilon}_{2}\n\\end{pmatrix}\\right)^{T}\n\\left(\\begin{pmatrix}\n\\upsilon_{1} \\\\\n\\upsilon_{2}\n\\end{pmatrix}\n-\\begin{pmatrix}\n1_{n_{1}} & 0_{n_{1}} \\\\\n0_{n_{2}} & 1_{n_{2}}\n\\end{pmatrix}\\begin{pmatrix}\n\\bar{\\upsilon}_{1} \\\\\n\\bar{\\upsilon}_{2}\n\\end{pmatrix}\\right) \\\\\n& =\n\\frac{1}{n_{1}+n_{2}-2}\n\\begin{pmatrix}\n\\upsilon_{11}-\\bar{\\upsilon}_{1} \\\\\n\\vdots \\\\\n\\upsilon_{1 n_{1}}-\\bar{\\upsilon}_{1} \\\\\n\\upsilon_{21}-\\bar{\\upsilon}_{2} \\\\\n\\vdots \\\\\n\\upsilon_{2 n_{2}}-\\bar{\\upsilon}_{2}\n\\end{pmatrix}\\begin{pmatrix}\n\\upsilon_{11}-\\bar{\\upsilon}_{1} \\\\\n\\vdots \\\\\n\\upsilon_{1 n_{1}}-\\bar{\\upsilon}_{1} \\\\\n\\upsilon_{21}-\\bar{\\upsilon}_{2} \\\\\n\\vdots \\\\\n\\upsilon_{2 n_{2}}-\\bar{\\upsilon}_{2}\n\\end{pmatrix} \\\\\n&\n= \\frac{\\sum_{j=1}^{n_{1}}\\left(\\upsilon_{1j}-\\bar{\\upsilon}_{1}\\right)^{2}+\\sum_{j=1}^{n_{2}}\\left(\\upsilon_{2j}-\\bar{\\upsilon}_{2}\\right)^{2}}{n_{1}+n_{2}-2} \\\\\n= & : s_{12}^{2} .\n\\end{split}\n\\end{align}\\]\n\nMan beachte, dass sich die Stichprobenvarianz \\(s_{\\upsilon}^{2}\\) der Komponenten von im Allgemeinen von der gepoolten Stichprobenvarianz \\(s_{12}^{2}\\) unterscheidet. Dies ist nicht zuletzt dadurch bedingt, dass die Stichprobenvarianz basierend auf dem Gesamtstichprobenmittel \\(\\bar{\\upsilon}\\), die gepoolte Stichprobenvarianz dagegen basierend auf den gruppenspezifischen Stichprobenmittel \\(\\bar{\\upsilon}_{1}\\) und \\(\\bar{\\upsilon}_{2}\\) ermittelt wird. Wir wollen das Konzept der gepoolten Stichprobenvarianz hier aber nicht weiter vertiefen.\n\n\nModellevaluation\nBasierend auf Theorem 29.1 formulieren wir nun die T-Teststatistik für das in Definition 31.2 in Designmatrixform definierte Zweistichproben-T-Test-Modell und geben ihre frequentistische Verteilung an.\n\nTheorem 31.4 (T-Teststatistik des Zweistichproben-T-Tests) Gegeben sei die Designmatrixform des Zweistichproben-T-Tests. Dann ergibt sich für die T-Teststatistik mit \\[\\begin{equation}\nc:=(1,-1)^{T} \\mbox{ und } c^{T} \\beta_{0}=: \\mu_{0},\n\\end{equation}\\] dass \\[\\begin{equation}\nT\n=\\sqrt{\\frac{n_{1} n_{2}}{n_{1}+n_{2}}}\n\\left(\\frac{\\bar{\\upsilon}_{1}-\\bar{\\upsilon}_{2}-\\mu_{0}}{s_{12}}\\right)\n\\end{equation}\\] und es gilt \\[\\begin{equation}\nT \\sim t\\left(\\delta, n_{1}+n_{2}-2\\right)\n\\mbox{ mit } \\delta=\\sqrt{\\frac{n_{1} n_{2}}{n_{1}+n_{2}}}\\left(\\frac{\\mu_{1}-\\mu_{2}-\\mu_{0}}{\\sigma}\\right) .\n\\end{equation}\\]\n\n\nBeweis. Mit Theorem 29.1 gilt zunächst für die Zähler von \\(T\\) und \\(\\delta\\), dass \\[\\begin{equation}\nc^{T} \\hat{\\beta}-c^{T} \\beta_{0}\n=\\begin{pmatrix}\n1 & -1\n\\end{pmatrix}\n\\begin{pmatrix}\n\\bar{\\upsilon}_{1} \\\\\n\\bar{\\upsilon}_{2}\n\\end{pmatrix}-\\mu_{0}\n=\\bar{\\upsilon}_{1}-\\bar{\\upsilon}_{2}-\\mu_{0}\n\\end{equation}\\] und \\[\\begin{equation}\nc^{T} \\beta-c^{T} \\beta_{0}\n=\\begin{pmatrix}\n1 & -1\n\\end{pmatrix}\n\\begin{pmatrix}\n\\mu_{1} \\\\\n\\mu_{2}\n\\end{pmatrix}-\\mu_{0}\n=\\mu_{1}-\\mu_{2}-\\mu_{0}\n\\end{equation}\\] respektive. Weiterhin gilt für die Nenner von \\(T\\) und \\(\\delta\\), dass \\[\\begin{equation}\nc^{T}\\left(X^{T}X\\right)^{-1}c\n=\n\\begin{pmatrix}\n1 & -1\n\\end{pmatrix}\n\\begin{pmatrix}\nn_{1}^{-1} & 0 \\\\\n0 & n_{2}^{-1}\n\\end{pmatrix}\n\\begin{pmatrix}\n1 \\\\\n-1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nn_{1}^{-1} & -n_{2}^{-1}\n\\end{pmatrix}\n\\begin{pmatrix}\n1 \\\\\n-1\n\\end{pmatrix}\n=\\frac{1}{n_{1}}+\\frac{1}{n_{2}}\n\\end{equation}\\] Außerdem gilt \\[\\begin{equation}\n\\left(\\frac{1}{n_{1}}+\\frac{1}{n_{2}}\\right)^{-\\frac{1}{2}}=\\left(\\frac{n_{2}}{n_{1} n_{2}}+\\frac{n_{1}}{n_{1} n_{2}}\\right)^{-\\frac{1}{2}}=\\left(\\frac{n_{1}+n_{2}}{n_{1} n_{2}}\\right)^{-\\frac{1}{2}}=\\left(\\frac{n_{1} n_{2}}{n_{1}+n_{2}}\\right)^{\\frac{1}{2}}\n\\end{equation}\\] Zusammengenommen folgt direkt, dass \\[\\begin{equation}\nT\n= \\sqrt{\\frac{n_{1} n_{2}}{n_{1}+n_{2}}}\n\\left(\\frac{\\bar{\\upsilon}_{1}-\\bar{\\upsilon}_{2}-\\mu_{0}}{s_{12}}\\right)\n\\mbox{ und }\n\\delta=\\sqrt{\\frac{n_{1} n_{2}}{n_{1}+n_{2}}}\\left(\\frac{\\mu_{1}-\\mu_{2}-\\mu_{0}}{\\sigma}\\right) .\n\\end{equation}\\]\n\nDie Formen der T-Teststatistik und ihre Verteilung im Zweistichproben-T-Test Modell in Designmatrixform sind also wiederum natürlicherweise mit den entsprechenden Formen im ALM-freien Kontext identisch. Die enstprechende zur Kontrolle des Testumfangs bei Zweistichproben-T-Tests sowie der Gebrauch der Testgütefunktion zur Evaluation der Testtrennschärfe (Power) folgt also analog.\n\n\nAnwendungsbeispiel\nFolgender R Code demonstriert die Evaluation von 95%-Konfidenzintervallen für die Erwartungswertparameter \\(\\mu_{1}\\) und \\(\\mu_{2}\\) sowie Durchführung eines zweiseitigen ZweistichprobenT-Tests mit Nullhypothese \\[\\begin{equation}\n\\Theta_{0}\n:=\n\\left\\{\n\\begin{pmatrix}\n\\mu_{1} \\\\\n\\mu_{2}\n\\end{pmatrix} \\in \\mathbb{R}^{2} \\mid\n\\mu_{1}=\\mu_{2}\n\\right\\}\n\\end{equation}\\] und Signifikanzlevel \\(\\alpha_{0}:=0.05\\) für das oben skizzierte Anwendungsbeispiel.\n\n# Dateneinlesen\nD           = read.csv(\"./_data/407-t-tests.csv\")               # Dataframe\ny_1         = D$dBDI[D$COND == \"F2F\"]                           # BDI Differenzwerte in der F2F Gruppe\ny_2         = D$dBDI[D$COND == \"ONL\"]                           # BDI Differenzwerte in der ONL Gruppe\n\n# Modellformulierung\nn_1         = length(y_1)                                       # Anzahl Datenpunkte Gruppe 1 (F2F)\nn_2         = length(y_1)                                       # Anzahl Datenpunkte Gruppe 2 (ONL)\nn           = n_1 + n_2                                         # Gesamtanzahl Datenpunkte\ny           = matrix(c(y_1, y_2), nrow = n)                     # Datenvektor\np           = 2                                                 # Anzahl Betaparameter\nX           = matrix(c(rep(1,n_1), rep(0,n_2),                  # Zweistichproben-T-Test Designmatrix\n                       rep(0,n_1), rep(1,n_2)),\n              nrow = n)\n\n# Parameterschätzng\nbeta_hat    = solve(t(X) %*% X) %*% t(X) %*% y                  # Betaparameterschätzer\neps_hat     = y - X %*% beta_hat                                # Residuenvektor\nsigsqr_hat  = (t(eps_hat) %*% eps_hat) /(n-p)                   # Varianzparameterschätzer\n\n# Konfidenzintervall\ndelta       = 0.95                                              # Konfidenzbedingung\nt_delta     = qt((1+delta)/2,n-1)                               # \\Psi^{-1}((1+\\delta)/2,n-1)\nlambda      = diag(solve(t(X) %*% X))                           # \\lambda_j Werte\nkappa       = matrix(rep(NaN,p*2), nrow = p)                    # \\beta_j Konfidenintervall array\nfor(j in 1:p){                                                  # Iteration über \\beta_j\n  kappa[j,1] = beta_hat[j]-sqrt(sigsqr_hat*lambda[j])*t_delta   # untere KI Grenze\n  kappa[j,2] = beta_hat[j]+sqrt(sigsqr_hat*lambda[j])*t_delta}  # obere KI Grenze\n\n# Hypothesentest\nc           = matrix(c(1,-1), nrow = 2)                         # Kontrastgewichtsvektor\nmu_0        = 0                                                 # Nullhypothese H_0\nalpha_0     = 0.05                                              # Signifikanzniveau\nk_alpha_0   = qt(1 - (alpha_0/2), n-1)                          # kritischer Wert\nt_num       = t(c) %*% beta_hat - mu_0                          # T-Teststatistik Zähler\nt_den       = sqrt(sigsqr_hat*t(c) %*% solve(t(X) %*% X)%*%c)   # T-Teststatistik Nenner\nt           = t_num/t_den                                       # T-Teststatistik\nif(abs(t) &gt;= k_alpha_0){phi = 1} else {phi = 0}                 # Test 1_{|T(X) &gt;= k_alpha_0|}\npval      = 2*(1-pt(abs(t), n_1+n_2-2))                         # p-Wert\n\n\n\nBetaparameterschätzer            :  5.17 5.75 \n95%-Konfidenzintervalle         :  2.6 3.18 7.74 8.32 \nVarianzparameterschätzer        :  18.54 \nalpha_0                         :  0.05 \nKritischer Wert                 :  2.07 \nEinstichproben-T-Teststatistik  :  -0.33 \nphi                             :  0 \np-Wert                          :  0.74\n\n\nDie Nullhypothese würde in diesem Fall bei einem kritischen Wert von \\(k_{0.05}=2.07\\) und einem Wert der T-Statistik von \\(T=-033\\) nicht verworfen werden. Inferenzstatisch besteht also keine Evidenz dafür, dass sich der wahre, aber unbekannte Erwartungswertparameter im F2F Therapiesetting vom wahren, aber unbekannte Erwartungswertparameter im ONL Therapiesetting unterscheidet. Die 95/%Konfidenzintervalle für die wahren, aber unbekannten, Erwartungswertparameter \\(\\mu_{1}\\) und \\(\\mu_{2}\\) sind \\([2.60,7.74]\\) und \\([3.18,8.32]\\), respektive.",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>T-Tests</span>"
    ]
  },
  {
    "objectID": "407-T-Tests.html#literaturhinweise",
    "href": "407-T-Tests.html#literaturhinweise",
    "title": "31  T-Tests",
    "section": "31.4 Literaturhinweise",
    "text": "31.4 Literaturhinweise\nObwohl die frequentistische Literatur der ersten Hälfte des 20. Jahrhunderts von der Äquivalenz regressions- und varianzanalytischer linearer Modelle durchdrungen ist, fällt es schwer eine definite Quelle anzugeben, die hinsichtlich der Beschreibung von T-Tests als Spezialfälle des ALM Priorität hätte. Es sei hier deshalb eher allgemein auf Fisher (1925) und Fisher (1935) verwiesen.",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>T-Tests</span>"
    ]
  },
  {
    "objectID": "407-T-Tests.html#selbstkontrollfragen",
    "href": "407-T-Tests.html#selbstkontrollfragen",
    "title": "31  T-Tests",
    "section": "31.5 Selbstkontrollfragen",
    "text": "31.5 Selbstkontrollfragen\n\nGeben Sie die Definition des Einstichproben-T-Test-Modells wieder.\nGeben Sie das Theorem zur Parameterschätzung im Einstichproben-T-Test-Modell wieder.\nGeben Sie das Theorem zur T-Teststatistik des Einstichproben-T-Tests wieder.\nGeben Sie die Definition des Zweistichproben-T-Test-Modells wieder.\nGeben Sie das Theorem zur Parameterschätzung im Zweistichproben-T-Test-Modell wieder.\nGeben Sie das Theorem zur T-Teststatistik des Zweistichproben-T-Tests wieder.\n\n\n\n\n\nFisher, R. A. (1925). Theory of Statistical Estimation. Mathematical Proceedings of the Cambridge Philosophical Society, 22(5), 700–725. https://doi.org/10.1017/S0305004100009580\n\n\nFisher, R. A. (1935). The Design of Experiments (1. ed). Hafner Press.",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>T-Tests</span>"
    ]
  },
  {
    "objectID": "408-Einfaktorielle-Varianzanalyse.html",
    "href": "408-Einfaktorielle-Varianzanalyse.html",
    "title": "32  Einfaktorielle Varianzanalyse",
    "section": "",
    "text": "32.1 Anwendungsszenario\nDas Anwendungsszenario einer einfaktoriellen Varianzanalyse ist durch das Vorliegen von \\(n\\) univariaten Datenpunkten von zwei oder mehr Gruppen randomisierter experimenteller Einhe\\(i\\)ten gekennzeichnet, die sich hinsichtlich der Level eines experimentellen Faktors unterscheiden. Ist die Anzahl an Datenpunkten in jeder Gruppe gleich, so spricht man auch von einem balancierten einfaktoriellen Varianzanalysedesign. Von den Datenpunkten der \\(i\\) ten Gruppe bzw. des \\(i\\)ten Faktorlevels wird dabei angenommen, dass sie Realisierungen von jeweils \\(n_{i}\\) unabhängigen und identisch normalverteilten Zufallsvariablen sind, deren wahre, aber unbekannte, Erwartungswertparameter sich potentiell über die Gruppen hinweg unterscheiden und deren wahrer, aber unbekannter, Varianzparameter über Gruppen hinweg identisch ist. In diesen Grundannahmen handelt es sich bei dem Szenario der einfaktoriellen Varianzanalyse also um eine direkte Generalisierung des Einstich- und Zweistichproben-T-Test Szenarios zu (potentiell) mehr als zwei Gruppen. Umgekehrt können die Einstichproben- und Zweistichproben-T-Test Szenarien natürlich auch als einfaktorielle Varianzanalyseszenarien betrachtet werden, bei denen der experimentelle Faktor (nur) ein oder zwei Level, respektive, aufweist. Schließlich wird wie im Falle der T-Test Szenarien meist vorausgesetzt, dass ein Interesse an einem inferentiellen Vergleich der wahren, aber unbekannten, faktorlevelspezifischen Erwartungswertparameter besteht.",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Einfaktorielle Varianzanalyse</span>"
    ]
  },
  {
    "objectID": "408-Einfaktorielle-Varianzanalyse.html#anwendungsbeispiel",
    "href": "408-Einfaktorielle-Varianzanalyse.html#anwendungsbeispiel",
    "title": "32  Einfaktorielle Varianzanalyse",
    "section": "32.2 Anwendungsbeispiel",
    "text": "32.2 Anwendungsbeispiel\nAls konkretes Anwendungsbeispiel betrachten wir die Analyse von Pre-Post-Interventions-BDI-Differenzwerten von drei Gruppen von jeweils 12 Patient:innen, die unterschiedliche Therapiesettings (Face-to-Face und Online) bzw. eine Wartelistenkontrollbedingung durchlaufen haben, wie in Tabelle 32.1 exemplarisch dargstellt. Die erste Spalte der Tabelle (COND) listet das patient:innenspezifische Therapiesetting (F2F: face-to-face, ONL: online, WLC: waitlist control) für jeweils drei Patient:innen jeder Studiengruppe auf. Die zweite Spalte der Tabelle (dBDI) list die entsprechenden patient:innenspezifischen Pre-PostInterventions-BDI-Differenzwerte. Positive Werte enstprechen hier wieder einer Abnahme der Depressionssymptomatik, negative Werte einer Zunahme der Depressionssymptomatik.\n\n\n\n\nTabelle 32.1: Exemplarische Pre-Post-Intervention-BDI-Differenzwerte des Beispieldatensatzes\n\n\n\n\n\n\n\nCOND\ndBDI\n\n\n\n\n1\nF2F\n9\n\n\n2\nF2F\n7\n\n\n3\nF2F\n10\n\n\n4\nF2F\n11\n\n\n13\nONL\n2\n\n\n14\nONL\n7\n\n\n15\nONL\n9\n\n\n16\nONL\n9\n\n\n17\nONL\n8\n\n\n25\nWLC\n-1\n\n\n26\nWLC\n2\n\n\n27\nWLC\n-3\n\n\n28\nWLC\n-1\n\n\n29\nWLC\n0\n\n\n\n\n\n\n\n\nAbbildung 32.1 zeigt eine Visualisierung des gesamten Datensatzes. Die Balken repräsentieren die gruppenspezifischen Stichprobenmittelwerte, die zugehörigen Fehlerbalken die gruppenspezifischen Stichprobenstandardabweichungen. Die Punktwolken repräsentieren die gruppenspezifischen Datenpunkte. In den F2F und ONL Gruppen ist die Veränderung des BDI Wertes stärker ausgeprägt als in der WLC Gruppe.\n\n\n\n\n\n\nAbbildung 32.1: Datendarstellung des Anwendungsbeispiels zur Einfaktoriellen Varianzanalyse.\n\n\n\nIn Tabelle 32.2 fassen wir die Deskriptivstatistiken des Beispieldatensatzes, aufgeschlüsselt nach Therapiebedingungen, zusammen.\n\n\n\n\nTabelle 32.2: Deskriptivstatistiken der Pre-Post BDI Differenzwerte\n\n\n\n\n\n\n\nn\nMax\nMin\nMedian\nMean\nVar\nStd\n\n\n\n\nF2F\n12\n13\n6\n10.0\n9.83\n3.79\n1.95\n\n\nONL\n12\n10\n2\n7.5\n7.00\n7.27\n2.70\n\n\nWLC\n12\n2\n-4\n-1.0\n-1.42\n4.45\n2.11",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Einfaktorielle Varianzanalyse</span>"
    ]
  },
  {
    "objectID": "408-Einfaktorielle-Varianzanalyse.html#modellformulierung",
    "href": "408-Einfaktorielle-Varianzanalyse.html#modellformulierung",
    "title": "32  Einfaktorielle Varianzanalyse",
    "section": "32.3 Modellformulierung",
    "text": "32.3 Modellformulierung\nWir definieren das Modell der EVA zunächst in Erwartungswertparameterdarstellung. Dabei nutzen wir den Index \\(i\\) um die experimentellen Gruppen zu indizieren und den Index j um die experimentellen Einhe\\(i\\)ten innerhalb der Gruppen zu indizieren.\n\nDefinition 32.1 (EVA-Modell in Erwartungswertparameterdarstellung) Für \\(i=1,\\ldots,p\\) und \\(j=1,\\ldots,n_{i}\\) seien \\(\\upsilon_{ij}\\) Zufallsvariablen, die die \\(n:=\\sum_{i=1}^{p} n_{i}\\) Datenpunkte eines EVA Szenarios modellieren. Dann hat das EVA-Modell in Erwartungswertparameterdarstellung die strukturelle Form \\[\\begin{equation}\n\\upsilon_{ij}=\\mu_{i}+\\varepsilon_{ij}\n\\mbox{ mit } \\varepsilon_{ij} \\sim N\\left(0, \\sigma^{2}\\right)\n\\mbox{ u.i.v. mit } \\mu_{i} \\in \\mathbb{R}, \\sigma^{2}&gt;0,\n\\end{equation}\\] die Datenverteilungsform \\[\\begin{equation}\n\\upsilon_{ij} \\sim N\\left(\\mu_{i}, \\sigma^{2}\\right)\n\\mbox{ u.v. mit } \\mu_{i} \\in \\mathbb{R}, \\sigma^{2}&gt;0.\n\\end{equation}\\] und für den \\(n\\)-dimensionalen Datenvektor definiert als \\[\\begin{equation}\n\\upsilon :=\n\\left(\n\\upsilon_{11},\\ldots, \\upsilon_{1n_{1}},\n\\upsilon_{21},\\ldots, \\upsilon_{2n_{2}},\\ldots,\n\\upsilon_{p1},\\ldots, \\upsilon_{pn_{p}}\n\\right)^{T}\n\\end{equation}\\] die Designmatrixform \\[\\begin{equation}\n\\upsilon = X\\beta+\\varepsilon\n\\end{equation}\\] mit \\[\\begin{equation}\nX:=\n\\begin{pmatrix}\n1_{n_{1}}   & 0_{n_{1}} & \\cdots & 0_{n_{1}} \\\\\n0_{n_{2}}   & 1_{n_{2}} & \\cdots & 0_{n_{2}} \\\\\n\\vdots      & \\vdots    & \\ddots & \\vdots    \\\\\n0_{n_{p}}   & 0_{n_{p}} & \\cdots & 1_{n_{p}}\n\\end{pmatrix},\n\\beta :=\n\\begin{pmatrix}\n\\mu_{1} \\\\\n\\mu_{2} \\\\\n\\vdots \\\\\n\\mu_{p}\n\\end{pmatrix} \\in \\mathbb{R}^{p}, \\varepsilon \\sim N\\left(0_{n}, \\sigma^{2} I_{n}\\right), \\sigma^{2}&gt;0 .\n\\end{equation}\\] Für \\(n_{i}:=m\\) für alle \\(i=1,\\ldots,p\\) heißt das Modell balanciert.\n\nDer Vergleich mit der Definition des Modells des Zweistichproben-T-Tests in Definition 31.2 zeigt, dass es sich bei dieser Formulierung des EVA-Modells um die direkte Generalisierung des Zweistichproben-T-Test Modells von \\(p=2\\) für ein beliebiges \\(p \\in \\mathbb{N}\\) handelt.\n\nMotivation der Effektdarstellung\nDas EVA-Modell in Erwartungswertparameterdarstellung ist ein valides Modell, auf dessen Grundlage sowohl Parameterschätzung als auch Parameter- und Modellinferenz für das EVA-Szenario entwickelt werden können (vgl. Georgii (2009)). Im Sinne der Konsistenz mit den Modellen der mehrfaktoriellen Varianzanalyse bietet sich jedoch eine Reparametrisierung des Betaparametervektors an. Kern dieser Reparametrisierung ist es, den Erwartungswertparameter der \\(i\\) ten Gruppe als Summe eines gruppenübergreifenden Erwartungswertparameters \\(\\mu_{0} \\in \\mathbb{R}\\) und eines gruppenspezifischen Effektparameters \\(\\alpha_{i} \\in \\mathbb{R}\\) zu modellieren, \\[\n\\mu_{i}:=\\mu_{0}+\\alpha_{i} \\mbox{ für } i=1,\\ldots,p.\n\\tag{32.1}\\] Dabei modelliert \\(\\alpha_{i}\\) die Differenz zwischen dem \\(i\\)ten Erwartungswertparameter \\(\\mu_{i}\\) und dem gruppenübergreifenden Erwartungswertparameter \\(\\mu_{0}\\), \\[\\begin{equation}\n\\alpha_{i}=\\mu_{i}-\\mu_{0} \\mbox{ für } i=1,\\ldots,p .\n\\end{equation}\\] Allerdings hat die in dieser Form vorgenommene Reparametrisierung einen entscheidenen Nachteil: es werden \\(p\\) Erwartungswertparameter \\(\\mu_{i}, i=1,\\ldots,p\\) durch die \\(p+1\\) Parameter \\(\\mu_{0}\\) und \\(\\alpha_{i}, i=1,\\ldots,p\\) dargestellt. Diese Darstellung ist im Allgemeinen nicht eindeutig. Zum Beispiel können die Erwartungswertparameter \\(\\mu_{1}=3, \\mu_{2}=5, \\mu_{3}=6\\) sowohl durch den gruppenspezifischen Erwartungswertparameter \\(\\mu_{0}=0\\) und die gruppenunspezifischen Effektparameter \\(\\alpha_{1}=3, \\alpha_{2}=5, \\alpha_{3}=6\\) als auch durch den gruppenunspezifischen Erwartungswertparameter \\(\\mu_{0}=1\\) und die gruppenspezifischen Effektparameter \\(\\alpha_{1}=2, \\alpha_{2}=4, \\alpha_{3}=5\\) dargestellt werden. Man sagt in diesem Kontext auch, dass das EVA-Modell in der Form von ?eq-eq-mu-i-1 überparametrisiert ist.\nDatenanalytisch hat die Überparametrisierung eines Varianzanalysemodells den Nachteil, dass aus \\(p\\) geschätzten Erwartungswertparametern \\(p+1\\) Betaparameterschätzer bestimmt werden müssten, was wie oben gesehen nicht eindeutig erfolgen kann. Um diese Probleme in der Effektparameterdarstellung des EVA-Modells zu umgehen und diese konsistent auf mehrfaktorielle Varianzanalysemodelle zu übertragen, bietet sich die Einführung der Nebenbedingung \\[\\begin{equation}\n\\alpha_{1}:=0\n\\end{equation}\\] an. Es wird also ein Effektparameter von vornherein als identisch Null angenommen. Für die gruppenspezifischen Erwartungswertparameter ergibt sich damit \\[\\begin{equation}\n\\begin{aligned}\n\\mu_{1} & :=\\mu_{0} \\\\\n\\mu_{i} & :=\\mu_{0}+\\alpha_{i} \\mbox{ für } i=2,\\ldots,p .\n\\end{aligned}\n\\end{equation}\\] Hierbei wird die erste Gruppe nun als Referenzgruppe bezeichnet und die \\(\\alpha_{i}\\) modellieren die Differenz zwischen dem Erwartungswertparameter der \\(i\\)ten Gruppe und dem Erwartungswertparameter der ersten Gruppe: \\[\\begin{equation}\n\\alpha_{i}=\\mu_{i}-\\mu_{0}=\\mu_{i}-\\mu_{1} \\mbox{ für } i=1,\\ldots,p .\n\\end{equation}\\] \\(\\mu_{0}\\) ist, unter der Nebenbedingung \\(\\alpha_{1}:=0\\) also kein gruppenübergreifender Erwartungswertparameter mehr, sondern identisch mit dem Erwartungswertparameter der ersten Gruppe. Welche tatsächliche experimentelle Gruppe dabei als “erste Gruppe” definiert wird, ist datenanalytisch unerheblich. Datenanalytisch entscheidend dagegegen ist, dass der entsprechenden Erwartungswertparameterschätzer \\(\\hat{\\mu}_{0}\\) korrekt als Erwartungswertparameterschätzer der Referenzgruppe und die \\(\\hat{\\alpha}_{i}\\) für \\(i=2,\\ldots,p\\) korrekt als geschätzte Erwartungswertparameterdifferenzen zwischen dem Erwartungswertparameter der Referenzgruppe und dem Erwartungswertparameter der \\(i\\)ten Gruppe verstanden werden.\nWir formalisieren das oben Gesagte in folgendem Theorem.\n\nTheorem 32.1 (EVA-Modell in Effektdarstellung mit Referenzgruppe) Gegeben sei das EVA-Modell in Erwartungswertparameterdarstellung. Dann können die Zufallsvariablen, die die Datenpunkte des EVA-Szenarios modellieren, äquivalent in der strukturellen Form \\[\\begin{equation}\n\\begin{aligned}\n& \\upsilon_{1j}=\\mu_{0}+\\varepsilon_{1 j} \\quad\n\\mbox{ mit } \\varepsilon_{1 j} \\sim N\\left(0, \\sigma^{2}\\right)\n\\mbox{ u.i.v. für } j=1,\\ldots, n_{1} \\\\\n& \\upsilon_{ij} = \\mu_{0}+\\alpha_{i}+\\varepsilon_{ij}\n\\mbox{ mit } \\varepsilon_{ij} \\sim N\\left(0, \\sigma^{2}\\right)\n\\mbox{ u.i.v. für } i=2,\\ldots,p, j=1,\\ldots,n_{i}\n\\end{aligned}\n\\end{equation}\\] mit \\[\\begin{equation}\n\\alpha_{i}:=\\mu_{i}-\\mu_{1} \\mbox{ für } i=2,\\ldots,p\n\\end{equation}\\] und in der entsprechenden Datenverteilungsform \\[\\begin{equation}\n\\begin{aligned}\n& \\upsilon_{1j} \\sim N\\left(\\mu_{0}, \\sigma^{2}\\right) \\quad\n\\mbox{ u.i.v. für } j=1,\\ldots,n_{i}\n\\mbox{ mit } \\mu_{1} \\in \\mathbb{R}, \\sigma^{2}&gt;0 \\\\\n& \\upsilon_{ij} \\sim N\\left(\\mu_{0}+\\alpha_{i}, \\sigma^{2}\\right)\n\\mbox{ u.v. für } i=2,\\ldots,p, j=1,\\ldots,n_{i}\n\\mbox{ mit } \\alpha_{i} \\in \\mathbb{R}, \\sigma^{2}&gt;0\n\\end{aligned}\n\\end{equation}\\] geschrieben werden.\n\n\nBeweis. Nach Definition gilt \\[\\begin{equation}\n\\mu_{i}=\\mu_{0}+\\mu_{i}-\\mu_{0}.\n\\end{equation}\\] Die Parametrisierungen mit \\(\\mu_{i}\\) und mit \\(\\mu_{0}+\\mu_{i}-\\mu_{0}\\) sind also gleich und damit äquivalent. Dann folgt aber auch \\[\\begin{equation}\n\\mu_{i}=\\mu_{0}+\\left(\\mu_{i}-\\mu_{0}\\right)=: \\mu_{0}+\\alpha_{i} \\mbox{ für } i=1,\\ldots,p.\n\\end{equation}\\] Mit \\(\\alpha_{1}:=0\\) gilt dann \\(\\mu_{1}=\\mu_{0}\\) und \\(\\mu_{i}=\\mu_{0}+\\alpha_{i}\\) für \\(i=2,\\ldots,p\\), wie im Theorem behauptet.\n\nBasierend auf Theorem 32.1 definieren wir nun das Modell der EVA in Effektdarstellung.\n\nDefinition 32.2 (EVA-Modell in Effektdarstellung mit Referenzgruppe) Für \\(i=1,\\ldots,p\\) und \\(j=1,\\ldots,n_{i}\\) seien \\(\\upsilon_{ij}\\) Zufallsvariablen, die die \\(n:=\\sum_{i=1}^{p} n_{i}\\) Datenpunkte eines EVA-Szenarios modellieren. Dann hat das EVA-Modell in Effektdarstellung die strukturelle Form \\[\\begin{equation}\n\\begin{aligned}\n& \\upsilon_{1j}=\\mu_{0}+\\varepsilon_{1 j} \\quad\n\\mbox{ mit } \\varepsilon_{1 j} \\sim N\\left(0, \\sigma^{2}\\right)\n\\mbox{ u.i.v. für } j=1,\\ldots, n_{1} \\\\\n& \\upsilon_{ij}=\\mu_{0}+\\alpha_{i}+\\varepsilon_{ij}\n\\mbox{ mit } \\varepsilon_{ij} \\sim N\\left(0, \\sigma^{2}\\right)\n\\mbox{ u.i.v. für } i=2,\\ldots,p, j=1,\\ldots,n_{i}\n\\end{aligned}\n\\end{equation}\\] mit \\[\\begin{equation}\n\\alpha_{i}:=\\mu_{i}-\\mu_{1} \\mbox{ für } i=2,\\ldots,p,\n\\end{equation}\\] die Datenverteilungsform \\[\\begin{equation}\n\\begin{aligned}\n& \\upsilon_{1j} \\sim N\\left(\\mu_{0}, \\sigma^{2}\\right) \\quad\n\\mbox{ u.i.v. für } j=1,\\ldots,n_{i}\n\\mbox{ mit } \\mu_{0} \\in \\mathbb{R}, \\sigma^{2}&gt;0 \\\\\n& \\upsilon_{ij} \\sim N\\left(\\mu_{0}+\\alpha_{i}, \\sigma^{2}\\right)\n\\mbox{ u.v. für } i=2,\\ldots,p, j=1,\\ldots,n_{i}\n\\mbox{ mit } \\alpha_{i} \\in \\mathbb{R}, \\sigma^{2}&gt;0\n\\end{aligned}\n\\end{equation}\\] und die Designmatrixform \\[\\begin{equation}\n\\upsilon = X\\beta+\\varepsilon \\mbox{ mit } \\varepsilon \\sim N\\left(0_{n}, \\sigma^{2} I_{n}\\right), n:=\\sum_{i=1}^{p} n_{i} \\\\\n\\upsilon :=\n\\begin{pmatrix}\n\\upsilon_{11} \\\\\n\\vdots \\\\\n\\upsilon_{1n_{1}} \\\\\n\\upsilon_{21} \\\\\n\\vdots \\\\\n\\upsilon_{2n_{2}} \\\\\n\\vdots \\\\\n\\upsilon_{p1} \\\\\n\\vdots \\\\\n\\upsilon_{pn_{p}}\n\\end{pmatrix}\n\\in \\mathbb{R}^{n},\nX:=\\begin{pmatrix}\n1_{n_{1}} & 0_{n_{1}} & \\cdots & 0_{n_{1}} \\\\\n1_{n_{2}} & 1_{n_{2}} & \\cdots & 0_{n_{2}} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n1_{n_{p}} & 0_{n_{p}} & \\cdots & 1_{n_{p}}\n\\end{pmatrix}\n\\in \\mathbb{R}^{n \\times p},\n\\beta:=\\begin{pmatrix}\n\\mu_{0} \\\\\n\\alpha_{2} \\\\\n\\vdots \\\\\n\\alpha_{p}\n\\end{pmatrix} \\in \\mathbb{R}^{p} \\mbox{ und } \\sigma^{2}&gt;0.\n\\end{equation}\\]\n\n\n\nBeispiel\nUm die Unterschiede und Gemeinsamkeiten zwischen der Erwartungswertparameterdarstellung und der Effektparameterdarstellung des EVA-Modells in ihrer Designmatrixform zu verdeutlichen, betrachten wir ein Beispielszenario mit \\(n_{i}:=4\\) und \\(p=3\\) für \\(i=1,\\ldots,p\\), also \\(n=12\\). Für die Erwartunswertparameterdarstellung gilt dann \\[\\begin{equation}\n\\upsilon = X\\beta+\\varepsilon \\mbox{ mit } \\varepsilon \\sim N\\left(0_{12}, \\sigma^{2} I_{12}\\right)\n\\end{equation}\\] mit \\[\\begin{equation}\n\\upsilon := \\begin{pmatrix}\n\\upsilon_{11} \\\\\n\\upsilon_{12} \\\\\n\\upsilon_{13} \\\\\n\\upsilon_{21} \\\\\n\\upsilon_{22} \\\\\n\\upsilon_{23} \\\\\n\\upsilon_{31} \\\\\n\\upsilon_{32} \\\\\n\\upsilon_{33} \\\\\n\\upsilon_{41} \\\\\n\\upsilon_{42} \\\\\n\\upsilon_{43}\n\\end{pmatrix}, X:=\\begin{pmatrix}\n1 & 0 & 0 \\\\\n1 & 0 & 0 \\\\\n1 & 0 & 0 \\\\\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 1\n\\end{pmatrix}\n\\in \\mathbb{R}^{12 \\times 3},\n\\beta :=\n\\begin{pmatrix}\n\\mu_{1} \\\\\n\\mu_{2} \\\\\n\\mu_{3}\n\\end{pmatrix} \\in \\mathbb{R}^{3} \\mbox{ und } \\sigma^{2}&gt;0.\n\\end{equation}\\] Für die Effektparameterdarstellung dagegen gilt \\[\\begin{equation}\n\\upsilon = X\\beta+\\varepsilon \\mbox{ mit } \\varepsilon \\sim N\\left(0_{12}, \\sigma^{2} I_{12}\\right)\n\\end{equation}\\] mit \\[\\begin{equation}\n\\upsilon :=\n\\begin{pmatrix}\n\\upsilon_{11} \\\\\n\\upsilon_{12} \\\\\n\\upsilon_{13} \\\\\n\\upsilon_{21} \\\\\n\\upsilon_{22} \\\\\n\\upsilon_{23} \\\\\n\\upsilon_{31} \\\\\n\\upsilon_{32} \\\\\n\\upsilon_{33} \\\\\n\\upsilon_{41} \\\\\n\\upsilon_{42} \\\\\n\\upsilon_{43}\n\\end{pmatrix},\nX:=\n\\begin{pmatrix}\n1 & 0 & 0 \\\\\n1 & 0 & 0 \\\\\n1 & 0 & 0 \\\\\n1 & 0 & 0 \\\\\n1 & 1 & 0 \\\\\n1 & 1 & 0 \\\\\n1 & 1 & 0 \\\\\n1 & 1 & 0 \\\\\n1 & 0 & 1 \\\\\n1 & 0 & 1 \\\\\n1 & 0 & 1 \\\\\n1 & 0 & 1\n\\end{pmatrix} \\in \\mathbb{R}^{12 \\times 3},\n\\beta :=\n\\begin{pmatrix}\n\\mu_{0}     \\\\\n\\alpha_{2}  \\\\\n\\alpha_{3}\n\\end{pmatrix}\n\\in \\mathbb{R}^{3} \\mbox{ und } \\sigma^{2}&gt;0\n\\end{equation}\\] Folgender R Code demonstriert die Realisierung von Daten in einem EVA-Szenario mit eben dieser Effekparameterdarstellung.\n\n# Modellformulierung\nlibrary(MASS)                                                                    # Multivariate Normalverteilung\nm      = 4                                                                       # Anzahl von Datenpunkten der iten Gruppe\np      = 3                                                                       # Anzahl Gruppen\nn      = p*m                                                                     # Gesamtanzahl Datenpunkte\nXt     = cbind(                                                                  # Designmatrix\n         matrix(1,nrow = n, ncol = 1),\n         kronecker(diag(p), matrix(1,nrow = m,ncol = 1)))\nX      = Xt[,-2]\nI_n    = diag(n)                                                                 # n x n Einheitsmatrix\nbeta   = matrix(c(10,-3,-12), nrow = p)                                          # \\beta = (\\mu_0,\\alpha_2,\\alpha_3,\\alpha_4)\nsigsqr = 14                                                                      # \\sigma^2\ny      = mvrnorm(1, X %*% beta, sigsqr*I_n)                                      # eine Realisierung eines n-dimensionalen ZVs\nprint(X)\n\n      [,1] [,2] [,3]\n [1,]    1    0    0\n [2,]    1    0    0\n [3,]    1    0    0\n [4,]    1    0    0\n [5,]    1    1    0\n [6,]    1    1    0\n [7,]    1    1    0\n [8,]    1    1    0\n [9,]    1    0    1\n[10,]    1    0    1\n[11,]    1    0    1\n[12,]    1    0    1",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Einfaktorielle Varianzanalyse</span>"
    ]
  },
  {
    "objectID": "408-Einfaktorielle-Varianzanalyse.html#modellschätzung",
    "href": "408-Einfaktorielle-Varianzanalyse.html#modellschätzung",
    "title": "32  Einfaktorielle Varianzanalyse",
    "section": "32.4 Modellschätzung",
    "text": "32.4 Modellschätzung\nWir betrachten nun die Betaparameterschätzung in der Effektparameterdarstellung des EVA-Modells mit Referenzgruppe. Entsprechend der Interpretation der Betaparameterkomponenten werden dabei \\(\\mu_{0}\\) durch das Stichprobenmittel der Referenzgruppe und die \\(\\alpha_{2},\\ldots, \\alpha_{p}\\) durch die Differenzen des jeweiligen Gruppenstichprobenmittels und des Referenzgruppenstichprobenmittels geschätzt. Auf die Schätzung des Varianzparameters, der sich wie im Zweistichproben-T-Test Modell zu einer gepoolten Stichprobenvarianz ergibt, wollen wir hier nicht weiter eingehen.\n\nTheorem 32.2 (Betaparameterschätzung im EVA-Modell) Gegeben sei die Designmatrixform des EVA in Effektdarstellung mit Referenzgruppe. Dann ergibt sich für den Betaparameterschätzer \\[\\begin{equation}\n\\hat{\\beta} :=\n\\begin{pmatrix}\n\\hat{\\mu}_{0} \\\\\n\\hat{\\alpha}_{2} \\\\\n\\vdots \\\\\n\\hat{\\alpha}_{p}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\bar{\\upsilon}_{1} \\\\\n\\bar{\\upsilon}_{2}-\\bar{\\upsilon}_{1} \\\\\n\\vdots \\\\\n\\bar{\\upsilon}_{p}-\\bar{\\upsilon}_{1}\n\\end{pmatrix}\n\\end{equation}\\] wobei \\[\\begin{equation}\n\\bar{\\upsilon}_{i}:=\\frac{1}{n_{i}} \\sum_{j=1}^{n_{i}} \\upsilon_{ij}\n\\end{equation}\\] das Stichprobenmittel der \\(i\\)ten Gruppe bezeichnet.\n\n\nBeweis. Wir halten zunächst fest, dass\n\\[\\begin{equation}\n\\begin{aligned}\nX^{T}X & =\n\\begin{pmatrix}\n1 & \\cdots & 1 & 1 & \\cdots & 1 & \\cdots & 1 & \\cdots & 1 \\\\\n0 & \\cdots & 0 & 1 & \\cdots & 1 & \\cdots & 0 & \\cdots & 0 \\\\\n& \\vdots & & & \\vdots & & \\vdots & & \\vdots & \\\\\n0 & \\cdots & 0 & 0 & \\cdots & 0 & \\cdots & 1 & \\cdots & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 0 & & 0 \\\\\n\\vdots & \\vdots & \\cdots & \\vdots \\\\\n1 & 0 & & 0 \\\\\n1 & 1 & & 0 \\\\\n\\vdots & \\vdots & \\cdots & \\vdots \\\\\n1 & 1 & & 0 \\\\\n& & & \\\\\n\\vdots & \\vdots & \\cdots & \\vdots \\\\\n1 & 0 & & 1 \\\\\n\\vdots & \\vdots & \\cdots & \\vdots \\\\\n1 & 0 & & 1\n\\end{pmatrix} \\\\\n& =\n\\begin{pmatrix}\nn & n_{2} & n_{3} & \\cdots & n_{p} \\\\\nn_{2} & n_{2} & 0 & \\cdots & 0 \\\\\nn_{3} & 0 & n_{3} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\nn_{p} & 0 & 0 & \\cdots & n_{p}\n\\end{pmatrix}.\n\\end{aligned}\n\\end{equation}\\] Die Inverse von \\(X^{T}X\\) ist \\[\\begin{equation}\n\\left(X^{T}X\\right)^{-1}=\n\\begin{pmatrix}\n\\frac{1}{n_{1}} & -\\frac{1}{n_{1}} & \\cdots & -\\frac{1}{n_{1}} \\\\\n-\\frac{1}{n_{1}} & \\frac{n_{1}+n_{2}}{n_{1} n_{2}} & \\cdots & \\frac{1}{n_{1}} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n-\\frac{1}{n_{1}} & \\frac{1}{n_{1}} & \\cdots & \\frac{n_{1}+n_{p}}{n_{1} n_{p}}\n\\end{pmatrix}\n\\end{equation}\\] So gilt zum Beispiel für \\(p=3\\), dass \\[\\begin{equation}\nX^{T}X =\n\\begin{pmatrix}\nn & n_{2} & n_{3} \\\\\nn_{2} & n_{2} & 0 \\\\\nn_{3} & 0 & n_{3}\n\\end{pmatrix}\n\\mbox{ und }\n\\left(X^{T}X\\right)^{-1}=\n\\begin{pmatrix}\n\\frac{1}{n_{1}} & -\\frac{1}{n_{1}} & -\\frac{1}{n_{1}} \\\\\n-\\frac{1}{n_{1}} & \\frac{n_{1}+n_{2}}{n_{1} n_{2}} & \\frac{1}{n_{1}} \\\\\n-\\frac{1}{n_{1}} & \\frac{1}{n_{1}} & \\frac{n_{1}+n_{3}}{n_{1} n_{3}}\n\\end{pmatrix}\n\\end{equation}\\] Wir halten weiterhin fest, dass \\[\\begin{equation}\n\\begin{aligned}\nX^{T}\\upsilon & =\n\\begin{pmatrix}\n1 & \\cdots & 1 & 1 & \\cdots & 1 & \\cdots & 1 & \\cdots & 1 \\\\\n0 & \\cdots & 0 & 1 & \\cdots & 1 & \\cdots & 0 & \\cdots & 0 \\\\\n& \\vdots & & & \\vdots & & \\vdots & & \\vdots & \\\\\n0 & \\cdots & 0 & 0 & \\cdots & 0 & \\cdots & 1 & \\cdots & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n\\upsilon_{11} \\\\\n\\vdots \\\\\n\\upsilon_{1n_{1}} \\\\\n\\upsilon_{21} \\\\\n\\vdots \\\\\n\\upsilon_{2n_{2}} \\\\\n\\vdots \\\\\n\\upsilon_{p1} \\\\\n\\vdots \\\\\n\\upsilon_{pn_{p}}\n\\end{pmatrix} \\\\\n& =\n\\begin{pmatrix}\n\\sum_{i=1}^{p} \\sum_{j=1}^{n_{i}} \\upsilon_{ij} \\\\\n\\sum_{j=1}^{n_{2}} \\upsilon_{2 j} \\\\\n\\vdots \\\\\n\\sum_{j=1}^{n_{p}} \\upsilon_{p j}\n\\end{pmatrix} .\n\\end{aligned}\n\\end{equation}\\] Es ergibt sich also \\[\\begin{equation}\n\\hat{\\beta} =\n\\left(X^{T}X\\right)^{-1}X^{T}\\upsilon =\n\\begin{pmatrix}\n\\frac{1}{n_{1}} & -\\frac{1}{n_{1}} & \\cdots & -\\frac{1}{n_{1}} \\\\\n-\\frac{1}{n_{1}} & \\frac{n_{1}+n_{2}}{n_{1} n_{2}} & \\cdots & \\frac{1}{n_{1}} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n-\\frac{1}{n_{1}} & \\frac{1}{n_{1}} & \\cdots & \\frac{n_{1}+n_{p}}{n_{1} n_{p}}\n\\end{pmatrix}\n\\begin{pmatrix}\n\\sum_{i=1}^{p} \\sum_{j=1}^{n_{i}} \\upsilon_{ij} \\\\\n\\sum_{j=1}^{n_{2}} \\upsilon_{2 j} \\\\\n\\vdots \\\\\n\\sum_{j=1}^{n_{p}} \\upsilon_{p j}\n\\end{pmatrix}\n\\end{equation}\\] Für die erste Komponente von \\(\\hat{\\beta}\\) ergibt sich damit \\[\\begin{equation}\n\\begin{aligned}\n\\hat{\\beta}_{0}\n& =\n\\frac{1}{n_{1}} \\sum_{i=1}^{p} \\sum_{j=1}^{n_{i}} \\upsilon_{ij}-\\frac{1}{n_{1}} \\sum_{j=1}^{n_{2}} \\upsilon_{2 j}-\\cdots-\\frac{1}{n_{1}} \\sum_{j=1}^{n_{p}} \\upsilon_{p j}\n\\\\\n& =\n\\frac{1}{n_{1}}\\left(\\left(\\sum_{j=1}^{n_{1}} \\upsilon_{1j}+\\sum_{j=1}^{n_{2}} \\upsilon_{2 j}+\\cdots+\\sum_{j=1}^{n_{p}} \\upsilon_{p j}\\right)-\\sum_{j=1}^{n_{2}} \\upsilon_{2 j}-\\cdots-\\sum_{j=1}^{n_{p}} \\upsilon_{p j}\\right)\n\\\\\n& =\n\\frac{1}{n_{1}} \\sum_{j=1}^{n_{1}} \\upsilon_{1j}\n\\\\\n& =\\bar{\\upsilon}_{1}.\n\\end{aligned}\n\\end{equation}\\] Für die zweite Komponente von \\(\\hat{\\beta}\\) und analog für alle weiteren ergibt sich \\[\\begin{equation}\n\\begin{aligned}\n\\hat{\\beta}_{2}\n& =\n-\\frac{1}{n_{1}} \\sum_{i=1}^{p} \\sum_{j=1}^{n_{i}} \\upsilon_{ij}+\\frac{n_{1}+n_{2}}{n_{1} n_{2}} \\sum_{j=1}^{n_{2}} \\upsilon_{2 j}+\\frac{1}{n_{1}} \\sum_{j=1}^{n_{3}} \\upsilon_{3 j}+\\cdots+\\frac{1}{n_{1}} \\sum_{j=1}^{n_{p}} \\upsilon_{p j}\n\\\\\n& =\n\\frac{n_{1}+n_{2}}{n_{1} n_{2}} \\sum_{j=1}^{n_{2}} \\upsilon_{2 j}-\\frac{1}{n_{1}}\\left(\\left(\\sum_{j=1}^{n_{1}} \\upsilon_{1j}+\\sum_{j=1}^{n_{2}} \\upsilon_{2 j}+\\cdots+\\sum_{j=1}^{n_{p}} \\upsilon_{p j}\\right)-\\sum_{j=1}^{n_{3}} \\upsilon_{3 j}-\\cdots-\\sum_{j=1}^{n_{p}} \\upsilon_{p j}\\right) \\\\\n& =\\frac{n_{1}+n_{2}}{n_{1} n_{2}} \\sum_{j=1}^{n_{2}} \\upsilon_{2 j}-\\frac{1}{n_{1}} \\sum_{j=1}^{n_{1}} \\upsilon_{1j}-\\frac{1}{n_{1}} \\sum_{j=1}^{n_{2}} \\upsilon_{2 j} \\\\\n& =\\frac{n_{1}+n_{2}}{n_{1} n_{2}} \\sum_{j=1}^{n_{2}} \\upsilon_{2 j}-\\frac{n_{2}}{n_{1} n_{2}} \\sum_{j=1}^{n_{2}} \\upsilon_{2 j}-\\frac{1}{n_{1}} \\sum_{j=1}^{n_{1}} \\upsilon_{1j} \\\\\n& =\\frac{n_{1}}{n_{1} n_{2}} \\sum_{j=1}^{n_{2}} \\upsilon_{2 j}-\\frac{1}{n_{1}} \\sum_{j=1}^{n_{1}} \\upsilon_{1j} \\\\\n& =\\frac{1}{n_{2}} \\sum_{j=1}^{n_{2}} \\upsilon_{2 j}-\\frac{1}{n_{1}} \\sum_{j=1}^{n_{1}} \\upsilon_{1j}\n\\\\\n& =\\bar{\\upsilon}_{2}-\\bar{\\upsilon}_{1} .\n\\end{aligned}\n\\end{equation}\\]\n\nFolgender R Code implementiert die Parameterschätzung des EVA-Modells für den Beispieldatensatz. Neben der mithilfe des Betaparameterschätzers gewonnnenen Schätzwerte für den Referenzgruppen- und die Effekparameter evaluiert der Code auch die stichprobenmittel(differenzen)basierten Schätzer aus @-betaparameterschaetzung-im-eva-modell. Weiterhin evaluiert der Code neben dem Varianzparameterschätzer auch die gepoolte Stichprobenvarianz sowie die Gesamtstichprobenvarianz, die sich deutlich unterscheiden.\n\n# Datenmanagement\nD           = read.csv(\"./_data/408-einfaktorielle-varianzanalyse.csv\")          # Dataframe \ny           = D$dBDI                                                             # Datenvektor\ny_1         = D$dBDI[D$COND == \"F2F\"]                                            # Daten F2F\ny_2         = D$dBDI[D$COND == \"ONL\"]                                            # Daten ONL \ny_3         = D$dBDI[D$COND == \"WLC\"]                                            # Daten WLC \n\n# Modellformulierung\np          = 3                                                                   # drei Gruppen\nm          = length(y_1)                                                         # balancierters Design \nn          = p*m                                                                 # Datenvektordimension\nXt         = cbind(                                                              # Designmatrix\n             matrix(1, nrow = n, ncol = 1),\n             kronecker(diag(p), matrix(1, nrow = m, ncol = 1)))\nX          = Xt[,-2]\n\n# Modellschätzung\nbeta_hat   = solve(t(X) %*% X) %*% t(X) %*% y                                    # Betaparameterschätzer\neps_hat    = y - X %*% beta_hat                                                  # Residuenvektor\nsigsqr_hat = (t(eps_hat) %*% eps_hat) /(n-p)                                     # Varianzparameterschätzer\ns_sqr_123  = ((m-1)*var(y_1) +                                                   # gepoolte Stichprobenvarianz\n              (m-1)*var(y_2) +\n              (m-1)*var(y_3))/(m+m+m-p)\n\n\n\nhat{beta}                                    :  9.83 -2.83 -11.25 \nbar{y}_1,bar{y}_2,bar{y}_3                   :  9.83 7 -1.42 \nbar{y}_1,bar{y}_2-bar{y}_1,bar{y}_3-bar{y}_1 :  9.83 -2.83 -11.25 \nhat{sigsqr}                                  :  5.17 \ns_123^2                                      :  5.17 \ns_y^2                                        :  28.35",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Einfaktorielle Varianzanalyse</span>"
    ]
  },
  {
    "objectID": "408-Einfaktorielle-Varianzanalyse.html#modellevaluation",
    "href": "408-Einfaktorielle-Varianzanalyse.html#modellevaluation",
    "title": "32  Einfaktorielle Varianzanalyse",
    "section": "32.5 Modellevaluation",
    "text": "32.5 Modellevaluation\nPrinzipiell sind alle Parameterschätzwerte in einem EVA-Modell von Interesse und können mithilfe von T-Statistiken im Sinne von Konfidenzintervallen oder Hypothesentests evaluiert werden. Traditionell steht im EVA-Szenario allerdings häufig die Evaluation der Nullhypothese, dass die wahren, aber unbekannten, Erwartungswertparameter aller Gruppen identisch sind, im Vordergrund. Vor dem Hintergrund der EVA Effektdarstellung mit Referenzgruppe entspricht dies der Nullhypothese, dass die Effektparameter gleich Null sind, formal \\[\n\\Theta_{0} :=\n\\left\\{\n\\begin{pmatrix}\n\\mu_{0} \\\\\n\\alpha_{2} \\\\\n\\vdots \\\\\n\\alpha_{p}\n\\end{pmatrix} \\in \\mathbb{R}^{p} \\mid\n\\alpha_{i}=0 \\mbox{ für } i=2,\\ldots,p\n\\right\\}\n=\n\\mathbb{R} \\times\\left\\{0_{p-1}\\right\\}\n\\tag{32.2}\\] und \\[\n\\Theta_{1}\n:=\n\\left\\{\n\\begin{pmatrix}\n\\mu_{0} \\\\\n\\alpha_{2} \\\\\n\\vdots \\\\\n\\alpha_{p}\n\\end{pmatrix} \\in \\mathbb{R}^{p}\n\\mid\n\\alpha_{i} \\neq 0 \\text { für mindestens ein } i=2,\\ldots,p\n\\right\\}\n=\n\\mathbb{R}^{p} \\backslash \\Theta_{0} .\n\\tag{32.3}\\] Zur Evaluation der Nullhypothese wird dabei wird im Allgemeinen eine F-Statistik genutzt. Im Folgenden wollen wir zunächst diese F-Statistik anhand einer Quadratsummenzerlegung der Datenvariabilität in einem EVA-Szenario entwickeln. In diesem Kontext führen wir mit \\(\\eta^{2}\\) (Eta-Quadrat) auch ein zu dem aus Kapitel 26 bekannten Bestimmheitsmaß \\(\\mathbf{R}^{2}\\) analoges Effektstärkenmaß ein. Ausgestattet mit der speziellen Form der F-Statistik für das EVA-Modell diskutieren wir dann den traditionellen Test der EVA Nullhypothese.\n\n32.5.1 Quadratsummenzerlegung und Bestimmheitsmaß \\(\\eta^{2}\\)\nDie Variabilität der Daten eines EVA-Szenarios lässt sich wie in folgendem Theorem dargestellt im Sinne einer Quadratsummenzerlegung schreiben.\n\nTheorem 32.3 (Quadratsummenzerlegung bei einfaktorieller Varianzanalyse) Für \\(i=\\) \\(1,\\ldots,p\\) und \\(j=1,\\ldots,n_{i}\\) sei \\(\\upsilon_{ij}\\) die \\(j\\)te Datenvariable in der \\(i\\)ten Gruppe eines EVASzenarios. Weiterhin seien mit \\(n:=\\sum_{i=1}^{p} n_{i}\\) \\[\\begin{equation}\n\\bar{\\upsilon} :=\n\\frac{1}{n} \\sum_{i=1}^{p} \\sum_{j=1}^{n_{i}} \\upsilon_{ij}\n\\mbox{ und }\n\\bar{\\upsilon}_{i}=\\frac{1}{n_{i}} \\sum_{j=1}^{n_{i}} \\upsilon_{ij}\n\\end{equation}\\] das Gesamtstichprobenmittel und \\(i\\)te Stichprobenmittel, respektive. Schließlich seien\n\ndie Total-Sum-of-Squares definiert als \\[\\begin{equation}\n\\mbox{SQT} := \\sum_{i=1}^{p} \\sum_{j=1}^{n_{i}}\\left(\\upsilon_{ij}-\\bar{\\upsilon}\\right)^{2}\n\\end{equation}\\]\ndie Between-Sum-of-Squares definert als \\[\\begin{equation}\n\\mbox{SQB} := \\sum_{i=1}^{p} n_{i}\\left(\\bar{\\upsilon}_{i}-\\bar{\\upsilon}\\right)^{2}\n\\end{equation}\\] und die Within-Sum-of-Squares definiert als \\[\\begin{equation}\n\\mbox{SQW} := \\sum_{i=1}^{p} \\sum_{j=1}^{n_{i}}\\left(\\upsilon_{ij}-\\bar{\\upsilon}_{i}\\right)^{2}\n\\end{equation}\\] Dann gilt \\[\\begin{equation}\n\\mbox{SQT}=\\mbox{SQB}+\\mbox{SQW}\n\\end{equation}\\]\n\n\n\nBeweis. Es gilt \\[\\begin{equation}\n\\begin{aligned}\n\\mbox{SQT}\n& = \\sum_{i=1}^{p} \\sum_{j=1}^{n_{i}}\\left(\\upsilon_{ij}-\\bar{\\upsilon}\\right)^{2} \\\\\n& = \\sum_{i=1}^{p} \\sum_{j=1}^{n_{i}}\\left(\\upsilon_{ij}-\\bar{\\upsilon}_{i}+\\bar{\\upsilon}_{i}-\\bar{\\upsilon}\\right)^{2} \\\\\n& = \\sum_{i=1}^{p} \\sum_{j=1}^{n_{i}}\\left(\\left(\\upsilon_{ij}-\\bar{\\upsilon}_{i}\\right)+\\left(\\bar{\\upsilon}_{i}-\\bar{\\upsilon}\\right)\\right)^{2} \\\\\n& = \\sum_{i=1}^{p} \\sum_{j=1}^{n_{i}}\\left(\\left(\\upsilon_{ij}-\\bar{\\upsilon}_{i}\\right)^{2}+2\\left(\\upsilon_{ij}-\\bar{\\upsilon}_{i}\\right)\\left(\\bar{\\upsilon}_{i}-\\bar{\\upsilon}\\right)+\\left(\\bar{\\upsilon}_{i}-\\bar{\\upsilon}\\right)^{2}\\right) \\\\\n& =\\sum_{i=1}^{p}\\left(\\sum_{j=1}^{n_{i}}\\left(\\upsilon_{ij}-\\bar{\\upsilon}_{i}\\right)^{2}+\\sum_{j=1}^{n_{i}} 2\\left(\\upsilon_{ij}-\\bar{\\upsilon}_{i}\\right)\\left(\\bar{\\upsilon}_{i}-\\bar{\\upsilon}\\right)+\\sum_{j=1}^{n_{i}}\\left(\\bar{\\upsilon}_{i}-\\bar{\\upsilon}\\right)^{2}\\right)\n\\\\\n& =\\sum_{i=1}^{p}\\left(\\sum_{j=1}^{n_{i}}\\left(\\upsilon_{ij}-\\bar{\\upsilon}_{i}\\right)^{2}+2\\left(\\bar{\\upsilon}_{i}-\\bar{\\upsilon}\\right) \\sum_{j=1}^{n_{i}}\\left(\\upsilon_{ij}-\\bar{\\upsilon}_{i}\\right)+n_{i}\\left(\\bar{\\upsilon}_{i}-\\bar{\\upsilon}\\right)^{2}\\right) \\\\\n& =\\sum_{i=1}^{p}\\left(\\sum_{j=1}^{n_{i}}\\left(\\upsilon_{ij}-\\bar{\\upsilon}_{i}\\right)^{2}+2\\left(\\bar{\\upsilon}_{i}-\\bar{\\upsilon}\\right) \\sum_{j=1}^{n_{i}}\\left(\\upsilon_{ij}-\\frac{1}{n_{i}} \\sum_{j=1}^{n_{i}} \\upsilon_{ij}\\right)+n_{i}\\left(\\bar{\\upsilon}_{i}-\\bar{\\upsilon}\\right)^{2}\\right) \\\\\n& =\\sum_{i=1}^{p}\\left(\\sum_{j=1}^{n_{i}}\\left(\\upsilon_{ij}-\\bar{\\upsilon}_{i}\\right)^{2}+2\\left(\\bar{\\upsilon}_{i}-\\bar{\\upsilon}\\right)\\left(\\sum_{j=1}^{n_{i}} \\upsilon_{ij}-\\sum_{j=1}^{n_{i}}\\left(\\frac{1}{n_{i}} \\sum_{j=1}^{n_{i}} \\upsilon_{ij}\\right)\\right)+n_{i}\\left(\\bar{\\upsilon}_{i}-\\bar{\\upsilon}\\right)^{2}\\right) \\\\\n& =\\sum_{i=1}^{p}\\left(\\sum_{j=1}^{n_{i}}\\left(\\upsilon_{ij}-\\bar{\\upsilon}_{i}\\right)^{2}+2\\left(\\bar{\\upsilon}_{i}-\\bar{\\upsilon}\\right)\\left(\\sum_{j=1}^{n_{i}} \\upsilon_{ij}-\\frac{n_{i}}{n_{i}} \\sum_{j=1}^{n_{i}} \\upsilon_{ij}\\right)+n_{i}\\left(\\bar{\\upsilon}_{i}-\\bar{\\upsilon}\\right)^{2}\\right)\n\\\\\n& =\\sum_{i=1}^{p}\\left(\\sum_{j=1}^{n_{i}}\\left(\\upsilon_{ij}-\\bar{\\upsilon}_{i}\\right)^{2}+2\\left(\\bar{\\upsilon}_{i}-\\bar{\\upsilon}\\right)\\left(\\sum_{j=1}^{n_{i}} \\upsilon_{ij}-\\sum_{j=1}^{n_{i}} \\upsilon_{ij}\\right)+n_{i}\\left(\\bar{\\upsilon}_{i}-\\bar{\\upsilon}\\right)^{2}\\right) \\\\\n& =\\sum_{i=1}^{p}\\left(\\sum_{j=1}^{n_{i}}\\left(\\upsilon_{ij}-\\bar{\\upsilon}_{i}\\right)^{2}+n_{i}\\left(\\bar{\\upsilon}_{i}-\\bar{\\upsilon}\\right)^{2}\\right) \\\\\n& =\\sum_{i=1}^{p} \\sum_{j=1}^{n_{i}}\\left(\\upsilon_{ij}-\\bar{\\upsilon}_{i}\\right)^{2}+\\sum_{i=1}^{p} n_{i}\\left(\\bar{\\upsilon}_{i}-\\bar{\\upsilon}\\right)^{2} \\\\\n& =\\mbox{SQW}+\\mbox{SQB}\n\\end{aligned}\n\\end{equation}\\] und damit direkt \\[\\begin{equation}\n\\mbox{SQT}=\\mbox{SQB}+\\mbox{SQW}\n\\end{equation}\\]\n\nDie Quadratsummenzerlegung im Szenario der EVA ist offenbar analog zur Quadratsummenzerlegung bei einer Ausgleichsgerade (vgl. ?sec-ausgleichsgerade). Dementsprechend wird die Within-Sum-of-Squares der EVA auch häufig als Residual-Sum-of-Squares bezeichnet. Die Begriffsbildungen von Theorem 32.3 ergeben sich intuitiv wiederrum wie folgt:\n\nSQT repräsentiert die Gesamtvariabilität der Daten \\(\\upsilon_{ij}\\) um das Gesamtstichprobenmittel \\(\\bar{\\upsilon}\\).\nSQB repräsentiert die anhand des jeweiligen Gruppenumfangs \\(n_{i}\\) gewichtete Variabilität der Gruppenstichprobenmittel um das Gesamtstichprobenmittel. Große Werte von SQB implizieren also eine große Abhängigkeit der Gruppenstichprobenmittel von dem jeweils betrachteten Faktorlevel \\(i\\), kleine Werte von SQB dagegen eine geringe Abhängigkeit der Gruppenstichprobenmittel von dem jeweils betrachteten Faktorlevel. In diesem Sinne repräsentiert SQB also die durch die Betrachtung des jeweiligen Faktorlevels erklärte Datenvariabilität und ist analog zur Explained-Sum-of-Squares (SQE) der Quadratsummenzerlegung bei einer Ausgleichsgerade.\nSQW repräsentiert die über Faktorlevel summierte Datenvariabilität die nach Erklärung der Datenvariabilitit in der \\(i\\) ten Gruppe durch ihr jeweiliges Stichprobenmittel verbleibt. Damit ist die SQW analog zur Residual-Sum-of-Squares der Quadratsummenzerlegung bei Ausgleichsgerade.\n\nIn der Zusammenschau quantifiziert SQB also die Stärke der Unterschiede zwischen den Faktorleveln. Das Effekstärkenmaß \\(\\eta^{2}\\) und die F-Statistik der EVA setzen die SQB nun jeweils in ein anderers Verhältnis: \\(\\eta^{2}\\) vergleicht die SQB mit der SQT und betrachtet damit den Anteil der Variabilität zwischen den Faktorleveln an der Gesamtvariabilität der Daten. Die F-Statistik dagegen vergleicht die SQB mit der SQW und setzt damit den Einfluß der Faktorenlevel in das Verhältnis mit der nicht erklärten Datenvariabilität nach Subtraktion dieses Einflußes. Wir betrachten an dieser Stelle zunächst das Effektstärkemaß \\(\\eta^{2}\\) mithilfe folgender Definition.\n\nDefinition 32.3 (Effektstärkenmaß \\(\\eta^{2}\\)) Für ein EVA-Szenario seien die Between-Sum-of-Squares SQB und die Total-Sum-of-Squares SQT definiert wie in Theorem 32.3. Dann ist das Effektstärkenmaß \\(\\eta^{2}\\) definiert als \\[\\begin{equation}\n\\eta^{2}:=\\frac{\\mbox{SQB}}{\\mbox{SQT}}\n\\end{equation}\\]\n\nDer Vergleich mit Definition 26.4, dass \\(\\eta^{2}\\) analog zum Bestimmtheitsmaß \\(\\mbox{R}^{2}\\) definiert ist. Wie oben beschrieben gibt \\(\\eta^{2}\\) den Anteil der Datenvariabilität zwischen den Faktorleveln an der Gesamtdatenvariabilität an. Schließlich folgt mit \\[\\begin{equation}\n\\mbox{SQT}=\\mbox{SQB}+\\mbox{SQW}\n\\end{equation}\\] analog zu \\(\\mbox{R}^{2}\\) sofort, dass für \\(\\mbox{SQT} \\neq 0\\) gilt, dass \\(\\eta^{2} \\in [0,1]\\), weil einerseits \\[\\begin{equation}\n\\mbox{SQB}=0 \\Rightarrow \\mbox{SQT}=\\mbox{SQW} \\mbox{ und } \\eta^{2}=0\n\\end{equation}\\] und andererseits \\[\\begin{equation}\n\\mbox{SQW}=0 \\Rightarrow \\mbox{SQT}=\\mbox{SQB} \\mbox{ und } \\eta^{2}=1.\n\\end{equation}\\]\n\n\n32.5.2 F-Teststatistik\nWir wollen nun zeigen, dass für die Designmatrixform der Effektdarstellung mit Referenzgruppe des EVA-Modells die F-Statistik bei der Partitionierung \\(p_{0}:=1\\) und \\(p_{1}:=p-1\\) (vgl. Kapitel 30) als Verhältnis skalierter Versionen der SQB und SQW geschrieben werden kann. Dabei impliziert \\(p_{0}:=1\\) hier insbesondere, dass das betrachtete reduzierte EVA-Modell die Designmatrix \\(X_{0}:=1_{n}\\) und den Betaparameter \\(\\beta:=\\mu_{0}\\) hat, und damit insbesondere auch, dass das reduzierte EVA-Modell keine Effektparameter hat. Die dabei relevanten Skalierungsfaktoren beziehen die SQB und SQW jeweils auf die Anzahl der Effektparameter bzw. die Differenz aus Gesamtanzahl der Datenpunkte und Anzahl der Betaparameter, respektive. Es gilt folgendes Theorem.\n\nTheorem 32.4 (F-Statistik der einfaktoriellen Varianzanalyse) Es sei \\[\\begin{equation}\n\\upsilon = X\\beta+\\varepsilon \\mbox{ mit } \\varepsilon \\sim N\\left(0_{n}, \\sigma^{2} I_{n}\\right)\n\\end{equation}\\] die Designmatrixform der Effektdarstellung mit Referenzgruppe des EVA-Modells. Weiterhin sei dieses Modell im Sinne von Definition 30.2 partioniert mit \\(p_{0}:=1\\) und \\(p_{1}:=p-1\\). Schließlich seien\n\ndie Mean-Between-Sum-of-Squares definiert als \\[\\begin{equation}\n\\mbox{MSB}:=\\frac{\\mbox{SQB}}{p-1}\n\\end{equation}\\]\nund die Mean-Within-Sum-of-Squares definiert als \\[\\begin{equation}\n\\mbox{MSW}:=\\frac{\\mbox{SQW}}{n-p}\n\\end{equation}\\] wobei \\(p-1\\) auch als Between-Freiheitsgrade und \\(n-p\\) auch als Within-Freiheitsgrade bezeichnet werden. Dann gilt mit der Definition der F-Statistik (Definition 30.3), dass \\[\\begin{equation}\nF=\\frac{\\mbox{MSB}}{\\mbox{MSW}}\n\\end{equation}\\]\n\n\n\nBeweis. Wir halten zunächst fest, dass für den Betaparameterschätzer des reduzierten Modells gilt, dass \\[\\begin{equation}\n\\hat{\\beta}_{0}\n=\\left(X_{0}^{T} X_{0}\\right)^{-1} X_{0}^{T}\\upsilon\n=\\left(1_{n}^{T} 1_{n}\\right)^{-1} 1_{n}^{T}\\upsilon\n=\\frac{1}{n} \\sum_{i=1}^{p} \\sum_{j=1}^{n_{i}} \\upsilon_{ij}\n=\\bar{\\upsilon}\n\\end{equation}\\] Weiterhin ergibt sich \\[\\begin{equation}\n\\hat{\\varepsilon}_{0}^{T} \\hat{\\varepsilon}_{0}\n= \\left(\\upsilon-X_{0} \\hat{\\beta}_{0}\\right)^{T}\\left(\\upsilon-X_{0} \\hat{\\beta}_{0}\\right)\n= \\left(\\upsilon-1_{n} \\bar{\\upsilon}\\right)^{T}\\left(\\upsilon-1_{n} \\bar{\\upsilon}\\right)\n= \\sum_{i=1}^{p} \\sum_{j=1}^{n_{i}}\\left(\\upsilon_{ij}-\\bar{\\upsilon}\\right)^{2}=\\mbox{SQT} .\n\\end{equation}\\] Der Betaparameterschätzer des vollständigen Modells ergibt sich wie oben gesehen zu \\[\\begin{equation}\n\\hat{\\beta}=\n\\begin{pmatrix}\n\\hat{\\mu}_{0} \\\\\n\\hat{\\alpha}_{2} \\\\\n\\vdots \\\\\n\\hat{\\alpha}_{m}\n\\end{pmatrix}=\\begin{pmatrix}\n\\frac{1}{n_{1}} \\sum_{j=1}^{n_{1}} \\upsilon_{1j} \\\\\n\\frac{1}{n_{2}} \\sum_{j=1}^{n_{2}} \\upsilon_{2 j}-\\frac{1}{n_{1}} \\sum_{j=1}^{n_{1}} \\upsilon_{1j} \\\\\n\\vdots \\\\\n\\frac{1}{n_{m}} \\sum_{j=1}^{n_{m}} \\upsilon_{m j}-\\frac{1}{n_{1}} \\sum_{j=1}^{n_{1}} \\upsilon_{1j}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\bar{\\upsilon}_{1} \\\\\n\\bar{\\upsilon}_{2}-\\bar{\\upsilon}_{1} \\\\\n\\vdots \\\\\n\\bar{\\upsilon}_{p}-\\bar{\\upsilon}_{1}\n\\end{pmatrix},\n\\end{equation}\\] so dass \\[\\begin{equation}\n\\begin{aligned}\n\\hat{\\varepsilon}^{T} \\hat{\\varepsilon}\n& =  (\\upsilon-X \\hat{\\beta})^{T}(\\upsilon-X \\hat{\\beta}) \\\\\n& =\n\\left(\\begin{pmatrix}\n\\upsilon_{11} \\\\\n\\vdots \\\\\n\\upsilon_{1n_{1}} \\\\\n\\upsilon_{21} \\\\\n\\vdots \\\\\n\\upsilon_{2n_{2}} \\\\\n\\vdots \\\\\n\\upsilon_{p1} \\\\\n\\vdots \\\\\n\\upsilon_{pn_{p}}\n\\end{pmatrix}-\\begin{pmatrix}\n1 & 0 & & 0 \\\\\n\\vdots & \\vdots & \\cdots & \\vdots \\\\\n1 & 0 & & 0 \\\\\n1 & 1 & & 0 \\\\\n\\vdots & \\vdots & \\cdots & \\vdots \\\\\n1 & 1 & & 0 \\\\\n\\vdots & \\vdots & \\cdots & \\vdots \\\\\n1 & 0 & & 1 \\\\\n\\vdots & \\vdots & \\cdots & \\vdots \\\\\n1 & 0 & & 1\n\\end{pmatrix}\\begin{pmatrix}\n\\bar{\\upsilon}_{1} \\\\\n\\bar{\\upsilon}_{2}-\\bar{\\upsilon}_{1} \\\\\n\\bar{\\upsilon}_{p}-\\bar{\\upsilon}_{1}\n\\end{pmatrix}\\right)^{T}\n\\left(\\begin{pmatrix}\n\\upsilon_{11} \\\\\n\\vdots \\\\\n\\upsilon_{1n_{1}} \\\\\n\\upsilon_{21} \\\\\n\\vdots \\\\\n\\upsilon_{2n_{2}} \\\\\n\\vdots \\\\\n\\upsilon_{p1} \\\\\n\\vdots \\\\\n\\upsilon_{pn_{p}}\n\\end{pmatrix}-\\begin{pmatrix}\n1 & 0 & & 0 \\\\\n\\vdots & \\vdots & \\cdots & \\vdots \\\\\n1 & 0 & & 0 \\\\\n1 & 1 & & 0 \\\\\n\\vdots & \\vdots & \\cdots & \\vdots \\\\\n1 & 1 & & 0 \\\\\n\\vdots & \\vdots & \\cdots & \\vdots \\\\\n1 & 0 & & 1 \\\\\n\\vdots & \\vdots & \\cdots & \\vdots \\\\\n1 & 0 & & 1\n\\end{pmatrix}\\begin{pmatrix}\n\\bar{\\upsilon}_{1} \\\\\n\\bar{\\upsilon}_{2}-\\bar{\\upsilon}_{1} \\\\\n\\bar{\\upsilon}_{p}-\\bar{\\upsilon}_{1}\n\\end{pmatrix}\\right)\n\\\\\n& = \\begin{pmatrix}\n\\upsilon_{11}-\\bar{\\upsilon}_{1} \\\\\n\\vdots \\\\\n\\upsilon_{1n_{1}}-\\bar{\\upsilon}_{1} \\\\\n\\upsilon_{21}-\\bar{\\upsilon}_{1}-\\bar{\\upsilon}_{2}+\\bar{\\upsilon}_{1} \\\\\n\\vdots \\\\\n\\upsilon_{2n_{2}}-\\bar{\\upsilon}_{1}-\\bar{\\upsilon}_{2}+\\bar{\\upsilon}_{1} \\\\\n\\vdots \\\\\n\\upsilon_{p1}-\\bar{\\upsilon}_{1}-\\bar{\\upsilon}_{p}+\\bar{\\upsilon}_{1} \\\\\n\\vdots \\\\\n\\upsilon_{pn_{p}}-\\bar{\\upsilon}_{1}-\\bar{\\upsilon}_{p}+\\bar{\\upsilon}_{1}\n\\end{pmatrix}^{T}\\begin{pmatrix}\n\\upsilon_{11}-\\bar{\\upsilon}_{1} \\\\\n\\vdots \\\\\n\\upsilon_{1n_{1}}-\\bar{\\upsilon}_{1} \\\\\n\\upsilon_{21}-\\bar{\\upsilon}_{1}-\\bar{\\upsilon}_{2}+\\bar{\\upsilon}_{1} \\\\\n\\vdots \\\\\n\\upsilon_{2n_{2}}-\\bar{\\upsilon}_{1}-\\bar{\\upsilon}_{2}+\\bar{\\upsilon}_{1} \\\\\n\\vdots \\\\\n\\upsilon_{p1}-\\bar{\\upsilon}_{1}-\\bar{\\upsilon}_{p}+\\bar{\\upsilon}_{1} \\\\\n\\vdots \\\\\n\\upsilon_{pn_{p}}-\\bar{\\upsilon}_{1}-\\bar{\\upsilon}_{p}+\\bar{\\upsilon}_{1}\n\\end{pmatrix}\n\\\\\n& =\n\\begin{pmatrix}\n\\upsilon_{11}-\\bar{\\upsilon}_{1} \\\\\n\\vdots \\\\\n\\upsilon_{1n_{1}}-\\bar{\\upsilon}_{1} \\\\\n\\upsilon_{21}-\\bar{\\upsilon}_{2} \\\\\n\\vdots \\\\\n\\upsilon_{2n_{2}}-\\bar{\\upsilon}_{2} \\\\\n\\vdots \\\\\n\\upsilon_{p1}-\\bar{\\upsilon}_{p} \\\\\n\\vdots \\\\\n\\upsilon_{pn_{p}}-\\bar{\\upsilon}_{p}\n\\end{pmatrix}^{T}\\begin{pmatrix}\n\\upsilon_{11}-\\bar{\\upsilon}_{1} \\\\\n\\vdots \\\\\n\\upsilon_{1n_{1}}-\\bar{\\upsilon}_{1} \\\\\n\\upsilon_{21}-\\bar{\\upsilon}_{2} \\\\\n\\vdots \\\\\n\\upsilon_{2n_{2}}-\\bar{\\upsilon}_{2} \\\\\n\\vdots \\\\\n\\upsilon_{p1}-\\bar{\\upsilon}_{p} \\\\\n\\vdots \\\\\n\\upsilon_{pn_{p}}-\\bar{\\upsilon}_{p}\n\\end{pmatrix} \\\\\n& = \\sum_{i=1}^{p} \\sum_{j=1}^{n_{i}}\\left(\\upsilon_{ij}-\\bar{\\upsilon}_{i}\\right)^{2} \\\\\n& = \\operatorname{SQW}.\n\\end{aligned}\n\\end{equation}\\] Mit dem Theorem zur Quadratsummenzerlegung bei einfaktorieller Varianzanalyse \\[\\begin{equation}\n\\mbox{SQT}=\\mbox{SQB}+\\mbox{SQW} \\Leftrightarrow \\mbox{SQB}=\\mbox{SQT}-\\mbox{SQW}\n\\end{equation}\\] folgt sofort, dass \\[\\begin{equation}\n\\begin{aligned}\n\\mbox{SQB}\n& =\\mbox{SQT}-\\mbox{SQW} \\\\\n& =\\hat{\\varepsilon}_{0}^{T} \\hat{\\varepsilon}_{0}-\\hat{\\varepsilon}^{T} \\hat{\\varepsilon} .\n\\end{aligned}\n\\end{equation}\\] Dann aber folgt auch direkt, dass \\[\\begin{equation}\n\\begin{aligned}\n\\frac{\\mbox{MSB}}{\\mbox{MSW}}\n& =\\frac{\\frac{\\mbox{SQB}}{p-1}}{\\frac{\\mbox{SQW}}{n-p}} \\\\\n& =\\frac{\\frac{\\hat{\\varepsilon}_{0}^{T} \\hat{\\varepsilon}_{0}-\\hat{\\varepsilon}^{T} \\hat{\\varepsilon}}{p-1}}{\\frac{\\hat{\\varepsilon}^{T} \\hat{\\varepsilon}}{n-p}} \\\\\n& = F\n\\end{aligned}\n\\end{equation}\\]\n\nGegeben die sehr ähnlichen Definitionen des Effektstärkemaßes \\(\\eta^{2}\\) und der F-Statistik der EVA ist folgendes Resultat naheliegend.\n\nTheorem 32.5 (Effektstärkenmaß \\(\\eta^{2}\\) und F-Teststatistik.) Für ein EVA-Szenario mit \\(p\\) Gruppen und Gesamtdatenpunktanzahl \\(n\\) seien das Effektstärkenmaß \\(\\eta^{2}\\) und die F-Statistik der einfaktoriellen Varianzanalyse gegeben. Dann gilt \\[\\begin{equation}\n\\eta^{2}=\\frac{F(p-1)}{F(p-1)+(n-p)}\n\\end{equation}\\]\n\n\nBeweis. Wir halten zunächst fest, dass \\[\\begin{equation}\nF =\n\\frac{\\mbox{SQB}}{\\mbox{SQW}} \\cdot \\frac{n-p}{p-1}\n\\Leftrightarrow\n\\mbox{SQB}=\\frac{p-1}{n-p} \\cdot \\mbox{SQW} \\cdot F.\n\\end{equation}\\] Damit folgt dann \\[\\begin{equation}\n\\begin{aligned}\n\\eta^{2}\n& =\\frac{\\mbox{SQB}}{\\mbox{SQT}} \\\\\n& =\\frac{\\mbox{SQB}}{\\mbox{SQB}+\\mbox{SQW}} \\\\\n& =\\frac{\\frac{p-1}{n-p} \\cdot \\mbox{SQW} \\cdot F}{\\frac{p-1}{n-p} \\cdot \\mbox{SQW} \\cdot F+\\mbox{SQW}}\\\\\n& =\\frac{\\frac{F(p-1)}{n-p} \\cdot \\mbox{SQW}}{\\frac{F(p-1)}{n-p} \\cdot \\mbox{SQW}+\\mbox{SQW}} \\\\\n& =\\frac{\\frac{F(p-1)}{n-p} \\cdot \\mbox{SQW}}{\\left(\\frac{F(p-1)}{n-p}+1\\right) \\cdot \\mbox{SQW}} \\\\\n& =\\frac{\\frac{F(p-1)}{n-p}}{\\frac{F(p-1)}{n-p}+\\frac{n-p}{n-p}} \\\\\n& =\\frac{\\frac{F(p-1)}{n-p}}{\\frac{F(p-1)+(n-p)}{n-p}} \\\\\n& =\\frac{F(p-1)}{F(p-1)+(n-p)}\n\\end{aligned}\n\\end{equation}\\]\n\nIntuitiv ist dabei das Verhältnis von \\(\\eta^{2}\\) und F-Statistik analog zum Verhältnis von Cohen’s \\(d\\) und der T-Statistik bei Einstich- und Zweistichproben-T-Tests. Insbesondere ist die gleichzeitige Angabe von \\(\\eta^{2}\\) und der F-Statistik bei bekannten Gruppengrößen redundant.\n\n\n32.5.3 F-Test der einfaktoriellen Varianzanalyse\nWir wollen nun den Gebrauch der F-Statistik zur Durchführung eines Tests der Nullhypothese (Gleichung 32.2) diskutieren. Wir erinnern daran, dass diese Nullhypothese intuitiv besagt, dass die Erwartungswerte über alle Level des Faktors identisch sind bzw. dass alle Effektparameter gleich Null sind. Das Verwerfen dieser Nullhypothese impliziert also, dass inferentielle Evidenz dahingehend besteht, dass zumindest ein Effektparameter von Null verschieden ist. Allerdings impliziert das Verwerfen der Nullhypothese des F-Tests der einfaktoriellen Varianzanalyse keine Aussage darüber, um welchen Effektparameter es sich dabei genau handelt. Wir definieren den F-Test der einfaktoriellen Varianzanalyse wie folgt.\n\nDefinition 32.4 (F-Test der einfaktoriellen Varianzanalyse) Gegeben sei das EVA-Modell in Effektparameterdarstellung mit Referenzgruppe sowie die Null- und Alternativhypothesen \\[\\begin{equation}\n\\Theta_{0} :=\n\\left\\{\n\\begin{pmatrix}\n\\mu_{0} \\\\\n\\alpha_{2} \\\\\n\\vdots \\\\\n\\alpha_{p}\n\\end{pmatrix} \\in \\mathbb{R}^{p}\n\\mid\n\\alpha_{i} = 0\n\\mbox{ für } i=2,\\ldots,p\n\\right\\}\n\\mbox{ und }\n\\Theta_{1} :=\n\\mathbb{R}^{p} \\backslash \\Theta_{0}.\n\\end{equation}\\] Weiterhin sei die F-Teststatistik definiert durch \\[\\begin{equation}\nF=\\frac{\\mbox{MSB}}{\\mbox{MSW}}\n\\end{equation}\\] mit der Mean-Between-Sum-of-Squares MSB und der Mean-Within-Sum-of-Squares definiert wie in Theorem 32.4. Dann ist der F-Test der einfaktoriellen Varianzanalyse definiert als der kritische Wert-basierte Test \\[\\begin{equation}\n\\phi(\\upsilon)\n:= 1_{\\{F \\geq k\\}}\n:=\n\\begin{cases}\n1 & F \\geq k \\\\\n0 & F &lt; k\n\\end{cases}\n\\end{equation}\\]\n\nDer Kontrolle der Typ-I-Fehlerwahrscheinlichkeit liegt die \\(f\\)-Verteilung der F-Statistik zugrunde. Dies ist die Kernaussage folgenden Theorems.\n\nTheorem 32.6 (Testumfangkontrolle des F-Tests der einfaktoriellen Varianzanalyse) \\(\\phi\\) sei der \\(F\\)-Test der einfaktoriellen Varianzanalyse. Dann ist \\(\\phi\\) ein Level-\\(\\alpha_{0}\\)-Test mit Testumfang \\(\\alpha_{0}\\), wenn der kritische Wert definiert ist durch \\[\\begin{equation}\nk_{\\alpha_{0}}:=\\varphi^{-1}\\left(1-\\alpha_{0} ; p-1, n-p\\right),\n\\end{equation}\\] wobei \\(\\varphi^{-1}(\\cdot ; p-1, n-p)\\) die inverse KVF der \\(f\\)-Verteilung mit Freiheitsgradparametern \\(p-1\\) und \\(n-p\\) ist.\n\n\nBeweis. Die Testgütefunktion des betrachteten Tests im vorliegenden Testszenario ist definiert als \\[\\begin{equation}\nq: \\mathbb{R} \\rightarrow[0,1], \\beta \\mapsto q_{\\phi}(\\beta):=\\mathbb{P}_{\\beta}(\\phi=1).\n\\end{equation}\\] Wir haben in ?sec-f-statiatik gesehen, dass die F-Statistik für \\(p_{1}=p-1\\) nach einer nichtzentralen \\(f\\)-Verteilung verteilt ist, \\[\\begin{equation}\nF \\sim f(\\delta,p-1, n-p).\n\\end{equation}\\] Weiterhin ist der Ablehnungsbereich des hier betrachteten Tests gegeben als \\([k, \\infty[\\). Für die funktionale Form der Testgütefunktion ergibt sich also \\[\\begin{equation}\n\\begin{aligned}\n\\mathbb{P}_{\\beta}(\\phi=1)\n& = \\mathbb{P}_{\\beta}(F \\in[k, \\infty[) \\\\\n& = \\mathbb{P}_{\\beta}(F \\geq k) \\\\\n& = 1-\\mathbb{P}_{\\beta}(F \\leq k) \\\\\n& = 1-\\varphi(k ; \\delta,p-1, n-p),\n\\end{aligned}\n\\end{equation}\\] wobei \\(\\varphi(k ; \\delta,p-1, n-p)\\) den Wert der KVF der nichtzentralen \\(f\\)-Verteilung an der Stelle \\(k\\) und mit Nichtzentralitätsparameter \\(\\delta\\) sowie Freiheitsgradparametern \\(p-1\\) und \\(n-p\\) bezeichnet (vgl. sec-f-statistik). Damit der betrachtete Test ein Level-\\(\\alpha_{0}\\)-Test ist, muss bekanntlich gelten, dass \\[\\begin{equation}\nq_{\\phi}(\\beta) \\leq \\alpha_{0} \\mbox{ für alle } \\beta \\in \\Theta_{0} \\mbox{ mit }\n\\Theta_{0} = \\mathbb{R} \\times\\left\\{0_{p-1}\\right\\}.\n\\end{equation}\\] Mit der Form des Nichtzentralitätsparameters gegeben durch (vgl. Theorem 30.3) \\[\\begin{equation}\n\\delta =\n\\frac{1}{\\sigma^{2}} c^{T} \\beta\\left(c^{T}\\left(X^{T}X\\right)^{-1} c\\right)^{-1} c^{T} \\beta\n\\end{equation}\\] folgt mit \\(\\beta \\in \\Theta_{0}\\) aus \\[\\begin{equation}\nc =\n\\begin{pmatrix}\n0 \\\\\n1_{p-1}\n\\end{pmatrix} \\in \\mathbb{R}^{p} \\mbox{ und }\n\\beta\n=\\begin{pmatrix}\n\\mu_{0} \\\\\n0_{p-1}\n\\end{pmatrix} \\in \\mathbb{R}^{p}\n\\end{equation}\\] dann aber \\(\\delta=0\\) und somit \\[\\begin{equation}\nq_{\\phi}(\\beta)=1-\\varphi(k ; p-1, n-p) \\mbox{ für alle } \\beta \\in \\Theta_{0} .\n\\end{equation}\\] wobei \\(\\varphi(k ; p-1, n-p)\\) den Wert der KVF der \\(f\\)-Verteilung an der Stelle \\(k\\) mit Freiheitsgradparametern \\(p-1\\) und \\(n-p\\) bezeichnet. Der Testumfang des betrachteten Tests ergibt sich als \\[\\begin{equation}\n\\alpha=\\max _{\\beta \\in \\Theta_{0}} q_{\\phi}(\\beta)=1-\\varphi(k ; p-1, n-p)\n\\end{equation}\\] da \\(q_{\\phi}(\\beta)\\) für \\(\\beta \\in \\Theta_{0}\\) nicht von \\(\\mu_{0}\\) abhängt. Wir müssen also lediglich zeigen, dass die Wahl von \\(k_{\\alpha_{0}}\\) wie im Theorem garantiert, dass \\(\\phi\\) den Testumfang \\(\\alpha_{0}\\) hat. Sei also \\(k:=k_{\\alpha_{0}}\\). Dann gilt für alle \\(\\beta \\in \\Theta_{0}\\) \\[\\begin{equation}\nq_{\\phi}(\\beta)\n= 1-\\varphi\\left(\\varphi^{-1}\\left(1-\\alpha_{0} ; p-1, n-p\\right); p-1, n-p\\right)\n= 1-1-\\alpha_{0}=\\alpha_{0}\n\\end{equation}\\] und damit ist alles gezeigt.\n\n\n\n\n\n\n\nAbbildung 32.2: Exemplarischer kritischer Wert und Ablehnungsbereich bei einem F-Test der einfaktorieller Varianzanalyse.\n\n\n\nIn Abbildung 32.2 visualisieren wir die Wahl des kritischen Wertes \\(k_{\\alpha_{0}}\\) zur Kontrolle des Testumfangs mithilfe der KVF der \\(f\\)-Verteilung sowie den resultierenden Ablehnungsbereich für \\(\\alpha_{0}:=0.05\\), einem Faktor mit drei Leveln \\(p=3\\) und einem balancierten Design mit Gruppenumfang \\(m=12\\), also \\(n=3 \\cdot 12=36\\). In diesem Fall würde die Nullhypothese bei einem Wert der F-Statistik von größer als 3.28 verworfen werden.\np-Wert\nEs sei \\(f\\) ein vorliegender Wert der \\(F\\)-Statistik im Kontext einer einfaktoriellen Varianzanalyse. Dann ergibt sich der zu \\(f\\) gehörige p-Wert anhand folgenden Theorems.\n\nTheorem 32.7 (p-Wert der F-Statistik bei einfaktorieller Varianzanalyse) Gegeben sei das in Definition 32.4 spezifizierte Szenario eines F-Tests bei einfaktorieller Varianzanalyse. Dann ergibt sich der zu einem vorliegenden Wert \\(f\\) der F-Statistik zugehörige p-Wert als \\[\\begin{equation}\n\\mbox{p-Wert} = \\mathbb{P}(F \\geq f)=1-\\varphi(f ; p-1, n-p)\n\\end{equation}\\]\n\n\nBeweis. Nach Definition ist der p-Wert das kleinste Signifikanzlevel \\(\\alpha_{0}\\), bei welchem man die Nullhypothese basierend auf einem vorliegenden Wert der Teststatistik ablehnen würde. Bei \\(F=f\\) würde \\(H_{0}\\) für jedes \\(\\alpha_{0}\\) mit \\(f \\geq \\psi^{-1}\\left(1-\\alpha_{0} ; p-1, n-p\\right)\\) abgelehnt werden. Für ein solches \\(\\alpha_{0}\\) gilt aber \\[\\begin{equation}\n\\alpha_{0} \\geq \\mathbb{P}(F \\geq f),\n\\end{equation}\\] denn \\[\\begin{equation}\n\\begin{aligned}\nf\n&\n\\geq \\psi^{-1}\\left(1-\\alpha_{0} ; p-1, n-p\\right)  \\\\\n\\Leftrightarrow \\psi(f ; p-1, n-p)\n&\n\\geq \\psi\\left(\\psi^{-1}\\left(1-\\alpha_{0} ; p-1, n-p\\right),p-1, n-p\\right) \\\\\n\\Leftrightarrow \\psi(f ; p-1, n-p)\n&\n\\geq 1-\\alpha_{0} \\\\\n\\Leftrightarrow \\mathbb{P}(F \\leq f)\n& \\geq 1-\\alpha_{0} \\\\\n\\Leftrightarrow \\alpha_{0}\n& \\geq 1-\\mathbb{P}(F \\leq f) \\\\\n\\Leftrightarrow \\alpha_{0}\n& \\geq \\mathbb{P}(F \\geq f)\n\\end{aligned}\n\\end{equation}\\] Das kleinste \\(\\alpha_{0} \\in[0,1]\\) mit \\(\\alpha_{0} \\geq \\mathbb{P}(F \\geq f)\\) ist dann \\(\\alpha_{0}=\\mathbb{P}(F \\geq f)\\), also folgt \\[\\begin{equation}\n\\mbox{p-Wert} = \\mathbb{P}(F \\geq f)=1-\\varphi(f ; p-1, n-p)\n\\end{equation}\\]\n\nPraktisches Vorgehen\nAus dem in diesem Abschnitt Gesagten ergibt sich folgendes Vorgehen zur Evaluation eines EVA-Modells mithilfe eines F-Tests: Man nimmt zunächst einmal an, dass ein vorliegender Datensatz von \\(p\\) Gruppendatensätzen \\[\\begin{equation}\ny_{11},\\ldots, y_{1 n_{1}}, y_{21},\\ldots, y_{2 n_{2}},\\ldots, y_{p 1},\\ldots, y_{p n_{p}}\n\\end{equation}\\] Realisationen von \\[\\begin{equation}\n\\upsilon_{1j} \\sim N\\left(\\mu_{0}, \\sigma^{2}\\right)\n\\end{equation}\\] und \\[\\begin{equation}\n\\upsilon_{ij} \\sim N\\left(\\mu_{0}+\\alpha_{i}, \\sigma^{2}\\right)\n\\end{equation}\\] für \\(i=2,\\ldots,p\\) mit wahren, aber unbekannten, Parametern \\(\\mu_{0},\\alpha_{i}, i=2,\\ldots,p\\) und \\(\\sigma^{2}&gt;\\) 0 sind. Weiterhin nimmt man an, dass man entscheiden möchte, ob die Nullhypothese identischer wahrer, aber unbekannter, Erwartungswertparameter über Faktorlevel bzw. zu Null identischer wahrer, aber unbekannter, Effektparameter eher zutrifft oder eher nicht (vgl. Gleichung 32.2). Dazu wählt man zunächst ein Signifikanzniveau \\(\\alpha_{0}\\) und bestimmt den zugehörigen Freiheitsgradparameter-abhängigen kritischen Wert \\(k_{\\alpha_{0}}\\). Zum Beispiel gilt bei Wahl von \\(\\alpha_{0}:=0.05,p=3, m=12, i=1,2,3\\) und somit \\(n=36\\), dass \\(k_{\\alpha_{0}}=\\varphi^{-1}(1-0.05 ; 2,33) \\approx 3.28\\) ist. Man nutzt dann den vorliegenden Datensatz zur Berechung der MSB und MSW und bestimmt damit den realisierten Wert der F-Statistik \\(f\\). Wenn \\(f\\) größer gleich \\(k_{\\alpha_{0}}\\) ist, lehnt man die Nullhypothese ab, andernfalls nicht. Insgesamt garantiert die in diesem Abschnitt entwickelte Theorie dann, dass man im Mittel in höchstens \\(\\alpha_{0} \\cdot 100\\) von 100 Fällen die Nullhypothese fälschlicherweise ablehnt. Schließlich mag man den zu dem Wert \\(f\\) assoziierten p-Wert als \\(1-\\varphi(f ; p-1, n-p)\\) bestimmen und in der Dokumentation der Analyse vermerken.\n\n\n32.5.4 Anwendungsbeispiel\nFolgender R Code implementiert das obige praktische Vorgehen für den Beispieldatensatz.\n\n# Datenmanagement\nD           = read.csv(\"./_data/408-einfaktorielle-varianzanalyse.csv\")          # Datensatz\ny           = D$dBDI                                                             # Datenvektor\nn           = length(y)                                                          # Gesamtdatenumfang\np           = 3                                                                  # Anzahl Gruppen \nm           = n/p                                                                # Anzahl Datenpunkte pro Gruppe   \n \n# Modellformulierung\nXt          = cbind(                                                              # Designmatrix vollständiges Modell\n              matrix(1, nrow = n, ncol = 1),\n              kronecker(diag(p),\n                       matrix(1, nrow = m, ncol = 1)))\nX           = Xt[,-2]\nX_0         = X[,1]                                                              # Designmatrix reduziertes Modell\n\n# F-Teststatistikevaluation\nbeta_hat    = solve(t(X) %*% X) %*% t(X) %*% y                                   # Betaparameterschätzer vollständiges Modell\nbeta_hat_0  = solve(t(X_0) %*% X_0) %*% t(X_0) %*% y                             # Betaparameterschätzer reduziertes Modell\neps_hat     = y - X %*% beta_hat                                                 # Residuenvektor vollständiges Modell\neps_hat_0   = y - X_0 %*% beta_hat_0                                             # Residuenvektor reduziertes Modell\nSQT         = t(eps_hat_0) %*% eps_hat_0                                         # Sum of Squares Total\nSQW         = t(eps_hat)   %*% eps_hat                                           # Sum of Squares Within\nSQB         = SQT - SQW                                                          # Sum of Squares Between\nDFB         = p - 1                                                              # Between Degrees of Freedom\nDFW         = n - p                                                              # Within  Degrees of Freedom\nDFB         = p - 1                                                              # Between Degrees of Freedom\nMSB         = SQB/DFB                                                            # Mean Sum of Squares Between\nMSW         = SQW/DFW                                                            # Mean Sum of Squares Within\nEff         = MSB/MSW                                                            # F-Teststatistik\npW          = 1 - pf(Eff, p-1, n-p)                                               # p-Wert\n\n# F-Test Evaluation\nalpha_0     = 0.05                                                               # Signifikanzlevel\nk_alpha_0   = qf(1 - alpha_0, p-1,n-p)                                           # kritischer Wert\nif(Eff &gt; k_alpha_0){phi = 1} else {phi = 0}                                      # Testwert\n\n\n# Ausgabe\ncat( \"DFB :\" , round(DFB,2),\n    \"\\nDFW :\", round(DFW,2),\n    \"\\nSQB :\", round(SQB,2),\n    \"\\nSQW :\", round(SQW,2),\n    \"\\nMSB :\", round(MSB,2),\n    \"\\nMSW :\", round(MSW,2),\n    \"\\nF   :\", round(Eff,2),\n    \"\\np   :\", round(pW,2),\n    \"\\nphi :\", round(phi),2)\n\nDFB : 2 \nDFW : 33 \nSQB : 821.72 \nSQW : 170.58 \nMSB : 410.86 \nMSW : 5.17 \nF   : 79.48 \np   : 0 \nphi : 1 2\n\n\nIm vorliegenden Fall würde die Nullhypothese \\[\\begin{equation}\n\\Theta_{0} :=\n\\left\\{\n\\begin{pmatrix}\n\\mu_{0} \\\\\n\\alpha_{2} \\\\\n\\alpha_{3}\n\\end{pmatrix}\n\\in \\mathbb{R}^{3} \\mid\n\\alpha_{i}=0 \\mbox{ für } i=2,3\n\\right\\}\n\\end{equation}\\] verworfen werden.\nFolgender R Code demonstriert die Durchführung und Dokumentation der gleichen Analyse mithilfe der R Funktion aov().\n\nD       = read.csv(\"./_data/408-einfaktorielle-varianzanalyse.csv\")              # Daten \nres.aov = aov(D$dBDI ~ D$COND, data = D)                                         # Modellformulierung und Modellschätzung\nsummary(res.aov)                                                                 # Modellevaluation\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nD$COND       2  821.7   410.9   79.48 2.41e-13 ***\nResiduals   33  170.6     5.2                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Einfaktorielle Varianzanalyse</span>"
    ]
  },
  {
    "objectID": "408-Einfaktorielle-Varianzanalyse.html#literaturhinweise",
    "href": "408-Einfaktorielle-Varianzanalyse.html#literaturhinweise",
    "title": "32  Einfaktorielle Varianzanalyse",
    "section": "32.6 Literaturhinweise",
    "text": "32.6 Literaturhinweise\nDie Popularität varianzanalytischer Verfahren wird im Allgemeinen auf Fisher (1925) und Fisher (1935) zurückgeführt. Everitt & Howell (2005) und Stigler (1986) geben einen kurzen und einen ausführlichen historischen Überblick, respektive.",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Einfaktorielle Varianzanalyse</span>"
    ]
  },
  {
    "objectID": "408-Einfaktorielle-Varianzanalyse.html#selbstkontrollfragen",
    "href": "408-Einfaktorielle-Varianzanalyse.html#selbstkontrollfragen",
    "title": "32  Einfaktorielle Varianzanalyse",
    "section": "32.7 Selbstkontrollfragen",
    "text": "32.7 Selbstkontrollfragen\n\nErläutern Sie das Anwendungsszenario einer einfaktoriellen Varianzanalyse (EVA).\nGeben Sie die Definition des EVA-Modells in Erwartungswertparameterdarstellung wieder.\nGeben Sie die strukturelle Form des EVA-Modells in Effektdarstellung wieder.\nErläutern Sie die Motivation für die Reparametrisierung des EVA-Modells\nWelche Bedeutung haben \\(\\mu_{0}, \\alpha_{2},\\ldots, \\alpha_{p}\\) in der Effektparameterdarstellung des EVA-Modells?\nWarum gibt es bei \\(p\\) Gruppen eines EVA-Szenarios nur die \\(p-1\\) Effektparameter \\(\\alpha_{2},\\ldots, \\alpha_{p}\\) ?\nGeben Sie die Designmatrixform des EVA-Modells in Effektdarstellung wieder.\nFormulieren Sie die Designmatrix eines EVA-Modells mit \\(n_{i}=3\\) und \\(p=2\\).\nFormulieren Sie die Designmatrix eines EVA-Modells mit \\(n_{i}=2\\) und \\(p=5\\).\nGeben Sie das Theorem zur Betaparameterschätzung im EVA-Modell wieder.\nMit welchen Deskriptivstatistiken werden die Parameter \\(\\mu_{0}, \\alpha_{2},\\ldots, \\alpha_{p}\\) geschätzt?\nGeben Sie das Theorem zur Quadratsummenzerlegung bei einfaktorieller Varianzanalyse wieder.\nErläutern Sie die Begriffe der Total, Between, und Within-Sum-of-Squares der EVA.\nGeben Sie die Definition des Effektstärkenmaßes \\(\\eta^{2}\\) an.\nWann nimmt das Effektstärkenmaß \\(\\eta^{2}\\) der EVA seinen Minimalwert an und wie lautet dieser?\nWann nimmt das Effektstärkenmaß \\(\\eta^{2}\\) der EVA seinen Maximalwert an und wie lautet dieser?\nGeben Sie das Theorem zur F-Teststatistik der EVA wieder.\nErläutern Sie die Begriffe Mean-Between-Sum-of-Squares und Mean-Within-Sum-of-Squares der EVA.\nGeben Sie das Theorem zum Zusammenhang von \\(\\eta^{2}\\) und F-Teststatistik wieder.\nGeben Sie die Definition des EVA F-Test wieder.\nErläutern sie die Null- und Alternativhypothesen des EVA F-Tests.\nGeben Sie das Theorem zur Testumfangkontrolle der EVA wieder.\nSkizzieren Sie den Beweis zur Testumfangkontrolle der EVA.\nGeben Sie den p-Wert zum F-Test der EVA wieder. –&gt;\n\n\n\n\n\nEveritt, B., & Howell, D. C. (Hrsg.). (2005). Encyclopedia of Statistics in Behavioral Science. John Wiley & Sons.\n\n\nFisher, R. A. (1925). Applications of \"Student’s\" Distribution. Metron, 5, 90–104.\n\n\nFisher, R. A. (1935). The Design of Experiments (1. ed). Hafner Press.\n\n\nStigler, S. M. (1986). The History of Statistics: The Measurement of Uncertainty before 1900. Belknap Press of Harvard University Press.",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Einfaktorielle Varianzanalyse</span>"
    ]
  },
  {
    "objectID": "409-Zweifaktorielle-Varianzanalyse.html",
    "href": "409-Zweifaktorielle-Varianzanalyse.html",
    "title": "33  Zweifaktorielle Varianzanalyse",
    "section": "",
    "text": "33.1 Anwendungsszenario\nDas klassische Anwendungsszenario einer zweifaktoriellen Varianzanalyse (ZVA) ist ein randomisiertes zweifaktorielles vollständig gekreuztes Studiendesign bestehend aus einer univariaten abhängige Variable bestimmt an randomisierten experimentellen Einheiten und zwei unabhängigen Variablen, die in der Regel jeweils mindestens zweistufig sind. Im Kontext der Varianzanalyse werden die unabhängigen Variablen bekanntlich auch Faktoren und ihre Ausprägungsstufen Faktorlevel genannt. Im Falle eines vollständig gekreuzten Designs wird jedes Level eines Faktors mit allen Level des anderen Faktors kombiniert. Die Kombination zweier spezifischer Faktorlevel wird dann auch Zelle des Designs genannt. Da den Zellen eines ZVA Designs üblicherweise Gruppen von experimentellen Einheiten zugeordnet werden, werden sie auch oft Gruppen oder experimentelle Bedingungen genant.\nSpezielle zweifaktorielle Studiendesigns werden üblicherweise anhand ihrer Faktorlevel bezeichnet. Ein \\(I \\times J\\) Studiendesign und seine entsprechende \\(I \\times J\\) zweifaktorielle Varianzanalyse impliziert also, dass \\(I\\) Faktorlevel des ersten Faktors, im Folgenden als Faktor A bezeichnet, mit \\(J\\) Faktorleveln des zweiten Faktors, im Folgenden als Faktor \\(B\\) bezeichnet, gekreuzt werden. Folgende Tabelle gibt für einige der möglichen ZVA Designs die Bezeichnung und die Level der jeweiligen Faktoren an.\nGenerell sind \\(2 \\times 2\\) ZVA Designs sehr populär und ermöglichen es, die wesentlichen Charakteristika zweifaktorieller Varianzanalysedesigns zu diskutieren. Wir fokussieren im Folgenden daher weitgehend auf diesen Fall. In Abbildung 33.1 A visualisieren wir das konzeptuelle Design einer \\(2 \\times 2\\) ZVA, wobei wir die Zellen des Designs hier und im Folgenden mit A1B1, A1B2, A2B1 und A2B2 bezeichnen.\nAbbildung 33.1 B visualisiert, dass zu jeder Zelle des Designs eine Gruppen von Datenpunkten korrespondiert. Wir nutzen dabei folgende Indexkonvention: \\(y_{ijk}\\) bezeichnet den Datenwert der \\(k\\) ten experimentellen Einheit im \\(i\\) ten Level von Faktor A und \\(j\\) ten Level von Faktor B, wobei \\(k=1, \\ldots, n_{ij}\\) und im vorliegenden Fall \\(i=1,2\\) sowie \\(j=1,2\\). Ist die Anzahl an Datenwerten \\(n_{ij}\\) in jeder Zelle identisch, so spricht man wiederrum von einem balancierten Design.",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Zweifaktorielle Varianzanalyse</span>"
    ]
  },
  {
    "objectID": "409-Zweifaktorielle-Varianzanalyse.html#anwendungsszenario",
    "href": "409-Zweifaktorielle-Varianzanalyse.html#anwendungsszenario",
    "title": "33  Zweifaktorielle Varianzanalyse",
    "section": "",
    "text": "Haupteffekte und Interaktionen\nIntuitiv ist man in ZVA Designs an Haupteffekten und Interaktionen interessiert, die sich zunächst rein deskriptiv auf das Muster der zellspezifischen Gruppenmittelwerte beziehen.\n\n\n\n\n\n\nAbbildung 33.1: Konzeptuelles Design und Datennotation einer \\(2 \\times 2\\) ZVA.\n\n\n\nFür den Fall einer \\(2 \\times 2\\) ZVA spricht man dabei intuitiv vom Vorliegen eines Haupteffekts von Faktor A, wenn sich die Gruppenmittelwerte zwischen Level 1 und Level 2 von Faktor A, jeweils gemittelt über die zwei Level von Faktor B, unterscheiden. Weiterhin spricht man vom Vorliegen eines Haupteffekts von Faktor B, wenn sich die Gruppenmittelwerte zwischen Level 1 und Level 2 von Faktor B, jeweils gemittelt über die zwei Level von Faktor A, unterscheiden.\nVom Vorliegen einer Interaktion der Faktoren \\(A\\) und \\(B\\) schließlich spricht man, wenn der Unterschied der Gruppenmittelwerte von Faktor A zwischen Level 1 und 2 unterschiedlich für Level 1 und Level 2 von Faktor B ausgeprägt ist bzw. wenn der Unterschied der Gruppenmittelwerte von Faktor B zwischen Level 1 und 2 unterschiedlich für Level 1 und Level 2 von Faktor A ausgeprägt ist.\nHaupteffekte beziehen sich intuitiv also auf (marginale) Unterschiede (Differenzen), während sich Interaktionen auf Unterschiede von Unterschieden (Differenzen von Differenzen) beziehen. Dabei besagt das Vorliegen einer Interaktion also lediglich, dass sich die Unterschiede der Gruppenmittelwerte zwischen den Leveln eines experimentellen Faktors in Abhängigkeit von den Leveln des anderen experimentellen Faktors ändern, impliziert aber keine Aussage darüber, warum dies so ist. In anderen Worten sind Haupteffekte und Interaktionen bei Varianzanalysen Datenmuster, aber keine wissenschaftlichen Theorien. Die frequentitistisch-inferenzstatistische Absicherung dieser Datenmuster ist das Thema dieses Kapitels.\n\n\nAnwendungsbeispiel\nFür ein konkretes Anwendungsbeispiel aus dem Bereich der Interventionsevaluation betrachten wir eine BDI-Pre-Postinterventionsdifferenzwertanalyse für je zwei Settings (Face-to-Face und Online) und Varianten (Mindfulness und Exercise) der kognitiven Verhaltenstherapie (CBT, Abbildung 38.1). Setting sei Faktor (A) mit Leveln (1) Face-to-face (F2F) und (2) Online (ONL) und Variante sei Faktor (B) mit Leveln (1) Mindfulness (MND) und (2) Exercise (EXC). Jeder der durch Kreuzung der Faktorlevel entstehenden Interventionsbedingungen sei eine Gruppe (Stichprobe) von \\(n_{ij}:=12\\) Patient:innen zugeordnet und für jede Patient:in sei die Differenz zwischen Preinterventions und Postinterventions BDI Wert bestimmt worden und mit dBDI bezeichnet.\n\n\n\n\n\n\nAbbildung 33.2: Konzeptuelles Design des Anwendungsbeispiels.\n\n\n\n\n\n\n\nTabelle 33.1: Pre-Post-BDI-Differenzwerte der Faktorlevelkombinationen\n\n\n\n\n\n\nSetting\nVariant\ndBDI\n\n\n\n\nF2F\nMND\n12\n\n\nF2F\nMND\n11\n\n\nF2F\nMND\n8\n\n\nF2F\nMND\n8\n\n\nF2F\nMND\n12\n\n\nF2F\nMND\n12\n\n\nF2F\nMND\n9\n\n\nF2F\nMND\n10\n\n\nF2F\nMND\n12\n\n\nF2F\nMND\n13\n\n\nF2F\nMND\n10\n\n\nF2F\nMND\n9\n\n\nF2F\nEXC\n14\n\n\nF2F\nEXC\n11\n\n\nF2F\nEXC\n15\n\n\nF2F\nEXC\n16\n\n\nF2F\nEXC\n15\n\n\nF2F\nEXC\n19\n\n\nF2F\nEXC\n16\n\n\nF2F\nEXC\n14\n\n\nF2F\nEXC\n11\n\n\nF2F\nEXC\n15\n\n\nF2F\nEXC\n15\n\n\nF2F\nEXC\n17\n\n\nONL\nMND\n4\n\n\nONL\nMND\n10\n\n\nONL\nMND\n12\n\n\nONL\nMND\n13\n\n\nONL\nMND\n12\n\n\nONL\nMND\n12\n\n\nONL\nMND\n13\n\n\nONL\nMND\n10\n\n\nONL\nMND\n10\n\n\nONL\nMND\n13\n\n\nONL\nMND\n4\n\n\nONL\nMND\n8\n\n\nONL\nEXC\n16\n\n\nONL\nEXC\n19\n\n\nONL\nEXC\n14\n\n\nONL\nEXC\n17\n\n\nONL\nEXC\n17\n\n\nONL\nEXC\n16\n\n\nONL\nEXC\n13\n\n\nONL\nEXC\n16\n\n\nONL\nEXC\n20\n\n\nONL\nEXC\n13\n\n\nONL\nEXC\n16\n\n\nONL\nEXC\n13\n\n\n\n\n\n\n\n\nTabelle 33.1 zeigt einen Beispieldatensatz. Dabei repräsentiert jede Zeile eine Patient:in, die Spalte Setting bezeichnet das dieser Patient:in entsprechende Level des Therapiesettingfaktors und die Spalte Variant das dieser Patient:in entsprechende Level des Therapievariantenfaktors. Der entsprechende dBDI Zeilenwert bildet den Pre-Post-InterventionsBDI-Differenzwert der entsprechenden Patient:in ab. Hohe Werte zeigen dabei eine starke Verbesserung, geringe Werte eher eine leichte Verbesserung der Depressionssymptomatik an.\nAbbildung 33.3 schließlich zeigt zwei häufig genutzte Visualisationsformen für Deskriptivstatistiken (hier Gruppenmittelwerte und Standardabweichungen) in \\(2 \\times 2\\) ZVA Szenarien. Abbildung 33.3 A zeigt die faktorlevelkombinationsspezifischen Gruppenmittelwerte und ihre Standardabweichungen als Balkendiagramm. Abbildung 33.3 B zeigt die gleichen Daten als Liniendiagramm mit Fehlerbalken. Dabei entsprechen die zwei Linien den beiden Leveln des Settingfaktors F2F (dunkelgraue Linie) und ONL (hellgraue Linie). Die auf der x-Achse abgetragenen Werte dagegen entsprechen den beiden Leveln des Variantenfaktors MND und EXC. Für das Verständnis dieser Art der Visualisierung ist es sicherlich hilfreich, die Korrespondenz zwischen den in Abbildung 33.3 A und Abbildung 33.3 B einmal explizit durch nachzeichnen herzustellen.\n\n\n\n\n\n\nAbbildung 33.3: Visualisierung des Beispieldatensatzes als Balken- und Liniendiagramm.",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Zweifaktorielle Varianzanalyse</span>"
    ]
  },
  {
    "objectID": "409-Zweifaktorielle-Varianzanalyse.html#modellformulierung",
    "href": "409-Zweifaktorielle-Varianzanalyse.html#modellformulierung",
    "title": "33  Zweifaktorielle Varianzanalyse",
    "section": "33.2 Modellformulierung",
    "text": "33.2 Modellformulierung\nUm nun den Spezialfall des ALMs für das ZVA Szenario zu diskutieren, gehen wir in zwei Schritten vor. In einem ersten Schritt führen mit dem Modell der additiven ZVA ein Modell ein, das lediglich die Haupteffekte der Faktoren, nicht aber ihre Interaktion modellieren kann. Entsprechend können auf Grundlage dieses Modells Haupteffektparameter geschätzt und inferenzstatistisch evaluiert werden. In einem zweiten Schritt führen wir mit dem Modell der ZVA mit Interaktion ein Modell ein, das sowohl die Haupteffekte der Faktoren als auch ihre Interaktion modelliert und auf dessen Grundlage entsprechend sowohl Haupteffekts- als auch Interaktionsparameter geschätzt und inferenzstatistisch evaluiert werden können. In beiden Fällen ist das zentrale Thema der Modellformulierung wie bei der EVA von einer Intuition zur Modellierung der Erwartungswertparameter der Zellen des ZVA Szenarios zu einer nicht überparameterisierten Darstellung mithilfe von Effektparametern zu kommen.\n\n33.2.1 Modell der additiven ZVA\nIn Analogie zur EVA möchte man im Modell der additiven ZVA die Gruppenerwartungswerte \\(\\mu_{ij}\\) mit \\(i=1, \\ldots, I\\) für die Level von Faktor A und \\(j=1, \\ldots, J\\) für die Level von Faktor B als Summe eines gruppenunspezifischen Erwartungswertes und den Effekten der Level von Faktor A und der Level von Faktor B modellieren.\nWir bezeichnen dabei den gruppenunspezifischen Erwartungswertparameter mit \\(\\mu_{0}\\), den Effekt von Level \\(i\\) von Faktor A mit \\(\\alpha_{i}\\) und den Effekt von Level \\(j\\) von Faktor B mit \\(\\beta_{j}\\) ( \\(\\beta_{j}\\) bezeichnet hier also den \\(j\\) ten Eintrag des Betaparametervektors). Dann ergibt sich zum Beispiel für \\(I:=J:=2\\)\n\\[\\begin{equation}\n\\begin{array}{l|l}\n\\mu_{11}:=\\mu_{0}+\\alpha_{1}+\\beta_{1}  & \\mu_{12}:=\\mu_{0}+\\alpha_{1}+\\beta_{2} \\\\\\hline\n\\mu_{21}:=\\mu_{0}+\\alpha_{2}+\\beta_{1}  & \\mu_{22}:=\\mu_{0}+\\alpha_{2}+\\beta_{2}\n\\end{array}\n\\end{equation}\\]\nWie im Falle der EVA ist diese Darstellung der Gruppenerwartungswerte \\(\\mu_{ij}\\) allerdings überparameterisiert, speziell werden die vier Erwartungswertparameter \\(\\mu_{11}, \\mu_{12}, \\mu_{21}\\) und \\(\\mu_{22}\\) in obiger Formulierung durch die fünf Parameter \\(\\mu_{0}, \\alpha_{1}, \\alpha_{2}, \\beta_{1}\\) und \\(\\beta_{2}\\) dargestellt. Um eine eindeutige Darstellung der \\(\\mu_{ij}\\) zu gewährleisten, bietet sich auch hier die Restriktion an, den Effekt des ersten Levels jedes Faktors als Null zu definieren \\[\\begin{equation}\n\\alpha_{1}:=\\beta_{1}:=0\n\\end{equation}\\] und damit die Faktorlevelkombination A1B1 als Referenzgruppe zu etablieren. Es ergibt sich somit zum Beispiel für für \\(I:=J:=2\\) \\[\\begin{equation}\n\\begin{array}{l|l}\n\\mu_{11} := \\mu_{0}             & \\mu_{12} := \\mu_{0}+\\beta_{2}         \\\\\\hline\n\\mu_{21} :=\\mu_{0}+\\alpha_{2}   & \\mu_{22}:=\\mu_{0}+\\alpha_{2}+\\beta_{2}\n\\end{array}\n\\end{equation}\\]\nHier werden nun also die vier Erwartungswertparameter \\(\\mu_{11}, \\mu_{12}, \\mu_{21}\\) und \\(\\mu_{22}\\) durch nur drei Effektparameter \\(\\mu_{0}, \\alpha_{2}\\) und \\(\\beta_{2}\\) dargestellt (in diesem Sinne ist das Modell der additiven ZVA sogar unterparameterisiert, was im folgenden Abschnitt die Einführung eines Interaktionsparameters erlaubt). Allerdings ändern sich bei dieser Effektdarstellung des Modells der additiven \\(2 \\times 2\\) ZVA mit Referenzgruppe analog zur EVA die Interpretationen der Parameter \\(\\mu_{0}, \\alpha_{2}, \\beta_{2}\\) im Vergleich zum überparameterisierten Fall ohne Referenzgruppe: \\(\\mu_{0}\\) entspricht dem Erwartungswert der Faktorlevelkombination A1B1, \\(\\alpha_{2}\\) der Differenz beim Übergang von Level 1 zu Level 2 von Faktor A und \\(\\beta_{2}\\) der Differenz beim Übergang von Level 1 zu Level 2 von Faktor B. Wir fassen obige Überlegungen in folgender Definition zusammen.\n\nDefinition 33.1 (Modell der additiven ZVA mit Referenzgruppe) \\(\\upsilon_{ijk}\\) mit \\(i=1, \\ldots, I, j=\\) \\(1, \\ldots, J, k=1, \\ldots, n_{ij}\\) sei die Zufallsvariable, die den \\(k\\)ten Datenpunkt zum \\(i\\)ten Level von Faktor A und dem \\(j\\)ten Level von Faktor B in einem ZVA Anwendungsszenario modelliert. Dann hat das Modell der additiven ZVA mit Referenzgruppe die strukturelle Form \\[\\begin{equation}\n\\upsilon_{ijk} = \\mu_{ij}+\\varepsilon_{ijk} \\sim N\\left(0, \\sigma^{2}\\right)\n\\mbox{ u.i.v. für } i=1, \\ldots, I, j=1, \\ldots, J, k=1, \\ldots, n_{ij}\n\\end{equation}\\] und die Datenverteilungsform \\[\\begin{equation}\n\\upsilon_{ijk} \\sim N\\left(\\mu_{ij}, \\sigma^{2}\\right) \\mbox{ u.v. für } i=1, \\ldots, I, j=1, \\ldots, J, k=1, \\ldots, n_{ij}\n\\end{equation}\\] mit \\[\\begin{equation}\n\\mu_{ij}:=\\mu_{0}+\\alpha_{i}+\\beta_{j} \\mbox{ für } i=1, \\ldots, I, j=1, \\ldots, J \\mbox{ mit } \\alpha_{1}:=\\beta_{1}:=.\n\\end{equation}\\] und \\(\\sigma^{2}&gt;0\\).\n\nWir verzichten in Definition 33.1 auf die Angabe einer allgemeinen Designmatrixform, die wir im Folgenden lediglich für den \\(2 \\times 2\\) ZVA Spezialfall betrachten.\nDie Expressivität des Modells der additiven ZVA ist, wie schon oben betont, auf das Abbilden von Haupteffekten beschränkt. Wir verdeutlichen dies in folgenden Parameterbeispielen und der zugehörigen fig-zva-add-beispiele.\nBeispiel (1)\nEs seien \\(\\mu_{0}:=1, \\alpha_{2}:=1\\) und \\(\\beta_{2}:=0\\), der Effektparameter für den Haupteffekt von Faktor A sei also von Null verschieden, der Effektparameter für den Haupteffekt von Faktor B dagegen gleich Null. Dann ergibt sich für die Gruppenerwartungswerte\n\\[\\begin{equation}\n\\begin{array}{l|l}\n\\mu_{11}=\\mu_{0}+\\alpha_{1}+\\beta_{1}=1+0+0=1 & \\mu_{12}=\\mu_{0}+\\alpha_{1}+\\beta_{2}=1+0+0=1 \\\\\\hline\n\\mu_{21}=\\mu_{0}+\\alpha_{2}+\\beta_{1}=1+1+0=2 & \\mu_{22}=\\mu_{0}+\\alpha_{2}+\\beta_{2}=1+1+0=2\n\\end{array}\n\\end{equation}\\]\nfig-zva-add-beispiele A zeigt Visualisierungen dieses Erwartungswertmusters in der Balkendiagramm- und Liniendiagrammform von Abbildung 33.3.\nBeispiel (2)\nEs seien \\(\\mu_{0}:=1, \\alpha_{2}:=0\\) und \\(\\beta_{2}:=1\\), der Effektparameter für den Haupteffekt von Faktor A sei also gleich Null, der Effektparameter für den Haupteffekt von Faktor B dagegen von Null verschieden. Dann ergibt sich für die Gruppenerwartungswerte\n\\[\\begin{equation}\n\\begin{array}{l|l}\n\\mu_{11}=\\mu_{0}+\\alpha_{1}+\\beta_{1}=1+0+0=1 & \\mu_{12}=\\mu_{0}+\\alpha_{1}+\\beta_{2}=1+0+1=2 \\\\\\hline\n\\mu_{21}=\\mu_{0}+\\alpha_{2}+\\beta_{1}=1+0+0=1 & \\mu_{22}=\\mu_{0}+\\alpha_{2}+\\beta_{2}=1+0+1=2\n\\end{array}\n\\end{equation}\\]\nfig-zva-add-beispiele B zeigt Visualisierungen dieses Erwartungswertmusters in der Balkendiagramm- und Liniendiagrammform von Abbildung 33.3.\nBeispiel (3)\nEs seien \\(\\mu_{0}:=1, \\alpha_{2}:=1\\) und \\(\\beta_{2}:=1\\), die Effektparameter für die Haupteffekte beider Faktoren seien also von Null verschieden. Dann ergibt sich für die Gruppenerwartungswerte\n\\[\\begin{equation}\n\\begin{array}{l|l}\n\\mu_{11}=\\mu_{0}+\\alpha_{1}+\\beta_{1}=1+0+0=1 & \\mu_{12}=\\mu_{0}+\\alpha_{1}+\\beta_{2}=1+0+1=2 \\\\\\hline\n\\mu_{21}=\\mu_{0}+\\alpha_{2}+\\beta_{1}=1+1+0=2 & \\mu_{22}=\\mu_{0}+\\alpha_{2}+\\beta_{2}=1+1+1=3\n\\end{array}\n\\end{equation}\\]\nfig-zva-add-beispiele C zeigt Visualisierungen dieses Erwartungswertmusters in der Balkendiagramm- und Liniendiagrammform von Abbildung 33.3.\n\n\npng \n  2 \n\n\nIn Hinblick auf die Liniendiagrammform der Gruppenerwartungswerte der additiven ZVA fällt auf, dass die den Leveln von Faktor A entsprechenden Linien immer parallel sind.\n\n\n\n\n\n\nAbbildung 33.4: Gruppenerwartungswerte der Beispielparameterszenarien in Balken- und Liniendiagrammform im Modell der additiven ZVA.\n\n\n\nFür den Fall der \\(2 \\times 2\\) ZVA gibt Definition 33.2 abschließend das Modell der additiven ZVA mit Referenzgruppe inklusive seiner Designmatrixform an.\n\nDefinition 33.2 (Modell der additiven \\(2 \\times 2\\) ZVA mit Referenzgruppe) \\(\\upsilon_{ijk}\\) mit \\(i=\\) \\(1,2, j=1,2, k=1, \\ldots, n_{ij}\\) sei die Zufallsvariable, die den \\(k\\)ten Datenpunkt zum \\(i\\)ten Level von Faktor A und dem \\(j\\)ten Level von Faktor B in einem \\(2 \\times 2\\) ZVA Anwendungsszenario modelliert. Dann hat mit \\(\\sigma^{2}&gt;0\\) das Modell der additiven \\(2 \\times 2\\) ZVA mit Referenzgruppe die strukturelle Form \\[\\begin{equation}\n\\upsilon_{ijk} = \\mu_{ij}+\\varepsilon_{ijk}\n\\mbox{ mit }\n\\varepsilon_{ijk} \\sim N\\left(0, \\sigma^{2}\\right)\n\\mbox{ u.i.v. für } i=1,2, j=1,2, k=1, \\ldots, n_{ij},\n\\end{equation}\\] die Datenverteilungsform \\[\\begin{equation}\n\\upsilon_{ijk} \\sim N\\left(\\mu_{ij}, \\sigma^{2}\\right) \\mbox{ u.v. für } i=1,2, j=1,2, k=1, \\ldots, n_{ij}\n\\end{equation}\\] mit \\[\\begin{equation}\n\\mu_{ij}:=\\mu_{0}+\\alpha_{i}+\\beta_{j} \\mbox{ für }i=1,2, j=1,2 \\mbox{ mit } \\alpha_{1}:=\\beta_{1}:=0,\n\\end{equation}\\] sowie die Designmatrixform \\[\\begin{equation}\n\\upsilon \\sim  N\\left(X \\beta, \\sigma^{2} I_{n}\\right)\n\\end{equation}\\] wobei \\[\\begin{equation}\n\\upsilon :=\n\\begin{pmatrix}\n\\upsilon_{111} \\\\\n\\vdots \\\\\n\\upsilon_{11 n_{11}} \\\\\n\\upsilon_{121} \\\\\n\\vdots \\\\\n\\upsilon_{12 n_{12}} \\\\\n\\upsilon_{211} \\\\\n\\vdots \\\\\n\\upsilon_{21 n_{21}} \\\\\n\\upsilon_{221} \\\\\n\\vdots \\\\\n\\upsilon_{22 n_{22}}\n\\end{pmatrix},\nX =\n\\begin{pmatrix}\n1 & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots \\\\\n1 & 0 & 0 \\\\\n1 & 0 & 1 \\\\\n\\vdots & \\vdots & \\vdots \\\\\n1 & 0 & 1 \\\\\n1 & 1 & 0 \\\\\n\\vdots & \\vdots & \\vdots \\\\\n1 & 1 & 0 \\\\\n1 & 1 & 1 \\\\\n\\vdots & \\vdots & \\vdots \\\\\n1 & 1 & 1\n\\end{pmatrix} \\in \\mathbb{R}^{n \\times 3},\n\\beta:=\\begin{pmatrix}\n\\mu_{0} \\\\\n\\alpha_{2} \\\\\n\\beta_{2}\n\\end{pmatrix} \\in \\mathbb{R}^{3} \\mbox{ und }\\sigma^{2}&gt;0.\n\\end{equation}\\]\n\n\n\n33.2.2 Modell der ZVA mit Interaktion\nIn der ZVA mit Interaktion möchte man die Gruppenerwartungswerte \\(\\mu_{ij}\\) mit \\(i=1, \\ldots, I\\) für die Level von Faktor A und \\(j=1, \\ldots, J\\) für die Level von Faktor B als Summe eines gruppenunspezifischen Erwartungswertes, der Effekte der Level von Faktor A und Faktor B und der Interaktion der Level der Faktoren modellieren. Wir bezeichnen dazu den gruppenunspezifischen Erwartungswertparameter mit \\(\\mu_{0}\\), den Effekt von Level \\(i\\) von Faktor A mit \\(\\alpha_{i}\\), den Effekt von Level \\(j\\) von Faktor \\(\\mathrm{B}\\) mit \\(\\beta_{j}\\), und die Interaktion von Level \\(i\\) von Faktor A mit Level \\(j\\) von Faktor B mit \\(\\gamma_{ij}\\). Dann ergibt sich zum Beispiel für \\(I:=J:=2\\)\n\\[\\begin{equation}\n\\begin{array}{l|l}\n\\mu_{11} :=\\ mu_{0}+\\alpha_{1}+\\beta_{1}+\\gamma_{11} & \\mu_{12}:=\\mu_{0}+\\alpha_{1}+\\beta_{2}+\\gamma_{12} \\\\\\hline\n\\mu_{21} :=\\mu_{0}+\\alpha_{2}+\\beta_{1}+\\gamma_{21}  & \\mu_{22}:=\\mu_{0}+\\alpha_{2}+\\beta_{2}+\\gamma_{22}\n\\end{array}\n\\end{equation}\\]\nWie in der die additiven ZVA ist diese Darstellung der Gruppenerwartungswerte \\(\\mu_{ij}\\) multipel überparameterisiert, speziell werden die vier Erwartungswertparameter \\(\\mu_{11}, \\mu_{12}, \\mu_{21}\\) und \\(\\mu_{22}\\) in obiger Formulierung durch die neun Parameter \\(\\mu_{0}, \\alpha_{1}, \\alpha_{2}, \\beta_{1}, \\beta_{2}, \\gamma_{11}, \\gamma_{12}, \\gamma_{21}\\) und \\(\\gamma_{11}\\) dargestellt. Um eine eindeutige Darstellung der \\(\\mu_{ij}\\) zu gewährleisten, bietet sich auch hier die Restriktion an, den Effekt des ersten Levels jedes Faktors und jeder Interaktion als Null zu definieren \\[\\begin{equation}\n\\alpha_{1}:=\\beta_{1}:=\\gamma_{i 1}:=\\gamma_{1 j}:=0 \\mbox{ für  }i=1, \\ldots, I, j=1, . ., J\n\\end{equation}\\] und damit die Faktorlevelkombination A1B1 als Referenzgruppe zu etablieren. Es ergibt sich somit zum Beispiel für \\(I:=J:=2\\): \\[\\begin{equation}\n\\begin{array}{l|l}\n\\mu_{11} :=\\mu_{0}              & \\mu_{12}:=\\mu_{0}+\\beta_{2} \\\\\\hline\n\\mu_{21} :=\\mu_{0}+\\alpha_{2}   & \\mu_{22}:=\\mu_{0}+\\alpha_{2}+\\beta_{2}+\\gamma_{22}\n\\end{array}\n\\end{equation}\\] Hier werden nun also die vier Erwartungswertparameter \\(\\mu_{11}, \\mu_{12}, \\mu_{21}\\) und \\(\\mu_{22}\\) durch die vier Effektparameter \\(\\mu_{0}, \\alpha_{2}, \\beta_{2}\\) und \\(\\gamma_{22}\\) dargestellt. Natürlich ändern sich auch bei dieser Effektdarstellung des Modells der \\(2 \\times 2\\) ZVA mit Interaktion und Referenzgruppe die Interpretationen der Parameter \\(\\mu_{0}, \\alpha_{2}, \\beta_{2}, \\gamma_{22}\\) im Vergleich zum überparameterisierten Fall ohne Referenzgruppe: \\(\\mu_{0}\\) entspricht dem Erwartungswert der Faktorlevelkombination A1B1, \\(\\alpha_{2}\\) der Differenz beim Übergang von Level 1 zu Level 2 von Faktor A, \\(\\beta_{2}\\) der Differenz beim Übergang von Level 1 zu Level 2 von Faktor B und \\(\\gamma_{22}\\) der Differenz beim Übergang von Level 1 zu Level 2 von Faktor B im Unterschied zum Übergang von Level 1 zu Level 2 von Faktor A. Wir fassen obige Überlegungen in folgender Definition zusammen, wobei wir wiederum auf die Angabe einer allgemeinen Designmatrixform verzichten wollen.\n\nDefinition 33.3 (Modell der ZVA mit Interaktion und Referenzgruppe) \\(\\upsilon_{ijk}\\) mit \\(i=\\) \\(1, \\ldots, I, j=1, \\ldots, J, k=1, \\ldots, n_{ij}\\) sei die Zufallsvariable, die den \\(k\\)ten Datenpunkt zum \\(i\\)ten Level von Faktor A und dem \\(j\\)ten Level von Faktor B in einem ZVA Anwendungsszenario modelliert. Dann hat das Modell der ZVA mit Interaktion und Referenzgruppe die strukturelle Form \\[\\begin{equation}\n\\upsilon_{ijk} = \\mu_{ij}+\\varepsilon_{ijk} \\sim N\\left(0, \\sigma^{2}\\right)\n\\mbox{ u.i.v. für } i=1, \\ldots, I, j=1, \\ldots, J, k=1, \\ldots, n_{ij}\n\\end{equation}\\] und die Datenverteilungsform \\[\\begin{equation}\n\\upsilon_{ijk} \\sim N\\left(\\mu_{ij}, \\sigma^{2}\\right) \\mbox{ u.i.v. für } i=1, \\ldots, I, j=1, \\ldots, J, k=1, \\ldots, n_{ij}\n\\end{equation}\\] mit \\[\\begin{equation}\n\\mu_{ij}:=\\mu_{0}+\\alpha_{i}+\\beta_{j}+\\gamma_{ij}\n\\end{equation}\\] sowie \\[\\begin{equation}\n\\alpha_{1}:=\\beta_{1}:=\\gamma_{i 1}:=\\gamma_{1 j}:=0 \\mbox{ für }i=1, \\ldots, I, j=1, \\ldots, J\n\\end{equation}\\] und \\(\\sigma^{2}&gt;0\\).\n\nDie Expressivität des Modells der ZVA mit Interaktion entspricht nun dem Betrachten von Haupteffekten und Interaktion wie einleitend zum Anwendungsszenario der ZVA diskutiert. Wir wollen dies mithilfe folgender Parameterbeispiele und der zugehörigen Abbildung 33.5 verdeutlichen.\nBeispiel (1)\nEs seien \\(\\mu_{0}:=1, \\alpha_{2}:=0, \\beta_{2}:=0\\) und \\(\\gamma_{22}=2\\), die Effektparameter für die Haupteffekte beider Faktoren seien also gleich Null, aber der Effektparameter für die Interaktion der Faktoren positiv. Dann ergibt sich für die Gruppenerwartungswerte\n\\[\\begin{equation}\n\\begin{array}{l|l}\n\\mu_{11}=\\mu_{0}+\\alpha_{1}+\\beta_{1}+\\gamma_{11}=1+0+0+0=1 & \\mu_{12}=\\mu_{0}+\\alpha_{1}+\\beta_{2}+\\gamma_{12}=1+0+0+0=1 \\\\\\hline\n\\mu_{21}=\\mu_{0}+\\alpha_{2}+\\beta_{1}+\\gamma_{21}=1+0+0+0=1 & \\mu_{22}=\\mu_{0}+\\alpha_{2}+\\beta_{2}+\\gamma_{22}=1+0+0+2=3\n\\end{array}\n\\end{equation}\\] \nAbbildung 33.5 A zeigt Visualisierungen dieses Erwartungswertmusters in Balkendiagramm- und Liniendiagrammform.\nBeispiel (2)\nEs seien \\(\\mu_{0}:=1, \\alpha_{2}:=1, \\beta_{2}:=1\\) und \\(\\gamma_{22}=-2\\). Die Effektparameter für die Haupteffekte beider Faktoren seien also gleich und der Effektparameter für die Interaktion der Faktoren negativ. Dann ergibt sich für die Gruppenerwartungswerte\n\\[\\begin{equation}\n\\begin{array}{l|l}\n\\mu_{11}=\\mu_{0}+\\alpha_{1}+\\beta_{1}+\\gamma_{11}=1+0+0+0=1 & \\mu_{12}=\\mu_{0}+\\alpha_{1}+\\beta_{2}+\\gamma_{12}=1+0+1+0=2 \\\\\\hline\n\\mu_{21}=\\mu_{0}+\\alpha_{2}+\\beta_{1}+\\gamma_{21}=1+0+1+0=2 & \\mu_{22}=\\mu_{0}+\\alpha_{2}+\\beta_{2}+\\gamma_{22}=1+1+1-2=1\n\\end{array}\n\\end{equation}\\] \nAbbildung 33.5 B zeigt Visualisierungen dieses Erwartungswertmusters in Balkendiagramm- und Liniendiagrammform.\nBeispiel (3)\nEs seien \\(\\mu_{0}:=1, \\alpha_{2}:=1, \\beta_{2}:=0\\) und \\(\\gamma_{22}=1\\), der Effektparameter für Faktor A sei also positiv, der Effektparameter für Faktor B gleich Null und der Effektparameter für die Interaktion der Faktoren positiv. Dann ergibt sich für die Gruppenerwartungswerte\n\\[\\begin{equation}\n\\begin{array}{l|l}\n\\mu_{11}=\\mu_{0}+\\alpha_{1}+\\beta_{1}+\\gamma_{11}=1+0+0+0=1 & \\mu_{12}=\\mu_{0}+\\alpha_{1}+\\beta_{2}+\\gamma_{12}=1+0+0+0=1 \\\\\\hline\n\\mu_{21}=\\mu_{0}+\\alpha_{2}+\\beta_{1}+\\gamma_{21}=1+1+0+0=2 & \\mu_{22}=\\mu_{0}+\\alpha_{2}+\\beta_{2}+\\gamma_{22}=1+1+0+1=3\n\\end{array}\n\\end{equation}\\] \nAbbildung 33.5 C zeigt Visualisierungen dieses Erwartungswertmusters in Balkendiagramm- und Liniendiagrammform.\nBeispiel (4)\nEs seien \\(\\mu_{0}:=1, \\alpha_{2}:=0, \\beta_{2}:=1\\) und \\(\\gamma_{22}=1\\), der Effektparameter für Faktor A sei also gleich Null, der Effektparameter für Faktor B positiv und der Effektparameter für die Interaktion der Faktoren wiederrum positiv. Dann ergibt sich für die Gruppenerwartungswerte\n\\[\\begin{equation}\n\\begin{array}{l|l}\n\\mu_{11}=\\mu_{0}+\\alpha_{1}+\\beta_{1}+\\gamma_{11}=1+0+0+0=1 & \\mu_{12}=\\mu_{0}+\\alpha_{1}+\\beta_{2}+\\gamma_{12}=1+0+1+0=2 \\\\\\hline\n\\mu_{21}=\\mu_{0}+\\alpha_{2}+\\beta_{1}+\\gamma_{21}=1+0+0+0=1 & \\mu_{22}=\\mu_{0}+\\alpha_{2}+\\beta_{2}+\\gamma_{22}=1+0+1+1=3\n\\end{array}\n\\end{equation}\\] \nAbbildung 33.5 D zeigt Visualisierungen dieses Erwartungswertmusters in Balkendiagramm- und Liniendiagrammform.\nDie obigen Beispiele zeigen, dass die Expressivität der ZVA mit Interaktion im Sinne der durch das Modell abzubildenden Datenmuster ungleich höher als die Expressivität des rein additiven Modells der ZVA ist. Insbesondere bilden obige vier Beispiele bei Weitem nicht den gesamten kombinatorischen qualitativen Parameterraum der ZVA mit Interaktion ab. Hinsichtlich der Liniendiagrammvisualisation der Gruppenerwartungswertparameter bei ZVA mit Interaktion fällt auf, dass ein von Null verschiedener Interaktionsterm immer impliziert, dass die die Level von Faktor A repräsentierenen Linien immer nicht parallel verlaufen.\n\n\n\n\n\n\nAbbildung 33.5: Gruppenerwartungswerte der Beispielparameterszenarien in Balken- und Liniendiagrammform im Modell der ZVA mit Interaktion.\n\n\n\nAbschließend geben wir in Definition 33.4 den Spezialfall des Modells der \\(2 \\times 2\\) ZVA mit Interaktion und Referenzgruppe inklusiver seiner Designmatrixform an.\n\nDefinition 33.4 (Modell der \\(2 \\times 2\\) ZVA mit Interaktion und Referenzgruppe.) \\(\\upsilon_{ijk}\\) mit \\(i=\\) \\(1,2, j=1,2, k=1, \\ldots, n_{ij}\\) sei die Zufallsvariable, die den \\(k\\) ten Datenpunkt zum \\(i\\) ten Level von Faktor A und dem \\(j\\) ten Level von Faktor B in einem \\(2 \\times 2\\) ZVA Anwendungsszenario modelliert. Dann hat das Modell der \\(2 \\times 2\\) ZVA mit Interaktion und Referenzgruppe für \\(\\sigma^{2}&gt;0\\) die strukturelle Form\n\\[\\begin{equation}\n\\upsilon_{ijk}=\\mu_{ij}+\\varepsilon_{ijk} \\mbox{ mit }\n\\varepsilon_{ijk} \\sim N\\left(0, \\sigma^{2}\\right) \\mbox{ u.i.v. für } i=1,2, j=1,2, k=1, \\ldots, n_{ij},\n\\end{equation}\\] die Datenverteilungsform \\[\\begin{equation}\n\\upsilon_{ijk} \\sim N\\left(\\mu_{ij}, \\sigma^{2}\\right) \\mbox{ u.i.v. für } i=1,2, j=1,2, k=1, \\ldots, n_{ij}\n\\end{equation}\\] mit \\[\\begin{equation}\n\\mu_{ij}:=\\mu_{0}+\\alpha_{i}+\\beta_{j}+\\gamma_{ij}\n\\end{equation}\\] sowie \\[\\begin{equation}\n\\alpha_{1}:=\\beta_{1}:=\\gamma_{i 1}:=\\gamma_{1 j}:=0 \\mbox{ für }i=1, \\ldots, I, j=1, \\ldots, J\n\\end{equation}\\] und für \\(n:=\\sum_{i=1}^{2} \\sum_{i=1}^{2} n_{ij}\\) die Designmatrixform \\[\\begin{equation}\n\\upsilon \\sim  N\\left(X \\beta, \\sigma^{2} I_{n}\\right), \\mbox{ mit }\n\\end{equation}\\] \\[\\begin{equation}\n\\upsilon :=\n\\begin{pmatrix}\n\\upsilon_{111} \\\\\n\\vdots \\\\\n\\upsilon_{11 n_{11}} \\\\\n\\upsilon_{121} \\\\\n\\vdots \\\\\n\\upsilon_{12 n_{12}} \\\\\n\\upsilon_{211} \\\\\n\\vdots \\\\\n\\upsilon_{21 n_{21}} \\\\\n\\upsilon_{221} \\\\\n\\vdots \\\\\n\\upsilon_{22 n_{22}}\n\\end{pmatrix},\nX =\n\\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\n1 & 0 & 0 & 0 \\\\\n1 & 0 & 1 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\n1 & 0 & 1 & 0 \\\\\n1 & 1 & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\n1 & 1 & 0 & 0 \\\\\n1 & 1 & 1 & 1 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\n1 & 1 & 1 & 1\n\\end{pmatrix} \\in \\mathbb{R}^{n \\times 4},\n\\beta :=\n\\begin{pmatrix}\n\\mu_{0} \\\\\n\\alpha_{2} \\\\\n\\beta_{2} \\\\\n\\gamma_{22}\n\\end{pmatrix} \\in \\mathbb{R}^{4} \\mbox{ und }\\sigma^{2}&gt;0.\n\\end{equation}\\]\n\nBeispiel\nAls konkretes Beispiel für die Designmatrixform des in Definition 33.4 definierten Modells betrachten wir das Szenario \\(n_{ij}:=4\\), also \\(n=16\\). Dann gilt \\[\\begin{equation}\n\\upsilon = X\\beta+\\varepsilon \\mbox{ mit } \\varepsilon \\sim N\\left(0_{16}, \\sigma^{2} I_{16}\\right)\n\\end{equation}\\] mit \\[\\begin{equation}\n\\upsilon :=\n\\begin{pmatrix}\n\\upsilon_{111} \\\\\n\\upsilon_{112} \\\\\n\\upsilon_{113} \\\\\n\\upsilon_{114} \\\\\n\\upsilon_{121} \\\\\n\\upsilon_{122} \\\\\n\\upsilon_{123} \\\\\n\\upsilon_{124} \\\\\n\\upsilon_{211} \\\\\n\\upsilon_{212} \\\\\n\\upsilon_{213} \\\\\n\\upsilon_{214} \\\\\n\\upsilon_{221} \\\\\n\\upsilon_{222} \\\\\n\\upsilon_{223} \\\\\n\\upsilon_{224}\n\\end{pmatrix},\nX :=\n\\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 0 \\\\\n1 & 0 & 1 & 0 \\\\\n1 & 0 & 1 & 0 \\\\\n1 & 0 & 1 & 0 \\\\\n1 & 0 & 1 & 0 \\\\\n1 & 1 & 0 & 0 \\\\\n1 & 1 & 0 & 0 \\\\\n1 & 1 & 0 & 0 \\\\\n1 & 1 & 0 & 0 \\\\\n1 & 1 & 1 & 1 \\\\\n1 & 1 & 1 & 1 \\\\\n1 & 1 & 1 & 1 \\\\\n1 & 1 & 1 & 1\n\\end{pmatrix} \\in \\mathbb{R}^{16 \\times 4},\n\\beta :=\n\\begin{pmatrix}\n\\mu_{0} \\\\\n\\alpha_{2} \\\\\n\\beta_{2} \\\\\n\\gamma_{22}\n\\end{pmatrix} \\in \\mathbb{R}^{4} \\mbox{ und }\\sigma^{2}&gt;0.\n\\end{equation}\\]\nFolgender R Code erlaubt die Realisierung von Daten im Falle dieses Beispiels.\n\nlibrary(MASS)                                                                    # Multivariate Normalverteilung\nI      = 2                                                                       # Anzahl Level Faktor A\nJ      = 2                                                                       # Anzahl Level Faktor B\nn_ij   = 4                                                                       # Anzahl von Datenpunkten der i,jten Gruppe\nn      = I*J*n_ij                                                                # Anzahl Datenpunkte\np      = 1 + (I-1)+(J-1)+(I*J-3)                                                 # Anzahl Parameter\nD      = matrix(c(1,0,0,0,                                                       # Prototypische Designmatrix für balancierte Designs\n                  1,0,1,0,\n                  1,1,0,0,\n                  1,1,1,1),\n                nrow  = p,\n                byrow = TRUE)\nC      = matrix(rep(1,n_ij),nrow = n_ij)                                         # Prototypischer Zellenvektor für balancierte Designs\nX      = kronecker(D,C)                                                          # Kroneckerprodukt Designmatrix Erzeugung \nI_n    = diag(n)                                                                 # n x n Einheitsmatrix\nbeta   = matrix(c(1,1,1,1), nrow = p)                                            # \\beta = (\\mu_0,\\alpha_2,\\alpha_3,\\alpha_4)\nsigsqr = 10                                                                      # \\sigma^2\ny      = mvrnorm(1, X %*% beta, sigsqr*I_n)                                      # eine Realisierung eines n-dimensionalen ZVs",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Zweifaktorielle Varianzanalyse</span>"
    ]
  },
  {
    "objectID": "409-Zweifaktorielle-Varianzanalyse.html#modellschätzung",
    "href": "409-Zweifaktorielle-Varianzanalyse.html#modellschätzung",
    "title": "33  Zweifaktorielle Varianzanalyse",
    "section": "33.3 Modellschätzung",
    "text": "33.3 Modellschätzung\nWir betrachten im Folgenden die Betaparameterschätzer im additiven \\(2 \\times 2\\) ZVA Modell mit Referenzgruppe und im \\(2 \\times 2\\) ZVA mit Interaktion und Referenzgruppe. Dabei stellt sich wenig überraschend heraus, dass sich die Schätzer für die Komponenten des Betaparameters der in Definition 33.2 und Definition 33.4 formulierten Modelle als gewichtete Summen der gruppenspezifischen Stichprobenmittel ergeben. Die genaue Form dieser gewichteten Summen ist Inhalt der beiden folgenden Theoreme. Der Einfachheit halber gehen wir dabei jeweils von einem balancierten Design aus. Wir beginnen mit der Betaparameterschätzung im additiven \\(2 \\times 2\\) ZVA Modell mit Referenzgruppe.\n\nTheorem 33.1 (Betaparameterschätzung im additiven \\(2 \\times 2\\) ZVA Modell mit Referenzgruppe) Gegeben sei die Designmatrixform eines balancierten additiven \\(2 \\times 2\\) ZVA Modells mit Referenzgruppe. Dann ergibt sich für den Betaparameterschätzer \\[\\begin{equation}\n\\hat{\\beta} :=\n\\begin{pmatrix}\n\\hat{\\mu}_{0} \\\\\n\\hat{\\alpha}_{2} \\\\\n\\hat{\\beta}_{2}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\frac{3}{4} \\bar{\\upsilon}_{11}+\\frac{1}{4}\\left(\\bar{\\upsilon}_{12}+\\bar{\\upsilon}_{21}\\right)-\\frac{1}{4} \\bar{\\upsilon}_{22} \\\\\n\\frac{1}{2}\\left(\\bar{\\upsilon}_{21}+\\bar{\\upsilon}_{22}\\right)-\\frac{1}{2}\\left(\\bar{\\upsilon}_{11}+\\bar{\\upsilon}_{12}\\right) \\\\\n\\frac{1}{2}\\left(\\bar{\\upsilon}_{12}+\\bar{\\upsilon}_{22}\\right)-\\frac{1}{2}\\left(\\bar{\\upsilon}_{11}+\\bar{\\upsilon}_{21}\\right)\n\\end{pmatrix},\n\\end{equation}\\] wobei \\[\\begin{equation}\n\\bar{\\upsilon}_{i j}:=\\frac{1}{n_{ij}} \\sum_{k=1}^{n_{ij}} \\upsilon_{ijk} \\mbox{ für }1 \\leq i, j \\leq 2\n\\end{equation}\\] das Stichprobenmittel der \\(i,j\\)ten Gruppe des \\(2 \\times 2\\) ZVA Designs bezeichnet.\n\n\nBeweis. Wir bestimmen zunächst \\(X^T\\upsilon, X^{T} X\\) und \\(\\left(X^{T} X\\right)^{-1}\\) bei konstantem \\(n_{ij}\\) für \\(1 \\leq i, j \\leq 2\\). Es ergeben sich \\[\\begin{equation}\nX^T\\upsilon\n=\n\\begin{pmatrix}\n1 & \\cdots & 1 & 1 & \\cdots & 1 & 1 & \\cdots & 1 & 1 & \\cdots & 1 \\\\\n0 & \\cdots & 0 & 0 & \\cdots & 0 & 1 & \\cdots & 1 & 1 & \\cdots & 1 \\\\\n0 & \\cdots & 0 & 1 & \\cdots & 1 & 0 & \\cdots & 0 & 1 & \\cdots & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n\\upsilon_{111} \\\\\n\\vdots \\\\\n\\upsilon_{11 n_{11}} \\\\\n\\upsilon_{121} \\\\\n\\vdots \\\\\n\\upsilon_{12 n_{12}} \\\\\n\\upsilon_{211} \\\\\n\\vdots \\\\\n\\upsilon_{21 n_{21}} \\\\\n\\upsilon_{221} \\\\\n\\vdots \\\\\n\\upsilon_{22 n_{22}} \\\\\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\sum_{i=1}^{2} \\sum_{k=1}^{2} \\sum_{k=1}^{n_{ij}} \\upsilon_{ijk} \\\\\n\\sum_{j=1}^{2} \\sum_{k=1}^{n_{2j}} \\upsilon_{2jk}              \\\\\n\\sum_{i=1}^{2} \\sum_{k=1}^{n_{i2}} \\upsilon_{i2k}\n\\end{pmatrix},\n\\end{equation}\\] \\[\\begin{equation}\nX^{T}X\n=\n\\begin{pmatrix}\n1 & \\cdots & 1 & 1 & \\cdots & 1 & 1 & \\cdots & 1 & 1 & \\cdots & 1 \\\\\n0 & \\cdots & 0 & 0 & \\cdots & 0 & 1 & \\cdots & 1 & 1 & \\cdots & 1 \\\\\n0 & \\cdots & 0 & 1 & \\cdots & 1 & 0 & \\cdots & 0 & 1 & \\cdots & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots \\\\\n1 & 0 & 0 \\\\\n1 & 0 & 1 \\\\\n\\vdots & \\vdots & \\vdots \\\\\n1 & 0 & 1 \\\\\n1 & 1 & 0 \\\\\n\\vdots & \\vdots & \\vdots \\\\\n1 & 1 & 0 \\\\\n1 & 1 & 1 \\\\\n\\vdots & \\vdots & \\vdots \\\\\n1 & 1 & 1\n\\end{pmatrix}\n=\nn_{ij}\n\\begin{pmatrix}\n4 & 2 & 2 \\\\\n2 & 2 & 1 \\\\\n2 & 1 & 2\n\\end{pmatrix},\n\\end{equation}\\] sowie \\[\\begin{equation}\n\\left(X^{T} X\\right)^{-1}=n_{ij}\n\\begin{pmatrix}\n4 & 2 & 2 \\\\\n2 & 2 & 1 \\\\\n2 & 1 & 2\n\\end{pmatrix}^{-1}=\\frac{1}{n_{ij}}\n\\begin{pmatrix}\n\\frac{3}{4} & -\\frac{1}{2} & -\\frac{1}{2} \\\\\n-\\frac{1}{2} & 1 & 0 \\\\\n-\\frac{1}{2} & 0 & 1\n\\end{pmatrix}.\n\\end{equation}\\] Es ergibt sich also \\[\\begin{equation}\n\\hat{\\beta} =\n\\begin{pmatrix}\n\\hat{\\mu}_{0} \\\\\n\\hat{\\alpha}_{2} \\\\\n\\hat{\\beta}_{2}\n\\end{pmatrix}\n=\n\\frac{1}{n_{ij}}\n\\begin{pmatrix}\n\\frac{3}{4} & -\\frac{1}{2} & -\\frac{1}{2} \\\\\n-\\frac{1}{2} & 1 & 0 \\\\\n-\\frac{1}{2} & 0 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n\\sum_{i=1}^{2} \\sum_{j=1}^{2} \\sum_{k=1}^{n_{ij}} \\upsilon_{ijk} \\\\\n\\sum_{j=1}^{2} \\sum_{k=1}^{n_{2 j}} \\upsilon_{2 j k} \\\\\n\\sum_{i=1}^{2} \\sum_{k=1}^{n_{i 2}} \\upsilon_{i 2 k}\n\\end{pmatrix}.\n\\end{equation}\\] Damit ergibt sich dann \\[\\begin{equation}\n\\begin{aligned}\n\\hat{\\mu}_{0}\n& = \\frac{1}{n_{ij}}\\left(\\frac{3}{4} \\sum_{i=1}^{2} \\sum_{j=1}^{2} \\sum_{k=1}^{n_{ij}} \\upsilon_{ijk}-\\frac{1}{2} \\sum_{j=1}^{2} \\sum_{k=1}^{n_{2 j}} \\upsilon_{2 j k}-\\frac{1}{2} \\sum_{i=1}^{2} \\sum_{k=1}^{n_{i 2}} \\upsilon_{i 2 k}\\right) \\\\\n& = \\frac{1}{n_{ij}}\\left(\\frac{3}{4} \\sum_{k=1}^{n_{11}} \\upsilon_{11 k}+\\frac{3}{4} \\sum_{k=1}^{n_{12}} \\upsilon_{12 k}+\\frac{3}{4} \\sum_{k=1}^{n_{21}} \\upsilon_{21 k}+\\frac{3}{4} \\sum_{k=1}^{n_{22}} \\upsilon_{22 k}\\right) \\\\\n& + \\frac{1}{n_{ij}}\\left(-\\frac{1}{2} \\sum_{k=1}^{n_{21}} \\upsilon_{21 k}-\\frac{1}{2} \\sum_{k=1}^{n_{22}} \\upsilon_{22 k}-\\frac{1}{2} \\sum_{k=1}^{n_{12}} \\upsilon_{12 k}-\\frac{1}{2} \\sum_{k=1}^{n_{22}} \\upsilon_{22 k}\\right) \\\\\n& = \\frac{1}{n_{ij}}\\left(\\frac{3}{4} \\sum_{k=1}^{n_{11}} \\upsilon_{11 k}+\\frac{1}{4} \\sum_{k=1}^{n_{12}} \\upsilon_{12 k}+\\frac{1}{4} \\sum_{k=1}^{n_{21}} \\upsilon_{21 k}-\\frac{1}{4} \\sum_{k=1}^{n_{22}} \\upsilon_{22 k}\\right) \\\\\n& = \\frac{3}{4} \\bar{\\upsilon}_{11}+\\frac{1}{4}\\left(\\bar{\\upsilon}_{12}+\\bar{\\upsilon}_{21}\\right)-\\frac{1}{4} \\bar{\\upsilon}_{22}\n\\end{aligned}\n\\end{equation}\\] sowie \\[\\begin{equation}\n\\begin{aligned}\n\\hat{\\alpha}_{2}\n& =\\frac{1}{n_{ij}}\\left(-\\frac{1}{2} \\sum_{i=1}^{2} \\sum_{j=1}^{2} \\sum_{k=1}^{n_{ij}} \\upsilon_{ijk}+\\sum_{j=1}^{2} \\sum_{k=1}^{n_{2 j}} \\upsilon_{2 j k}\\right) \\\\\n& =\\frac{1}{n_{ij}}\\left(-\\frac{1}{2} \\sum_{k=1}^{n_{11}} \\upsilon_{11 k}-\\frac{1}{2} \\sum_{k=1}^{n_{12}} \\upsilon_{12 k}-\\frac{1}{2} \\sum_{k=1}^{n_{21}} \\upsilon_{21 k}-\\frac{1}{2} \\sum_{k=1}^{n_{22}} \\upsilon_{22 k}+\\sum_{k=1}^{n_{21}} \\upsilon_{21 k}+\\sum_{k=1}^{n_{22}} \\upsilon_{22 k}\\right) \\\\\n& =\\frac{1}{n_{ij}}\\left(-\\frac{1}{2} \\sum_{k=1}^{n_{11}} \\upsilon_{11 k}-\\frac{1}{2} \\sum_{k=1}^{n_{12}} \\upsilon_{12 k}+\\frac{1}{2} \\sum_{k=1}^{n_{21}} \\upsilon_{21 k}+\\frac{1}{2} \\sum_{k=1}^{n_{22}} \\upsilon_{22 k}\\right) \\\\\n& =\\frac{1}{2}\\left(\\bar{\\upsilon}_{21}+\\bar{\\upsilon}_{22}\\right)-\\frac{1}{2}\\left(\\bar{\\upsilon}_{11}+\\bar{\\upsilon}_{12}\\right)\n\\end{aligned}\n\\end{equation}\\] und analog für \\(\\hat{\\beta}_{2}\\).\n\nBeispiel\nWir verdeutlichen den in Theorem 33.1 formulierten Zusammenhang zwischen den Betaparameterschätzerkomponenten und den gruppenspezifischen Stichprobenmitteln mithilfe folgenden R Codes am Beispieldatensatz.\n\n# Datenreformatierung\nD          = read.csv(\"./_data/409-zweifaktorielle-varianzanalyse.csv\")         # Datensatz\nA1B1       = D$dBDI[D$Setting == \"F2F\" & D$Variant == \"MND\" ]                   # Face-to-face, mindfulness\nA1B2       = D$dBDI[D$Setting == \"F2F\" & D$Variant == \"EXC\" ]                   # Face-to-face, exercise\nA2B1       = D$dBDI[D$Setting == \"ONL\" & D$Variant == \"MND\" ]                   # Online      , mindfulness\nA2B2       = D$dBDI[D$Setting == \"ONL\" & D$Variant == \"EXC\" ]                   # Online      , exercise\n\n# Datenmatrix für Gruppenmittelwerte\nn_ij       = length(A1B1)                                                       # Anzahl Datenpunkte pro Gruppe\nY          = matrix(c(A1B1,A1B2,A2B1,A2B2), nrow = n_ij)                        # Datenmatrix\nbar_y      = colMeans(Y)                                                        # Zellenmittelwerte\n\n# Modellschätzung\nI          = 2                                                                  # Anzahl Level Faktor A (Therapiesetting)\nJ          = 2                                                                  # Anzahl Level Faktor B (Therapievariante)\nn          = I*J*n_ij                                                           # Anzahl Datenpunkte\np          = 1 + (I-1)+(J-1)+(I*J-3)                                            # Anzahl Parameter\nD          = matrix(c(1,0,0,                                                    # Prototypische Designmatrix für balancierte Designs\n                      1,0,1,\n                      1,1,0,\n                      1,1,1), nrow = p, byrow = TRUE)\nC          = matrix(rep(1,n_ij),nrow = n_ij)                                    # Prototypischer Zellenvektor für balancierte Designs\nX          = kronecker(D,C)                                                     # Kroneckerprodukt Designmatrix\ny          = matrix(c(A1B1,A1B2,A2B1,A2B2), nrow = n)                           # Datenvektor\nbeta_hat   = solve(t(X) %*% X) %*% t(X) %*% y                                   # Betaparameterschätzer\neps_hat    = y - X %*% beta_hat                                                 # Residuenvektor\nsigsqr_hat = (t(eps_hat) %*% eps_hat) /(n-p)                                    # Varianzparameterschätzer\n\n\n\nhat{beta}                                                :  10.15 0.29 5.04 \nhat{sigsqr}                                              :  6.07 \nbar{y}_11,bar{y}_12,bar{y}_21,bar{y}_22                  :  10.5 14.83 10.08 15.83 \n3/4bar{y}_11 + 1/4(bar{y}_12 + bar{y}_21) - 1/4bar{y}_22 :  10.15 \n1/2(bar{y}_21 + bar{y}_22) - 1/2(bar{y}_11 + bar{y}_12)  :  0.29 \n1/2(bar{y}_12 + bar{y}_22) - 1/2(bar{y}_11 + bar{y}_21)  :  5.04\n\n\nFür die Betaparameterschätzung im \\(2 \\times 2\\) ZVA Modell mit Interaktion und Referenzgruppe ergibt sich folgendes Theorem.\n\nTheorem 33.2 (Betaparameterschätzung im \\(2 \\times 2\\) ZVA Modell mit Interaktion und Referenzgruppe) Gegeben sei die Designmatrixform eines balancierten \\(2 \\times 2\\) ZVA Modells mit Interaktion und Referenzgruppe. Dann ergibt sich für den Betaparameterschätzer \\[\\begin{equation}\n\\hat{\\beta}:=\n\\begin{pmatrix}\n\\hat{\\mu}_{0} \\\\\n\\hat{\\alpha}_{2} \\\\\n\\hat{\\beta}_{2} \\\\\n\\hat{\\gamma}_{22}\n\\end{pmatrix}=\\begin{pmatrix}\n\\bar{\\upsilon}_{11}-\\bar{\\upsilon}_{11} \\\\\n\\bar{\\upsilon}_{12}-\\bar{\\upsilon}_{11} \\\\\n\\bar{\\upsilon}_{11}+\\bar{\\upsilon}_{22}-\\bar{\\upsilon}_{12}-\\bar{\\upsilon}_{21}\n\\end{pmatrix}\n\\end{equation}\\] wobei \\[\\begin{equation}\n\\bar{\\upsilon}_{i j}:=\\frac{1}{n_{ij}} \\sum_{k=1}^{n_{ij}} \\upsilon_{ijk} \\mbox{ für }1 \\leq i, j \\leq 2\n\\end{equation}\\] das Stichprobenmittel der i,jten Gruppe des \\(2 \\times 2\\) ZVA Designs bezeichnet.\n\n\nBeweis. Wir bestimmen zunächst \\(X^T\\upsilon, X^{T} X\\) und \\(\\left(X^{T} X\\right)^{-1}\\) bei konstantem \\(n_{ij}\\) für \\(1 \\leq i, j \\leq 2\\). Es ergeben sich \\[\\begin{equation}\nX^T\\upsilon\n=\n\\begin{pmatrix}\n1 & \\cdots & 1 & 1 & \\cdots & 1 & 1 & \\cdots & 1 & 1 & \\cdots & 1 \\\\\n0 & \\cdots & 0 & 0 & \\cdots & 0 & 1 & \\cdots & 1 & 1 & \\cdots & 1 \\\\\n0 & \\cdots & 0 & 1 & \\cdots & 1 & 0 & \\cdots & 0 & 1 & \\cdots & 1 \\\\\n0 & \\cdots & 0 & 0 & \\cdots & 0 & 0 & \\cdots & 0 & 1 & \\cdots & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n\\upsilon_{111} \\\\\n\\vdots \\\\\n\\upsilon_{11 n_{11}} \\\\\n\\upsilon_{121} \\\\\n\\vdots \\\\\n\\upsilon_{12 n_{12}} \\\\\n\\upsilon_{211} \\\\\n\\vdots \\\\\n\\upsilon_{21 n_{21}} \\\\\n\\upsilon_{221} \\\\\n\\vdots \\\\\n\\upsilon_{22 n_{22}}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\sum_{i=1}^{2} \\sum_{j=1}^{2} \\sum_{k=1}^{n_{ij}} \\upsilon_{ijk} \\\\\n\\sum_{j=1}^{2} \\sum_{k=1}^{n_{2 j}} \\upsilon_{2 j k} \\\\\n\\sum_{i=1}^{2} \\sum_{k=1}^{n} \\upsilon_{i 2 k} \\\\\n\\sum_{k=1}^{n_{22}} \\upsilon_{22 k}\n\\end{pmatrix},\n\\end{equation}\\]\n\\[\\begin{equation}\nX^{T}X =\n\\begin{pmatrix}\n1 & \\cdots & 1 & 1 & \\cdots & 1 & 1 & \\cdots & 1 & 1 & \\cdots & 1 \\\\\n0 & \\cdots & 0 & 0 & \\cdots & 0 & 1 & \\cdots & 1 & 1 & \\cdots & 1 \\\\\n0 & \\cdots & 0 & 1 & \\cdots & 1 & 0 & \\cdots & 0 & 1 & \\cdots & 1 \\\\\n0 & \\cdots & 0 & 0 & \\cdots & 0 & 0 & \\cdots & 0 & 1 & \\cdots & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\n1 & 0 & 0 & 0 \\\\\n1 & 0 & 1 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\n1 & 0 & 1 & 0 \\\\\n1 & 1 & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\n1 & 1 & 0 & 0 \\\\\n1 & 1 & 1 & 1 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\n1 & 1 & 1 & 1\n\\end{pmatrix}\n= n_{ij}\n\\begin{pmatrix}\n4 & 2 & 2 & 1 \\\\\n2 & 2 & 1 & 1 \\\\\n2 & 1 & 2 & 1 \\\\\n1 & 1 & 1 & 1\n\\end{pmatrix},\n\\end{equation}\\] sowie \\[\\begin{equation}\n\\left(X^{T} X\\right)^{-1}=n_{ij}\n\\begin{pmatrix}\n4 & 2 & 2 & 1 \\\\\n2 & 2 & 1 & 1 \\\\\n2 & 1 & 2 & 1 \\\\\n1 & 1 & 1 & 1\n\\end{pmatrix}^{-1}\n=\n\\frac{1}{n_{ij}}\n\\begin{pmatrix}\n1 & -1 & -1 & 1 \\\\\n-1 & 2 & 1 & -2 \\\\\n-1 & 1 & 2 & -2 \\\\\n1 & -2 & -2 & 4\n\\end{pmatrix}\n\\end{equation}\\] Es ergibt sich also \\[\\begin{equation}\n\\hat{\\beta} =\n\\begin{pmatrix}\n\\hat{\\mu}_{0} \\\\\n\\hat{\\alpha}_{2} \\\\\n\\hat{\\beta}_{2} \\\\\n\\hat{\\gamma}_{22}\n\\end{pmatrix}\n=\n\\frac{1}{n_{ij}}\n\\begin{pmatrix}\n1 & -1 & -1 & 1 \\\\\n-1 & 2 & 1 & -2 \\\\\n-1 & 1 & 2 & -2 \\\\\n1 & -2 & -2 & 4\n\\end{pmatrix}\n\\begin{pmatrix}\n\\sum_{i=1}^{2} \\sum_{j=1}^{2} \\sum_{k=1}^{n_{ij}} \\upsilon_{ijk} \\\\\n\\sum_{j=1}^{2} \\sum_{k=1}^{n_{2 j}} \\upsilon_{2 j k} \\\\\n\\sum_{i=1}^{2} \\sum_{k=1}^{n_{i 2}} \\upsilon_{i 2 k} \\\\\n\\sum_{k=1}^{n 22} \\upsilon_{22 k}\n\\end{pmatrix}\n\\end{equation}\\] Damit ergibt sich dann \\[\\begin{equation}\n\\begin{aligned}\n\\hat{\\mu}_{0}\n& = \\frac{1}{n_{ij}}\\left(\\sum_{i=1}^{2} \\sum_{j=1}^{2} \\sum_{k=1}^{n_{ij}} \\upsilon_{ijk}-\\sum_{j=1}^{2} \\sum_{k=1}^{n_{2 j}} \\upsilon_{2 j k}-\\sum_{i=1}^{2} \\sum_{k=1}^{n_{i 2}} \\upsilon_{i 2 k}+\\sum_{k=1}^{n_{22}} \\upsilon_{22 k}\\right) \\\\\n& = \\frac{1}{n_{ij}}\\left(\\sum_{k=1}^{n_{11}} \\upsilon_{11 k}+\\sum_{k=1}^{n_{12}} \\upsilon_{12 k}+\\sum_{k=1}^{n_{21}} \\upsilon_{21 k}+\\sum_{k=1}^{n_{22}} \\upsilon_{22 k}\\right) \\\\\n& + \\frac{1}{n_{ij}}\\left(-\\sum_{k=1}^{n_{21}} \\upsilon_{21 k}-\\sum_{k=1}^{n_{22}} \\upsilon_{22 k}-\\sum_{k=1}^{n_{12}} \\upsilon_{12 k}-\\sum_{k=1}^{n_{22}} \\upsilon_{22 k}+\\sum_{k=1}^{n_{22}} \\upsilon_{22 k}\\right) \\\\\n& = \\frac{1}{n_{11}} \\sum_{k=1}^{n_{11}} \\upsilon_{11 k} \\\\\n& = \\bar{\\upsilon}_{11}\n\\end{aligned}\n\\end{equation}\\] sowie \\[\\begin{equation}\n\\begin{aligned}\n\\hat{\\alpha}_{2}\n& = \\frac{1}{n_{ij}}\\left(-\\sum_{i=1}^{2} \\sum_{j=1}^{2} \\sum_{k=1}^{n_{ij}} \\upsilon_{ijk}+2 \\sum_{j=1}^{2} \\sum_{k=1}^{n_{2 j}} \\upsilon_{2 j k}+1 \\sum_{i=1}^{2} \\sum_{k=1}^{n_{i 2}} \\upsilon_{i 2 k}-2 \\sum_{k=1}^{n_{22}} \\upsilon_{22 k}\\right) \\\\\n& = \\frac{1}{n_{ij}}\\left(-\\sum_{k=1}^{n_{11}} \\upsilon_{11 k}-\\sum_{k=1}^{n_{12}} \\upsilon_{12 k}-\\sum_{k=1}^{n_{21}} \\upsilon_{21 k}-\\sum_{k=1}^{n_{22}} \\upsilon_{22 k}\\right) \\\\\n& + \\frac{1}{n_{ij}}\\left(2 \\sum_{k=1}^{n_{21}} \\upsilon_{21 k}+2 \\sum_{k=1}^{n_{22}} \\upsilon_{22 k}+\\sum_{k=1}^{n_{12}} \\upsilon_{12 k}+\\sum_{k=1}^{n_{22}} \\upsilon_{22 k}-2 \\sum_{k=1}^{n_{22}} \\upsilon_{22 k}\\right) \\\\\n& = \\frac{1}{n_{ij}}\\left(\\sum_{k=1}^{n_{21}} \\upsilon_{21 k}-\\sum_{k=1}^{n_{11}} \\upsilon_{11 k}\\right) \\\\\n& = \\bar{\\upsilon}_{21}-\\bar{\\upsilon}_{11}\n\\end{aligned}\n\\end{equation}\\] und analog für \\(\\hat{\\beta}_{2}\\). Schließlich ergibt sich \\[\\begin{equation}\n\\begin{aligned}\n\\hat{\\gamma}_{22}\n& = \\frac{1}{n_{ij}}\\left(\\sum_{i=1}^{2} \\sum_{j=1}^{2} \\sum_{k=1}^{n_{ij}} \\upsilon_{ijk}-2 \\sum_{j=1}^{2} \\sum_{k=1}^{n_{2 j}} \\upsilon_{2 j k}-2 \\sum_{i=1}^{2} \\sum_{k=1}^{n_{i 2}} \\upsilon_{i 2 k}+4 \\sum_{k=1}^{n_{22}} \\upsilon_{22 k}\\right) \\\\\n& = \\frac{1}{n_{ij}}\\left(\\sum_{k=1}^{n_{11}} \\upsilon_{11 k}+\\sum_{k=1}^{n_{12}} \\upsilon_{12 k}+\\sum_{k=1}^{n_{21}} \\upsilon_{21 k}+\\sum_{k=1}^{n_{22}} \\upsilon_{22 k}\\right) \\\\\n& + \\frac{1}{n_{ij}}\\left(-2 \\sum_{k=1}^{n_{21}} \\upsilon_{21 k}-2 \\sum_{k=1}^{n_{22}} \\upsilon_{22 k}-2 \\sum_{k=1}^{n_{12}} \\upsilon_{12 k}-2 \\sum_{k=1}^{n_{22}} \\upsilon_{22 k}+4 \\sum_{k=1}^{n_{22}} \\upsilon_{22 k}\\right) \\\\\n& = \\frac{1}{n_{ij}}\\left(\\sum_{k=1}^{n_{11}} \\upsilon_{11 k}+\\sum_{k=1}^{n_{22}} \\upsilon_{22 k}-\\sum_{k=1}^{n_{12}} \\upsilon_{12 k}-\\sum_{k=1}^{n_{21}} \\upsilon_{21 k}\\right) \\\\\n& = \\bar{\\upsilon}_{11}+\\bar{\\upsilon}_{22}-\\bar{\\upsilon}_{12}-\\bar{\\upsilon}_{21} .\n\\end{aligned}\n\\end{equation}\\]\n\nAuch hier vedeutlichen wir den in obigem Theorem formulierten Zusammenhang zwischen den Betaparameterschätzerkomponenten und den gruppenspezifischen Stichprobenmitteln anhand des Beispieldatensatzes mithilfe folgenden R Codes.\n\n# Datenreformatierung\nD          = read.csv(\"./_data/409-zweifaktorielle-varianzanalyse.csv\")         # Datensatz\nA1B1       = D$dBDI[D$Setting == \"F2F\" & D$Variant == \"MND\" ]                   # Face-to-face, mindfulness\nA1B2       = D$dBDI[D$Setting == \"F2F\" & D$Variant == \"EXC\" ]                   # Face-to-face, exercise\nA2B1       = D$dBDI[D$Setting == \"ONL\" & D$Variant == \"MND\" ]                   # Online      , mindfulness\nA2B2       = D$dBDI[D$Setting == \"ONL\" & D$Variant == \"EXC\" ]                   # Online      , exercise\n\n# Datenmatrix für Gruppenmittelwerte\nn_ij       = length(A1B1)                                                       # Anzahl Datenpunkte pro Gruppe\nY          = matrix(c(A1B1,A1B2,A2B1,A2B2), nrow = n_ij)                        # Datenmatrix\nbar_y      = colMeans(Y)                                                        # Zellenmittelwerte\n\n# Modellschätzung\nI          = 2                                                                  # Anzahl Level Faktor A (Therapiesetting)\nJ          = 2                                                                  # Anzahl Level Faktor B (Therapievariante)\nn          = I*J*n_ij                                                           # Anzahl Datenpunkte\np          = 1 + (I-1)+(J-1)+(I*J-3)                                            # Anzahl Parameter\nD          = matrix(c(1,0,0,0,                                                  # Prototypische Designmatrix  \n                      1,0,1,0,\n                      1,1,0,0,\n                      1,1,1,1), nrow = p, byrow = TRUE)\nC          = matrix(rep(1,n_ij),nrow = n_ij)                                    # Prototypischer Zellenvektor  \nX          = kronecker(D,C)                                                     # Kroneckerprodukt Designmatrix\ny          = matrix(c(A1B1,A1B2,A2B1,A2B2), nrow = n)                           # Datenvektor\nbeta_hat   = solve(t(X) %*% X) %*% t(X) %*% y                                   # Betaparameterschätzer\neps_hat    = y - X %*% beta_hat                                                 # Residuenvektor\nsigsqr_hat = (t(eps_hat) %*% eps_hat) /(n-p)                                    # Varianzparameterschätzer\n\n\n\nhat{beta}                                        :  10.5 -0.42 4.33 1.42 \nhat{sigsqr}                                      :  5.94 \nbar{y}_11,bar{y}_12,bar{y}_21,bar{y}_22          :  10.5 14.83 10.08 15.83 \nbar{y}_11                                        :  10.5 \nbar{y}_21 - bar{y}_11                            :  -0.42 \nbar{y}_12 - bar{y}_11                            :  4.33 \nbar{y}_11 + bar{y}_22 - bar{y}_12 + bar{y}_21    :  1.42",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Zweifaktorielle Varianzanalyse</span>"
    ]
  },
  {
    "objectID": "409-Zweifaktorielle-Varianzanalyse.html#modellevaluation",
    "href": "409-Zweifaktorielle-Varianzanalyse.html#modellevaluation",
    "title": "33  Zweifaktorielle Varianzanalyse",
    "section": "33.4 Modellevaluation",
    "text": "33.4 Modellevaluation\nDie inferenzstatistische Evaluation von Haupteffekt- und Interaktionsparametern in ZVA Modellen in allgemeiner Form kann aus mindestens drei Blickwinkeln betrachtet werden. Analog zur EVA könnnen auch für die ZVA Quadratsummenzerlegungen entwickelt werden und zur Definition entsprechender F-Statistiken eingesetzt werden. Wir folgen hier dem alternativen Ansatz, F-Statistiken für Haupteffekte und Interaktionen vor dem Hintergrund von Modellvergleichen in den entsprechenden ZVA Modellen zu formulieren. Einen integrativen Blick auf beide Ansätze bietet die Theorie der Allgemeinen Linearen Hypothese, auf deren Diskussion wir hier aber verzichten. Dementsprechend beschränken wir uns hier auf (1) die Evaluation der Haupteffekte im additiven Modell der \\(2 \\times 2\\) ZVA mit Referenzgruppe und (2) die Evaluation der Interaktion im Modell der \\(2 \\times 2\\) ZVA mit Interaktion und Referenzgruppe. Für beide Evaluationen formulieren wir (1) die zugrundeliegenden Modelle und Teststatistiken, (2) die Testhypothesen und Tests und (3) die Prozeduren zur Testumfangkontrolle und Auswertung des p-Wertes.\n\nEvaluation von Haupteffekten im additiven Modell der \\(2 \\times 2\\) ZVA\n\nTheorem 33.3 (Teststatistiken für Haupteffekte) Es sei \\[\\begin{equation}\n\\upsilon = X\\beta+\\varepsilon \\mbox{ mit } \\varepsilon \\sim N\\left(0_{n}, \\sigma^{2} I_{n}\\right)\n\\end{equation}\\] die Designmatrixform des additiven Modells der \\(2 \\times 2 Z V A\\) mit Referenzgruppe, wobei die \\(n \\times 1\\) Spalten von \\(X\\) bezeichnet seien durch \\[\\begin{equation}\nX:=\n\\begin{pmatrix}\nX_{\\mu_{0}} & X_{\\alpha_{2}} & X_{\\beta_{2}}\n\\end{pmatrix}\n\\in \\mathbb{R}^{n \\times 3}.\n\\end{equation}\\] Dann gelten\n\nEine F-Teststatistik für den Haupteffekt von Faktor A ist die F-Statistik des ALMs mit \\[\\begin{equation}\nX_{A} :=\n\\begin{pmatrix}\nX_{\\mu_{0}} & X_{\\beta_{2}} & X_{\\alpha_{2}}\n\\end{pmatrix} \\in \\mathbb{R}^{n \\times 3},\n\\beta_{A} :=\n\\begin{pmatrix}\n\\mu_{0} \\\\\n\\beta_{2} \\\\\n\\alpha_{2}\n\\end{pmatrix} \\in \\mathbb{R}^{3} \\mbox{ und }p_{0}:=2, p_{1}:=1\n\\end{equation}\\]\nEine F-Teststatistik für den Haupteffekt von Faktor B ist die F-Statistik des ALMs mit \\[\\begin{equation}\nX_{B}\n:=\n\\begin{pmatrix}\nX_{\\mu_{0}} & X_{\\alpha_{2}} & X_{\\beta_{2}}\n\\end{pmatrix} \\in \\mathbb{R}^{n \\times 3},\n\\beta_{B}\n:= \\begin{pmatrix}\n\\mu_{0} \\\\\n\\alpha_{2} \\\\\n\\beta_{2}\n\\end{pmatrix} \\in \\mathbb{R}^{3} \\mbox{ und }p_{0}:=2, p_{1}:=1\n\\end{equation}\\]\n\n\n\nDefinition 33.5 (Testhypothesen und Tests für Haupteffekte) Gegeben sei das Modell der additiven \\(2 \\times 2\\) ZVA und die F-Teststatistiken für die Haupteffekte von Faktor A und B seien mit \\(F_{A}\\) und \\(F_{B}\\) bezeichnet und wie oben definiert. Dann gelten folgende Definitionen:\n\nDer kritische Wert-basierte Test \\[\\begin{equation}\n\\phi_{A}(\\upsilon):=1_{\\left\\{F_{A} \\geq k\\right\\}} \\text { mit Nullhypothese } H_{0}^{A}: \\alpha_{2}=0\n\\end{equation}\\] definiert den \\(F\\)-Test des Haupteffekts von Faktor \\(A\\) im Modell mit \\(X_{A}, \\beta_{A}\\).\nDer kritische Wert-basierte Test \\[\\begin{equation}\n\\phi_{B}(\\upsilon):=1_{\\left\\{F_{B} \\geq k\\right\\}} \\text { mit Nullhypothese } H_{0}^{B}: \\beta_{2}=0\n\\end{equation}\\] definiert den \\(F\\)-Test des Haupteffekts von Faktor \\(B\\) im Modell mit \\(X_{B}, \\beta_{B}\\).\n\n\n\nTheorem 33.4 (Testumfangkontrolle und p-Werte für Haupteffekte) Mit obigen Definitionen und der KVF \\(\\varphi\\) der f-Verteilung gelten:\n\n\\(\\phi_{A}\\) ist ein Level- \\(\\alpha_{0}\\)-Test mit Testumfang \\(\\alpha_{0}\\), wenn der kritische Wert definiert ist durch \\[\\begin{equation}\nk_{\\alpha_{0}}^{A}:=\\varphi^{-1}\\left(1-\\alpha_{0} ; 1, n-3\\right) .\n\\end{equation}\\] Der zu einem beobachteten Wert \\(f_{A}\\) von \\(F_{A}\\) assoziierte \\(p\\)-Wert ist gegeben durch \\[\\begin{equation}\np_{A}-\\text { Wert }:=1-\\varphi\\left(f_{A} ; 1, n-3\\right).\n\\end{equation}\\]\n\\(\\phi_{B}\\) ist ein Level- \\(\\alpha_{0}\\)-Test mit Testumfang \\(\\alpha_{0}\\), wenn der kritische Wert definiert ist durch \\[\\begin{equation}\nk_{\\alpha_{0}}^{B}:=\\varphi^{-1}\\left(1-\\alpha_{0} ; 1, n-3\\right) .\n\\end{equation}\\] Der zu einem beobachteten Wert \\(f_{B}\\) von \\(F_{B}\\) assoziierte \\(p\\)-Wert ist gegeben durch \\[\\begin{equation}\np_{B}-\\text { Wert }:=1-\\varphi\\left(f_{B} ; 1, n-3\\right) .\n\\end{equation}\\]\n\n\nAnwendungsbeispiel\nDie Anwendung des oben formulierten Evaluationsverfahrens für die Haupteffekte im Modell der additiven \\(2 \\times 2\\) ZVA mit Referenzgruppe anhand des Beispieldatensatzes demonstriert folgender R Code.\n\nD          = read.csv(\"./_data/409-zweifaktorielle-varianzanalyse.csv\")         # Datensatz\nA1B1       = D$dBDI[D$Setting == \"F2F\" & D$Variant == \"MND\" ]                   # Face-to-face, mindfulness\nA1B2       = D$dBDI[D$Setting == \"F2F\" & D$Variant == \"EXC\" ]                   # Face-to-face, exercise\nA2B1       = D$dBDI[D$Setting == \"ONL\" & D$Variant == \"MND\" ]                   # Online      , mindfulness\nA2B2       = D$dBDI[D$Setting == \"ONL\" & D$Variant == \"EXC\" ]                   # Online      , exercise\nI          = 2                                                                  # Anzahl Level Faktor Setting\nJ          = 2                                                                  # Anzahl Level Faktor Variante \nn_ij       = length(A1B1)                                                       # balanciertes ANOVA Design\nn          = I*J*n_ij                                                           # Anzahl Datenpunkte\np          = 3                                                                  # Anzahl Parameter vollständiges Modell\ny          = matrix(c(A1B1,A1B2,A2B1,A2B2), nrow = n)                           # Datenvektor\nD          = matrix(c(1,0,0,1,0,1,1,1,0,1,1,1),                                 # Prototypische Designmatrix\n                     nrow = I*J,byrow=TRUE)\nC          = matrix(rep(1,n_ij),nrow = n_ij)                                    # Prototypischer Zellenvektor \nX          = kronecker(D,C)                                                     # ZVA Kroneckerprodukt Designmatrix\nXH         = list(X[,c(1,3,2)], X)                                              # Modellvarianten\nalpha_0    = 0.05                                                               # Signifikanzlevel\nEff        = rep(NaN,2)                                                         # F-Teststatistik Arrayinitialisierung\nk_alpha_0  = rep(NaN,2)                                                         # Kritischer Wert Arrayinitialisierung\nphi        = rep(NaN,2)                                                         # Testwert Arrayinitialisierung\np_vals     = rep(NaN,2)                                                         # p-Wert Arrayinitialisierung\nfor(i in 1:2){                                                                  # Iteration über Modellvarianten\n    X            = XH[[i]]                                                      # Designmatrix vollständiges Modell\n    X_0          = X[,-3]                                                       # Designmatrix reduziertes Modell\n    p            = ncol(X)                                                      # Anzahl Parameter vollständiges Modell\n    p_0          = ncol(X_0)                                                    # Anzahl Parameter reduziertes Modell\n    p_1          = p - p_0                                                      # Anzahl zusätzlicher Parameter im vollst. Modell\n    beta_hat_0   = solve(t(X_0)%*%X_0)%*%t(X_0)%*%y                             # Betaparameterschätzer reduziertes Modell\n    beta_hat     = solve(t(X) %*%X )%*%t(X) %*%y                                # Betaparameterschätzer vollständiges Modell\n    eps_hat_0    = y-X_0%*%beta_hat_0                                           # Residuenvektor reduziertes Modell\n    eps_hat      = y - X%*%beta_hat                                             # Residuenvektor vollständiges Modell\n    eh0_eh0      = t(eps_hat_0) %*% eps_hat_0                                   # RQS reduziertes Modell\n    eh_eh        = t(eps_hat) %*% eps_hat                                       # RQS vollständiges Modell\n    sigsqr_hat   = eh_eh/(n-p)                                                  # Varianzparameterschätzer vollst. Modell\n    Eff[i]       = ((eh0_eh0-eh_eh)/p_1)/sigsqr_hat                             # F-Statistik\n    k_alpha_0[i] = qf(1-alpha_0, p_1, n-p)                                      # kritischer Wert\n    if(Eff[i] &gt;= k_alpha_0[i]){ phi[i] = 1 }                                    # H_0 Ablehnen\n    else {                      phi[i] = 0 }                                    # H_A Annehmen\n    p_vals[i]    = 1 - pf(Eff[i], p_1,n-p)                                      # p-Wert\n}\n\n\n\n             f     k phi p.Wert\nSetting  0.172 4.057   0   0.68\nVariant 51.356 4.057   1   0.00\n\n\nFür den Haupteffekt des Faktors Settings ergibt sich dabei ein Wert der \\(F\\)-Statistik von \\(F_{A}=0.17\\) bei einem kritischen Wert von \\(k_{0.05}=4.06\\). Die Nullhypothese eines wahren, aber unbekannten, Effektparameterwertes von Null für den Faktor Setting würde im Lichte der in Abbildung 33.3 visualisierten Daten also nicht verworfen werden. Für den Haupteffekt des Faktors Variant dagegen ergibt ein Wert der \\(F\\)-Statistik von \\(F_{B}=51.36\\) bei dem gleichen kritischen Wert von \\(k_{0.05}=4.06\\). Die Nullhypothese eines wahren, aber unbekannten, Effektparameterwertes von Null für den Faktor Variant würde im Lichte der in Abbildung 33.3 visualisierten Daten mit einem Signifikanzlevel von \\(\\alpha_{0}:=0.05\\) also verworfen werden.\nFolgender R Code demonstriert die gleiche Analyse mithilfe der R Funktion aov().\n\nD          = read.csv(\"./_data/409-zweifaktorielle-varianzanalyse.csv\")         # Datensatz\nres.aov    = aov(dBDI ~ Setting + Variant, data = D)                            # Modellformulierung und Modellschätzung\nsummary(res.aov)                                                                # Modellevaluation\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nSetting      1   1.02    1.02   0.172     0.68    \nVariant      1 305.02  305.02  51.356 5.78e-09 ***\nResiduals   45 267.27    5.94                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nnormalsize\nFolgender R Code schließlich demonstriert die gleiche Analyse mithilfe der R Funktionen lm() und anova().\n\nD          = read.csv(\"./_data/409-zweifaktorielle-varianzanalyse.csv\")         # Datensatz\nglm        = lm(dBDI ~ Setting + Variant, data = D)                             # Modellformulierung und Modellschätzung\nanova(glm)                                                                      # Modellevaluation\n\nAnalysis of Variance Table\n\nResponse: dBDI\n          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nSetting    1   1.021   1.021  0.1719    0.6804    \nVariant    1 305.021 305.021 51.3559 5.779e-09 ***\nResiduals 45 267.271   5.939                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nEvaluation der Interaktion im Modell der \\(2 \\times 2\\) ZVA mit Interaktion\n\nTheorem 33.5 (Teststatistik für die Interaktion) Es sei \\[\\begin{equation}\n\\upsilon = X\\beta+\\varepsilon \\mbox{ mit } \\varepsilon \\sim N\\left(0_{n}, \\sigma^{2} I_{n}\\right)\n\\end{equation}\\] die Designmatrixform des \\(2 \\times 2\\) ZVA Modells mit Interaktion und Referenzgruppe. Dann ist die \\(F\\)-Statistik mit \\(p_{0}:=3\\) und \\(p_{1}:=1\\) eine \\(F\\)-Teststatistik für die Interaktion von Faktor \\(A\\) und Faktor \\(B\\).\n\n\nDefinition 33.6 (Testhypothese und Test für die Interaktion) Die F-Teststatistik für die Interaktion von Faktor A und Faktor B sei mit \\(F_{A \\times B}\\) bezeichnet und wie oben definiert. Dann definiert der kritische Wert-basierte Test \\[\\begin{equation}\n\\phi_{A \\times B}(\\upsilon):=1_{\\left\\{F_{A \\times B} \\geq k\\right\\}} \\mbox{ mit Nullhypothese } H_{0}^{A \\times B}: \\gamma_{22}=0\n\\end{equation}\\] den \\(F\\)-Test der Interaktion von Faktor \\(A\\) und Faktor B.\n\n\nTheorem 33.6 (Testumfangkontrolle und p-Wert für die Interaktion) Mit obigen Definition und der KVF \\(\\varphi\\) der \\(f\\)-Verteilung gilt, dass \\(\\phi_{A \\times B}\\) ein Level- \\(\\alpha_{0}\\)-Test mit Testumfang \\(\\alpha_{0}\\) ist, wenn der kritische Wert definiert ist durch \\[\\begin{equation}\nk_{\\alpha_{0}}^{A \\times B}:=\\varphi^{-1}\\left(1-\\alpha_{0} ; 1, n-4\\right).\n\\end{equation}\\] Der zu einem beobachteten Wert \\(f_{A \\times B}\\) von \\(F_{A \\times B}\\) assoziierte \\(p\\)-Wert ist gegeben durch \\[\\begin{equation}\np_{A \\times B}-\\text { Wert }:=1-\\varphi\\left(f_{A \\times B} ; 1, n-4\\right) \\text {. }\n\\end{equation}\\]\n\nAnwendungsbeispiel\nDie Anwendung des oben formulierten Evaluationsverfahrens für die Interaktion der Faktoren im Modell der \\(2 \\times 2\\) ZVA mit Interaktion und Referenzgruppe anhand des Beispieldatensatzes demonstriert folgender R Code.\n\nD          = read.csv(\"./_data/409-zweifaktorielle-varianzanalyse.csv\")         # Datensatz\nA1B1       = D$dBDI[D$Setting == \"F2F\" & D$Variant == \"MND\" ]                   # Face-to-face, mindfulness\nA1B2       = D$dBDI[D$Setting == \"F2F\" & D$Variant == \"EXC\" ]                   # Face-to-face, exercise\nA2B1       = D$dBDI[D$Setting == \"ONL\" & D$Variant == \"MND\" ]                   # Online      , mindfulness\nA2B2       = D$dBDI[D$Setting == \"ONL\" & D$Variant == \"EXC\" ]                   # Online      , exercise\nI          = 2                                                                  # Anzahl Level Faktor Setting\nJ          = 2                                                                  # Anzahl Level Faktor Variante \nn_ij       = length(A1B1)                                                       # balanciertes ANOVA Design\nn          = I*J*n_ij                                                           # Anzahl Datenpunkte\np          = 4                                                                  # Anzahl Parameter vollständiges Modell\ny          = matrix(c(A1B1,A1B2,A2B1,A2B2), nrow = n)                           # Datenvektor\nD          = matrix(c(1,0,0,0,1,0,1,0, 1,1,0,0,1,1,1,1),nrow = I*J,byrow=TRUE)  # Prototypische Designmatrix\nC          = matrix(rep(1,n_ij),nrow = n_ij)                                    # Prototypischer Zellenvektor   \nX          = kronecker(D,C)                                                     # ZVA Kroneckerprodukt Designmatrix\nXH         = list(X[,c(1,3,2)], X)                                              # Modellvarianten\nalpha_0    = 0.05                                                               # Signifikanzlevel\nX          = XH[[i]]                                                            # Designmatrix vollständiges Modell\nX_0        = X[,-4]                                                             # Designmatrix reduziertes Modell\np          = ncol(X)                                                            # Anzahl Parameter vollständiges Modell\np_0        = ncol(X_0)                                                          # Anzahl Parameter reduziertes Modell\np_1        = p - p_0                                                            # Anzahl zusätzlicher Parameter im vollst. Modell\nbeta_hat_0 = solve(t(X_0)%*%X_0)%*%t(X_0)%*%y                                   # Betaparameterschätzer reduziertes Modell\nbeta_hat   = solve(t(X) %*%X )%*%t(X) %*%y                                      # Betaparameterschätzer vollständiges Modell\neps_hat_0  = y-X_0%*%beta_hat_0                                                 # Residuenvektor reduziertes Modell\neps_hat    = y - X%*%beta_hat                                                   # Residuenvektor vollständiges Modell\neh0_eh0    = t(eps_hat_0) %*% eps_hat_0                                         # RQS reduziertes Modell\neh_eh      = t(eps_hat) %*% eps_hat                                             # RQS vollständiges Modell\nsigsqr_hat = eh_eh/(n-p)                                                        # Varianzparameterschätzer vollst. Modell\nf          = ((eh0_eh0-eh_eh)/p_1)/sigsqr_hat                                   # F-Statistik\nk_alpha_0  = qf(1-alpha_0, p_1, n-p)                                            # kritischer Wert\nif(f &gt;= k_alpha_0){phi = 1} else {phi = 0}                                      # Test\np_val      = 1 - pf(f, p_1,n-p)                                                 # p-Wert\n\n\n\n                      f     k phi p.Wert\nSetting x Variant 1.014 4.062   0  0.319\n\n\nEs ergibt sich dabei eine Wert der \\(F\\)-Statistik von \\(F_{A \\times B}=1.01\\) bei einem kritischen Wert von \\(k_{0.05}=4.06\\). Die Nullhypothese eines wahren, aber unbekannten, Effektparameterwertes von Null für den Interaktionseffekt würde im Lichte der in Abbildung 33.3 visualisierten Daten also nicht verworfen werden.\nFolgender R Code demonstriert die gleiche Analyse mithilfe der R Funktion aov().\n\nD          = read.csv(\"./_data/409-zweifaktorielle-varianzanalyse.csv\")         # Datensatz\nres.aov    = aov(dBDI ~ Setting + Variant + Setting:Variant, data = D)          #  Modellformulierung und Modellschätzung\nsummary(res.aov)                                                                # Modellevaluation\n\n                Df Sum Sq Mean Sq F value  Pr(&gt;F)    \nSetting          1   1.02    1.02   0.172   0.680    \nVariant          1 305.02  305.02  51.372 6.5e-09 ***\nSetting:Variant  1   6.02    6.02   1.014   0.319    \nResiduals       44 261.25    5.94                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nFolgender R Code schließlich demonstriert die gleiche Analyse mithilfe der R Funktionen lm und anova().\n\nD          = read.csv(\"./_data/409-zweifaktorielle-varianzanalyse.csv\")         # Datensatz\nglm        = lm(dBDI ~ Setting + Variant + Setting:Variant, data = D)           # Modellformulierung und Modellschätzung\nanova(glm)                                                                      # Modellevaluation\n\nAnalysis of Variance Table\n\nResponse: dBDI\n                Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nSetting          1   1.021   1.021  0.1719    0.6804    \nVariant          1 305.021 305.021 51.3719 6.502e-09 ***\nSetting:Variant  1   6.021   6.021  1.0140    0.3194    \nResiduals       44 261.250   5.938                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Zweifaktorielle Varianzanalyse</span>"
    ]
  },
  {
    "objectID": "409-Zweifaktorielle-Varianzanalyse.html#literaturhinweise",
    "href": "409-Zweifaktorielle-Varianzanalyse.html#literaturhinweise",
    "title": "33  Zweifaktorielle Varianzanalyse",
    "section": "33.5 Literaturhinweise",
    "text": "33.5 Literaturhinweise\nDie Popularität varianzanalytischer Verfahren wird im Allgemeinen auf Fisher (1925) und Fisher (1935) zurückgeführt. Everitt & Howell (2005) und Stigler (1986) geben einen kurzen und einen ausführlichen historischen Überblick, respektive.",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Zweifaktorielle Varianzanalyse</span>"
    ]
  },
  {
    "objectID": "409-Zweifaktorielle-Varianzanalyse.html#selbstkontrollfragen",
    "href": "409-Zweifaktorielle-Varianzanalyse.html#selbstkontrollfragen",
    "title": "33  Zweifaktorielle Varianzanalyse",
    "section": "33.6 Selbstkontrollfragen",
    "text": "33.6 Selbstkontrollfragen\n\nErläutern Sie das Anwendungsszenario der zweifaktoriellen Varianzanalyse.\nAus wie vielen Datenpunkten besteht ein Datensatz eines 3 x 4 ZVA Designs mit 10 Datenpunkten pro Zelle?\nErläutern Sie die intuitive Bedeutung eines Haupteffektes in einem ZVA Design.\nErläutern Sie die intuitive Bedeutung einer Interaktion ein einem ZVA Design.\nGeben Sie die Definition des additiven Modells der ZVA mit Referenzgruppe wieder.\nErläutern Sie die Bedeutung der Parameter \\(\\mu_{0}, \\alpha_{2}\\) und \\(\\beta_{2}\\) im additiven Modell der ZVA mit Referenzgruppe.\nBestimmen Sie \\(\\mu_{ij}\\) für \\(\\mu_{0}:=2, \\alpha_{2}=-1\\) und \\(\\beta_{2}:=3\\) im additiven Modell der ZVA mit Referenzgruppe.\nGeben Sie die Designmatrixform des Modells einer additiven \\(2 \\times 2\\) ZVA mit Referenzgruppe für \\(n_{ij}:=1\\) an.\nGeben Sie die Designmatrixform des Modells einer additiven \\(2 \\times 2\\) ZVA mit Referenzgruppe für \\(n_{ij}:=3\\) an.\nGeben Sie die Definition des Modells der ZVA mit Interaktion und Referenzgruppe wieder.\nErläutern Sie die Bedeutung der Parameter \\(\\mu_{0}, \\alpha_{2}, \\beta_{2}\\) und \\(\\gamma_{22}\\) im Modell der ZVA mit Interaktion und Referenzgruppe.\nGeben Sie die Designmatrixform des Modells einer \\(2 \\times 2\\) ZVA mit Interaktion und Referenzgruppe für \\(n_{ij}:=1\\) an.\nGeben Sie die Designmatrixform des Modells einer \\(2 \\times 2\\) ZVA mit Interaktion und Referenzgruppe für \\(n_{ij}:=3\\) an.\nGeben Sie das Theorem zur Betaparameterschätzung im additiven \\(2 \\times 2\\) ZVA Modell mit Referenzgruppe wieder.\nGeben Sie das Theorem zur Betaparameterschätzung im \\(2 \\times 2\\) ZVA Modell mit Interaktion und Referenzgruppe wieder. \n\n\n\n\n\nEveritt, B., & Howell, D. C. (Hrsg.). (2005). Encyclopedia of Statistics in Behavioral Science. John Wiley & Sons.\n\n\nFisher, R. A. (1925). Applications of \"Student’s\" Distribution. Metron, 5, 90–104.\n\n\nFisher, R. A. (1935). The Design of Experiments (1. ed). Hafner Press.\n\n\nStigler, S. M. (1986). The History of Statistics: The Measurement of Uncertainty before 1900. Belknap Press of Harvard University Press.",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Zweifaktorielle Varianzanalyse</span>"
    ]
  },
  {
    "objectID": "410-Partielle-Korrelation.html",
    "href": "410-Partielle-Korrelation.html",
    "title": "34  Partielle Korrelation",
    "section": "",
    "text": "34.1 Motivation\nZur Motivation des Begriffs der partiellen Korrelation betrachten wir zunächst den in Abbildung 34.1 visualisierten Beispieldatensatz zum Zusammenhang von Eiskonsum und Sonnenbrandinzidenz. Wir stellen uns vor, dass jeder der in Abbildung 34.1 abgebildeten Datenpunkte ein Wertepaar aus einem durchschnittlichen und normalisierten Eiskonsum und einer durchschnittlichen und normalisierten Sonnenbrandinzidenz eines Landes über einen gewissen Erhebungszeitraum ist. Visuell betrachtet sieht man eine Tendenz dafür, dass hohe Werte des Eiskonsums mit eher hohen Werten der Sonnenbrandinzidenz auftreten, während niedrige Werte des Eiskonsums mit eher niedrigen Werten der Sonnenbrandinziden zusammen auftreten. Die Bestimmung des Stichprobenkorrelationskoeffizienten zu diesem Datensatz ergibt mit \\(r=0.46\\) eine mittelstarke positive Korrelation.\nIntuitiv ist es jedoch eher unplausibel, dass Eiskonsum ursächlich Sonnenbrand hervorruft bzw. das Sonnenbrand den Eiskonsum erhöht (allerdings sind diese Szenarien auch nicht gänzlich auszuschließen: ein bestimmter Eiskonsum könnte eine allergische Reaktion hervorrufen mit Symptomen, die dem Sonnenbrand sehr ähnlich sind, andersherum wäre es denkbar, dass bei Sonnenbrand zur Abkühlung gerne Eis konsumiert wird. Wir wollen diese eher unplausiblen Erklärungsansätze hier jedoch nicht weiter verfolgen). Der in Abbildung 34.1 dargestellte Datensatz ist also ein Beispiel dafür, dass Korrelation als Maß für den linearen Zusammenhang zweier Zufallsvariablen lediglich ein Maß für die Koinzidenz bestimmter Datenwerte ist, jedoch keine Kausalerklärung der Werte einer abhängigen Variable aus den Werten einer unabhängigen Variable impliziert. In Kurzform hat sich zur Beschreibung dieser Tatsache seit Beginn der modernen Inferenzstatistik am Anfang des 20. Jahrhunderts der Leitsatz “Correlation is not causation” eingebürgert.\nBasierend auf dem negativen Ergebnis, dass eine mittelstarke Korrelation wie im Beispiel von Eiskonsum und Sonnenbrandinzidenz nur sehr unplausibel durch eine direkte kausale Beziehung der beiden Variablen zu erklären ist, stellt sich die Frage, inwieweit andere datenanalytische Verfahren hier Abhilfe schaffen können. Dabei stellt sich natürlich zunächst das philosophische Problem, was Kausalität eigentlich bedeuten soll und als nächstes die Frage, wie ein solcher Begriff mit den Mitteln der Wahrscheinlichkeitstheorie und Inferenzstatistik evaluiert werden könnte. Diesen Ansatz verfolgt das Gebiet der Kausalen Inferenz, wie zum Beispiel durch die Arbeiten von Pearl (2000) und Imbens & Rubin (2015) repräsentiert. Wir wollen an dieser Stelle diesen Ansatz nicht vertiefen, sondern stattdessen fragen, wie im obigen Beispiel anhand weiterer Daten die beobachtete Korrelation von Eiskonsum und Sonnenbrandinzidenz so aufgeklärt werden kann, dass die statistische Beschreibung des in Abbildung 34.1 dargestellten Datensatzes plausibler erscheint. Dies ist das zentrale Thema der partiellen Korrelation.\nDazu nehmen wir an, dass der Zusammenhang von Eiskonsum und Sonnenbrandinzidenz (Abbildung 34.2 A) plausibel durch die Kovariation beider Variablen mit einer dritten Variable, nämlich der Anzahl der im Erhebungszeitraum und Land auftretenden Anzahl an Sommertagen, d.h. Tagen mit einer maximalen Temperatur von über \\(25^{\\circ}\\) Celsius, erklärt werden kann (Abbildung 34.2 B).\nAbbildung 34.2\nIntuitiv erklärt sich die positive Korrelation von Eiskonsum und Sonnenbrandinziden dann wie folgt. Treten im Erhebungszeitraum in einem Land mehr Sommertage auf, so steigt in diesem Land sowohl der Eiskonsum als auch die Sonnenbrandinzidenz, treten dagegen weniger Sommertage auf, so fallen in diesem Land sowohl der Eiskonsum als auch die Sonnenbrandinzidenz. Lässt man die Anzahl der Sommertage außer Acht, so treten also hohe Werte von Eiskonsum und Sonnenbrandinzidenz als auch niedrige Werte von Eiskonsum und Sonnenbrandinzidenz häufig zusammen auf und es ergibt sich die in Abbildung 34.1 implizierte positive Korrelation.\nDie entscheidende Frage in diesem Kontext ist also, ob bei gleicher Anzahl von Sommertagen Evidenz für eine Korrelation von Eiskonsum und Sonnenbrandinzidenz besteht oder nicht. In diesem Fall würde die Kovarianz von Eiskonsum und Sonnenbrandinzidenz also bedingt auf einer konstanten Anzahl von Sommertagen betrachtet werden. Die datenanalytischen Werkzeuge, bei Vorliegen von Realisationen von drei Zufallsvariablen eben diese Form einer bedingten Korrelation zu evaluieren, stellen der Begriff der bedingten Korrelation und der eng verwandte Begriff der partiellen Korrelation bereit. Intuitiv handelt es sich dabei um die Korrelation zweier Zufallsvariablen (.B. Eiskonsum und Sonnenbrandinziden) nachdem aus beiden Zufallsvariablen der Einfluß einer dritten Zufallsvariable (.B. Anzahl an Sommertagen) “herausgerechnet” wurde. Die Begriffe der bedingten und partiellen Korrelation sind dabei nicht auf das Szenario von drei Zufallsvariablen beschränkt, sondern können für beliebig viele Zufallsvariablen generalisiert werden. Wir beschränken uns in diesem Abschnitt allerdings auf das Szenario dreier Zufallsvariablen um die Grundlagen der Theorie zu verdeutlichen.\nWir gehen dabei wie folgt vor. In Kapitel 12.2 führen wir mit der bedingten Kovarianz und der bedingten Korrelation zunächst allgemeine Maße für den auf den Werten einer dritten Zufallsvariable bedingten linear-affinen Zusammenhang zweier Zufallsvariablen ein, verdeutlichen dann die Begriffe anhand des Szenarios dreier gemeinsam multivariat normalverteilter Zufallsvariablen und diskutieren schließlich den Zusammenhang zwischen bedingter Korrelation und paarweisen (unbedingten) Korrelation. In Kapitel 12.3 führen wir mit der partiellen Korrelation dann ein regressionsbasiertes Maß für den bedingten Zusammenhang zweier Zufallsvariablen ein. Dabei ergibt sich insbesondere, dass im Falle von gemeinsam multivariat normalverteilten Zufallsvariablen bedingte und partielle Korrelation identisch sind. Wir schließen diesen Abschnitt mit der Evaluation der partiellen Korrelation von Eiskonsum und Sonnenbrandinzidenz im Lichte des Wissens um die Anzahl an Sommertagen für den in Abbildung 34.1 visualisierten Beispieldatensatz.",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Partielle Korrelation</span>"
    ]
  },
  {
    "objectID": "410-Partielle-Korrelation.html#motivation",
    "href": "410-Partielle-Korrelation.html#motivation",
    "title": "34  Partielle Korrelation",
    "section": "",
    "text": "Abbildung 34.1: Beispielszenario zur Evaluation bedingter und partieller Korrelationen.\n\n\n\n\n\n\n\n\n\n\n\n\nAbbildung 34.2: Zufallsvariablen im Beispielszenario.",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Partielle Korrelation</span>"
    ]
  },
  {
    "objectID": "410-Partielle-Korrelation.html#bedingte-korelation",
    "href": "410-Partielle-Korrelation.html#bedingte-korelation",
    "title": "34  Partielle Korrelation",
    "section": "34.2 Bedingte Korelation",
    "text": "34.2 Bedingte Korelation\nWir definieren zunächst die bedingte Kovarianz und die bedingte Korrelation zweier Zufallsvariablen gegeben eine dritte Zufallsvariable.\n\nDefinition 34.1 (Bedingte Kovarianz und bedingte Korrelation) Gegeben seien drei Zufallsvariablen \\(\\xi,\\upsilon,\\zeta\\) einer gemeinsamen Verteilung \\(\\mathbb{P}_{\\xi,\\upsilon,\\zeta}(\\xi,\\upsilon,\\zeta)\\). Weiterhin sei \\(\\mathbb{P}_{\\xi, \\upsilon \\vert \\zeta}(\\xi,\\upsilon)\\) die bedingte Verteilung von \\(\\xi\\) und \\(\\upsilon\\) gegeben \\(\\zeta\\). Dann heißt die Kovarianz von \\(\\xi\\) und \\(\\upsilon\\) in der Verteilung \\(\\mathbb{P}_{\\xi, \\upsilon \\vert \\zeta}(\\xi,\\upsilon)\\) die bedingte Kovarianz von \\(\\xi\\) und \\(\\upsilon\\) gegeben \\(\\zeta\\) und wird mit \\(\\mathbb{C}(\\xi, \\upsilon \\vert \\zeta)\\) bezeichnet. Weiterhin seien \\(\\mathbb{P}_{\\xi, \\upsilon \\vert \\zeta}(\\xi)\\) und \\(\\mathbb{P}_{\\xi, \\upsilon \\vert \\zeta}(\\upsilon)\\) die marginalen Verteilungen von \\(\\xi\\) und \\(\\upsilon\\) gegeben \\(\\zeta\\), respektive, und \\(\\mathbb{S}(\\xi\\vert\\zeta),\n\\mathbb{S}(\\upsilon\\vert\\zeta)\\) die Standardabweichungen von \\(\\xi\\) und \\(\\upsilon\\) hinsichtlich \\(\\mathbb{P}_{\\xi, \\upsilon \\vert \\zeta}(\\upsilon)\\) und \\(\\mathbb{P}_{\\xi, \\upsilon \\vert \\zeta}(\\xi)\\), respektive. Dann heißt die Korrelation von \\(\\xi\\) und \\(\\upsilon\\) in der Verteilung \\(\\mathbb{P}_{\\xi, \\upsilon \\vert \\zeta}(\\xi,\\upsilon)\\), \\[\\begin{equation}\n\\rho(\\xi, \\upsilon \\vert \\zeta):=\\frac{\\mathbb{C}(\\xi, \\upsilon \\vert \\zeta)}{\\mathbb{S}(\\xi\\vert\\zeta) \\mathbb{S}(\\upsilon\\vert\\zeta)}\n\\end{equation}\\] die bedingte Korrelation von \\(\\xi\\) und \\(\\upsilon\\) gegeben \\(\\zeta\\)\n\nDie bedingte Kovarianz zweier Zufallsvariablen ist also definiert als die Kovarianz zweier Zufallsvariablen in einer auf einer dritten Zufallsvariable bedingten Verteilung. Gleiches gilt für die bedingte Korrelation zweier Zufallsvariablen. Durch Vertauschen in obiger Definition kann man analog \\(\\rho(\\upsilon,\\zeta\\vert\\xi)\\) und \\(\\rho(\\xi,\\zeta\\vert\\upsilon)\\) definieren. Wir verdeutlichen Definition 34.1 als nächstes an einem Beispiel.\n\nBeispiel\nDie Zufallsvariablen \\(\\xi,\\upsilon,\\zeta\\) seien multivariat normalverteilt, d.h. für \\(\\gamma := (\\xi,\\upsilon,\\zeta)^{T}\\) gelte, dass \\[\\begin{equation}\n\\gamma \\sim N(\\mu, \\Sigma)\n\\end{equation}\\] mit \\[\\begin{equation}\n\\mu:=\n\\begin{pmatrix}\n\\mu_{\\upsilon} \\\\\n\\mu_{\\xi}   \\\\\n\\mu_{\\zeta}\n\\end{pmatrix}\n\\mbox{ und }\n\\Sigma:=\\begin{pmatrix}\n\\sigma_{\\xi}^{2}            & \\sigma_{\\xi,\\upsilon}^{2}     & \\sigma_{\\xi,\\zeta}^{2} \\\\\n\\sigma_{\\upsilon,\\xi}^{2}   & \\sigma_{\\upsilon}^{2}         & \\sigma_{\\upsilon, \\zeta}^{2} \\\\\n\\sigma_{\\zeta,\\xi}^{2}      & \\sigma_{\\zeta,\\upsilon}^{2}   & \\sigma_{\\zeta}^{2}\n\\end{pmatrix}\n\\end{equation}\\] Wir nehmen an, dass wir die bedingte Korrelation von \\(\\xi\\) und \\(\\upsilon\\) gegeben \\(\\zeta\\) bestimmen wollen und wenden uns entprechend der bedingten Verteilung von \\(\\xi\\) und \\(\\upsilon\\) gegeben \\(\\zeta\\) zu. Nach Theorem 20.8 wissen wir, dass diese bedingte Verteilung ebenfalls eine Normalverteilung ist, deren Kovarianzmatrixparameter wir aus dem Kovarianzmatrixparameter der gemeinsamen Verteilung von \\(\\xi,\\upsilon,\\zeta\\) bestimmen können. Wir definieren zu diesem Zweck zunächst \\[\\begin{equation}\n\\Sigma_{\\xi,\\upsilon} :=\n\\begin{pmatrix}\n\\sigma_{\\xi}^{2} & \\sigma_{\\xi,\\upsilon}^{2} \\\\\n\\sigma_{\\upsilon,\\xi}^{2} & \\sigma_{\\upsilon}^{2}\n\\end{pmatrix},\n\\Sigma_{\\zeta} := \\left(\\sigma_{\\zeta}^{2}\\right)\n\\mbox{ und }\n\\Sigma_{(\\xi,\\upsilon), \\zeta}\n:=\\Sigma_{\\zeta,(\\xi,\\upsilon)}^{T}\n:=\n\\begin{pmatrix}\n\\sigma_{\\xi,\\zeta}^{2} \\\\\n\\sigma_{\\upsilon, \\zeta}^{2}\n\\end{pmatrix}\n\\end{equation}\\] so dass für den Kovarianzmatrixparameter der gemeinsamen Verteilung von \\(\\xi,\\upsilon,\\zeta\\) gilt, dass \\[\\begin{equation}\n\\Sigma\n=\n\\begin{pmatrix}\n\\Sigma_{\\xi,\\upsilon} & \\Sigma_{(\\xi,\\upsilon), \\zeta} \\\\\n\\Sigma_{\\zeta,(\\xi,\\upsilon)} & \\Sigma_{\\zeta}\n\\end{pmatrix}\n\\end{equation}\\] Mit Theorem 4.8 ergibt sich der Kovarianzmatrixparameter des Zufallsvektors \\((\\xi,\\upsilon)^{T}\\) dann zu \\[\\begin{equation}\n\\Sigma_{\\xi, \\upsilon \\vert \\zeta}\n= \\Sigma_{\\xi,\\upsilon}-\\Sigma_{(\\xi,\\upsilon), \\zeta} \\Sigma_{\\zeta}^{-1} \\Sigma_{\\zeta,(\\xi,\\upsilon)}\n\\end{equation}\\] Mit den Eigenschaften von multivariaten Normalverteilungen gilt dann, dass die Diagonaleinträge von \\(\\Sigma_{\\xi, \\upsilon \\vert \\zeta}\\) den bedingten Varianzen von \\(\\xi\\) und \\(\\upsilon\\) gegeben \\(\\zeta\\) entsprechen und dass der Nichtdiagonaleintrag von \\(\\Sigma_{\\xi, \\upsilon \\vert \\zeta}\\) die bedingte Kovarianz von \\(\\xi\\) und \\(\\upsilon\\) gegeben \\(\\zeta\\) ist. In anderen Worten gilt \\[\\begin{equation}\n\\Sigma_{\\xi, \\upsilon \\vert \\zeta}\n=\n\\begin{pmatrix}\n\\mathbb{C}(\\xi,\\xi\\vert\\zeta)       & \\mathbb{C}(\\xi,\\upsilon \\vert \\zeta) \\\\\n\\mathbb{C}(\\upsilon,\\xi\\vert\\zeta)  & \\mathbb{C}(\\upsilon,\\upsilon\\vert\\zeta)\n\\end{pmatrix}\n\\end{equation}\\] Die bedingte Korrelation \\(\\rho(\\xi, \\upsilon \\vert \\zeta)\\) von \\(\\xi\\) und \\(\\upsilon\\) gegeben \\(\\zeta\\) ergibt sich dann aus den Einträgen von \\(\\Sigma_{\\xi, \\upsilon \\vert \\zeta}\\) gemäß \\[\\begin{equation}\n\\rho(\\xi, \\upsilon \\vert \\zeta) =\n\\frac{\\mathbb{C}(\\xi, \\upsilon \\vert \\zeta)}{\\sqrt{\\mathbb{C}(\\xi,\\xi\\vert\\zeta)}\\sqrt{\\mathbb{C}(\\upsilon, \\upsilon\\vert\\zeta)}}\n\\end{equation}\\] Sei konkret etwa der Kovarianzmatrixparameter von \\((\\xi,\\upsilon,\\zeta)^{T}\\) gegeben als \\[\\begin{equation}\n\\Sigma :=\n\\begin{pmatrix}\n1.0 & 0.5 & 0.9 \\\\\n0.5 & 1.0 & 0.5 \\\\\n0.9 & 0.5 & 1.0\n\\end{pmatrix}\n\\end{equation}\\] Dann ergibt sich \\[\\begin{equation}\n\\rho(\\xi,\\upsilon)=0.50 \\mbox{ und } \\rho(\\xi, \\upsilon \\vert \\zeta) \\approx 0.13\n\\end{equation}\\] Folgender R Code demonstriert die Auswertung dieser bedingten Korrelation.\n\n# Bedingte Korrelation bei Normalverteilung \nS         = matrix(c( 1,.5,.9,                                                  # \\Sigma\n                     .5, 1,.5,\n                     .9,.5, 1), nrow  = 3, byrow = TRUE)\nrho_xy    = S[1,2]/(sqrt(S[1,1])*sqrt(S[2,2]))                                  # \\rho(x,y)\nS_xy_z    = S[1:2,1:2] -  S[1:2,3] %*% solve(S[3,3]) %*%S[3,1:2]                # \\Sigma_{x,y|z} \nrho_xy_z  = S_xy_z[1,2]/(sqrt(S_xy_z[1,1])*sqrt(S_xy_z[2,2]))                   # \\rho(x,y|z)\n\n\n\nrho(x,y)   : 0.5 \nrho(x,y|z) : 0.13",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Partielle Korrelation</span>"
    ]
  },
  {
    "objectID": "410-Partielle-Korrelation.html#bedingte-korrelation-bei-normalverteilung",
    "href": "410-Partielle-Korrelation.html#bedingte-korrelation-bei-normalverteilung",
    "title": "34  Partielle Korrelation",
    "section": "34.3 Bedingte Korrelation bei Normalverteilung",
    "text": "34.3 Bedingte Korrelation bei Normalverteilung\nFür den Fall dreier gemeinsam normalverteilter Zufallsvariablen eröffnet folgendes Theorem eine Möglichkeit, die bedingte Korrelation zweier dieser Zufallsvariablen gegeben die dritte auf Grundlage der (unbedingten) paarweisen Korrelationen der Zufallsvariablen zu bestimmen. So kann bei gemeinsamer Normalverteilung von \\(\\xi,\\upsilon,\\zeta\\) zum Beispiel \\(\\rho(\\xi, \\upsilon \\vert \\zeta)\\) aus den Korrelationen \\(\\rho(\\xi,\\upsilon), \\rho(\\xi,\\zeta)\\), und \\(\\rho(\\upsilon, \\zeta)\\) bestimmt werden. Speziell gilt folgendes Theorem.\n\nTheorem 34.1 (Bedingte Korrelation und Korrelationen bei Normalverteilung) \\(\\xi,\\upsilon,\\zeta\\) seien drei gemeinsam multivariat normalverteilte Zufallsvariablen. Dann gilt \\[\\begin{equation}\n\\rho(\\xi,\\upsilon \\vert \\zeta)\n= \\frac{\\rho(\\xi,\\upsilon)-\\rho(\\xi,\\zeta) \\rho(\\upsilon, \\zeta)}{\\sqrt{\\left(1-\\rho(\\xi,\\zeta)^{2}\\right)} \\sqrt{\\left(1-\\rho(\\upsilon, \\zeta)^{2}\\right)}}\n\\end{equation}\\]\n\n\nBeweis. Ohne Beschränkung der Allgemeinheit betrachten wir den Fall eines standardisierten multivariat normalverteilten Zufallsvektors \\(\\gamma := (\\xi,\\upsilon,\\zeta)^{T}\\) mit Kovarianzmatrixparameter \\[\\begin{equation}\n\\Sigma :=\n\\begin{pmatrix}\n1                       & \\rho(\\xi,\\upsilon)    & \\rho(\\xi,\\zeta) \\\\\n\\rho(\\upsilon, \\xi)     & 1                     & \\rho(\\upsilon, \\zeta) \\\\\n\\rho(\\zeta, \\xi)        & \\rho(\\zeta, \\upsilon) & 1\n\\end{pmatrix}\n\\end{equation}\\] Wir definieren nun zunächst \\[\\begin{equation}\n\\Sigma_{\\xi,\\upsilon} :=\n\\begin{pmatrix}\n1                   & \\rho(\\xi,\\upsilon) \\\\\n\\rho(\\upsilon, \\xi) & 1\n\\end{pmatrix},\n\\Sigma_{\\zeta}:=(1)\n\\mbox{ und }\n\\Sigma_{(\\xi,\\upsilon), \\zeta}:=\\Sigma_{\\zeta,(\\xi,\\upsilon)}^{T} :=\n\\begin{pmatrix}\n\\rho(\\xi,\\zeta) \\\\\n\\rho(\\upsilon, \\zeta)\n\\end{pmatrix},\n\\end{equation}\\] so dass \\[\\begin{equation}\n\\Sigma\n= \\begin{pmatrix}\n\\Sigma_{\\xi,\\upsilon}           & \\Sigma_{(\\xi,\\upsilon), \\zeta} \\\\\n\\Sigma_{\\zeta,(\\xi,\\upsilon)}   & \\Sigma_{\\zeta}\n\\end{pmatrix}.\n\\end{equation}\\] Mit dem Theorem 20.8 ist dann die Kovarianzmatrix des Zufallsvektors \\((\\xi,\\upsilon)\\) gegeben durch \\[\\begin{equation}\n\\Sigma_{\\xi, \\upsilon \\vert \\zeta} = \\Sigma_{\\xi,\\upsilon} - \\Sigma_{(\\xi,\\upsilon), \\zeta} \\Sigma_{\\zeta}^{-1} \\Sigma_{\\zeta,(\\xi,\\upsilon)}\n\\end{equation}\\] Es ergibt sich also \\[\\begin{equation}\n\\begin{aligned}\n\\begin{pmatrix}\n\\sigma_{\\xi, \\xi\\vert\\zeta}^{2}      & \\sigma_{\\xi, \\upsilon \\vert \\zeta}^{2} \\\\\n\\sigma_{\\upsilon, \\xi\\vert\\zeta}^{2} & \\sigma_{\\upsilon, \\upsilon\\vert\\zeta}^{2}\n\\end{pmatrix}\n&  =\n\\begin{pmatrix}\n1                   & \\rho(\\xi,\\upsilon) \\\\\n\\rho(\\upsilon, \\xi) & 1\n\\end{pmatrix}\n-\n\\begin{pmatrix}\n\\rho(\\xi,\\zeta) \\\\\n\\rho(\\upsilon, \\zeta)\n\\end{pmatrix}(1)^{-1}\n\\begin{pmatrix}\n\\rho(\\xi,\\zeta) & \\rho(\\upsilon, \\zeta)\n\\end{pmatrix}\n\\\\\n&\n= \\begin{pmatrix}\n1 & \\rho(\\xi,\\upsilon) \\\\\n\\rho(\\upsilon, \\xi) & 1\n\\end{pmatrix}-\n\\begin{pmatrix}\n\\rho(\\xi,\\zeta) \\rho(\\xi,\\zeta)         & \\rho(\\xi,\\zeta) \\rho(\\upsilon, \\zeta) \\\\\n\\rho(\\upsilon, \\zeta) \\rho(\\xi,\\zeta)   & \\rho(\\upsilon, \\zeta) \\rho(\\upsilon, \\zeta)\n\\end{pmatrix} \\\\\n& =\n\\begin{pmatrix}\n1-\\rho(\\xi,\\zeta)^{2}                                       & \\rho(\\xi,\\upsilon)-\\rho(\\xi,\\zeta) \\rho(\\upsilon, \\zeta) \\\\\n\\rho(\\upsilon, \\xi)-\\rho(\\upsilon, \\zeta) \\rho(\\xi,\\zeta)   & 1-\\rho(\\upsilon, \\zeta)^{2}\n\\end{pmatrix}.\n\\end{aligned}\n\\end{equation}\\] Es ergibt sich also \\[\\begin{equation}\n\\rho(\\xi, \\upsilon \\vert \\zeta) =\n\\frac{\\sigma_{\\xi, \\upsilon \\vert \\zeta}^{2}}{\\sqrt{\\sigma_{\\xi, \\xi\\vert\\zeta}^{2}} \\sqrt{\\sigma_{\\upsilon, \\upsilon\\vert\\zeta}^{2}}}\n=\\frac{\\rho(\\xi,\\upsilon)-\\rho(\\xi,\\zeta) \\rho(\\upsilon, \\zeta)}{\\sqrt{1-\\rho(\\xi,\\zeta)^{2}} \\sqrt{1-\\rho(\\upsilon, \\zeta)^{2}}}\n\\end{equation}\\] Im Falle des Vorliegens von Realisierungen von \\(\\xi,\\upsilon,\\zeta\\) ergibt sich ein entsprechender Schätzer für \\(\\rho(\\xi, \\upsilon \\vert \\zeta)\\) mit den Stichprobenkorrelationen \\(r_{\\xi,\\upsilon}, r_{\\xi,\\zeta}, r_{\\upsilon, \\zeta}\\) dann zu \\[\\begin{equation}\nr_{\\xi, \\upsilon \\vert \\zeta}\n= \\frac{r_{\\xi,\\upsilon}-r_{\\xi,\\zeta} r_{\\upsilon, \\zeta}}{\\sqrt{\\left(1-r_{\\xi,\\zeta}^{2}\\right)} \\sqrt{\\left(1-r_{\\upsilon, \\zeta}^{2}\\right)}}\n\\end{equation}\\]",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Partielle Korrelation</span>"
    ]
  },
  {
    "objectID": "410-Partielle-Korrelation.html#partielle-korrelation",
    "href": "410-Partielle-Korrelation.html#partielle-korrelation",
    "title": "34  Partielle Korrelation",
    "section": "34.4 Partielle Korrelation",
    "text": "34.4 Partielle Korrelation\nWir defininieren als nächstes die partielle Korrelation zweier Zufallsvariablen gegeben eine dritte Zufallsvariable.\n\nDefinition 34.2 (Partielle Korrelation.) \\(\\xi,\\upsilon,\\zeta\\) seien Zufallsvariablen mit linear-affinen Abhängigkeiten zwischen \\(\\xi\\) und \\(\\zeta\\) sowie zwischen \\(\\upsilon\\) und \\(\\zeta\\), \\[\\begin{equation}\n\\begin{aligned}\n\\xi         & :=\\beta_{0}^{\\xi, \\zeta}     + \\beta_{1}^{\\xi, \\zeta}\\zeta \\\\\n\\upsilon    & :=\\beta_{0}^{\\upsilon, \\zeta}+ \\beta_{1}^{\\upsilon, \\zeta}\\zeta\n\\end{aligned}\n\\end{equation}\\] mit Residualvariablen \\[\\begin{equation}\n\\begin{aligned}\n& \\varepsilon^{\\xi, \\zeta}      := \\xi-\\beta_{0}^{\\xi, \\zeta}          -\\beta_{1}^{\\xi, \\zeta}\\zeta \\\\\n& \\varepsilon^{\\upsilon, \\zeta} := \\upsilon-\\beta_{0}^{\\upsilon, \\zeta}-\\beta_{1}^{\\upsilon, \\zeta}\\zeta\n\\end{aligned}\n\\end{equation}\\] Dann ist die partielle Korrelation von \\(\\xi\\) und \\(\\upsilon\\) mit auspartialisiertem \\(\\zeta\\) definiert als \\[\\begin{equation}\n\\rho(\\xi,y \\backslash \\zeta):=\\rho\\left(\\varepsilon^{\\xi, \\zeta}, \\varepsilon^{\\upsilon, \\zeta}\\right) .\n\\end{equation}\\]\n\nIntuitiv entsprechen in obiger Definition die Zufallsvariable \\(\\varepsilon^{\\xi, \\zeta}\\) der Zufallsvariable \\(\\xi\\), aus der der Einfluss von \\(\\zeta\\) “herausgerechnet” wurde, und die Zufallsvariable \\(\\varepsilon^{\\upsilon, \\zeta}\\) der Zufallsvariable \\(\\upsilon\\), aus der der Einfluss von \\(\\zeta\\) “herausgerechnet” wurde. Damit entspricht \\(\\rho(\\xi,y \\backslash \\zeta)\\) dann intuitiv der Korrelation von \\(\\xi\\) und \\(\\upsilon\\), aus denen jeweils der Einfluss von \\(\\zeta\\) “herausgerechnet” wurde. Wir geben als nächstes einen Schätzer für die partielle Korrelation zweier Zufallsvariablen gegeben eine dritte Zufallsvariable an.\n\nDefinition 34.3 (Partielle Stichprobenkorrelation) \\(\\xi,\\upsilon,\\zeta\\) seien Zufallsvariablen mit linear-affinen Abhängigkeiten zwischen \\(\\upsilon\\) und \\(\\zeta\\) sowie zwischen \\(\\xi\\) und \\(\\zeta\\) wie in der Definition der partiellen Korrelation. Weiterhin seien\n\n\\(\\left\\{\\left(x_{i}, y_{i}, z_{i}\\right)\\right\\}_{i=1, \\ldots, n}\\) eine Menge von Realisierungen des Zufallsvektors \\((\\xi,\\upsilon,\\zeta)^{T}\\),\n\\(\\hat{\\beta}_{0}^{\\xi, \\zeta}, \\hat{\\beta}_{1}^{\\xi, \\zeta}\\) die Ausgleichsgeradenparameter für \\(\\left\\{\\left(x_{i}, z_{i}\\right)\\right\\}_{i=1, \\ldots, n}\\),\n\\(\\hat{\\beta}_{0}^{\\upsilon, \\zeta}, \\hat{\\beta}_{1}^{\\upsilon, \\zeta}\\) die Ausgleichsgeradenparameter für \\(\\left\\{\\left(y_{i}, z_{i}\\right)\\right\\}_{i=1, \\ldots, n}\\).\n\nSchließlich seien für \\(i=1, \\ldots, n\\)\n\n\\(e_{i}^{\\xi, \\zeta}     := x_{i}-\\hat{\\beta}_{0}^{\\xi, \\zeta}      + \\hat{\\beta}_{1}^{\\xi, \\zeta}\\zeta_{i}\\)\n\\(e_{i}^{\\upsilon, \\zeta}:= y_{i}-\\hat{\\beta}_{0}^{\\upsilon, \\zeta} + \\hat{\\beta}_{1}^{\\upsilon, \\zeta}\\zeta_{i}\\)\n\ndie Residualwerte der jeweiligen Ausgleichsgeraden. Dann heißt die Stichprobenkorrelation der Wertemenge \\(\\left\\{\\left(e_{i}^{\\upsilon, \\zeta}, e_{i}^{\\xi, \\zeta}\\right)\\right\\}_{i=1, \\ldots, n}\\) partielle Stichprobenkorrelation der \\(x_{i}\\) und \\(y_{i}\\) mit auspartialisierten \\(z_{i}\\).\n\nFür den Fall, dass \\(\\xi,\\upsilon,\\zeta\\) multivariat normalverteilt sind, gibt folgendes Theorem, auf dessen Beweis wir hier verzichten wollen, den Zusammenhang zwischen bedingter und partieller Korrelation an.\n\nTheorem 34.2 (Bedingte und Partielle Korrelation bei Normalverteilung) \\(\\xi,\\upsilon,\\zeta\\) seien drei gemeinsam multivariat normalverteilte Zufallsvariablen. Dann gilt \\[\\begin{equation}\n\\rho(\\xi, \\upsilon \\vert \\zeta)=\\rho(\\xi,y \\backslash \\zeta)\n\\end{equation}\\]\n\nMan beachte, dass obiges Theorem im Falle dreier multivariat normalverteilter Zufallsvariablen gilt. Im Allgemeinen, also für beliebige Verteilungen der drei Zufallsvariablen gilt die Identität von bedingter und partieller Korrelationen nicht. Weitere Details in diesem Zusammenhang diskutieren zum Beispiel Lawrance (1976) und Baba et al. (2004).\nAus Theorem 34.2 folgt mit Theorem 34.2 dann unmittelbar, dass bei gemeinsamer Normalverteilung von \\(\\xi,\\upsilon,\\zeta\\) die partielle Korrelation \\(\\rho(\\xi, \\upsilon \\vert \\zeta)\\) ebenso wie die bedingte Korrelation \\(\\rho(\\xi,y \\backslash \\zeta)\\) basierend auf den (unbedingten) Korrelationen \\(\\rho(\\xi,\\upsilon), \\rho(\\xi,\\zeta)\\) und \\(\\rho(\\upsilon, \\zeta)\\) bestimmt werden kann, bzw. im Falle der jeweiligen Stichprobenäquivalente durch diese geschätzt werden kann.\nFolgender R Code demonstriert die Auswertung der partiellen Stichprobenkorrelation basierend auf einem simulierten Datensatz dreier multivariat normalverteilter Zufallsvariablen. Dabei bestimmen wir die partielle Korrelation einmal basierend aus den Residualstichprobenkorrelation wie in Definition 34.3 und einmal basierend auf den paarweisen Stichprobenkorrelationen anhand von Theorem 34.2. Schließlich stellt das R Paket ppcormit pcor() eine Funktion zur automatisierten Auswertung partieller Stichprobenkorrelationen bereit, auch ihre Anwendung demonstrieren wir untenstehend. Das Resultat ist natürlich in allen drei Fällen identisch.\n\n# Modellformulierung und Datenrealisierung\nlibrary(MASS)                                                                   # Multivariate Normalverteilung\nset.seed(1)                                                                     # reproduzierbare Daten\nS     = matrix(c( 1,.5,.9,                                                      # Kovarianzmatrixparameter \\Sigma\n                 .5, 1,.5,\n                 .9,.5, 1),nrow=3,byrow=TRUE)\nn     = 1e6                                                                     # Anzahl Realisierungen   \nxyz   = mvrnorm(n,rep(0,3),S)                                                   # Realisierungen          \n\n# Partielle Stichprobenkorrelation als Residualstichprobenkorrelation\nbars  = apply(xyz, 2, mean)                                                     # Stichprobenmittel\ns     = apply(xyz, 2, sd)                                                       # Stichprobenstandardabweichungen\nc     = cov(xyz)                                                                # Stichprobenkovarianzen\nb_xz1 = c[1,3]/c[3,3]                                                           # beta_1 (x,z)\nb_xz0 = bars[1] - b_xz1*bars[3]                                                 # beta_0 (x,z)\nb_yz1 = c[2,3]/c[3,3]                                                           # beta_1 (y,z)\nb_yz0 = bars[2] - b_yz1*bars[3]                                                 # beta_0 (y,z)\ne_xz  = xyz[,1] - b_xz1*xyz[,3] - b_xz0                                         # Residualwerte e^{x,z}\ne_yz  = xyz[,2] - b_yz1*xyz[,3] - b_yz0                                         # Residualwerte e^{y,z}\npr_e  = cor(e_xz,e_yz)                                                          # \\rho(x,y\\z)\n\n# Partielle Stichprobenkorrelation aus Stichprobenkorrelationen\nr      = cor(xyz)                                                               # Stichprobenkorrelationsmatrix\npr_r_n = r[1,2]-r[1,3]*r[2,3]                                                   # \\rho(x,y\\z) Formel Zähler\npr_r_d = sqrt((1-r[1,3]^2)*(1-r[2,3]^2))                                        # \\rho(x,y\\z) Formel Nenner\npr_r   = pr_r_n/pr_r_d                                                          # \\rho(x,y\\z)          \n\n# partielle Stichprobenkorrelation aus Toolbox\nlibrary(ppcor)                                                                  # Laden der Toolbox\npr_t   = pcor(xyz)                                                              # \\rho(x,y\\z),\\rho(x,z\\y),\\rho(y,z\\x) \n\n\n\nr(x,y)                           : 0.5 \nr(x,y/z) aus Residuenkorrelation : 0.13 \nr(x,y/z) aus Korrelationen       : 0.13 \nr(x,y/z) aus Toolbox             : 0.13\n\n\n\nAnwendungsbeispiel\nMithilfe oben eingeführten R Codes wenden wir uns nun abschließend dem eingangs diskutierten Beispiel zum Zusammenhang von Eiskonsum und Sonnenbrandinzidenz zu. Wir nehmen an, dass zu jedem Wertepaar von Eiskonsum \\(\\left(x_{i}\\right)\\) und Sonnenbrandinzidenz \\(\\left(y_{i}\\right)\\) der korrespondierende Wert der Anzahl der Sommertage \\(\\left(z_{i}\\right)\\) im betrachteten Erhebungszeitraum und Land verfügbar ist. Dann eröffnet obige Theorie die Möglichkeit, die partielle Korrelation von Eiskonsum und Sonnenbrandinziden nach Korrektur für die Anzahl der Sommertage zu bestimmen.\n\n\n\n\n\n\nAbbildung 34.3: Evaluation der partiellen Korrelation im Beispielszenario.\n\n\n\nAbbildung 34.3\nDazu stellt Abbildung 34.3 mit der Achsenbeschriftung Eiskonsum | Sommertage die Residualwerte \\[\\begin{equation}\ne_{i}^{\\xi, \\zeta} := x_{i} -\\ hat{\\beta}_{0}^{\\xi, \\zeta}+\\hat{\\beta}_{1}^{\\xi, \\zeta}\\zeta_{i}\n\\end{equation}\\] und mit der Achsenbeschriftung Sonnenbrandinzidenz | Sommertage die Residualwerte \\[\\begin{equation}\ne_{i}^{\\upsilon, \\zeta} := y_{i}-\\hat{\\beta}_{0}^{\\upsilon, \\zeta}+\\hat{\\beta}_{1}^{\\upsilon, \\zeta}\\zeta_{i}\n\\end{equation}\\] dar. Man erkennt, dass kein systematischer Zusammenhang hoher bzw. niedriger Werte von Eiskonsum | Sommertage mit hohen bzw. niedrigen Werten von Sonnenbrandinzidenz | Sommertage besteht. Die Korrelation dieser Residualwerte beträgt dementsprechend auch nur \\(r=0.17\\) und nicht, wie im Falle der nicht für die Kovariation mit der Anzahl der Sommertage korrigierten Werte von Eiskonsum und Sonnenbrandinzidenz, \\(r=0.46\\) (vgl. Abbildung 34.1). Der bei der nicht durch die Anzahl der Sommertage informierten Korrelationsanalyse implizierte Zusammenhang von Eiskonsum und Sonnenbrandinzidenz lässt sich also durch die Kovariation beider Variablen mit der Drittvariable Sommertage aufklären bzw. “wegerklären”.",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Partielle Korrelation</span>"
    ]
  },
  {
    "objectID": "410-Partielle-Korrelation.html#literaturhinweise",
    "href": "410-Partielle-Korrelation.html#literaturhinweise",
    "title": "34  Partielle Korrelation",
    "section": "34.5 Literaturhinweise",
    "text": "34.5 Literaturhinweise\nDie Theorie partielle und bedingter Korrelationen findet spätestens seit Beginn der modernen Korrelationsanalyse zu Beginn des 20. Jahrhunderts Beachtung, man vergleiche hierzu zum Beispiel Pearson (1920), Yule (1907) oder Fisher (1924).",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Partielle Korrelation</span>"
    ]
  },
  {
    "objectID": "410-Partielle-Korrelation.html#selbstkontrollfragen",
    "href": "410-Partielle-Korrelation.html#selbstkontrollfragen",
    "title": "34  Partielle Korrelation",
    "section": "34.6 Selbstkontrollfragen",
    "text": "34.6 Selbstkontrollfragen\n\nErläutern Sie die Motivation zur Bestimmung bedingter und partieller Korrelationen.\nGeben Sie die Definition der Begriffe der bedingten Kovarianz und der bedingten Korrelation wieder.\nGeben Sie das Theorem zu bedingter Korrelation und Korrelationen bei Normalverteilung an.\nGeben Sie die Definition des Begriffs der partiellen Korrelation wieder.\nGeben Sie die Definition des Begriffs der partiellen Stichprobenkorrelation wieder.\nGeben Sie das Theorem zu bedingter und partieller Korrelation bei Normalverteilung wieder.\nErläutern Sie die Auswertung einer partiellen Korrelation anhand eines Anwendungsbeispiels.\n\n\n\n\n\n\nBaba, K., Shibata, R., & Sibuya, M. (2004). Partial Correlation and Conditional Correlation as Measures of Conditional Independence. Australian \\(&lt;\\)Html_ent Glyph=\"@amp;\" Ascii=\"&amp;\"/\\(&gt;\\) New Zealand Journal of Statistics, 46(4), 657–664. https://doi.org/10.1111/j.1467-842X.2004.00360.x\n\n\nFisher, R. A. (1924). The Distribution of the Partial Correlation Coefficient. Metron, 3, 329–332.\n\n\nImbens, G., & Rubin, D. B. (2015). Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction. Academic Press.\n\n\nLawrance, A. J. (1976). On Conditional and Partial Correlation. The American Statistician, 30(3), 146–149. https://doi.org/10.1080/00031305.1976.10479163\n\n\nPearl, J. (2000). Causality: Models, Reasoning, and Inference. Cambridge University Press.\n\n\nPearson, K. (1920). Notes on the History of Correlation. Biometrika, 13(1), 25–45. https://doi.org/10.1093/biomet/13.1.25\n\n\nYule, G. U. (1907). On the Theory of Correlation for Any Number of Variables, Treated by a New System of Notation. Proceedings of the Royal Society of London. Series A, Containing Papers of a Mathematical and Physical Character, 79(529), 182–193. https://www.jstor.org/stable/92723",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Partielle Korrelation</span>"
    ]
  },
  {
    "objectID": "411-Multiple-Regression.html",
    "href": "411-Multiple-Regression.html",
    "title": "35  Multiple Regression",
    "section": "",
    "text": "35.1 Anwendungsszenario\nBei der multiplen Regression handelt es sich um die Generalisierung der einfachen linearen Regression im Kontext von mehr als einer kontinuierlichen unabhängigen Variable. Wie bei der einfachen linearen Regression betrachtet man eine univariate abhängige Variable, deren Werte an randomisierten experimentellen Einheiten bestimmt werden. Die unabhängigen Variablen werden bei der multiplen Regression je nach Kontext Regressoren, Prädiktoren, Kovariaten, Features oder einfach Spalten der Designmatrix genannt. Ähnlich wie bei der einfachen linearen Regression ist das Ziel einer multiplen Regressionsanalyse, das Erklärungspotential für Variationen der abhängigen Variablen durch Variation der unabhängigen Variablen zu quantifizieren. In Erweitertung der einfachen linearen Regression liegt ein Augenmerk dabei insbesondere darauf, den Einfluss einzelner unabhängiger Variablen auf die abhängige Variable im Kontext der Variation anderer unabängiger Variablen zu ermitteln. Darüber hinaus mag ein Ziel auch die Prädiktion der Werte der abhängigen Variablen für Werte der unabhängigen Variable nach Schätzung der entsprechenden Wichtungsbetaparameter anhand eines “Trainingsdatensatzes” sein.",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "411-Multiple-Regression.html#anwendungsszenario",
    "href": "411-Multiple-Regression.html#anwendungsszenario",
    "title": "35  Multiple Regression",
    "section": "",
    "text": "Anwendungsbeispiel\nUm einige Grundprinzipien der multiplen Regressionsanalyse zu verdeutlichen, beschränken wir uns in diesem Abschnitt meist auf den Fall zweier kontinuierlicher unabhängiger Variablen. Als Anwendungsbeispiel betrachten wir dazu den Effekt einer Psychotherapie auf die Depressionssymptomatik in Abhängigkeit vom Alter der Patient:innen und der Dauer der Therapie. Tabelle 35.1 zeigt einen Beispieldatensatz bestehend aus Datenwerten von \\(n=20\\) Patient:innen, wobei Alter das Patient:innenalter, Dauer die Therapiedauer und dBDI die Pre-Post-Interventions-BDI-Differenzwerte bezeichnen.\n\n\n\n\nTabelle 35.1: Pre-Post-BDI-Differenzwerte in Abhängigkeit von Patient:innenalter und Therapiedauer\n\n\n\n\n\n\nAlter\nDauer\ndBDI\n\n\n\n\n36\n23\n35.4\n\n\n42\n15\n17.5\n\n\n54\n20\n17.8\n\n\n74\n14\n-5.2\n\n\n32\n15\n17.7\n\n\n74\n17\n-2.4\n\n\n77\n12\n-9.7\n\n\n60\n17\n10.2\n\n\n58\n22\n19.7\n\n\n24\n16\n29.3\n\n\n32\n18\n26.3\n\n\n31\n19\n26.0\n\n\n61\n18\n5.8\n\n\n43\n14\n11.0\n\n\n66\n22\n15.8\n\n\n50\n20\n22.0\n\n\n63\n22\n11.2\n\n\n80\n13\n-8.8\n\n\n43\n21\n28.0\n\n\n67\n17\n8.4\n\n\n\n\n\n\n\n\nAbbildung 35.1 visualisiert den Beispieldatensatz, wobei die dBDI Werte der Höhe gemäß im dreidimensionalen Raum anhand ihrer Koordinaten in der Alter-Dauer-Ebene abgetragen sind. Die Visualisierung der multiple Regression, insbesondere für mehr als zwei unabhängige Variablen, stößt sehr schnell an ihre Grenzen.\n\n\n\n\n\n\nAbbildung 35.1: Visualisierung des Beispieldatensatzes als Punktwolke",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "411-Multiple-Regression.html#modellformulierung",
    "href": "411-Multiple-Regression.html#modellformulierung",
    "title": "35  Multiple Regression",
    "section": "35.2 Modellformulierung",
    "text": "35.2 Modellformulierung\nDas Modell der multiplen Regression ist mit der allgemeinen Form des Allgemeinen Linearen Modells identisch. Der Vollständigkeit halber geben wir folgende Definition.\n\nDefinition 35.1 (Modell der multiplen Regression) \\(\\upsilon_{i}\\) mit \\(i=1, \\ldots, n\\) sei die Zufallsvariable, die den \\(i\\)ten Wert einer abhängigen Variable modelliert. Dann hat das Modell der multiplen Regression die strukturelle Form \\[\n\\upsilon_{i}=x_{i1} \\beta_{1}+\\cdots+x_{ip} \\beta_{p}+\\varepsilon_{i} \\mbox{ mit } \\varepsilon_{i} \\sim N\\left(0, \\sigma^{2}\\right) \\mbox{ u.i.v. für } i=1, \\ldots, n \\mbox{ und } \\sigma^{2}&gt;0,\n\\] wobei \\(x_{i j} \\in \\mathbb{R}\\) mit \\(1 \\leq i \\leq n\\) und \\(1 \\leq j \\leq p\\) den \\(i\\)ten Wert der \\(j\\) ten unabhängigen Variable bezeichnet. Die unabhängigen Variablen werden auch Regressoren, Prädiktoren, Kovariaten oder Features genannt. Mit \\[\nx_{i} := \\left(x_{i1}, \\ldots, x_{ip}\\right)^{T} \\in \\mathbb{R}^{p} \\mbox{ und } \\beta:=\\left(\\beta_{1}, \\ldots, \\beta_{p}\\right)^{T} \\in \\mathbb{R}^{p}\n\\] hat das Modell der multiplen Regression die Datenverteilungsform \\[\n\\upsilon_{i} \\sim N\\left(\\mu_{i}, \\sigma^{2}\\right) \\text { u.v. für } i=1, \\ldots, n, \\mbox{ wobei } \\mu_{i}:=x_{i}^{T} \\beta.\n\\] In diesem Zusammenhang wird \\(x_{i} \\in \\mathbb{R}^{p}\\) auch als \\(i\\)ter Featurevektor bezeichnet. Die Designmatrixform des Modells der multiplen Regression schließlich ist gegeben durch \\[\n\\upsilon = X\\beta+\\varepsilon \\mbox{ mit } \\varepsilon \\sim N\\left(0_{n}, \\sigma^{2} I_{n}\\right)\n\\] mit \\[\n\\upsilon := \\left(\\upsilon_{1}, \\ldots, \\upsilon_{n}\\right)^{T},\nX:=\\left(x_{i j}\\right)_{1 \\leq i \\leq n, 1 \\leq j \\leq p} \\in \\mathbb{R}^{n \\times p},\n\\beta:=\\left(\\beta_{1}, \\ldots, \\beta_{p}\\right)^{T} \\in \\mathbb{R}^{p}\n\\mbox{ und } \\sigma^{2}&gt;0.\n\\]\n\nFolgender R Code nutzt das Modell der multiplen Regression auf Definition 13.1, um den in Tabelle 35.1 dargestellten Beispieldatensatz zu erzeugen.\n\nlibrary(MASS)                                                                   # Multivariate Normalverteilung\nset.seed(1)                                                                     # reproduzierbare Daten\nn             = 20                                                              # Anzahl Datenpunkte\np             = 3                                                               # Anzahl Parameter\nx_1           = round(runif(n,20,80))                                           # Regressorwerte Alter\nx_2           = round(runif(n,12,24))                                           # Regressorwerte Therapiedauer\nX             = matrix(c(rep(1,n),x_1,x_2), nrow = n)                           # Designmatrix\nI_n           = diag(n)                                                         # Identitätsmatrix\nbeta          = matrix(c(5,-.5,2), nrow = p)                                    # Betaparametervektor\nsigsqr        = 10                                                              # Varianzparameter\ny             = mvrnorm(1, X %*% beta, sigsqr*I_n)                              # eine Realisierung eines n-dimensionalen ZVs\nD             = data.frame(Alter = x_1, Dauer = x_2, dBDI  = y)                 # Dataframeformatierung\nwrite.csv(D, \"./_data/411-multiple-regression.csv\", row.names = FALSE)          # Datenspeichern",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "411-Multiple-Regression.html#modellschätzung",
    "href": "411-Multiple-Regression.html#modellschätzung",
    "title": "35  Multiple Regression",
    "section": "35.3 Modellschätzung",
    "text": "35.3 Modellschätzung\nDas Modell der multiplen Regression erlaubt eine allgemeine Einsicht in die Bedeutung des Betaparameterschätzers \\[\n\\hat{\\beta}:=\\left(X^{T}X\\right)^{-1}X^{T}\\upsilon\n\\] Insbesondere wird bei der Beschäftigung mit dem Betaparameterschätzer im Kontext der multiplen Regression deutlich, dass \\(X^{T}\\upsilon\\) die Kovariation der Regressoren mit den Daten und \\(X^{T}X\\) die Kovariation der Regressoren untereinander quantifiziert, so dass für den Betaparameterschätzer eine Intuition als “regressorkovariationsnormalisierte Regressordatenkovariation” ergibt. Wir wollen dies im Folgenden am Beispiel einer multiplen Regression mit einem Interzeptregressor und zwei Regressoren für zwei unabhängige Variablen vertiefen. Dabei zeigen wir einmal die Form des Betaparameterschätzers in Form von Stichprobenkorrelationen (Theorem 13.1) und einmal in Form von partiellen Stichprobenkorrelationen (Theorem 13.2).\n\nTheorem 35.1 (Betaparameterschätzer und Korrelationen) Gegeben sei ein multiples Regressionsmodel der Form \\[\n\\upsilon = X\\beta+\\varepsilon, \\varepsilon \\sim N\\left(0_{n}, \\sigma^{2} I_{n}\\right)\n\\mbox{ mit }\nX:=\\begin{pmatrix}\n1 & x_{11} & x_{12} \\\\\n\\vdots & \\vdots & \\vdots \\\\\n1 & x_{n 1} & x_{n 2}\n\\end{pmatrix} \\mbox{ und }\n\\beta :=\n\\begin{pmatrix}\n\\beta_{0} \\\\\n\\beta_{1} \\\\\n\\beta_{2}\n\\end{pmatrix}.\n\\] Dann gilt \\[\n\\hat{\\beta}\n=\n\\begin{pmatrix}\n\\bar{\\upsilon}-\\hat{\\beta}_{1} \\bar{x}_{1}-\\hat{\\beta}_{2} \\bar{x}_{2} \\\\\n\\frac{r_{\\upsilon, x_{1}}-r_{\\upsilon, x_{2}} r_{x_{1}, x_{2}}}{1-r_{x_{1}, x_{2}}^{2}} \\frac{s_{\\upsilon}}{s_{x_{1}}} \\\\\n\\frac{r_{\\upsilon, x_{2}}-r_{\\upsilon, x_{1}} r_{x_{1}, x_{2}}}{1-r_{x_{1}, x_{2}}^{2}} \\frac{s_{\\upsilon}}{s_{x_{2}}}\n\\end{pmatrix},\n\\] wobei für die \\(\\upsilon_{i}, x_{i1}\\) und \\(x_{i2}\\) mit \\(i=1, \\ldots, n, \\bar{\\cdot},\ns_{\\cdot}\\) und \\(r_{\\cdot,\\cdot}\\), die entsprechenden Stichprobenmittel, Stichprobenstandardabweichungen, und Stichprobenkorrelationen bezeichnen.\n\n\nBeweis. Wir erinnern zunächst daran, dass die Form des Betaparameterschätzers bekanntlich zum System der Normalengleichungen äquivalent ist (vgl. Kapitel 28) \\[\n\\hat{\\beta}=\\left(X^{T}X\\right)^{-1}X^{T}\\upsilon \\Leftrightarrow X^{T}X \\hat{\\beta}=X^{T}\\upsilon .\n\\] Ausschreiben des Normalengleichungssystems für den hier betrachteten Spezialfall ergibt dann zunächst \\[\n\\begin{aligned}\nX^{T}X \\hat{\\beta}\n& =\nX^{T}\\upsilon\n\\\\\n\\Leftrightarrow\n\\begin{pmatrix}\n1 & \\cdots & 1 \\\\\nx_{11} & \\cdots & x_{n 1} \\\\\nx_{12} & \\cdots & x_{n 2}\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & x_{11} & x_{12} \\\\\n\\vdots & \\vdots & \\vdots \\\\\n1 & x_{n 1} & x_{n 2}\n\\end{pmatrix}\n\\begin{pmatrix}\n\\hat{\\beta}_{0} \\\\\n\\hat{\\beta}_{1} \\\\\n\\hat{\\beta}_{2}\n\\end{pmatrix}\n& =\n\\begin{pmatrix}\n1 & \\cdots & 1 \\\\\nx_{11} & \\cdots & x_{n 1} \\\\\nx_{12} & \\cdots & x_{n 2}\n\\end{pmatrix}\n\\begin{pmatrix}\n\\upsilon_{1} \\\\\n\\vdots \\\\\n\\upsilon_{n}\n\\end{pmatrix}\n\\\\\n\\Leftrightarrow\n\\begin{pmatrix}\nn & \\sum_{i=1}^{n} x_{i1} & \\sum_{i=1}^{n} x_{i2}\\\\\n\\sum_{i=1}^{n} x_{i1} & \\sum_{i=1}^{n} x_{i1}^{2} & \\sum_{i=1}^{n} x_{i1} x_{i2}\\\\\n\\sum_{i=1}^{n} x_{i2}& \\sum_{i=1}^{n} x_{i2}x_{i1} & \\sum_{i=1}^{n} x_{i2}^{2}\n\\end{pmatrix}\n\\begin{pmatrix}\n\\hat{\\beta}_{0} \\\\\n\\hat{\\beta}_{1} \\\\\n\\hat{\\beta}_{2}\n\\end{pmatrix}\n& =\n\\begin{pmatrix}\n\\sum_{i=1}^{n} \\upsilon_{i} \\\\\n\\sum_{i=1}^{n} \\upsilon_{i} x_{i1} \\\\\n\\sum_{i=1}^{n} \\upsilon_{i} x_{i2}\n\\end{pmatrix}\n\\\\\n\\Leftrightarrow\n\\begin{pmatrix}\nn & \\sum_{i=1}^{n} x_{i1} & \\sum_{i=1}^{n} x_{i2}\\\\\n\\sum_{i=1}^{n} x_{i1} & \\sum_{i=1}^{n} x_{i1}^{2} & \\sum_{i=1}^{n} x_{i1} x_{i2}\\\\\n\\sum_{i=1}^{n} x_{i2}& \\sum_{i=1}^{n} x_{i2}x_{i1} & \\sum_{i=1}^{n} x_{i2}^{2}\n\\end{pmatrix}\n\\begin{pmatrix}\n\\hat{\\beta}_{0} \\\\\n\\hat{\\beta}_{1} \\\\\n\\hat{\\beta}_{2}\n\\end{pmatrix}\n& =\n\\begin{pmatrix}\n\\sum_{i=1}^{n} \\upsilon_{i} \\\\\n\\sum_{i=1}^{n} \\upsilon_{i} x_{i1} \\\\\n\\sum_{i=1}^{n} \\upsilon_{i} x_{i2}\n\\end{pmatrix}\n\\end{aligned}\n\\] und damit \\[\n\\begin{aligned}\nX^{T}X \\hat{\\beta}\n& =\nX^{T}\\upsilon\n\\\\\n\\Leftrightarrow\n\\begin{pmatrix}\nn \\hat{\\beta}_{0}+\\hat{\\beta}_{1} \\sum_{i=1}^{n} x_{i1}+\\hat{\\beta}_{2} \\sum_{i=1}^{n} x_{i2}\\\\\n\\hat{\\beta}_{0} \\sum_{i=1}^{n} x_{i1}+\\hat{\\beta}_{1} \\sum_{i=1}^{n} x_{i1}^{2}+\\hat{\\beta}_{2} \\sum_{i=1}^{n} x_{i1} x_{i2}\\\\\n\\hat{\\beta}_{0} \\sum_{i=1}^{n} x_{i2}+\\hat{\\beta}_{1} \\sum_{i=1}^{n} x_{i1} x_{i2}+\\hat{\\beta}_{2} \\sum_{i=1}^{n} x_{i2}^{2}\n\\end{pmatrix}\n& =\n\\begin{pmatrix}\n\\sum_{i=1}^{n} \\upsilon_{i} \\\\\n\\sum_{i=1}^{n} \\upsilon_{i} x_{i1} \\\\\n\\sum_{i=1}^{n} \\upsilon_{i} x_{i2}\n\\end{pmatrix}.\n\\end{aligned}\n\\] Aus der Gleichung der ersten Vektorkomponenten folgt dann direkt die Form von \\(\\hat{\\beta}_{0}\\) mit \\[\n\\begin{aligned}\n\\sum_{i=1}^{n} \\upsilon_{i}\n& = n\\hat{\\beta}_{0}+\\hat{\\beta}_{1} \\sum_{i=1}^{n} x_{i1}+\\hat{\\beta}_{2} \\sum_{i=1}^{n} x_{i2}\n\\\\\n\\Leftrightarrow\n\\frac{1}{n} \\sum_{i=1}^{n} \\upsilon_{i}\n& =\n\\hat{\\beta}_{0}+\\hat{\\beta}_{1} \\frac{1}{n} \\sum_{i=1}^{n} x_{i1}+\\hat{\\beta}_{2} \\frac{1}{n} \\sum_{i=1}^{n} x_{i2}\n\\\\\n\\Leftrightarrow\n\\hat{\\beta}_{0} & = \\bar{\\upsilon}-\\hat{\\beta}_{1} \\bar{x}_{1}-\\hat{\\beta}_{2} \\bar{x}_{2}.\n\\end{aligned}\n\\] Einsetzen dieser Form von \\(\\hat{\\beta}_{0}\\) in die Gleichung der zweiten Vektorkomponenten ergibt dann \\[\n\\begin{aligned}\n\\hat{\\beta}_{0} \\sum_{i=1}^{n} x_{i1}+\\hat{\\beta}_{1} \\sum_{i=1}^{n} x_{i1}^{2}+\\hat{\\beta}_{2} \\sum_{i=1}^{n} x_{i1} x_{i2}\n& =\\sum_{i=1}^{n} \\upsilon_{i} x_{i1}\n\\Leftrightarrow\n\\\\\n\\left(\\bar{\\upsilon}-\\hat{\\beta}_{1} \\bar{x}_{1}-\\hat{\\beta}_{2} \\bar{x}_{2}\\right) \\sum_{i=1}^{n} x_{i1}+\\hat{\\beta}_{1} \\sum_{i=1}^{n} x_{i1}^{2}+\\hat{\\beta}_{2} \\sum_{i=1}^{n} x_{i1} x_{i2}\n& =\\sum_{i=1}^{n} \\upsilon_{i} x_{i1}\n\\\\\n\\Leftrightarrow\n\\bar{\\upsilon} \\sum_{i=1}^{n} x_{i1}-\\hat{\\beta}_{1} \\bar{x}_{1} \\sum_{i=1}^{n} x_{i1}-\\hat{\\beta}_{2} \\bar{x}_{2} \\sum_{i=1}^{n} x_{i1}+\\hat{\\beta}_{1} \\sum_{i=1}^{n} x_{i1}^{2}+\\hat{\\beta}_{2} \\sum_{i=1}^{n} x_{i1} x_{i2}\n& =\n\\sum_{i=1}^{n} \\upsilon_{i} x_{i1}\n\\\\\n\\Leftrightarrow\n\\hat{\\beta}_{1} \\sum_{i=1}^{n} x_{i1}^{2}-\\hat{\\beta}_{1} \\bar{x}_{1} \\sum_{i=1}^{n} x_{i1}+\\hat{\\beta}_{2} \\sum_{i=1}^{n} x_{i1} x_{i2}-\\hat{\\beta}_{2} \\bar{x}_{2} \\sum_{i=1}^{n} x_{i1}\n& =\n\\sum_{i=1}^{n} \\upsilon_{i} x_{i1}-\\bar{\\upsilon} \\sum_{i=1}^{n} x_{i1}\n\\\\\n\\Leftrightarrow\n\\hat{\\beta}_{1}\\left(\\sum_{i=1}^{n} x_{i1}^{2}-\\bar{x}_{1} \\sum_{i=1}^{n} x_{i1}\\right)+\\hat{\\beta}_{2}\\left(\\sum_{i=1}^{n} x_{i1} x_{i2}-\\bar{x}_{2} \\sum_{i=1}^{n} x_{i1}\\right) & =\\sum_{i=1}^{n} \\upsilon_{i} x_{i1}-\\bar{\\upsilon} \\sum_{i=1}^{n} x_{i1}\n\\end{aligned}\n\\] Im Beweis des Theorems zur Ausgleichsgerade (vgl. Kapitel 25) haben wir gesehen, dass \\[\n\\begin{aligned}\n\\sum_{i=1}^{n} x_{i1}^2-\\bar{x}_{1} \\sum_{i=1}^{n} x_{i1} & =\\sum_{i=1}^{n}\\left(x_{i1}-\\bar{x}_{1}\\right)\\left(x_{i1}-\\bar{x}_{1}\\right) \\\\\n\\sum_{i=1}^{n} x_{i1} x_{i2}-\\bar{x}_{2} \\sum_{i=1}^{n} x_{i1} & =\\sum_{i=1}^{n}\\left(x_{i1}-\\bar{x}_{1}\\right)\\left(x_{i2}-\\bar{x}_{2}\\right) \\\\\n\\sum_{i=1}^{n} \\upsilon_{i} x_{i1}-\\bar{\\upsilon} \\sum_{i=1}^{n} x_{i1} & =\\sum_{i=1}^{n}\\left(\\upsilon_{i}-\\bar{\\upsilon}\\right)\\left(x_{i1}-\\bar{x}_{1}\\right)\n\\end{aligned}\n\\] Es ergibt sich also, dass \\[\n\\begin{aligned}\n\\hat{\\beta}_{1} \\sum_{i=1}^{n}\\left(x_{i1}-\\bar{x}_{1}\\right)\\left(x_{i1}-\\bar{x}_{1}\\right)+\\hat{\\beta}_{2} \\sum_{i=1}^{n}\\left(x_{i1}-\\bar{x}_{1}\\right)\\left(x_{i2}-\\bar{x}_{2}\\right)\n& =\n\\sum_{i=1}^{n}\\left(\\upsilon_{i}-\\bar{\\upsilon}\\right)\\left(x_{i1}-\\bar{x}_{1}\\right)\n\\\\\n\\Leftrightarrow\n\\hat{\\beta}_{1} \\frac{\\sum_{i=1}^{n}\\left(x_{i1}-\\bar{x}_{1}\\right)\\left(x_{i1}-\\bar{x}_{1}\\right)}{n-1}+\\hat{\\beta}_{2} \\frac{\\sum_{i=1}^{n}\\left(x_{i1}-\\bar{x}_{1}\\right)\\left(x_{i2}-\\bar{x}_{2}\\right)}{n-1}\n& =\\frac{\\sum_{i=1}^{n}\\left(\\upsilon_{i}-\\bar{\\upsilon}\\right)\\left(x_{i1}-\\bar{x}_{1}\\right)}{n-1}\n\\end{aligned}\n\\] Mit den Definitionen von Stichprobenstandardabweichung und -korrelation folgt dann weiter \\[\n\\begin{aligned}\n\\hat{\\beta}_{1} s_{x_{1}} s_{x_{1}}+\\hat{\\beta}_{2} c_{x_{1}, x_{2}}\n& =\nc_{\\upsilon, x_{1}}\n\\\\\\Leftrightarrow\n\\hat{\\beta}_{1} \\frac{s_{x_{1}} s_{x_{1}}}{s_{\\upsilon} s_{x_{1}}}+\\hat{\\beta}_{2} \\frac{c_{x_{1}, x_{2}}}{s_{\\upsilon} s_{x_{1}}}\n& =\n\\frac{c_{\\upsilon, x_{1}}}{s_{\\upsilon} s_{x_{1}}}\n\\\\\\Leftrightarrow\n\\hat{\\beta}_{1} \\frac{s_{x_{1}}}{s_{\\upsilon}}+\\hat{\\beta}_{2} \\frac{c_{x_{1}, x_{2}}}{s_{\\upsilon} s_{x_{1}}}\n& = r_{\\upsilon, x_{1}}\n\\\\\\Leftrightarrow\n\\hat{\\beta}_{1} \\frac{s_{x_{1}}}{s_{\\upsilon}}+\\hat{\\beta}_{2} \\frac{c_{x_{1}, x_{2}} s_{x_{2}}}{s_{\\upsilon} s_{x_{1}} s_{x_{2}}}\n& = r_{\\upsilon, x_{1}}\n\\\\\\Leftrightarrow\n\\hat{\\beta}_{1} \\frac{s_{x_{1}}}{s_{\\upsilon}}+\\hat{\\beta}_{2} \\frac{s_{x_{2}}}{s_{\\upsilon}} r_{x_{1}, x_{2}}\n& =\nr_{\\upsilon, x_{1}}\n\\end{aligned}\n\\] Definition von \\[\nb_{j}:=\\frac{s_{x_{j}}}{s_{\\upsilon}}, j=1,2\n\\] erlaubt dann die Schreibweise \\[\nb_{1}+b_{2} r_{x_{1}, x_{2}}=r_{\\upsilon, x_{1}}\n\\] Schließlich folgt analog durch Vertauschen der Subskripte aus der Gleichung der dritten Vektorkomponenten \\[\nb_{1} r_{x_{1}, x_{2}}+b_{2}=r_{\\upsilon, x_{2}}\n\\] Insgesamt haben wir also gesehen, dass die Definition des Betaparameterschätzers Spezialfall des Allgemeinen Linearen Modells ergibt, dass mit \\[\n\\hat{\\beta}_{j}=b_{j} \\frac{s_{\\upsilon}}{s_{x_{j}}}, j=1,2\n\\] gilt, dass \\[\n\\begin{aligned}\n& r_{\\upsilon, x_{1}}=b_{1}+b_{2} r_{x_{1}, x_{2}} \\\\\n& r_{\\upsilon, x_{2}}=b_{1} r_{x_{1}, x_{2}}+b_{2}\n\\end{aligned}\n\\] Damit folgt aus der zweiten Gleichung dann sofort \\[\nb_{2}=r_{\\upsilon, x_{2}}-b_{1} r_{x_{1}, x_{2}}.\n\\] Einsetzen in die erste Gleichung ergibt dann \\[\n\\begin{aligned}\nb_{1}+\\left(r_{\\upsilon, x_{2}}-b_{1} r_{x_{1}, x_{2}}\\right) r_{x_{1}, x_{2}}\n& =r_{\\upsilon, x_{1}}\n\\\\\\Leftrightarrow\nb_{1}+r_{\\upsilon, x_{2}} r_{x_{1}, x_{2}}-b_{1} r_{x_{1}, x_{2}}^{2}\n& = r_{\\upsilon, x_{1}}\n\\\\\\Leftrightarrow\nr_{\\upsilon, x_{2}} r_{x_{1}, x_{2}}+b_{1}\\left(1-r_{x_{1}, x_{2}}^{2}\\right)\n& =\nr_{\\upsilon, x_{1}}\n\\\\\\Leftrightarrow\nb_{1}\\left(1-r_{x_{1}, x_{2}}^{2}\\right)\n& =r_{\\upsilon, x_{1}}-r_{\\upsilon, x_{2}} r_{x_{1}, x_{2}}\n\\\\\\Leftrightarrow b_{1}\n& =\n\\frac{r_{\\upsilon, x_{1}}-r_{\\upsilon, x_{2}} r_{x_{1}, x_{2}}}{1-r_{x_{1}, x_{2}}^{2}}\n\\end{aligned}\n\\] Für \\(b_{2}\\) ergibt sich damit weiterhin \\[\n\\begin{aligned}\nb_{2}\n& = r_{\\upsilon, x_{2}}-b_{1} r_{x_{1}, x_{2}}\n\\\\\\Leftrightarrow\nb_{2}\n& =\nr_{\\upsilon, x_{2}}-\\left(\\frac{r_{\\upsilon, x_{1}}-r_{\\upsilon, x_{2}} r_{x_{1}, x_{2}}}{1-r_{x_{1}, x_{2}}^{2}}\\right) r_{x_{1}, x_{2}}\n\\\\\\Leftrightarrow b_{2}\n& = \\frac{r_{\\upsilon, x_{2}}\\left(1-r_{x_{1}, x_{2}}^{2}\\right)}{1-r_{x_{1}, x_{2}}^{2}}-\\frac{r_{\\upsilon, x_{1}} r_{x_{1}, x_{2}}-r_{\\upsilon, x_{2}} r_{x_{1}, x_{2}}^{2}}{1-r_{x_{1}, x_{2}}^{2}}\n\\\\\\Leftrightarrow b_{2}\n& =\n\\frac{r_{\\upsilon, x_{2}}-r_{\\upsilon, x_{2}} r_{x_{1}, x_{2}}^{2}-r_{\\upsilon, x_{1}} r_{x_{1}, x_{2}}+r_{\\upsilon, x_{2}} r_{x_{1}, x_{2}}^{2}}{1-r_{x_{1}, x_{2}}^{2}}\n\\\\\n\\Leftrightarrow\nb_{2}\n& =\\frac{r_{\\upsilon, x_{2}}-r_{\\upsilon, x_{1}} r_{x_{1}, x_{2}}}{1-r_{x_{1}, x_{2}}^{2}}\n\\end{aligned}\n\\] Damit folgen dann aber \\[\n\\begin{aligned}\n\\hat{\\beta}_{1} & =b_{1} \\frac{s_{\\upsilon}}{s_{x_{1}}}=\\left(\\frac{r_{\\upsilon, x_{1}}-r_{\\upsilon, x_{2}} r_{x_{1}, x_{2}}}{1-r_{x_{1}, x_{2}}^{2}}\\right) \\frac{s_{\\upsilon}}{s_{x_{1}}} \\\\\n\\hat{\\beta}_{2} & =b_{2} \\frac{s_{\\upsilon}}{s_{x_{2}}}=\\left(\\frac{r_{\\upsilon, x_{2}}-r_{\\upsilon, x_{1}} r_{x_{1}, x_{2}}}{1-r_{x_{1}, x_{2}}^{2}}\\right) \\frac{s_{\\upsilon}}{s_{x_{2}}}\n\\end{aligned}\n\\] und es ist alles gezeigt.\n\nExemplarisch wollen wir die Stichprobenkorrelationsform des Betaparameterschätzers für \\(\\beta_{1}\\) \\[\n\\hat{\\beta}_{1}=\\frac{r_{\\upsilon, x_{1}}-r_{\\upsilon, x_{2}} r_{x_{1}, x_{2}}}{1-r_{x_{1}, x_{2}}^{2}} \\frac{s_{\\upsilon}}{s_{x_{1}}}\n\\] betrachten. Man erkennt unter anderem:\n\nNur im Fall \\(r_{x_{1}, x_{2}}=0\\) und \\(s_{\\upsilon}=s_{x_{1}}\\) gilt \\(\\hat{\\beta}_{1}=r_{\\upsilon, x_{1}}\\). Die Betaparameterschätzer der multiplen Regression sind also im Allgemeinen nicht mit den Stichprobenkorrelationen zwischen dem entsprechenden Regressor und den Daten identisch.\nIm Fall \\(r_{x_{1}, x_{2}}= \\pm 1\\) ist \\(\\hat{\\beta}_{1}\\) nicht definiert. Vollständig korrelierte Regressoren ergeben also ein nicht schätzbares Modell.\nJe größer \\(\\left|r_{x_{1}, x_{2}}\\right|\\), desto größer der von \\(r_{\\upsilon, x_{1}}\\) subtrahierte Term \\(r_{\\upsilon, x_{2}} r_{x_{1}, x_{2}}\\). Die paarweise Korrelation von Regressoren untereinander reduziert also den Betaparameterschätzerwert für einen Regressor.\nJe größer \\(\\left|r_{\\upsilon, x_{2}}\\right|\\), desto größer der von \\(r_{\\upsilon, x_{1}}\\) subtrahierte Term \\(r_{\\upsilon, x_{2}} r_{x_{1}, x_{2}}\\). Neben der paarweisen Korrelation von Regressoren untereinander reduziert also auch eine hohe Stichprobenkorrelation eines anderen Regressors mit den Daten den Wert eines Betaparameterschätzers.\nBei identischen Korrelationen und gleich bleibender Regressorstandabweichung steigt \\(\\hat{\\beta}_{1}\\) mit \\(s_{\\upsilon}\\). Intuitiv erhält also ein Regressor einen höhreren Betaparameterschätzerwert, wenn er bei gleich Korrelationsstruktur mehr Datenvariabilität erklärt.\n\nZusammengefasst verdeutlichen obige Punkte, dass der Wert des Betaparameterschätzers eines Regressors im Modell der multiplen Regression allein im Kontext der Korrelationsstruktur der anderen Regressoren mit sich selbst und den Daten betrachtet werden kann und damit keinen modellunabhängigen Absolutbeitrag eines spezifischen Regressors zur Erklärung der Daten abbildet.\nFolgender R Code demonstriert die Äquivalenz der Matrixschreibweise des Betaparameterschätzers und seiner Darstellung durch Stichprobenmittel, Stichprobenkorrelationen, und Stichprobenstandardabweichungen.\nFolgendes Theorem stellt den Bezug zwischen multipler Regression und partiellen Korrelationen zwischen unabhängigen und abhängigen Variablen für das betrachtete Szenario her.\n\nTheorem 35.2 (Betaparameterschätzer und partielle Korrelationen) Gegeben sei ein multiples Regressionsmodel der Form \\[\n\\upsilon = X\\beta+\\varepsilon, \\varepsilon \\sim N\\left(0_{n}, \\sigma^{2} I_{n}\\right)\n\\mbox{ mit }\nX :=\n\\begin{pmatrix}\n1 & x_{11} & x_{12} \\\\\n\\vdots & \\vdots & \\vdots \\\\\n1 & x_{n 1} & x_{n 2}\n\\end{pmatrix}\n\\mbox{ und }\n\\beta :=\n\\begin{pmatrix}\n\\beta_{0} \\\\\n\\beta_{1} \\\\\n\\beta_{2}\n\\end{pmatrix}.\n\\] Dann gilt \\[\n\\hat{\\beta}\n=\n\\begin{pmatrix}\n\\bar{\\upsilon}-\\hat{\\beta}_{1} \\bar{x}_{1}-\\hat{\\beta}_{2} \\bar{x}_{2} \\\\\nr_{\\upsilon, x_{1} \\backslash x_{2}} \\sqrt{\\frac{1-r_{\\upsilon, x_{2}}^{2}}{1-r_{x_{1}, x_{2}}^{2}}} \\frac{s_{\\upsilon}}{s_{x_{1}}} \\\\\nr_{\\upsilon, x_{2} \\backslash x_{1}} \\sqrt{\\frac{1-r_{\\upsilon, x_{1}}^{2}}{1-r_{x_{2}, x_{1}}^{2}}} \\frac{s_{\\upsilon}}{s_{x_{2}}}\n\\end{pmatrix},\n\\] wobei für \\(1 \\leq k, l \\leq 2\\) und \\(i=1, \\ldots, n\\)\n\n\\(r_{\\upsilon, x_{k} \\backslash x_{l}}\\) die partielle Stichprobenkorrelation der \\(\\upsilon_{i}\\) und \\(x_{i k}\\) gegeben die \\(x_{i l}\\) ist,\n\\(r_{\\upsilon, x_{k}}\\) die Stichprobenkorrelation der \\(\\upsilon_{i}\\) und \\(x_{i k}\\) ist, und\n\\(r_{x_{k}, x_{l}}\\) die Stichprobenkorrelation der \\(x_{ik}\\) und \\(x_{il}\\) ist.\n\n\n\nBeweis. Wir betrachten \\(\\hat{\\beta}_{1}\\), das Resultat für \\(\\hat{\\beta}_{2}\\) folgt dann durch Vertauschen der Indizes. Wir haben in vorherigem Theorem gesehen, dass \\[\n\\hat{\\beta}_{1}=\\frac{r_{\\upsilon, x_{1}}-r_{\\upsilon, x_{2}} r_{x_{1}, x_{2}}}{1-r_{x_{1}, x_{2}}^{2}} \\frac{s_{\\upsilon}}{s_{x_{1}}}\n\\] Weiterhin haben wir in Kapitel 34 gesehen, dass unter der Annahme der multivariaten Normalverteilung von \\(\\upsilon, x_{1}, x_{2}\\) ein Schätzer für die partielle Korrelation von \\(\\upsilon\\) und \\(x_{1}\\) gegeben \\(x_{2}\\) durch \\[\nr_{\\upsilon, 2} = \\frac{r_{\\upsilon, x_{1}}-r_{\\upsilon, x_{2}} r_{x_{1}, x_{2}}}{\\sqrt{1-r_{\\upsilon, x_{2}}^{2}} \\sqrt{1-r_{x_{1}, x_{2}}^{2}}}\n\\] gegeben ist. Für \\(\\hat{\\beta}_{1}\\) ergibt sich somit \\[\n\\begin{aligned}\n\\hat{\\beta}_{1}\n& = \\frac{r_{\\upsilon, x_{1}}-r_{\\upsilon, x_{2}} r_{x_{1}, x_{2}}}{1-r_{x_{1}, x_{2}}^{2}} \\frac{s_{\\upsilon}}{s_{x_{1}}}\n\\\\\\Leftrightarrow\n\\left(1-r_{x_{1}, x_{2}}^{2}\\right) \\hat{\\beta}_{1}\n& = \\left(r_{\\upsilon, x_{1}}-r_{\\upsilon, x_{2}} r_{x_{1}, x_{2}}\\right) \\frac{s_{\\upsilon}}{s_{x_{1}}}\n\\\\\\Leftrightarrow\n\\frac{1-r_{x_{1}, x_{2}}^{2}}{\\sqrt{1-r_{\\upsilon, x_{2}}^{2}} \\sqrt{1-r_{x_{1}, x_{2}}^{2}}} \\hat{\\beta}_{1}\n& =\\frac{r_{\\upsilon, x_{1}}-r_{\\upsilon, x_{2}} r_{x_{1}, x_{2}}}{\\sqrt{1-r_{\\upsilon, _{2}}^{2}} \\sqrt{1-r_{x_{1}, x_{2}}^{2}}} \\frac{s_{\\upsilon}}{s_{x_{1}}}\n\\\\\\Leftrightarrow\n\\frac{1-r_{x_{1}, x_{2}}^{2}}{\\sqrt{1-r_{\\upsilon, x_{2}}^{2}} \\sqrt{1-r_{x_{1}, x_{2}}^{2}}} \\hat{\\beta}_{1}\n& = r_{\\upsilon, x_{1} \\backslash x_{2}} \\frac{s_{\\upsilon}}{s_{x_{1}}}\n\\end{aligned}\n\\] und damit weiter \\[\n\\begin{aligned}\n\\hat{\\beta}_{1}\n& = r_{\\upsilon, x_{1} \\backslash x_{2}} \\frac{\\sqrt{1-r_{\\upsilon, x_{2}}^{2}} \\sqrt{1-r_{x_{1}, x_{2}}^{2}}}{1-r_{x_{1}, x_{2}}^{2}} \\frac{s_{\\upsilon}}{s_{x_{1}}}\n\\\\\\Leftrightarrow\n\\hat{\\beta}_{1}\n& = r_{\\upsilon, x_{1} \\backslash x_{2}} \\frac{\\sqrt{1-r_{\\upsilon, x_{2}}^{2}} \\sqrt{1-r_{x_{1}, x_{2}}^{2}}}{\\left(\\sqrt{1-r_{x_{1}, x_{2}}^{2}}\\right)^{2}} \\frac{s_{\\upsilon}}{s_{x_{1}}}\n\\\\\\Leftrightarrow\n\\hat{\\beta}_{1}\n& = r_{\\upsilon, x_{1} \\backslash x_{2}} \\frac{\\sqrt{1-r_{\\upsilon, x_{2}}^{2}}}{\\sqrt{1-r_{x_{1}, x_{2}}^{2}}} \\frac{s_{\\upsilon}}{s_{x_{1}}}\n\\\\\\Leftrightarrow\n\\hat{\\beta}_{1}\n& = r_{\\upsilon, x_{1} \\backslash x_{2}} \\sqrt{\\frac{1-r_{\\upsilon, x_{2}}^{2}}{1-r_{x_{1}, x_{2}}^{2}}} \\frac{s_{\\upsilon}}{s_{x_{1}}}\n\\end{aligned}\n\\]\n\nIm Allgemeinen gilt für \\(1 \\leq i, l \\leq k\\), also dass \\[\n\\hat{\\beta}_{k} \\neq r_{\\upsilon, x_{k} \\backslash x_{l}}.\n\\] Betaparameterschätzer sind also im Allgemeinen keine partiellen Stichprobenkorrelationen. # Allerdings gilt \\[\n\\hat{\\beta}_{k}=r_{\\upsilon, x_{k} \\backslash x_{l}} \\text { für } 1 \\leq i, l \\leq k\n\\] genau dann, wenn \\[\ns_{\\upsilon}=s_{x_{1}}=s_{x_{2}}\n\\] und außerdem \\[\nr_{\\upsilon, x_{k}}=r_{x_{k}, x_{l}}=0\n\\] wenn also die Stichprobenkorrelationen der Daten und der Werte des zweiten Regressors, sowie die Stichprobenkorrelation der Werte der beiden Regressoren gleich Null sind. Dies kann der Fall sein, wenn einer der Regressoren die Daten “sehr gut erklärt” und der andere Regressor von dem ersten “sehr verschieden” ist. Schließlich gilt obige Identität von Betaparameterschätzerkomponente und partieller Stichprobenkorrelation auch dann, wenn \\[\n\\left|r_{\\upsilon, x_{l}}\\right|=\\left|r_{x_{k}, x_{l}}\\right|,\n\\] wenn also die obigen Stichprobenkorrelationen dem Betrage nach gleich sind. Dies ist in der Anwendung aber vermutlich selten der Fall.\nFolgender R Code demonstriert die Äquivalenz der Matrixschreibweise des Betaparameterschätzers und seiner Darstellung durch Stichprobenmittel, Stichprobenkorrelationen, Stichprobenstandardabweichungen und partielle Stichprobenkorrelationen.\n\nD          = read.csv(\"./_data/411-multiple-regression.csv\")                    # Dateneinlesen\ny          = D$dBDI                                                             # Abhängige Variable\nn          = length(y)                                                          # Anzahl Datenpunkte\nX          = matrix(c(rep(1,n), D$Alter, D$Dauer), nrow = n)                    # Designmatrix\np          = ncol(X)                                                            # Anzahl Parameter\nbeta_hat   = solve(t(X) %*% X) %*% t(X) %*% y                                   # Betaparameterschätzer\neps_hat    = y - X %*% beta_hat                                                 # Residuenvektor\nsigsqr_hat = (t(eps_hat) %*% eps_hat) /(n-p)                                    # Varianzparameterschätzer\n\n# Betaparameterschätzer aus partiellen Korrelationen und Korrelationen\nlibrary(ppcor)                                                                  # partielle Korrelationentoolbox\ny12        = cbind(y,X[,2:3])                                                   # y,x_1,x_2 Matrix\nbars       = apply(y12, 2, mean)                                                # Stichprobenmittel\ns          = apply(y12, 2, sd)                                                  # Stichprobenstandardabweichungen\nr          = cor(y12)                                                           # Stichprobenkorrelationen\npr         = pcor(y12)                                                          # partielle Stichprobenkorrelationen\npr         = pr$estimate                                                        # partielle Stichprobenkorrelationen\nbeta_hat_1 = pr[1,2]*sqrt((1-r[1,3]^2)/(1-r[2,3]^2))*(s[1]/s[2])                # \\hat{\\beta}_1\nbeta_hat_2 = pr[1,3]*sqrt((1-r[1,2]^2)/(1-r[3,2]^2))*(s[1]/s[3])                # \\hat{\\beta}_2\nbeta_hat_0 = bars[1] - beta_hat_1*bars[2] - beta_hat_2*bars[3]                  # \\hat{\\beta}_0\n\n\n\nKorrelationen  r(dPOMS,AGE),r(dPOMS,BRI),r(AGE,BRI)        : -0.87 0.64 -0.22 \nPartielle Korrelationen r(dPOMS,AGE|BRI), r(dPOMS,BRI|AGE) : -0.97 0.92 \nbeta_hat ALM Schätzer                                      : 11.44 -0.57 1.85 \nbeta_hat aus partieller Korrelation                        : 11.44 -0.57 1.85\n\n\nAbbildung 35.2 visualisiert den Beispieldatensatz zusammen mit der durch den Betaparameterschätzer definierten Regressionsebene.\n\\[\nf_{\\hat{\\beta}}: \\mathbb{R}^{2} \\rightarrow \\mathbb{R},\n\\left(x_{1}, x_{2}\\right) \\mapsto f_{\\hat{\\beta}}\\left(x_{1} x, x_{2}\\right)\n:= \\hat{\\beta}_{0}+\\hat{\\beta}_{1} x_{1}+\\hat{\\beta}_{2} x_{2}\n\\]\n\n\n\n\n\n\nAbbildung 35.2: Visualisierung von Beispieldatensatz und Regressionebene",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "411-Multiple-Regression.html#modellevaluation",
    "href": "411-Multiple-Regression.html#modellevaluation",
    "title": "35  Multiple Regression",
    "section": "35.4 Modellevaluation",
    "text": "35.4 Modellevaluation\nDie allgemeine Theorie der T- und F-Statistiken bietet eine Vielzahl von Möglichkeiten, verschiedenste Hypothesen im Kontext der multiplen Regression inferenzstatistisch zu evaluieren. Für das Anwendungsbeispiel könnte zum Beispiel folgende Auswahl von Kontrastgewichtsvektoren und Null- und Alternativenhypothesen zunächst von Interesse sein:\n\\[\nc:=\\begin{pmatrix}\n0 \\\\\n1 \\\\\n0\n\\end{pmatrix}\n\\Leftrightarrow H_{0}: \\beta_{2}=0, H_{A}: \\beta_{2} \\neq 0\n\\mbox{ und }\nc:=\\begin{pmatrix}\n0 \\\\\n0 \\\\\n1\n\\end{pmatrix}\n\\Leftrightarrow H_{0}: \\beta_{3}=0, H_{A}: \\beta_{3} \\neq 0\n\\]\nDabei würde das Ablehnen der jeweiligen Nullhypothese jeweils inferenzstatistische Evidenz für einen Effekt des Patient:innenalters bzw. der Therapiedauer auf die Pre-PostBDI-Differenz im Kontext der Präsenz der jeweils anderen unabhängigen Variable und des Interzeptterms implizieren.\nWeiterhin könnte folgender Kontrastgewichtsvektor mit folgenden Null- und Alternativhypothesen von Interesse sein:\n\\[\nc:=\n\\begin{pmatrix}\n0 \\\\\n1 \\\\\n-1\n\\end{pmatrix}\n\\Leftrightarrow\nH_{0}: \\beta_{2}-\\beta_{3}=0, H_{A}: \\beta_{2}-\\beta_{3} \\neq 0\n\\] In diesem Fall würde das Ablehnen der Nullhypothese inferenzstatistische Evidenz für einen differentiellen Einfluss von Patient:innenalter und Therapiedauer implizieren, je nach Vorzeichen der T-Statistik dabei einen stärkeren Effekt des Patient:innenalters oder der Therapiedauer.\nFolgender R Code evaluiert evaluiert die angesprochenen T-Tests.\n\nD          = read.csv(\"./_data/411-multiple-regression.csv\")                    # Datensatz\ny          = D$dBDI                                                             # Abhängige Variable\nn          = length(y)                                                          # Anzahl Datenpunkte\nX          = matrix(c(rep(1,n), D$Alter, D$Dauer), nrow = n)                    # Designmatrix\np          = ncol(X)                                                            # Anzahl Parameter\nbeta_hat   = solve(t(X) %*% X) %*% t(X) %*% y                                   # Betaparameterschätzer\neps_hat    = y - X %*% beta_hat                                                 # Residuenvektor\nsigsqr_hat = (t(eps_hat) %*% eps_hat) /(n-p)                                    # Varianzparameterschätzer\nC          = cbind(diag(p), matrix(c(0,1,-1), nrow = 3))                        # Kontrastgewichtsvektoren\nste        = rep(NaN, ncol(C))                                                  # Konstraststandardfehler\ntee        = rep(NaN, ncol(C))                                                  # T-Statistiken\npvals      = rep(NaN, ncol(C))                                                  # p-Werte\nfor(i in 1:ncol(C)){                                                            # Kontrastiterationen\n    c        = C[,i]                                                            # Kontrastgewichtsvektor\n    t_num    = t(c)%*%beta_hat                                                  # Zähler der T-Statistik\n    ste[i]   = sqrt(sigsqr_hat*t(c)%*%solve(t(X)%*%X)%*%c)                      # Kontraststandardfehler/Nenner der T-Statistik\n    tee[i]   = t_num/ste[i]                                                     # T-Statistik\n    pvals[i] = 2*(1 - pt(abs(tee[i]),n-p))                                      # p-Wert\n}\n\n\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Interzept)    11.44       4.20    2.73     0.01\nAlter          -0.57       0.04  -16.00     0.00\nDauer           1.85       0.19    9.97     0.00\nAlter-Dauer    -2.42       0.18  -13.37     0.00\n\n\nDer Beispieldatensatz liefert also inferenstatistische Inferenz für einen negativen Zusammenhang zwischen Therapieerfolg und Patient:innenalter und einen positiven Zusammenhang zwischen Therapieerfolg und Therapiedauer. Die Differenz der beiden Effekte ist dabei deutlich ausgeprägt.\nEtwas globaler könnte man einen F-Test basierend auf einer Modellpartition mit \\(p_{0}:=\\) 1 nutzen, um etwa zu klären, ob die Daten die Annahme, dass Patient:innenalter und Therapie überhaupt zur Erklärung der Pre-Post-BDI-Differenzwertvariation, bestätigen. Folgender R Code evaluiert den angesprochenen F-Test.\n\nD          = read.csv(\"./_data/411-multiple-regression.csv\")                    # Datensatz\ny          = D$dBDI                                                             # Abhängige Variable\nn          = length(y)                                                          # Anzahl Datenpunkte\nX          = matrix(c(rep(1,n), D$Alter, D$Dauer), nrow = n)                    # Desigmatrix vollständiges Modell\np          = ncol(X)                                                            # Anzahl Parameter vollständiges Modell\np_0        = 1                                                                  # Anzahl Parameter reduziertes Modell\np_1        = p - p_0                                                            # Anzahl zusätzlicher Parameter im vollst. Modell\nX_0        = X[,1:p_0]                                                          # Designmatrix reduzierters Modell\nbeta_hat_0 = solve(t(X_0)%*%X_0)%*%t(X_0)%*%y                                   # Betaparameterschätzer reduziertes Modell\nbeta_hat   = solve(t(X) %*%X )%*%t(X) %*%y                                      # Betaparameterschätzer vollständiges Modell\neps_hat_0  = y - X_0 %*% beta_hat_0                                             # Residuenvektor reduziertes Modell\neps_hat    = y - X %*% beta_hat                                                 # Residuenvektor vollständiges Modell\neh0_eh0    = t(eps_hat_0) %*% eps_hat_0                                         # RQS reduziertes Modell\neh_eh      = t(eps_hat) %*% eps_hat                                             # RQS vollständiges Modell\nsigsqr_hat = eh_eh/(n-p)                                                        # Varianzparameterschätzer vollst. Modell\nf          = ((eh0_eh0-eh_eh)/p_1)/sigsqr_hat                                   # F-Statistik\npval       = 1 - pf(f,p_1,n-p)                                                  # p-Wert\n\n\n\nF-statistic: 223.7 on 2 and 17 DF p-value:  6.17950135506362e-13\n\n\nDer Beispieldatenzsatz enthält also deutliche Evidenz dafür, dass sowohl Patient:innenalter als auch Therapiedauer zur Erklärung der Pre-Post-BDI-Differenzwertvariation über Patient:innen beiträgt.\nEine direkte Implementation obiger Analyse erlaubt das Zusammenspiel der R Funktionen lm() und summary(), wie untenstehender R Code demonstriert.\n\nD     = read.csv(\"./_data/411-multiple-regression.csv\")                         # Datensatz\nalm   = lm(dBDI ~ Alter + Dauer, data = D)                                      # Modellformulierung und Modellschätzung\nsummary(alm)                                                                    # Modellevaluation\n\n\nCall:\nlm(formula = dBDI ~ Alter + Dauer, data = D)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.9474 -2.0549  0.6748  1.9590  3.7926 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 11.43949    4.19776   2.725   0.0144 *  \nAlter       -0.57162    0.03573 -15.997 1.11e-11 ***\nDauer        1.85132    0.18577   9.966 1.63e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.612 on 17 degrees of freedom\nMultiple R-squared:  0.9634,    Adjusted R-squared:  0.9591 \nF-statistic: 223.7 on 2 and 17 DF,  p-value: 6.179e-13",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "411-Multiple-Regression.html#literaturhinweise",
    "href": "411-Multiple-Regression.html#literaturhinweise",
    "title": "35  Multiple Regression",
    "section": "35.5 Literaturhinweise",
    "text": "35.5 Literaturhinweise\nDie moderne Geschichte der multiplen Regression wird oft in den Arbeiten von Legendre (1805) und Gauss (1809) zur Berechnung von Planetenbahnen verankert. Die heutige Theorie der multiplen Regression sehr breit gefächert, so dass wir in diesem Abschnitt nur einen ersten Eindruck von ihrem Wesen vermitteln können. Weiterführende Einsichten bieten zum Beispiel Draper & Smith (1998), Hocking (2003) und Fox & Tishby (2016).",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "411-Multiple-Regression.html#selbstkontrollfragen",
    "href": "411-Multiple-Regression.html#selbstkontrollfragen",
    "title": "35  Multiple Regression",
    "section": "35.6 Selbstkontrollfragen",
    "text": "35.6 Selbstkontrollfragen\n\nErläutern Sie das Anwendungsszenario und die Ziele der multiplen Regression.\nGeben Sie die Definition des Modells der multiplen Regression wieder.\nErläutern Sie die Begriffe Regressor, Prädiktor, Kovariate und Feature im Rahmen der multiplen Regression.\nErläutern Sie, warum \\(\\hat{\\beta} \\approx\\) Regressorkovariabilität \\({ }^{-1}\\) Regressordatenkovariabilität gilt.\nErläutern Sie den Zusammenhang zwischen Betaparameterschätzern und Korrelationen in einem multiplen Regressionmodell mit Interzeptprädiktor und zwei kontinuierlichen Prädiktoren anhand der Formel \\[\n\\hat{\\beta}_{1}=\\frac{r_{\\upsilon, x_{1}}-r_{\\upsilon, x_{2}} r_{x_{1}, x_{2}}}{1-r_{x_{1}, x_{2}}^{2}} \\frac{s_{\\upsilon}}{s_{x_{1}}}\n\\]\nErläutern Sie den Zusammenhang zwischen Betaparameterschätzern und partieller Korrelation in einem multiplen Regressionmodell mit Interzeptprädiktor und zwei kontinuierlichen Prädiktoren anhand der Formel \\[\n\\hat{\\beta}_{1}=r_{\\upsilon, x_{1} \\backslash x_{2}} \\sqrt{\\frac{1-r_{y}^{2}, x_{2}}{1-r_{x_{1}, x_{2}}^{2}}} \\frac{s_{\\upsilon}}{s_{x_{1}}}\n\\]\n\\(X \\in \\mathbb{R}^{n \\times 2}\\) sei die Designmatrix eines multiplen Regressionsmodells mit zwei Prädiktoren und Betaparametervektor \\(\\beta:=\\left(\\beta_{1}, \\beta_{2}\\right)^{2}\\). Geben Sie den Kontrastgewichtsvektor an, um die Nullhypothese \\(H_{0}: \\beta_{1}=\\beta_{2}\\) mithilfe der T-Statistik zu testen. \n\n\n\n\n\nDraper, N., & Smith, H. (1998). Applied Regression Analysis. Wiley-Interscience.\n\n\nFox, R., & Tishby, N. (2016). Minimum-Information LQG Control Part I: Memoryless Controllers. 5610–5616. https://doi.org/10.1109/CDC.2016.7799131\n\n\nGauss, C. F. (1809). Theoria Motus Corporum Coelestium in Sectionibus Conicis Solem Ambientium. Cambridge University Press.\n\n\nHocking, R. (2003). Methods and Applications of Linear Models - Regression and the Analysis of Variance. Wiley.\n\n\nLegendre, A. M. (1805). Nouvelles Methodes Pour La Determination Des Orbites Des Cometes. Didot Paris.",
    "crumbs": [
      "Das Allgemeine Lineare Modell",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "501-Datendeskription.html",
    "href": "501-Datendeskription.html",
    "title": "36  Datendeskription",
    "section": "",
    "text": "36.1 Datenanalyseszenarien\nWir wollen hier zunächst die im folgenden zu betrachtenden Datenanalyseverfahren anhand der Dimensionalität ihrer unabhängigen Variablen und abhängigen Variablen klassifizieren. Dazu bezeichnen wir wie üblich eine unabhängige Variable mit \\(x\\) und eine abhängige Variable mit \\(y\\). Weiterhin sei mit den Subskripten \\(i\\) und \\(j\\) bei \\(x_{ij}\\) und \\(y_{ij}\\) der Wert der \\(j\\)ten univariaten Komponente der jeweiligen Variable, beispielsweise ein Testwert, bei der \\(i\\)ten experimentellen Einheit, beispielsweise einer Proband:in, bezeichnet. Die Gesamtzahl experimenteller Einheiten sei mit \\(n\\) bezeichnet.\nTabelle 36.1 zeigt das Szenario einer univariaten unabhängigen Variable und einer univariaten abhängigen Variable. Typische in diesem Szenario genutzte Inferenzverfahren sind die Bestimmung der Korrelation von \\(x_1\\) und \\(y_1\\), die Durchführung einer einfachen linearen Regression von \\(y_1\\) auf \\(x_1\\) und, wenn \\(x_1\\) eine kategoriale Faktorvariable ist, T-Tests und Varianzanalysen.\nTabelle 36.2 zeigt das Szenario einer multivariaten unabhängigen Variablen und einer univariaten abhängigen Variablen. Typische in diesem Szenario eingesetzte Inferenzverfahren sind die Bestimmung von multiplen und partiellen Korrelationen zwischen \\(x_1,...,x_m\\) und \\(y_1\\), die Durchführung von multiplen Regressionsanalysen und Kovarianzanalysen und generell alle datenanalytischen Spezialfälle des Allgemeinen Linearen Modells.\nTabelle 36.3 zeigt das im Kontext von Einstichproben-T\\(^2\\)-Tests, der einfaktoriellen multivariaten Varianzanalyse und vielen Szenarien der prädiktiven Modellierung relevante Szenario. In diesem Fall ist die unabhängige Variable univariat und kodiert kategorial ein Faktorlevel bzw. eine Gruppenzugehörigkeit, während die abhängige Variable multivariat ist. Insbesondere in prädiktiven Modellierung wird die unabhängige Variable in diesem Kontext auch als Targetvariable oder Label und die Komponenten der abhängigen Variable als Features bezeichnet.\nTabelle 36.4 schließlich zeigt das Szenario einer multivariaten unabhängigen Variablen und einer multivariaten abhängigen Variablen. Dies ist das datenanalytische Szenario, das wir im Rahmen der Kanonischen Korrelationsanalyse genauer betrachten wollen und das generell durch das Multivariate Allgemeine Lineare Modell abgebildet und datenanalytisch behandelt werden kann.",
    "crumbs": [
      "Multivariate Inferenz",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Datendeskription</span>"
    ]
  },
  {
    "objectID": "501-Datendeskription.html#sec-datenanalyseszenarien",
    "href": "501-Datendeskription.html#sec-datenanalyseszenarien",
    "title": "36  Datendeskription",
    "section": "",
    "text": "Tabelle 36.1: Univariate unabhängige Variable \\(x\\) und univariate abhängige Variable \\(y\\)\n\n\n\n\n\n\\(x_{1}\\)\n\\(y_1\\)\n\n\n\n\n\\(x_{11}\\)\n\\(y_{11}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(x_{n1}\\)\n\\(y_{n1}\\)\n\n\n\n\n\n\n\n\n\n\nTabelle 36.2: Multivariate unabhängige Variable \\(x\\) und univariate abhängige Variable \\(y\\)\n\n\n\n\n\n\\(x_{1}\\)\n\\(\\cdots\\)\n\\(x_{m}\\)\n\\(y_{1}\\)\n\n\n\n\n\\(x_{11}\\)\n\\(\\cdots\\)\n\\(x_{1m}\\)\n\\(y_{11}\\)\n\n\n\\(\\vdots\\)\n\\(\\ddots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(x_{n1}\\)\n\\(\\cdots\\)\n\\(x_{nm}\\)\n\\(y_{nm}\\)\n\n\n\n\n\n\n\n\n\n\nTabelle 36.3: Univariate unabhängige Variable \\(x\\) und multivariate abhängige Variable \\(y\\)\n\n\n\n\n\n\\(x_{1}\\)\n\\(y_{1}\\)\n\\(\\cdots\\)\n\\(y_{m}\\)\n\n\n\n\n\\(x_{11}\\)\n\\(y_{11}\\)\n\\(\\cdots\\)\n\\(y_{1m}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(x_{n1}\\)\n\\(y_{n1}\\)\n\\(\\cdots\\)\n\\(y_{nm}\\)\n\n\n\n\n\n\n\n\n\n\nTabelle 36.4: Multivariate unabhängige Variable \\(x\\) und multivariate abhängige Variable \\(y\\)\n\n\n\n\n\n\\(x_1\\)\n\\(\\cdots\\)\n\\(x_{m_x}\\)\n\\(y_1\\)\n\\(\\cdots\\)\n\\(y_{m_y}\\)\n\n\n\n\n\\(x_{11}\\)\n\\(\\cdots\\)\n\\(x_{1m_x}\\)\n\\(y_{11}\\)\n\\(\\cdots\\)\n\\(y_{1m_y}\\)\n\n\n\\(\\vdots\\)\n\\(\\ddots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\ddots\\)\n\\(\\vdots\\)\n\n\n\\(x_{n1}\\)\n\\(\\cdots\\)\n\\(x_{nm_x}\\)\n\\(y_{n1}\\)\n\\(\\cdots\\)\n\\(y_{nm_y}\\)",
    "crumbs": [
      "Multivariate Inferenz",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Datendeskription</span>"
    ]
  },
  {
    "objectID": "501-Datendeskription.html#sec-deskriptivstatistiken",
    "href": "501-Datendeskription.html#sec-deskriptivstatistiken",
    "title": "36  Datendeskription",
    "section": "36.2 Deskriptivstatistiken",
    "text": "36.2 Deskriptivstatistiken\nWir wollen nun einige Standarddeskriptivstatistiken zur Beschreibung multivariater Datensätze diskutieren. Dazu verallgemeinern wir zunächst die aus der univariaten Deskriptivstatistik bekannten Begriffe des Stichprobenmittels und der Stichprobenvarianz und betrachten dann mit den Mahalanobis-Distanzen multivariate Maße für das Verhältnis von Signal zu Rauschen. Wie immer entwickeln sich diese Begriffe vor dem Hintergrund der Annahme, dass es sich bei beobachteten Daten um Realisierungen entsprechender Zufallsvektoren handelt. Im Gegensatz zu der in Kapitel 36.1 betrachteten und aus dem empirischen Kontext bekannten Organisation von Daten experimenteller Einheiten in Zeilen und ihrer jeweiligen abhängigen Variablenkomponenten in Spalten ist dabei eine Organisation der zu einer experimentellen Einheit gehörenden Variablenkomponenten in Spaltenform zielführender und mit den Schreibweisen des univariaten Falles konsistenter.\n\nStichprobenmittel, -kovarianzmatrix, und -korrelationsmatrix\n\nDefinition 36.1 (Stichprobenmittel, -kovarianmatrix und -korrelationsmatrix) \\(\\upsilon_1,...,\\upsilon_n\\) sei eine Menge von \\(m\\)-dimensionalen Zufallsvektoren, genannt Stichprobe.\n\nOhne Beweis halten wir fest, dass analog zum univariaten Fall das Stichprobenmittel bei unabhängig und identisch verteilten Zufallsvektoren \\(\\upsilon_1,...,\\upsilon_n\\) ein unverzerrter Schätzer des Stichprobenvariablenerwartungswerts \\(\\mathbb{E}(\\upsilon_i) \\in \\mathbb{R}^m, i = 1,...,n\\) ist. Ebenso ist in diesem Fall die Stichprobenkovarianzmatrix ein unverzerrter Schätzer der Stichprobenvariablenkovarianzmatrix \\(\\mathbb{C}(\\upsilon_i) \\in \\mathbb{R}^m, i = 1,...,n\\). Zur konkreten Berechnung von Stichprobenmittel, Stichprobenkovarianzmatrix und Stichprobenkorrrelationsmatrix basierend auf einem Datensatz bieten sich die Aussagen des folgenden Theorems an.\n\nTheorem 36.1 (Datenmatrix und Stichprobenstatistiken)  \nEs sei \\[\\begin{equation}\n\\Upsilon :=\n\\begin{pmatrix}\n\\upsilon_1 & \\cdots & \\upsilon_n\n\\end{pmatrix}\n\\end{equation}\\] eine \\(m \\times n\\) , die durch die spaltenweise Konkatenation von \\(n\\) \\(m\\)-dimensionalen Zufallvektoren \\(\\upsilon_1, ...,\\upsilon_n\\) gegeben sei. Dann ergeben sich\n\n\nBeweis. Die Darstellung des Stichprobenmittels ergibt sich aus \\[\\begin{align}\n\\begin{split}\n\\bar{\\upsilon}\n& := \\frac{1}{n} \\sum_{i=1}^n\\upsilon_i \\\\\n&  = \\frac{1}{n}\\begin{pmatrix} \\sum_{i=1}^n\\upsilon_{i1} \\\\ \\vdots \\\\ \\sum_{i=1}^n\\upsilon_{im} \\end{pmatrix} \\\\\n&  = \\frac{1}{n}\\left(\\begin{pmatrix}\\upsilon_{11}    & \\cdots  &\\upsilon_{n1} \\\\\n                                      \\vdots    & \\ddots  & \\vdots     \\\\\n                                     \\upsilon_{1m}    & \\cdots  &\\upsilon_{nm} \\\\\n                   \\end{pmatrix}\n                   \\begin{pmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{pmatrix}\n              \\right) \\\\\n& = \\frac{1}{n}\\Upsilon 1_{n}.\n\\end{split}\n\\end{align}\\] Hinsichtlich der Darstellung der Stichprobenkovarianzmatrix halten wir zunächst fest, dass nach Definition gilt, dass \\[\\begin{align}\n\\begin{split}\nC  \n& := \\frac{1}{n-1}\\sum_{i=1}^n (\\upsilon_i - \\bar{\\upsilon})(\\upsilon_i - \\bar{\\upsilon})^T \\\\\n&  = \\frac{1}{n-1}\\sum_{i=1}^n \\left(\\upsilon_i\\upsilon_i^T-\\upsilon_i\\bar{\\upsilon}^T - \\bar{\\upsilon}\\upsilon_i^T+ \\bar{\\upsilon}\\bar{\\upsilon}^T\\right) \\\\\n&  = \\frac{1}{n-1}\\left(\\sum_{i=1}^n\\upsilon_i\\upsilon_i^T- \\sum_{i=1}^n\\upsilon_i\\bar{\\upsilon}^T - \\sum_{i=1}^n \\bar{\\upsilon}\\upsilon_i^T+ \\sum_{i=1}^n \\bar{\\upsilon}\\bar{\\upsilon}^T\\right) \\\\\n&  = \\frac{1}{n-1}\\left(\\sum_{i=1}^n\\upsilon_i\\upsilon_i^T- n\\bar{\\upsilon}\\bar{\\upsilon}^T - n\\bar{\\upsilon}\\bar{\\upsilon}^T + n\\bar{\\upsilon}\\bar{\\upsilon}^T\\right) \\\\\n&  = \\frac{1}{n-1}\\left(\\sum_{i=1}^n\\upsilon_i\\upsilon_i^T- n\\bar{\\upsilon}\\bar{\\upsilon}^T\\right).\n\\end{split}\n\\end{align}\\] Mit \\(1_{n}1_{n}^T = 1_{nn}\\) ergibt sich dann weiterhin \\[\\begin{align}\n\\begin{split}\n\\Upsilon\\left(I_n - \\frac{1}{n}1_{nn}\\right)\\Upsilon^T\n& = \\left(\\Upsilon I_n - \\frac{1}{n}\\Upsilon 1_{nn}\\right)\\Upsilon^T                                         \\\\\n& = \\Upsilon\\Upsilon^T - \\frac{1}{n}\\Upsilon 1_{nn}\\Upsilon^T                                                      \\\\\n& = \\begin{pmatrix} \\upsilon_1 & \\cdots & \\upsilon_n\\end{pmatrix} \\begin{pmatrix} \\upsilon_1^T \\\\ \\vdots \\\\ \\upsilon_n^T\\end{pmatrix} - \\frac{1}{n}\\Upsilon 1_n 1_n^T\\Upsilon^T    \\\\\n& = \\sum_{i=1}^n\\upsilon_i\\upsilon_i^T- n\\left(\\frac{1}{n}\\Upsilon 1_n\\right)\\left(\\frac{1}{n}1_n^T\\Upsilon^T\\right)              \\\\\n& = \\sum_{i=1}^n\\upsilon_i\\upsilon_i^T- n\\bar{\\upsilon}\\bar{\\upsilon}^T                                                          \\\\\n& = \\frac{1}{n-1}\\sum_{i=1}^n (\\upsilon_i - \\bar{\\upsilon})(\\upsilon_i - \\bar{\\upsilon})^T \\\\\n& = C.\n\\end{split}\n\\end{align}\\] Hinsichtlich der Korrelationsmatrix ergibt sich nach Definition und für ein beliebiges Indexpaar \\(i,j\\) mit \\(1 \\le i,j \\le m\\) schließlich, dass \\[\\begin{align}\n\\begin{split}\nR_{{y}_{ij}}\n& = \\frac{(C)_{ij}}{\\sqrt{ (C)_{ii}}\\sqrt{ (C)_{jj}}}             \\\\\n& = \\frac{1}{\\sqrt{(C)_{ii}}}(C)_{ij}\\frac{1}{\\sqrt{(C)_{jj}}}    \\\\\n& = (DCD)_{ij}.\n\\end{split}\n\\end{align}\\]\n\nFolgender R Code wendet die in Theorem 36.1 diskutierten Resultate auf einen Beispieldatensatz mit Datendimensionalität \\(m = 4\\) und Anzahl experimenteller Einheiten \\(n := 12\\) an.\n\n# Simulation eines Datensatzes\nlibrary(MASS)                                                                   # multivariate Normalverteilung\nm       = 4                                                                     # Datenvektordimension\nn       = 12                                                                    # Anzahl Datenvektoren\nmu      = matrix(c(1,2,3,4))                                                    # Erwartungswertparameter\nSigma   = diag(4)                                                               # Kovarianzmatrixparameter\nY       = t(mvrnorm(n, mu,Sigma))                                               # Datensatzrealisierung    \n\n\n\n         [,1]     [,2]      [,3]       [,4]      [,5]     [,6]      [,7]\n[1,] 0.380757 3.206102 0.7449730 -0.4244947 0.8556004 1.207538 3.3079784\n[2,] 1.681932 1.070638 0.5125397  0.9248077 3.0000288 1.378733 0.6155732\n[3,] 1.085641 4.176583 1.3350276  2.5364696 1.8840799 2.249181 5.0871665\n[4,] 3.318340 3.675730 4.0601604  3.4111055 4.5314962 2.481606 4.3065579\n         [,8]     [,9]     [,10]     [,11]    [,12]\n[1,] 1.105802 1.456999 0.9228471 0.6659992 0.965274\n[2,] 3.869291 2.425100 1.7613529 3.0584830 2.886423\n[3,] 3.017396 1.713699 1.3593945 3.4501871 2.981440\n[4,] 2.463550 3.699024 3.4717201 3.3479052 3.943103\n\n\n\n# Auswertung von Deskriptivstatistiken\nn       = ncol(Y)                                                               # Anzahl Datenvektorealisierungen\nI_n     = diag(n)                                                               # Einheitsmatrix I_n\nJ_n     = matrix(rep(1,n^2), nrow = n)                                          # 1_{nn}\ny_bar   = (1/n)* Y %*% J_n[,1]                                                  # Stichprobenmittel\nC       = (1/(n-1))*(Y %*% (I_n-(1/n)*J_n) %*% t(Y))                            # Stichprobenkovarianzmatrix\nD       = diag(1/sqrt(diag(C)))                                                 # Kov-Korr-Transformationsmatrix\nR       = D %*% C %*% D                                                         # Stichprobenkorrelationsmatrix\n\n\n\n         [,1]\n[1,] 1.199615\n[2,] 1.932075\n[3,] 2.573022\n[4,] 3.559191\n\n\n           [,1]        [,2]        [,3]        [,4]\n[1,]  1.1450788 -0.29291622  0.91837375  0.16929650\n[2,] -0.2929162  1.20170927 -0.09630563 -0.16923746\n[3,]  0.9183737 -0.09630563  1.50573951  0.08718263\n[4,]  0.1692965 -0.16923746  0.08718263  0.40266165\n\n\n           [,1]        [,2]        [,3]       [,4]\n[1,]  1.0000000 -0.24970431  0.69940198  0.2493218\n[2,] -0.2497043  1.00000000 -0.07159407 -0.2432913\n[3,]  0.6994020 -0.07159407  1.00000000  0.1119657\n[4,]  0.2493218 -0.24329134  0.11196568  1.0000000\n\n\n\n\nMahalanobis-Distanzen\nAbschließend wollen wir mit den sogenannten Mahalanobis-Distanzen multivariate Generalisierungen von aus der univariaten Anwendung bekannten Signal-zu-Rauschen-Maßen betrachten. Wir definieren den Begriff der Mahalanobis-Distanz wie folgt.\n\nDefinition 36.2 (Mahalanobis-Distanzen) \\(\\xi_1\\) sei ein Zufallsvektor, eine Realisation eines Zufallsvektors, ein multivariater Erwartungswert oder ein multivariates Stichprobenmittel, \\(\\xi_2\\) sei ein Zufallsvektor, eine Realisation eines Zufallsvektors, ein multivariater Erwartungswert oder ein multivariates Stichprobenmittel und \\(\\Xi\\) sei eine Kovarianzmatrix oder eine Stichprobenkovarianzmatrix. Dann heißt \\[\\begin{equation}\nD = \\left(\\xi_1 - \\xi_2 \\right)^T\\Xi^{-1}\\left(\\xi_1 - \\xi_2\\right)\n\\end{equation}\\] von \\(\\xi_1\\) und \\(\\xi_2\\) hinsichtlich \\(\\Xi\\).\n\nEine Mahalanobis-Distanz ist damit eine durch eine Kovarianzmatrix normalisierte quadrierte Euklidische Distanz (vgl. Kapitel 8.2). Ähnliche Maße für das Verhältnis eines Abstandes und einer Variabilität sind bekanntlich die \\(z\\)-Transformation \\(z = (y - \\mu)/\\sigma\\) für \\(y \\in \\mathbb{R}\\) und die Parameter \\(\\mu,\\sigma^2&gt;0\\) einer univariaten Normalverteilung sowie Cohen’s \\(d = (\\bar{\\upsilon}_1-\\bar{\\upsilon}_2)/s_{12}\\) für zwei Stichprobenmittel \\(\\bar{\\upsilon}_1\\) und \\(\\bar{\\upsilon}_2\\) und ihre korrespondierende gepoolte Stichprobenstandardabweichung \\(s_{12}\\). Im Unterschied zur Mahalanobis-Distanz sind diese Maße allerdings nicht quadriert und damit Vorzeichen behaftet. In Analogie zur \\(z\\)-Transformation oder zu Cohen’s \\(d\\) wird allerdings auch bei Mahalanobis-Distanzen ein Abstand in Einheiten von Variabilität gemessen. Bei Cohen’s \\(d\\) bedeutet ja ein Wert von \\(d = 1\\) gerade, dass der Abstand von \\(\\bar{\\upsilon}_1\\) und \\(\\bar{\\upsilon}_2\\) eine gepoolte Standardabweichung beträgt. Ebenso verhält es sich mit den Mahalanobis-Distanzen.\nAnhand von Abbildung 36.1 und Abbildung 36.2 wollen wir den Einfluss der Varianz und der Kovarianz von Komponenten der \\(\\xi_1\\) und \\(\\xi_2\\) auf ihre Mahalanobis-Distanz noch etwas genauer betrachten. Die Titel der Unterabbildungen von Abbildung 36.1 zeigen die Mahalanobis-Distanzen der Vektoren \\(\\xi_1 := (-1,-1)^T\\) und \\(\\xi_2 := (1,1)^T\\) bei Kovarianzmatrizen von \\[\\begin{equation}\n\\Sigma_1 := \\begin{pmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0\\end{pmatrix},\n\\Sigma_2 := \\begin{pmatrix} 0.5 & 0.0 \\\\ 0.0 & 0.5\\end{pmatrix} \\mbox{ und }\n\\Sigma_3 := \\begin{pmatrix} 1.5 & 0.0 \\\\ 0.0 & 1.5\\end{pmatrix},\n\\end{equation}\\] die mithilfe von Normalverteilungsisokonturen dargestellt sind. Für \\(\\Sigma_1\\) entspricht die Mahalanobis-Distanz dabei der quadrierten Euklidischen Distanz von \\(\\xi_1\\) und \\(\\xi_2\\). An der Darstellung zu \\(\\Sigma_2\\) erkennt man, dass im Fall sphärischer Kovarianzmatrizen eine geringere Komponentenvarianz von \\(\\xi_1\\) und \\(\\xi_2\\) zu einer größeren Mahalanobis-Distanz führt. Umgekehrt erkennt man an der Darstellung zu \\(\\Sigma_3\\), dass im Fall sphärischer Kovarianzmatrizen eine höhere Komponentenvarianz von \\(\\xi_1\\) und \\(\\xi_2\\) in einer kleineren Mahalanobis-Distanz resultiert. Intuitiv nähert die Komponentenvarianz die Komponenten also an.\n\n\n\n\n\n\nAbbildung 36.1: Mahalanobis-Distanzen als Funktion von Komponentenvarianzen\n\n\n\nDie Titel der Unterabbildungen von Abbildung 36.2 zeigen die Mahalanobis-Distanzen derselben Vektoren bei Kovarianzmatrizen von \\[\\begin{equation}\n\\Sigma_1 := \\begin{pmatrix} 1.0 & 0.0       \\\\ 0.0 & 1.0\\end{pmatrix},\n\\Sigma_2 := \\begin{pmatrix} 1.0 & 0.9       \\\\ 0.9 & 1.0\\end{pmatrix}\\mbox{ und }\n\\Sigma_3 := \\begin{pmatrix*}[r] 1.0 & -0.9  \\\\ -0.9 & 1.0\\end{pmatrix*},\n\\end{equation}\\] Für \\(\\Sigma_1\\) entspricht dabei wiederrum die die Mahalanobis-Distanz der quadrierten Euklidischen Distanz von \\(\\xi_1\\) und \\(\\xi_2\\). An der Darstellung zu \\(\\Sigma_2\\) erkennt man, dass eine stark positive Kovarianz der Komponenten von \\(\\xi_1\\) und \\(\\xi_2\\) in einer kleineren Mahalanobis-Distanz resultiert. Umgekehrt erkennt man an der Darstellung zu \\(\\Sigma_3\\), dass eine stark negative Kovarianz der Komponenten von \\(\\xi_1\\) und \\(\\xi_2\\) zu einer größeren Mahalanobis-Distanz führt. Intuitiv nähert also auch die Kovarianz von Komponenten \\(\\xi_1\\) und \\(\\xi_2\\) an. Alternativ kann man die Höhe einer Mahalanobis-Distanz dabei auch als ein Maß für die Unwahrscheinlichkeit der Realisierung zweier Werte eines Zufallsvektors bei einer gegebenen Kovarianzmatrix verstehen.\n\n\n\n\n\n\nAbbildung 36.2: Mahalanobis-Distanzen als Funktion von Komponentenkovarianzen",
    "crumbs": [
      "Multivariate Inferenz",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Datendeskription</span>"
    ]
  },
  {
    "objectID": "501-Datendeskription.html#literaturhinweise",
    "href": "501-Datendeskription.html#literaturhinweise",
    "title": "36  Datendeskription",
    "section": "36.3 Literaturhinweise",
    "text": "36.3 Literaturhinweise\nDie Resultate zur Matrixdarstellung von Stichprobenmittel, Stichprobenkovarianzmatrix und Stichprobenkorrelationsmatrix folgen Rencher & Christensen (2012). Der Begriff der Mahalanobis-Distanz geht zurück auf Mahalanobis (1936).",
    "crumbs": [
      "Multivariate Inferenz",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Datendeskription</span>"
    ]
  },
  {
    "objectID": "501-Datendeskription.html#selbstkontrollfragen",
    "href": "501-Datendeskription.html#selbstkontrollfragen",
    "title": "36  Datendeskription",
    "section": "36.4 Selbstkontrollfragen",
    "text": "36.4 Selbstkontrollfragen\n\nErläutern Sie vier prinzipielle Datenanalyseszenarien anhand der Dimensionalität ihrer unabhängigen und abhängigen Variablen. Nennen Sie Beispiele für die in dem jeweiligen Szenario häufig eingesetzten Datenanalyseverfahren.\nGeben Sie die Definition des Stichprobenmittels wieder.\nGeben Sie die Definition der Stichprobenkovarianzmatrix wieder.\nGeben Sie die Definition des Stichprobenkorrelationsmatrix wieder.\nGeben Sie das Theorem zu Datenmatrix und Stichprobenstatistiken wieder.\nGeben Sie die Definition einer Mahalanobis-Distanz wieder.\nErläutern Sie die intuitive Bedeutung einer Mahalanobis-Distanz.\n\n\n\n\n\nMahalanobis, S. A. (1936). Reprint of: Mahalanobis, P.C. (1936) \"On the Generalised Distance in Statistics.\". 80(S1), 1–7. https://doi.org/10.1007/s13171-019-00164-5\n\n\nRencher, A. C., & Christensen, W. F. (2012). Methods of Multivariate Analysis (Third Edition). Wiley.",
    "crumbs": [
      "Multivariate Inferenz",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Datendeskription</span>"
    ]
  },
  {
    "objectID": "502-Einstichproben-T2-Tests.html",
    "href": "502-Einstichproben-T2-Tests.html",
    "title": "37  Einstichproben-T\\(^2\\)-Tests",
    "section": "",
    "text": "37.1 Anwendungsszenario\nWie im univariaten Fall ist das Anwendungsszenario eines Einstichproben-T\\(^2\\)-Tests dadurch gekennzeichnet, dass \\(n\\) Datenpunkte einer Stichprobe (Gruppe) randomisierter experimenteller Einheiten betrachtet werden. In Generalisierung des univariaten Falls sind die \\(n\\) Datenpunkte allerdings multivariat, jeder Datenpunkt besteht also aus zwei oder mehr Zahlen und kann als Vektor in \\(\\mathbb{R}^m\\) mit \\(m&gt;1\\) betrachtet werden. In Analogie zum univariaten Fall wird von den \\(n\\) Datenpunkten angenommen, dass sie Realisierungen von \\(n\\) unabhängigen und identisch multivariat normalverteilten Zufallsvektoren sind. Hinsichtlich der identischen multivariaten Normalverteilung \\(N(\\mu,\\Sigma)\\) dieser Zufallsvektoren wird angenommen, dass sowohl der wahre Erwartungswertparameter \\(\\mu\\) als auch der wahre Kovarianzmatrixparameter \\(\\Sigma\\) unbekannt sind. Schließlich wird voraussgesetzt, dass ein Interesse an einem inferentiellen Vergleich des wahren, aber unbekannten, Erwartungswertparameters \\(\\mu\\) mit einem vorgegebenen Wert \\(\\mu_0\\), beispielsweise \\(\\mu_0 := 0_m\\), besteht. Wie im univariaten Fall ergeben sich auch in diesem Anwendungsszenario eine Reihe möglicher Hypothesenszenarien mit jeweils unterschiedlichen Testgütefunktionen und damit Herangehensweisen an Testumfangkontrolle und Stichprobengrößenoptimierung. Wir wollen im Rahmen dieser Einführung nur das Szenario einer einfachen Nullhypothese und einer zusammengesetzten Alternativhypothese, \\[\nH_0 :\\mu = \\mu_0 \\Leftrightarrow \\Theta_0 :=  \\{\\mu_0\\}\n\\mbox{ und }\nH_1 : \\mu \\neq \\mu_0 \\Leftrightarrow \\Theta_1 :=  \\mathbb{R}^m \\setminus \\{\\mu_0\\}\n\\tag{37.1}\\] genauer untersuchen.\nAnwendungsbeispiel\nFür ein konkretes Anwendungsbeispiel betrachten wir die Analyse simulierter\nPrä-Post-Interventions-Differenzwerte von BDI Scores (dBDI) und Glukokortikoidplasmaleveln (dGLU), die, wie in Tabelle 38.1 dargestellt, an einer Gruppe von \\(n = 20\\) Patient:innen erhoben worden sein könnten. Positive Werte von dBDI und dGLU entsprächen dabei einer Reduktion der Depressionssymptomatik, negative Werte zeigen eine Verschlechterung des Depressionszustandes an.\nBei der Anwendung eines Einstichproben-T\\(^2\\)-Tests auf die Daten dieses simulierten Datensatzes nehmen wir an, dass die zweidimensionalen Datenvektoren (dBDI, dGLU) Realisierungen von \\(n = 20\\) unabhängig normalverteilten zweidimensionalen Zufallsvektoren \\(\\upsilon_i \\sim N(\\mu,\\Sigma)\\) sind. Wir nehmen weiterhin an, dass wir daran interessiert sind, unsere Unsicherheit beim inferentiellen Vergleich des wahren, aber unbekannten, Erwartungswertparameters \\(\\mu \\in \\mathbb{R}^2\\) mit einem Vergleichswert \\(\\mu_0 \\in \\mathbb{R}^2\\), etwa einem Therapieerfolgsnormwert, zu quantifizieren.\nTabelle 37.1: Prä-Post-Interventions-Differenzwerte von BDI Scores und Glukokortikoidplasmaleveln von \\(n = 20\\) Patient:innen\n\n\n\n\n\n\ndBDI\ndGLU\n\n\n\n\n35\n6.1\n\n\n25\n4.0\n\n\n20\n1.7\n\n\n29\n2.6\n\n\n29\n1.9\n\n\n17\n0.9\n\n\n33\n2.0\n\n\n28\n4.1\n\n\n26\n3.9\n\n\n31\n3.8\n\n\n14\n2.1\n\n\n18\n2.0\n\n\n19\n5.0\n\n\n28\n2.6\n\n\n20\n2.1\n\n\n35\n4.4\n\n\n28\n4.0\n\n\n32\n3.9\n\n\n32\n1.0\n\n\n25\n1.9\nUnabhängig von diesem inferenzstatistischen Vorgehen betrachten wir zunächst zu diesem Datensatz einige Deskriptivstatistiken wie durch folgenden R Code ausgewertet und in Abbildung 37.1 dargestellt. Verglichen mit einem Therapienormwert von \\(\\mu_0 := (30,3.5)^T\\) fallen die Komponenten des Stichprobenmittels mit \\(\\bar{\\upsilon} = (26.3, 3.0)^T\\) etwas geringer aus, allerdings bei einer nicht zu vernachlässigen Datenvariabilität, die sich in einer Mahalanobisdistanz von \\(D = 0.4\\) des Stichprobenmittels vom Therapienormwert in bezug auf die Stichprobenkovarianzmatrix des Datensatzes wiederspiegelt.\n# Datendeskription\nD      = read.csv(\"_data/502-einstichproben-t2-tests.csv\")                      # Daten einlesen\nY      = rbind(D$y_1i, D$y_2i)                                                  # Datenmatrix               \nmu_0   = matrix(c(30,3.5), nrow = 2)                                            # Normwert\nn      = ncol(Y)                                                                # Anzahl Datenpunkte\nj_n    = matrix(rep(1,n), nrow = n)                                             # 1_n\nI_n    = diag(n)                                                                # I_n\nJ_n    = matrix(rep(1,n^2), nrow = n)                                           # 1_{nn}\nY_bar  = (1/n)*(Y %*% j_n)                                                      # Stichprobenmittel  \nC      = (1/(n-1))*(Y %*% (I_n-(1/n)*J_n) %*% t(Y))                             # Stichprobenkovarianzmatrix  \nD      = t(Y_bar - mu_0) %*% solve(C) %*% (Y_bar - mu_0)                        # Mahalanobis Distanz\nY_bar = 26.25615 2.991039 \nD     = 0.3773184",
    "crumbs": [
      "Multivariate Inferenz",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Einstichproben-T$^2$-Tests</span>"
    ]
  },
  {
    "objectID": "502-Einstichproben-T2-Tests.html#sec-anwendungsszenario",
    "href": "502-Einstichproben-T2-Tests.html#sec-anwendungsszenario",
    "title": "37  Einstichproben-T\\(^2\\)-Tests",
    "section": "",
    "text": "Abbildung 37.1: Deskriptivstatisken der dBDI, dGLU Daten des Beispieldatensatzes. Jeder Punkt visualisiert die Daten einer Patient:in, die Stichprobenkovarianz ist durch die 0.4 Isokontur einer zweidimensionalen Normalverteilung mit Erwartungswertparameter und Kovarianzmatrixparameter entsPrächend dem Stichprobenmittel und der Stichprobenkovarianz dargestellt",
    "crumbs": [
      "Multivariate Inferenz",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Einstichproben-T$^2$-Tests</span>"
    ]
  },
  {
    "objectID": "502-Einstichproben-T2-Tests.html#sec-modellformulierung-modellevaluation",
    "href": "502-Einstichproben-T2-Tests.html#sec-modellformulierung-modellevaluation",
    "title": "37  Einstichproben-T\\(^2\\)-Tests",
    "section": "37.2 Modellformulierung und Modellevaluation",
    "text": "37.2 Modellformulierung und Modellevaluation\nWir definieren zunächst das Einstichproben-T\\(^2\\)-Test-Modell wie folgt.\n\nDefinition 37.1 (Einstichproben-T\\(^2\\)-Test-Modell) Für \\(i = 1,...,n\\) seien \\(\\upsilon_i\\) \\(m\\)-dimensionale Zufallsvektoren, die die \\(n\\) Datenpunkte eines Einstichproben-T\\(^2\\)-Test Szenarios modellieren. Dann hat das Einstichproben-T\\(^2\\)-Test-Modell die strukturelle Form \\[\n\\upsilon_i = \\mu + \\varepsilon_i\n\\mbox{ mit } \\varepsilon_i \\sim N(0_m, \\Sigma) \\mbox{ u.i.v. für } i = 1,...,n\n\\mbox{ mit } \\mu \\in \\mathbb{R}^m,  \\Sigma \\in \\mathbb{R}^{m \\times m} \\mbox{ pd}\n\\tag{37.2}\\] und die Datenverteilungsform \\[\\begin{equation}\n\\upsilon_i \\sim N(\\mu, \\Sigma) \\mbox{ u.i.v. für } i = 1,...,n\n\\mbox{ mit } \\mu \\in \\mathbb{R}^m, \\Sigma \\in \\mathbb{R}^{m \\times m} \\mbox{ pd}.\n\\end{equation}\\]\n\nDie Äquivalenz von struktureller Form und Datenverteilungsform des Einstichproben-T\\(^2\\)-Test-Modells folgt dabei direkt mit Theorem 20.4 durch Transformation der Zufallsvektoren \\(\\varepsilon_i\\) anhand von Gleichung 37.2.",
    "crumbs": [
      "Multivariate Inferenz",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Einstichproben-T$^2$-Tests</span>"
    ]
  },
  {
    "objectID": "502-Einstichproben-T2-Tests.html#sec-modellevaluation",
    "href": "502-Einstichproben-T2-Tests.html#sec-modellevaluation",
    "title": "37  Einstichproben-T\\(^2\\)-Tests",
    "section": "37.3 Modellevaluation",
    "text": "37.3 Modellevaluation\n\nTeststatistik und Test\nWir definieren als nächstes eine Teststatistik für das Einstichproben-T\\(^2\\)-Test Szenario.\n\nDefinition 37.2 (Einstichproben-T\\(^2\\)-Teststatistik) Gegeben seien das Einstichproben-T\\(^2\\)-Test-Modell und ein Nullhypothesenparameter \\(\\mu_0 \\in \\mathbb{R}^m\\). Dann ist die Einstichproben-T\\(^2\\)-Teststatistik definiert als \\[\\begin{equation}\nT^2 := n(\\bar{\\upsilon} - \\mu_0)^T C^{-1}(\\bar{\\upsilon} - \\mu_0),\n\\end{equation}\\] wobei \\(\\bar{\\upsilon}\\) und \\(C\\) das Stichprobenmittel und die Stichprobenkovarianzmatrix der \\(\\upsilon_1,...,\\upsilon_n\\) bezeichnen.\n\nDie Einstichproben-T\\(^2\\)-Teststatistik ist offenbar die mit dem Stichprobenumfang \\(n\\) skalierte Mahalanobis-Distanz von \\(\\bar{\\upsilon}\\) und \\(\\mu_0\\) hinsichtlich \\(C\\) (vgl. Kapitel 36.2). Damit gilt entsprechend, dass bei konstanter Stichprobenkovarianzmatrix die Einstichproben-T\\(^2\\)-Test Teststatistik \\(T^2\\) größere Werte für eine größere Euklidische Distanz von \\(\\bar{\\upsilon}\\) und \\(\\mu_0\\) annimmt und bei konstanter Euklidischer Distanz von \\(\\bar{\\upsilon}\\) und \\(\\mu_0\\) der Wert der Teststatistik \\(T^2\\) von der Höhe der Datenvariabilität abhängt. Hinsichtlich der Verteilung der Einstichproben-T\\(^2\\)-Teststatistik halten wir zunächst folgendes Theorem fest, das wir nicht beweisen wollen.\n\nTheorem 37.1 (Verteilung der skalierten Einstichproben-T\\(^2\\)-Teststatistik) Es seien \\(\\upsilon_1,...,\\upsilon_n \\sim N(\\mu,\\Sigma)\\) mit \\(\\mu \\in \\mathbb{R}^m\\) und \\(\\Sigma \\in \\mathbb{R}^{m\\times m} \\mbox{pd}\\), \\[\n\\nu:= \\frac{n-m}{(n-1)m}\n\\tag{37.3}\\] und für \\(\\mu \\in \\mathbb{R}^m\\) sei die Einstichproben-T\\(^2\\)-Teststatistik definiert als \\[\\begin{equation}\nT^2 := n(\\bar{\\upsilon} - \\mu_0)^T C^{-1} (\\bar{\\upsilon} - \\mu_0).\n\\end{equation}\\] Dann gilt \\[\n\\nu T^2 \\sim f(\\delta, m, n-m),\n\\tag{37.4}\\] wobei \\(f(\\delta,m,n-m)\\) die nichtzentrale \\(f\\)-Verteilung mit Nichtzentralitätsparameter \\[\n\\delta := n(\\mu - \\mu_0)^T\\Sigma^{-1}(\\mu - \\mu_0)\n\\tag{37.5}\\] sowie mit Freiheitsgradparametern \\(m\\) und \\(n-m\\) bezeichnet.\n\nFür einen Beweis von Theorem 37.1 verweisen wir auf Hotelling (1931) und Anderson (2003). Wir erinnern in diesem Zusammenhang an die Begriffe der \\(f\\)-Zufallsvariable und der nichtzentralen \\(f\\)-Zufallsvariable, für die wir exemplarische WDFen in Abbildung 37.2 und Abbildung 37.3 darstellen.\n\nDefinition 37.3 (\\(f\\)-Zufallsvariable) \\(\\xi\\) sei eine Zufallsvariable mit Ergebnisraum \\(\\mathbb{R}_{&gt;0}\\) und WDF \\[\\begin{equation}\np_\\xi : \\mathbb{R} \\to \\mathbb{R}_{&gt;0}, x \\mapsto p_\\xi(x)\n:= \\nu_1^{\\frac{\\nu_1}{2}}\\nu_2^{\\frac{\\nu_2}{2}}\n   \\frac{\\Gamma\\left(\\frac{\\nu_1+\\nu_2}{2}\\right)}{\\Gamma\\left(\\frac{\\nu_1}{2}\\right)\\Gamma\\left(\\frac{\\nu_2}{2}\\right)}\n   \\frac{x^{\\frac{\\nu_1}{2}-1}}{\\left(\\nu_1 x  + \\nu_2 \\right)^{\\frac{\\nu_1+\\nu_2}{2}}},\n\\end{equation}\\] wobei \\(\\Gamma\\) die Gammafunktion bezeichne. Dann sagen wir, dass \\(\\xi\\) einer \\(f\\)-Verteilung mit Freiheitsgradparametern \\(\\nu_1\\) und \\(\\nu_2\\) unterliegt und nennen \\(\\xi\\) eine \\(f\\)-Zufallsvariable mit Freiheitsgradparametern \\(\\nu_1\\) und \\(\\nu_2\\). Wir kürzen dies mit \\(\\xi \\sim f(\\nu_1,\\nu_2)\\) ab. Die WDF einer \\(f\\)-Zufallsvariable bezeichnen wir mit \\(f(x;\\nu_1,\\nu_2)\\), die KVF einer \\(f\\)-Zufallsvariable bezeichnen wir mit \\(F(x;\\nu_1,\\nu_2)\\), und die inverse KVF einer \\(f\\)-Zufallsvariable bezeichnen wir mit \\(F^{-1}(x;\\nu_1,\\nu_2)\\).\n\n\n\n\n\n\n\nAbbildung 37.2: Exemplarische WDFen einer \\(f\\)-Zufallsvariable\n\n\n\n\nDefinition 37.4 (Nichtzentrale \\(f\\)-Zufallsvariable) \\(\\xi\\) sei eine Zufallsvariable mit Ergebnisraum \\(\\mathbb{R}_{&gt;0}\\) und WDF \\[\\begin{multline}\np_\\xi : \\mathbb{R} \\to \\mathbb{R}_{&gt;0}, x \\mapsto \\\\\np_\\xi(x)\n:= \\sum_{k=0}^\\infty \\frac{e^{-\\delta/2}(\\delta/2)^k}{\\frac{\\Gamma(\\nu_2/2)\\Gamma(\\nu_1/2 + k)}{\\Gamma(\\nu_2/2 + \\nu_1/2 + k)}k!}\n    \\left(\\frac{\\nu_1}{\\nu_2}\\right)^{\\nu_1/2 + k}\n    \\left(\\frac{\\nu_2}{\\nu_2+\\nu_1x}\\right)^{(\\nu_1+\\nu_2)/2 + k}\n    x^{\\nu_1/2 - 1 + k}\n\\end{multline}\\] wobei \\(\\Gamma\\) die Gammafunktion bezeichne. Dann sagen wir, dass \\(\\xi\\) einer nichtzentralen \\(f\\)-Verteilung mit Nichtzentralitätsparameter \\(\\delta\\) und Freiheitsgradparametern \\(\\nu_1\\) und \\(\\nu_2\\) unterliegt und nennen \\(\\xi\\) eine nichtzentrale \\(f\\)-Zufallsvariable mit Nichtzentralitätsparameter \\(\\delta\\) und Freiheitsgradparametern \\(\\nu_1\\) und \\(\\nu_2\\). Wir kürzen dies mit \\(\\xi \\sim f(\\delta,\\nu_1,\\nu_2)\\) ab. Die WDF einer \\(f\\)-Zufallsvariable bezeichnen wir mit \\(f(x;\\delta,\\nu_1,\\nu_2)\\), die KVF einer nichtzentralen \\(f\\)-Zufallsvariable bezeichnen wir mit \\(F(x;\\delta,\\nu_1,\\nu_2)\\), und die inverse KVF einer nichtzentralen \\(f\\)-Zufallsvariable bezeichnen wir mit \\(F^{-1}(x;\\delta,\\nu_1,\\nu_2)\\).\n\n\n\n\n\n\n\nAbbildung 37.3: Exemplarische WDFen einer nichtzentralen \\(f\\)-Zufallsvariable\n\n\n\nIm univariaten Fall sind bekanntlich die \\(F\\)-Statistiken der Varianzanalyse bei Zutreffen der Nullhypothese \\(f\\)-verteilt und bei Zutreffen der Alternativhypothese nichtzentral-\\(f\\)-verteilt. Für den Fall \\(\\mu = \\mu_0\\), dass also der wahre, aber unbekannte Erwartungswertparameter mit dem Nullhypothesenparameter identisch ist, gilt nach Gleichung 37.5 \\(\\delta = 0\\) und \\(f(\\delta,m,n-m)\\) entspricht der \\(f\\)-Verteilung \\(f(m,n-m)\\).\nWir halten weiterhin fest, dass aus Theorem 37.1 im univariaten Fall \\(m := 1\\) aus Gleichung 37.3 folgt, dass \\[\\begin{equation}\n\\nu = \\frac{n-1}{(n-1)\\cdot 1} = 1\n\\end{equation}\\] und mit der Stichprobenvarianz \\(S^2\\) einer univariaten Stichprobe entsPrächend folgt, dass \\[\\begin{equation}\nT^2 = n\\frac{(\\bar{\\upsilon} - \\mu_0)^2}{S^2} = \\left(\\sqrt{n}\\frac{\\bar{\\upsilon} - \\mu_0}{S} \\right)^2.\n\\end{equation}\\] Dies ist offenbar das Quadrat der bekannten univariaten Einstichproben-T-Test Teststatistik. Damit ist nach Theorem 37.1 das Quadrat der univariaten Einstichproben-T-Test Teststatistik nach \\(f(\\delta,1,n-1)\\) verteilt. Intuitiv und verkürzt ausgedrückt ist also eine quadrierte \\(t\\)-Zufallsvariable eine \\(f\\)-Zufallsvariable.\nAus Theorem 37.1 folgen nun für Einstichproben-T\\(^2\\)-Teststatistik unmittelbar folgende Formen für ihre WDF und KVF.\n\nTheorem 37.2 (WDF und KDF der Einstichproben-T\\(^2\\)-Teststatistik) Im Einstichproben-T\\(^2\\)-Test Szenario sei \\[\\begin{equation}\n\\nu:= \\frac{n-m}{(n-1)m}\n\\end{equation}\\] Dann ist eine WDF der Einstichproben-T\\(^2\\)-Teststatistik gegeben durch \\[\\begin{equation}\\label{eq:pT1}\np_{T^2} : \\mathbb{R}_{\\ge 0} \\to \\mathbb{R},\nt^2 \\mapsto p_{T^2}(t^2) := \\nu f(\\nu t^2;\\delta, m,n-m)\n\\end{equation}\\] und eine KVF der Einstichproben-T\\(^2\\)-Teststatistik ist gegeben durch \\[\\begin{equation}\\label{eq:PT2}\nP_{T^2} : \\mathbb{R}_{\\ge 0} \\to [0,1],\nt^2 \\mapsto P_{T^2}(t^2) := F(\\nu t^2;\\delta, m,n-m)\n\\end{equation}\\]\n\n\nBeweis. Wir halten zunächst fest, dass das Theorem zur univariaten WDF Transformation bei linear-affinen Abbildungen besagt, dass für eine Zufallsvariable \\(\\xi\\) mit WDF \\(p_\\xi\\) und der Definition \\(\\upsilon = f(\\xi)\\) mit \\(f(\\xi) := a\\xi + b\\) für \\(a\\neq 0\\) eine WDF von \\(\\upsilon\\) definiert ist durch \\(p_\\upsilon(y) := (1/|a|)p_\\xi((y-b)/a)\\). Im vorliegenden Fall ist \\(\\xi = \\nu T^2\\) mit WDF \\(f(\\delta,m,n-m)\\) und \\(\\upsilon := T^2 = \\frac{1}{\\nu}\\nu T^2\\), also \\(a = 1/\\nu\\) und \\(b = 0\\). Mit \\(\\nu &gt; 0\\) ergibt sich \\(\\eqref{eq:pT1}\\) also aus \\[\\begin{equation}\np_{T^2}(t^2) = \\frac{1}{a}p_{\\nu T^2}\\left(\\frac{t^2}{a}\\right) = \\nu f(\\nu t^2; m, n-m).\n\\end{equation}\\] \\(\\eqref{eq:PT2}\\) folgt dann damit, dass WDFen bei kontinuierlichen Zufallsvariablen die Ableitungen der entsprechenden KVFen sind. Mit der Kettenregel der Differentiation ergibt sich \\[\\begin{align}\n\\begin{split}\n\\frac{d}{dt^2}P_{T^2}\\left(t^2\\right)\n& = \\frac{d}{dt^2}\\left(F(\\nu t^2;m,n-m,\\delta)\\right) \\\\\n& = \\frac{d}{dt^2}F(\\nu t^2;m,n-m,\\delta)\\frac{d}{dt^2}\\left(\\nu t^2 \\right) \\\\\n& = \\nu f(\\nu t^2;m,n-m,\\delta) \\\\\n& = p_{T^2}(t^2).\n\\end{split}\n\\end{align}\\]\n\nWir halten fest, dass die skalierte Einstichproben-T\\(^2\\)-Test Teststatitik \\(\\nu T^2\\) nach \\(f(\\delta,m,n-m)\\) nichtzentral \\(f\\)-verteilt ist, die WDF der Einstichproben-T\\(^2\\)-Test Teststatitik \\(T^2\\) selbst dagegen durch \\(\\nu f(\\nu t^2;\\delta, m,n-m)\\) geben ist. Wir simulieren diese Verteilung mithilfe folgenden R Codes und visualisieren diese Simulation in Abbildung 37.4.\n\n# Modellparameter\nm     = 2                                                                       # Dimensionalität der Zufallsvektoren/Daten\nn     = 15                                                                      # Anzahl der Datenpunkte\nmu_0  = matrix(c(1,1) , nrow = 2)                                               # Nullhypothesenparameter\nmu    = matrix(c(2,2) , nrow = 2)                                               # wahrer, aber unbekannter, Erwartungswertparameter\nSigma = matrix(c(0.5,0.3, 0.3,0.5), nrow = 2, byrow = TRUE)                     # wahrer, aber unbekannter, Kovarianzmatrixparameter\n\n# Simulation\nlibrary(MASS)                                                                   # R Paket für multivariate Normalverteilungen\nnsim  = 1e4                                                                     # Anzahl Simulationen/Datensatzrealisierungen\nYb    = matrix(rep(NaN,m*nsim), nrow = 2)                                       # Stichprobenmittelarray\nT2    = rep(NaN,nsim)                                                           # Einstichproben-T$^2$-Teststatistik Array\nj_n   = matrix(rep(1,n), nrow = n)                                              # 1_n\nI_n   = diag(n)                                                                 # I_n\nJ_n   = matrix(rep(1,n^2), nrow = n)                                            # 1_{nn}\nfor(s in 1:nsim){                                                               # Simulationsiterationen\n    Y      = t(mvrnorm(n,mu,Sigma))                                             # \\upsilon_i \\sim N(\\mu,\\Sigma), i = 1,...,n\n    Y_bar  = (1/n)*(Y %*% j_n)                                                  # Stichprobenmittel\n    C      = (1/(n-1))*(Y %*% (I_n-(1/n)*J_n) %*% t(Y))                         # Stichprobenkovarianzmatrix\n    T2[s]  = n*t(Y_bar - mu_0) %*% solve(C) %*% (Y_bar - mu_0)                  # Einstichproben-T$^2$-Teststatistik\n    Yb[,s] = Y_bar                                                              # Stichprobenmittel für Visualisierung\n}\n\n\n\n\n\n\n\nAbbildung 37.4: Verteilung der Einstichproben-T\\(^2\\)-Teststatistik. A Stichprobenmittel von 10.000 Realisierungen eines Einstichproben-T\\(^2\\)-Test-Modells mit \\(m:=2\\) und \\(n:=15\\) und wahren, aber unbekannten, Parametern \\[\\begin{equation}\n\\mu := (2,2)^T \\mbox{ und } \\Sigma = \\begin{pmatrix} 0.5 & 0.3 \\\\ 0.3 & 0.5 \\end{pmatrix}.\n\\end{equation}\\] B Histogramm der entsprechenden Realisierungen der skalierten Einstichproben-T\\(^2\\)-Teststatistik für \\(\\mu_0 := (1,1)^T\\) (grau) und analytische Form dieser Verteilung (orange). C Histogramm der entsprechenden Realisierungen der Einstichproben-T\\(^2\\)-Teststatistik (grau) und ihre analytische Form (orange). D Empirische KVF der entsprechenden Realisierungen der Einstichproben-T\\(^2\\)-Teststatistik (grau) und ihre analytische Form (orange).\n\n\n\nDen Einstichproben-T\\(^2\\)-Test definieren wir schließlich als einen kritischen Wert-basierten Test wie folgt.\n\nDefinition 37.5 (Einstichproben-T\\(^2\\)-Test) Gegeben seien das Einstichproben-T\\(^2\\)-Modell und die Einstichproben-T\\(^2\\)-Teststatistik. Dann ist für einen kritischen Wert \\(k\\ge 0\\) der Einstichproben-T\\(^2\\)-Test definiert als der kritische Wert-basierte Test \\[\\begin{equation}\n\\phi(\\upsilon) := 1_{\\{T^2 &gt; k\\}} := \\begin{cases} 1 & T^2  &gt; k \\\\ 0 & T^2 \\le k \\end{cases}.\n\\end{equation}\\]\n\nIn Definition 37.5 repräsentiert wie üblich \\(\\phi(\\upsilon) = 1\\) den Vorgang des Ablehnens von \\(H_0\\) und \\(\\phi(\\upsilon) = 0\\) den Vorgang des Nichtablehnens von \\(H_0\\).\n\n\nAnalyse der Testgütefunktion\nUm für den in Definition 37.5 definierten Test Prozeduren zur Testumfangkontrolle (Typ I Fehlerbegrenzung) und zur Stichprobengrößenoptimierung (Typ II Fehlerbegrenzung) zu entwickeln, betrachten wir zunächst seine Testgütefunktion. Es gilt folgendes Theorem.\n\nTheorem 37.3 (Testgütefunktion des Einstichproben-T\\(^2\\)-Tests) \\(\\phi\\) sei der Einstichproben-T\\(^2\\)-Test. Dann ist die Testgütefunktion von \\(\\phi\\) gegeben durch \\[\\begin{equation}\nq_\\phi : \\mathbb{R}^m \\to [0,1], \\mu \\mapsto q_\\phi(\\mu) := 1 - F(\\nu k;\\delta_\\mu,m,n-m)\n\\end{equation}\\] wobei \\(F(\\cdot;\\delta_\\mu, m,n-m)\\) die KVF der nichtzentralen \\(f\\)-Verteilung mit Freiheitsgradparametern\\(m\\) und \\(n-m\\) sowie mit Nichtzentralitätsparameter \\[\\begin{equation}\n\\delta_\\mu := n(\\mu - \\mu_0)^T\\Sigma^{-1}(\\mu - \\mu_0)\n\\end{equation}\\] bezeichnet.\n\n\nBeweis. Die Testgütefunktion des betrachteten Tests ist definiert als \\[\\begin{equation}\nq_\\phi : \\mathbb{R}^m \\to [0,1], \\mu \\mapsto q_\\phi(\\mu) := \\mathbb{P}_{\\mu}(\\phi = 1).\n\\end{equation}\\] Da die Wahrscheinlichkeiten für \\(\\phi = 1\\) und dafür, dass die zugehörige Teststatistik im Ablehnungsbereich des Tests liegt, gleich sind, benötigen wir also zunächst die Verteilung der Teststatistik. Wir haben oben aber bereits gesehen, dass \\[\\begin{equation}\n\\frac{n-m}{m(n-1)}T^2 \\sim f(m,n-m,\\delta_\\mu) \\mbox{ mit } \\delta_\\mu := n(\\mu -\\mu_0)^T\\Sigma^{-1}(\\mu-\\mu_0)\n\\end{equation}\\] gilt. Der Ablehnungsbereich des betrachteten Tests ist \\(A := ]k,\\infty[\\). Also ergibt sich \\[\\begin{align}\n\\begin{split}\nq_\\phi(\\mu)\n&  = \\mathbb{P}_\\mu(\\phi = 1) \\\\\n&  = \\mathbb{P}_\\mu\\left(T^2 \\in \\,\\,]k,\\infty[\\right) \\\\\n&  = \\mathbb{P}_\\mu\\left(T^2 &gt; k \\right) \\\\\n&  = 1 - \\mathbb{P}_\\mu\\left(T^2 \\le k \\right) \\\\\n&  = 1 - F(\\nu k; \\delta_\\mu,m, m-n)\n\\end{split}\n\\end{align}\\]\n\nWir wollen diese Testgütefunktion beispielhaft für zwei Szenarien mit \\(m := 2\\) und \\(n := 15\\) in Abhängigkeit des kritischen Wertes \\(k\\) betrachten. Abbildung 37.5 und Abbildung 37.6 visualisieren \\(q_\\phi\\) in diesen Szenarien für einen Nullhypothesenparameter \\(\\mu_0 := (1,1)^T\\) und die wahren, aber unbekannten, Kovarianzmatrixparameter \\[\\begin{equation}\n\\Sigma_1 := \\begin{pmatrix} 1.0 & 0.0  \\\\ 0.0 & 1.0 \\end{pmatrix}\n\\mbox{ und }\n\\Sigma_2 := \\begin{pmatrix} 1.0 & 0.9  \\\\ 0.9 & 1.0 \\end{pmatrix},\n\\end{equation}\\] respektive. In beiden Fällen und unabhängig von \\(k\\) resultiert eine größere Distanz des wahren, aber unbekannten, Erwartungswertparameters \\(\\mu\\) vom Nullhypothesenparamter \\(\\mu_0\\) in einer höhereren Wahrscheinlichkeit dafür, dass der Test \\(\\phi\\) den Wert \\(1\\) annimmt, also die Nullhypothese abgelehnt wird. Die Zunahme dieser Wahrscheinlichkeit ist im ersten Szenario isotropisch, im zweiten dagegen aufgrund der Form des wahren, aber unbekannten, Kovarianzparameters nicht. Bei einem kleinen kritischen Wert \\(k\\) werden hohe Wahrscheinlichkeiten für eine Ablehnung der Nullhypothese schon bei geringen Distanzen zwischen \\(\\mu\\) und \\(\\mu_0\\) erreicht, bei einem größeren kritischen Wert \\(k\\) dagegen erst für größere Distanzen. Untenstehender R Code demonstriert das Vorgehen zur Evaluation dieser Testgütefunktionen\n\n# Modellparameter\nm           = 2                                                                 # m\nn           = 15                                                                # n\nnu          = (n-m)/((n-1)*m)                                                   # \\nu\nSigma       = diag(m)                                                           # \\Sigma = I_2\niSigma      = solve(Sigma)                                                      # \\Sigma^{-1}\n\n# Testparameter\nmu_0        = matrix(c(1,1), nrow = 2)                                          # \\mu_0\nk_all       = c(2,4,6)                                                          # k &lt;-&gt; \\phi\nn_k         = length(k_all)                                                     # Anzahl k Werte/Tests\n\n# q_\\phi(\\mu) Evaluation\nmu_min      = 0                                                                 # \\mu_i Minimum\nmu_max      = 2                                                                 # \\mu_i Maximum\nmu_res      = 1e3                                                               # \\mu_i Auflösung\nmu_i        = seq(mu_min,mu_max,len = mu_res)                                   # mu_i\nq_phi       = array(dim = c(mu_res, mu_res, length(k_all)))                     # q_\\phi Array\nfor(k in 1:n_k){\n    for(i in 1:mu_res){\n        for(j in 1:mu_res){\n            mu           = matrix(c(mu_i[i],mu_i[j]), nrow = 2)                 # \\mu\n            delta_mu     = n*t(mu - mu_0) %*% iSigma %*% (mu -mu_0)             # \\delta_\\mu\n            q_phi[i,j,k] = 1 - pf(nu*k_all[k], m, n-m, delta_mu)}}}             # q_\\phi(\\mu)\n\n\n\n\n\n\n\nAbbildung 37.5: Einstichproben-T\\(^2\\)-Test Testgütefunktionen für kritische Werte \\(k = 2\\), \\(k = 4\\) und \\(k = 6\\) im Szenario \\[\\begin{equation}\nm:=2,\nn:=15,\n\\mu_0 := (1,1)^T,\n\\Sigma_1 :=   \\begin{pmatrix} 1.0 & 0.0  \\\\ 0.0 & 1.0 \\end{pmatrix}\n\\end{equation}\\]\n\n\n\n\n\n\n\n\n\nAbbildung 37.6: Einstichproben-T\\(^2\\)-Test Testgütefunktionen für kritische Werte \\(k = 2\\), \\(k = 4\\) und \\(k = 6\\) im Szenario \\[\\begin{equation}\nm:=2,\nn:=15,\n\\mu_0 := (1,1)^T,\n\\Sigma_1 :=   \\begin{pmatrix} 1.0 & 0.9  \\\\ 0.9 & 1.0 \\end{pmatrix}\n\\end{equation}\\]\n\n\n\n\n\nTestumfangkontrolle\nBekanntlich erlaubt die Testumfangkontrolle die Begrenzung der größtmöglichen Wahrscheinlichkeit für einen Typ I Fehler. Im aktuellen Testszenario haben wir folgendes Theorem.\n\nTheorem 37.4 (Testumfangkontrolle des Einstichproben-T\\(^2\\)- Tests) \\(\\phi\\) sei der im obigen Testszenario definierte Test. Dann ist \\(\\phi\\) ein Level-\\(\\alpha_0\\)-Test mit Testumfang \\(\\alpha_0\\), wenn der kritische Wert definiert ist durch \\[\\begin{equation}\nk_{\\alpha_0} := \\nu^{-1}F^{-1}\\left(1-\\alpha_0; m, n-m \\right),\n\\end{equation}\\] wobei \\(\\nu := (n-m)/((n-1)m)\\) und \\(F^{-1}(\\cdot;m,n-m)\\) die inverse KVF der \\(f\\)-Verteilung mit Freiheitsgradparametern \\(m\\) und \\(n-m\\) ist.\n\n\nBeweis. Damit der betrachtete Test ein Level-\\(\\alpha_0\\)-Test ist, muss bekanntlich \\(q_\\phi(\\mu)\\le \\alpha_0\\) für alle \\(\\mu \\in \\{\\mu_0\\}\\), also hier \\(q_\\phi(\\mu_0)\\le \\alpha_0\\) gelten. Weiterhin ist der Testumfang des betrachteten Tests durch \\(\\alpha = \\max_{\\mu \\in \\{\\mu_0\\}} q_\\phi(\\mu)\\), also hier durch \\(\\alpha = q_\\phi(\\mu_0)\\) gegeben. Wir müssen also zeigen, dass die Wahl von \\(k_{\\alpha_0}\\) garantiert, dass \\(\\phi\\) ein Level-\\(\\alpha_0\\)-Test mit Testumfang \\(\\alpha_0\\) ist. Dazu merken wird zunächst an, dass für \\(\\mu = \\mu_0\\) gilt, dass \\[\\begin{equation}\nq_\\phi(\\mu_0) = 1 - F(\\nu k;\\delta, m, n-m ) = 1 - F(\\nu k;0, m,n-m)  = 1 - F(\\nu k;m,n-m),\n\\end{equation}\\] wobei \\(F(\\nu k; \\delta, m, n-m)\\) und \\(F(\\nu k;m,n-m)\\) die KVF der nichtzentralen \\(f\\)-Verteilung mit Nichtzentralitätsparameter \\(\\delta\\) und Freiheitsgradparametern \\(m\\) und \\(n-m\\) sowie der \\(f\\)-Verteilung mit Freiheitsgradparametern \\(m\\) und \\(n-m\\), respektive, bezeichnen. Sei nun also \\(k := k_{\\alpha_0}\\). Dann gilt \\[\\begin{align}\n\\begin{split}\nq_\\phi(\\mu_0)\n& = 1 - F(\\nu k_{\\alpha_0};m,n-m) \\\\\n& = 1 - F\\left(\\nu \\nu^{-1}F^{-1}\\left(1-\\alpha_0; m, n-m \\right);m,n-m\\right) \\\\\n& = 1 - F\\left(F^{-1}\\left(1-\\alpha_0; m, n-m \\right);m,n-m\\right) \\\\\n& = 1 - (1 - \\alpha_0)  = \\alpha_0.\n\\end{split}\n\\end{align}\\] Es folgt also direkt, dass bei der Wahl von \\(k = k_{\\alpha_0}, q_\\phi(\\mu_0) \\le \\alpha_0\\) gilt und der betrachtete Test somit ein Level-\\(\\alpha_0\\)-Test ist. Weiterhin folgt direkt, dass der Testumfang des betrachteten Tests bei Wahl von \\(k = k_{\\alpha_0}\\) gleich \\(\\alpha_0\\) ist.\n\n\n\n\n\n\n\nAbbildung 37.7: Testumfangkontrolle durch Selektion eines \\(\\alpha_0\\)-abhängigen kritischen Wertes für den Einstichproben-T\\(^2\\)-Test anhand von KVF unf WDF der Einstichproben-T\\(^2\\)-Teststatistik.\n\n\n\nWir visualisieren die Wahl von \\[\\begin{equation}\nk_{\\alpha_0} = \\nu^{-1}F^{-1}\\left(1-\\alpha_0; m, n-m \\right)\n\\end{equation}\\] für den Fall \\(m = 2, n = 15\\) und ein Signifikanzlevel von \\(\\alpha_0 := 0.05\\) in Abbildung 37.7. Untenstehender R Code simuliert die Testumfangkontrolle für ein Einstichproben-T\\(^2\\)-Test Szenario mit \\[\\begin{equation}\nm := 2, n := 15, \\mu := \\mu0 := \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n\\mbox{ und }\n\\Sigma :=\\begin{pmatrix} 0.5 & 0.3 \\\\ 0.3 & 0.5 \\end{pmatrix}.\n\\end{equation}\\] Der auf Grundlage von \\(10^4\\) Datensatzrealisationen geschätzte Testumfang stimmt gut mit dem Signifikanzlevel überein.\n\n# Modellparameter\nm         = 2                                                                   # Dimensionalität der Zufallsvektoren/Daten\nn         = 15                                                                  # Anzahl der Datenpunkte\nnu        = (n-m)/(m*(n-1))                                                     # Parameter\nmu_0      = matrix(c(1,1) , nrow = 2)                                           # Nullhypothesenparameter\nmu        = mu_0                                                                # w.a.u. Erwartungswertparameter bei Zutreffen von H0\nSigma     = matrix(c(0.5,0.3, 0.3,0.5), nrow = 2, byrow = TRUE)                 # wahrer, aber unbekannter, Kovarianzmatrixparameter\n\n# Testparameter\nalpha_0   = 0.05                                                                # Signifikanzlevel\nk_alpha_0 = (1/nu)*qf(1-alpha_0, m,n-m)                                         # kritischer Wert\n\n# Simulation der Testumfangkontrolle\nlibrary(MASS)                                                                   # R Paket für multivariate Normalverteilungen\nnsim  = 1e4                                                                     # Testentscheidungsarray\nphi   = rep(NaN,nsim)                                                           # Testentscheidungsarray\nj_n   = matrix(rep(1,n), nrow = n)                                              # 1_n\nI_n   = diag(n)                                                                 # I_n\nJ_n   = matrix(rep(1,n^2), nrow = n)                                            # 1_{nn}\nfor(s in 1:nsim){                                                               # Simulationsiterationen\n    Y      = t(mvrnorm(n,mu,Sigma))                                             # Y_i \\sim N(\\mu,\\Sigma), i = 1,...,n\n    Y_bar  = (1/n)*(Y %*% j_n)                                                  # Stichprobenmittel\n    C      = (1/(n-1))*(Y %*% (I_n-(1/n)*J_n) %*% t(Y))                         # Stichprobenkovarianzmatrix\n    T2     = n*t(Y_bar - mu_0) %*% solve(C) %*% (Y_bar - mu_0)                  # Einstichproben-T$^2$-Test Statistik\n    if(T2 &gt; k_alpha_0){                                                         # Test 1_{T^2 &gt;= k_alpha_0}\n        phi[s] = 1                                                              # Ablehnen von H_0\n    } else {\n        phi[s] = 0}}                                                            # Nicht Ablehnen von H_0\n\n\n\n\nKritischer Wert              =  8.196602 \nGeschätzter Testumfang alpha =  0.0505\n\n\nIn der Praxis entsPrächen obige Ergebnisse dann folgendem Vorgehen bei der Durchführung eines Einstichproben-T\\(^2\\)-Tests. Man unterstellt, dass ein vorliegender Datensatz von \\(m\\)-dimensionalen Datenvektoren eine Realisation von \\(n\\) u.i.v. \\(m\\)-dimensionalen Zufallsvektoren \\(\\upsilon_1,...,\\upsilon_n \\sim N(\\mu,\\Sigma)\\) mit unbekannten Parametern \\(\\mu \\in \\mathbb{R}^m\\) und \\(\\Sigma \\in \\mathbb{R}^{m \\times m} \\mbox{ pd }\\) ist und möchte entscheiden, ob für ein \\(\\mu_0 \\in \\mathbb{R}^m\\) eher die Nullhypothese \\(H_0 : \\mu = \\mu_0\\) oder die Alternativhypothese \\(H_1: \\mu \\neq \\mu_0\\) zutrifft. Zu diesem Zweck wählt man zunächst ein Signifikanzlevel \\(\\alpha_0\\) und bestimmt dann den zugehörigen kritischen Wert \\(k_{\\alpha_0}\\). Beispielsweise gilt für \\(m = 2\\) und \\(n=15\\) bei Wahl von \\(\\alpha_0 := 0.05\\), dass \\(k_{0.05}=\\nu^{-1}F^{-1}(1 - 0.05;2,13) \\approx 8.2\\) ist. Anhand von \\(m, n, \\mu_0\\), dem Stichprobenmittel \\(\\bar{\\upsilon}\\) und der Stichprobenkovarianzmatrix \\(C\\) berechnet man dann die Realisierung der Einstichproben-T\\(^2\\)-Teststatistik \\[\\begin{equation}\nT^2 := n(\\bar{\\upsilon} - \\mu_0)^T C^{-1}(\\bar{\\upsilon} - \\mu_0).\n\\end{equation}\\] Wenn das berechnete \\(T^2\\) größer als \\(k_{\\alpha_0}\\) ist, lehnt man die Nullhypothese ab, andernfalls nicht. Die oben entwickelte Theorie zur Testumfangkontrolle des Einstichproben-T\\(^2\\)-Test garantiert dann, dass man in höchstens \\(\\alpha_0 \\cdot 100\\) von \\(100\\) Fällen die Nullhypothese fälschlicherweise ablehnt.\n\n\np-Wert\nWir erinnern daran, dass per Definition der p-Wert das kleinste Signifikanzlevel \\(\\alpha_0\\) ist, bei welchem man die Nullhypothese basierend auf einem vorliegenden Wert der Teststatistik ablehnen würde. Wir haben folgendes Theorem\n\nTheorem 37.5 Für den p-Wert des in Definition 37.5 definierten Test gilt \\[\\begin{equation}\n\\mbox{ p-Wert } = \\mathbb{P}\\left(T^2 \\ge t^2\\right) = 1 - F(\\nu t^2;m,n-m).\n\\end{equation}\\]\n\n\nBeweis. Bei einem beobachteten Wert \\(t^2\\) der Einstichproben-T\\(^2\\)-Teststatistik \\(T^2\\) würde \\(H_0\\) für jedes \\(\\alpha_0\\) mit \\(t^2 \\ge \\nu^{-1}F^{-1}(1-\\alpha_0;m,n-m)\\) abgelehnt werden. Für diese \\(\\alpha_0\\) gilt, wie unten gezeigt \\[\\begin{equation}\n\\alpha_0 \\ge \\mathbb{P}\\left(T^2 \\ge t^2\\right).\n\\end{equation}\\] Das kleinste \\(\\alpha_0 \\in [0,1]\\) mit \\(\\alpha_0 \\ge \\mathbb{P}\\left(T^2 \\ge t^2\\right)\\) ist dann \\(\\alpha_0 = \\mathbb{P}(T^2 \\ge t^2)\\), also folgt \\[\\begin{equation}\n\\mbox{ p-Wert } = \\mathbb{P}\\left(T^2 \\ge t^2\\right) = 1 - F(\\nu t^2;m,n-m).\n\\end{equation}\\] Es bleibt zu zeigen, dass gilt \\[\\begin{align}\n\\begin{split}\nt^2 \\ & \\ge \\nu^{-1}F^{-1}(1-\\alpha_0;m,n-m) \\\\\n\\Leftrightarrow\n\\nu t^2 & \\ge F^{-1}(1-\\alpha_0;m,n-m) \\\\\n\\Leftrightarrow\n\\alpha_0 & \\ge \\mathbb{P}\\left(T^2 \\ge t^2\\right).\n\\end{split}\n\\end{align}\\] Dies aber folgt aus \\[\\begin{align}\n\\begin{split}\nt^2\n& \\ge \\nu^{-1}F^{-1}(1-\\alpha_0;m,n-m) \\\\\n\\nu t^2\n& \\ge F^{-1}(1-\\alpha_0;m,n-m) \\\\\nF(\\nu t^2; m,n-m)\n& \\ge F\\left(F^{-1}(1-\\alpha_0;m,n-m); m,n - m\\right) \\\\\nF(\\nu t^2; m,n-m)\n& \\ge 1 -\\alpha_0\\\\\n\\mathbb{P}\\left(T^2 \\le t^2\\right)\n& \\ge 1-\\alpha_0 \\\\\n\\alpha_0\n& \\ge 1-\\mathbb{P}\\left(T^2 \\le t^2\\right).\n\\end{split}\n\\end{align}\\]\n\n\n\n[1] 0.07168146 0.03968271 0.03521954 0.30464619\n\n\nZum Beispiel ergeben sich bei \\(m = 2\\) und \\(n=15\\) der p-Wert für \\(t^2 = 7.00\\) zu 0.071 und bei \\(m = 4\\) und \\(n=15\\) der p-Wert für \\(t^2 = 7.00\\) zu 0.304. Die gleiche Anzahl an Datenpunkten resultiert bei höherer Datendimensionalität also in einem höheren p-Wert. Weiterhin ergeben sich bei \\(m = 2\\) und \\(n=15\\) der p-Wert für \\(t^2 = 9.00\\) zu 0.040 und bei \\(m = 2\\) und \\(n=99\\) der p-Wert für \\(t^2 = 7.00\\) zu 0.035. Geringere Verhältnisse von geschätzter Nullhypothesenabweichung und geschätzter Daten(ko)varianz können also hinsichtlich des p-Wertes durch eine höhere Anzahl an Datenpunkten ausgeglichen werden.\n\n\nAnalyse der Powerfunktion\nBekanntlich ist man manchmal an der Optimierung der Stichprobengröße vor der Durchführung einer Studie interessiert. Zu diesem Zweck betrachtet man die Testgütefunktion \\[\\begin{equation}\nq_\\phi : \\mathbb{R}^m \\to [0,1], \\mu \\mapsto q_\\phi(\\mu) := 1 - F(\\nu k;\\delta_\\mu,m,n-m)\n\\end{equation}\\] bei kontrolliertem Testumfang, also für \\[\\begin{equation}\nk_{\\alpha_0} := \\nu^{-1}F^{-1}\\left(1-\\alpha_0; m, n-m \\right)\n\\end{equation}\\] mit festem \\(\\alpha_0\\) als Funktion des Nichtzentralitätsparameters, also der wahren, aber unbekannten, Effektstärke und des Stichprobenumfangs. Insbesondere hängt hier \\(k_{\\alpha_0}\\) auch von \\(n\\) ab. Es ergibt sich dabei die bivariate reellwertige Funktion \\[\\begin{equation}\n\\pi : \\mathbb{R} \\times \\mathbb{N} \\to [0,1],\n(\\delta_\\mu,n) \\mapsto\n\\pi(\\delta_\\mu,n) :=  1 - F(\\nu k_{\\alpha_0};\\delta_\\mu,m,n-m).\n\\end{equation}\\] Bei festgelegtem \\(\\alpha_0\\) hängt diese sogenannten Powerfunktion des Einstichproben-T\\(^2\\)-Tests also vom wahren, aber unbekannten, Nichtzentralitätsparameter \\(\\delta_\\mu\\), der Datendimensionalität \\(m\\) und von der Stichprobengröße \\(n\\) ab. Wir evaluieren diese Abhängigkeiten mithilfe untenstehenden R Codes und visualisieren sie exemplarisch in Abbildung 37.8.\n\n# Szenariospezifikationen\na_0_all   = c(0.05,0.01)                                                        # \\alpha_0 Raum\nd_mu_min  = 0                                                                   # \\delta_\\mu Minimum\nd_mu_max  = 20                                                                  # \\delta_\\mu Maximum\nd_mu_res  = 30                                                                  # \\delta_\\mu Auflösung\nd_mu_all  = seq(d_mu_min, d_mu_max, len = d_mu_res)                             # \\delta_\\mu d Raum\nn_min     = 5                                                                   # n Minimum\nn_max     = 20                                                                  # n Maximum\nn_res     = 30                                                                  # n Auflösung\nn_all     = seq(n_min,n_max, len = n_res)                                       # n Raum\nm_all     = c(2,4)                                                              # m Raum\n\n# Evaluation der Powerfunktion\npi        = array(dim = c(d_mu_res, n_res, 2,2))                                # Powerfunktionsarray\nfor (a in 1:length(a_0_all)){\n    for (l in 1:length(m_all)){                                                 # m Iterationen\n        for(i in 1:length(d_mu_all)){                                           # \\delta_\\mu Iterationen\n            for(j in 1:length(n_all)){                                          # n Iterationen\n                m           = m_all[l]                                          # Datendimensionalität\n                n           = n_all[j]                                          # Stichprobenumfang\n                d_mu        = d_mu_all[i]                                       # wahrer, aber unbekannter, Parameter\n                nu          = (n-m)/(m*(n-1))                                   # Parameter\n                alpha_0     = a_0_all[a]                                        # Signifikanzlevel\n                k_alpha_0   = (1/nu)*qf(1-alpha_0,m,n-m)                        # kritischer Wert\n                pi[i,j,l,a] = 1 - pf(nu*k_alpha_0, m, n-m, d_mu)}}}}            # Powerfunktionswert\n\n\n\n\n\n\n\nAbbildung 37.8: Powerfunktionen des Einstichproben-T\\(^2\\)-Tests. Die Abbildungen zeigen die Wahrscheinlichkeit dafür, dass der Einstichproben-T\\(^2\\)-Test den Wert 1 annimmt, dass also die Nullhypothese abgelehnt wird, als Funktion des wahren, aber unbekannten, Nichtzentralitätsparameters \\(\\delta_\\mu\\) sowie des Stichprobenumfangs \\(n\\). Dabei bilden die Abbildungen der ersten Zeile die Powerfunktionen des Einstichproben-T\\(^2\\)-Tests für das Signifikanzlevel \\(\\alpha_0 = 0.05\\) und die Abbildungen der zweiten Zeile die Powerfunktionen des Einstichproben-T\\(^2\\)-Tests für das Signifikanzlevel \\(\\alpha_0 = 0.01\\) ab. Ein niedrigeres Signifikanzlevel resultiert wie üblich in einer geringeren Wahrscheinlichkeit für die Ablehung der Nullhypothese über den gesamten Bereich von \\(\\delta_\\mu\\) und \\(n\\). Die erste Spalte der Abbildung zeigt die beiden Signifikanzlevelszenarien für eine Datendimensionalität von \\(m := 2\\), die zweite Spalte für eine Datendimensionalität von \\(m := 4\\). Eine Erhöhung der Datendimensionalität führt, bei Konstanthalten aller weiteren Parameter, zu einer Reduktion der Wahrscheinlichkeit für das Ablehnen der Nullhypothese. Äquivalent sind bei höherer Datendimensionalität höhere Stichprobenumfänge nötig um bei vergleichbarem Nichtzentraltitätsparameter die Nullhypothese gleichwahrscheinlich abzulehnen.\n\n\n\nGenerell lässt sich aus der Perspektive der Anwendung festhalten, dass \\(\\pi\\) als Funktion von \\(n\\) monoton steig. Ein größerer Stichprobenumfang resultiert damit also im Allgemeinem in einer kleineren Wahrscheinlichkeit für einen Typ II Fehler. Dabei bleiben allerdings mögliche weitere Kosten für die Erhöhung des Stichprobenumfangs unberücksichtigt. Weiterhin hängen die Werte der Powerfunktion \\(\\pi\\) offensichtlich vom wahren, aber unbekannten, Nichtzentralitätsparameterwert \\[\\begin{equation}\n\\delta_\\mu = n(\\mu-\\mu_0)^T\\Sigma^{-1}(\\mu-\\mu_0)\n\\end{equation}\\] ab. Würde man diesen Wert schon mit großer Präzision kennen, so gäbe es keinen Grund eine Studie und ihren Stichprobenumfang zu planen. Es wird deshalb zur Stichprobengrößenoptimierung im Vorfeld einer Studie im Allgemeinen folgendes Vorgehen favorisiert:\n\nMan legt zunächst das Signifikanzlevel \\(\\alpha_0\\) zur Kontrolle der Wahrscheinlichkeit eines Typ 1 Fehlers fest und evaluiert die entsprechende Powerfunktion.\nMan wählt einen Mindestparameterwert \\(\\delta_\\mu^*\\), den man mit einer Wahrscheinlichkeit von \\[\\begin{equation}\n\\pi(\\delta_\\mu,n) = \\beta\n\\end{equation}\\] detektieren möchte, bei dem man also die Nullhypothese ablehnen möchte. Der Wert von \\(\\delta_\\mu^*\\) ergibt sich dabei aus problemspezifischen Überlegungen, wie zum Beispiel der Frage nach einem klinisch bedeutsamen Wert. Ein konventioneller Wert für die gewünschte Detektionswahrscheinlichkeit ist \\(\\beta := 0.8\\).\nBasierend auf der evaluierten Powerfunktion liest man die für \\[\\begin{equation}\n\\pi(\\delta_\\mu = \\delta_\\mu^*,n) = \\beta\n\\end{equation}\\] minimal nötige Stichprobengröße \\(n\\) ab. Größere Stichprobengrößen führen aufgrund der Monotonie von \\(\\pi\\) als Funktion von \\(n\\) sicher zu einer gleichen oder höheren Wahrscheinlichkeit für das Ablehnen der Nullhypothese.\n\nFür eine Datendimensionalität von \\(m := 2\\) und Mindestparameterwert von \\(\\delta_\\mu^* = 12\\) evaluiert untenstehender R Code wie in Abbildung 37.9 dargestellt die minimale Stichprobengröße um mit einer Wahrscheinlichkeit von \\(\\beta = 0.8\\) die Nullhypothese abzulehnen.\n\n# Szenariospezifikation\nn_min      = 5                                                                  # n Minimum\nn_max      = 20                                                                 # n Maximum\nn_res      = 1e2                                                                # n Auflösung\nn          = seq(n_min,n_max, len = n_res)                                      # n Raum\nalpha_0    = 0.05                                                               # Signifikanzlevel\n\n# Poweranalyse\nm          = 2                                                                  # Datendimensionalität\nd_mu_fix   = 12                                                                 # fester Nichtzentralitätsparameter\nnu         = (n-m)/(m*(n-1))                                                    # Parameter\nk_alpha_0  = (1/nu)*qf(1-alpha_0,m,n-m)                                         # kritischer Wert\npi_n       = 1 - pf(nu*k_alpha_0, m, n-m, d_mu_fix)                             # Powerfunktionswert\nbeta       = 0.8                                                                # gewünschter Powerfunktionswert\ni          = 1                                                                  # Indexinitialisierung\nn_min      = NaN                                                                # minimales n Initialisierung\nwhile(pi_n[i] &lt; beta){                                                          # Solange \\pi(\\delta_\\mu*,n) &lt; \\beta\n    n_min = n[i]                                                                # Aufnahme des minimal nötigen ns\n    i     = i + 1                                                               # und Erhöhung des Indexes\n}\n\n\n\nMinimal nötiges n = 17\n\n\n\n\n\n\n\n\nAbbildung 37.9: Bestimmung eines minimalen Stichprobenumfangs zur Detektion eines Nichtzentralitätsmindestparameterwert von \\(\\delta_\\mu = 12\\). Die Abbildung zeigt die entsprechende Schnittfunktion der in Abbildung 37.8 dargestellten Funktion.",
    "crumbs": [
      "Multivariate Inferenz",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Einstichproben-T$^2$-Tests</span>"
    ]
  },
  {
    "objectID": "502-Einstichproben-T2-Tests.html#anwendungsbeispiel",
    "href": "502-Einstichproben-T2-Tests.html#anwendungsbeispiel",
    "title": "37  Einstichproben-T\\(^2\\)-Tests",
    "section": "37.4 Anwendungsbeispiel",
    "text": "37.4 Anwendungsbeispiel\nWir betrachten das eingangs diskutierte Anwendungsbeispiel eines simulierten zweidimensionalen Datensatzes dreier Studiengruppen. Wir wollen abschließen für diesen Datensatz die Nullhypothese z für ein Abweichen des wahren, aber unbekannten, Erwartungswertparameters der Daten von \\(\\mu_0\\) ist. Wir betrachten also weiterhin die einfache Nullhypothese \\(H_0 :\\mu = \\mu_0\\) und die zusammegesetzte Alternativhypothese \\(H_1 :\\mu \\neq \\mu_0\\). Folgender R Code implementiert das praktische Vorgehen für ein Signifikanzlevel von \\(\\alpha_0 := 0.05\\).\n\n# Datenbereitstellung\nD         = read.csv(\"./_data/502-einstichproben-t2-tests.csv\")                 # Datensatzeinlesen \nY         = rbind(D$y_1i, D$y_2i)                                               # Datenmatrix\n\n# Testparameter\nm         = nrow(Y)                                                             # Dimensionalität der Zufallsvektoren/Daten\nn         = ncol(Y)                                                             # Anzahl der Datenpunkte\nnu        = (n-m)/(m*(n-1))                                                     # Parameter\nmu_0      = matrix(c(30,3.5) , nrow = 2)                                        # H0 Hypothesenparameter (\"Normwert\")\nalpha_0   = 0.05                                                                # Signifikanzlevel\nk_alpha_0 = (1/nu)*qf(1-alpha_0,m,n-m)                                          # kritischer Wert\n\n# Testevaluation\nj_n       = matrix(rep(1,n), nrow = n)                                          # 1_n\nI_n       = diag(n)                                                             # I_n\nJ_n       = matrix(rep(1,n^2), nrow = n)                                        # 1_{nn}\nY_bar     = (1/n)*(Y %*% j_n)                                                   # Stichprobenmittel\nC         = (1/(n-1))*(Y %*% (I_n-(1/n)*J_n) %*% t(Y))                          # Stichprobenkovarianzmatrix\nT2        = n*t(Y_bar - mu_0) %*% solve(C) %*% (Y_bar - mu_0)                   # T^2 Statistik\nif(T2 &gt; k_alpha_0){                                                             # Test 1_{T^2 &gt;= k_alpha_0}\n    phi = 1                                                                     # Ablehnen von H_0\n} else {\n    phi = 0                                                                     # Nicht Ablehnen von H_0\n}\np         = 1 - pf(nu*T2,m,n-m)                                                 # p-Wert\n\n\n\nY_bar   =  26.25615 2.991039 \nC       =  38.8981 3.549813 3.549813 1.972143 \nT^2     =  7.546368 \nalpha_0 =  0.05 \nk       =  7.504065 \nphi     =  1 \np       =  0.04928746\n\n\nIm vorliegenden Fall nimmt die Einstichproben-T\\(^2\\)-Teststatistik einen größeren Wert als der kritische Wert an, es gilt damit \\(\\phi(\\Upsilon) = 1\\) und man lehnt die Nullhypothese ab. Der korrespondiere p-Wert ist durch 0.049 gegeben.",
    "crumbs": [
      "Multivariate Inferenz",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Einstichproben-T$^2$-Tests</span>"
    ]
  },
  {
    "objectID": "502-Einstichproben-T2-Tests.html#sec-literaturhinweise",
    "href": "502-Einstichproben-T2-Tests.html#sec-literaturhinweise",
    "title": "37  Einstichproben-T\\(^2\\)-Tests",
    "section": "37.5 Literaturhinweise",
    "text": "37.5 Literaturhinweise\nDie Theorie des Einstichproben-T\\(^2\\)-Tests geht zurück auf Hotelling (1931).",
    "crumbs": [
      "Multivariate Inferenz",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Einstichproben-T$^2$-Tests</span>"
    ]
  },
  {
    "objectID": "502-Einstichproben-T2-Tests.html#sec-selbskontrollfragen",
    "href": "502-Einstichproben-T2-Tests.html#sec-selbskontrollfragen",
    "title": "37  Einstichproben-T\\(^2\\)-Tests",
    "section": "37.6 Selbstkontrollfragen",
    "text": "37.6 Selbstkontrollfragen\n\nBeschreiben Sie das Anwendungsszenario für einen Einstichproben-T\\(^2\\)-Test.\nGeben Sie die Definition des Einstichproben-T\\(^2\\)-Test Modells wieder\nGeben Sie die Definition der Einstichproben-T\\(^2\\)-Teststatistik wieder.\nErläutern Sie, wann die Einstichproben-T\\(^2\\)-Teststatistik hohe Werte annimmt.\nGeben Sie das Theorem zu WDF und KDF der Einstichproben-T\\(^2\\)-Teststatistik wieder.\nGeben Sie das Theorem zur Testumfangkontrolle eins Einstichproben-T\\(^2\\)-Tests wieder.\nErläutern Sie das praktische Vorgehen bei der Durchführung eines Einstichproben-T\\(^2\\)-Tests.\nGeben Sie das Theorem zum p-Wert eines Einstichproben-T\\(^2\\)-Test an und erläutern Sie die Komponenten des entsprechenden Ausdrucks. \n\n\n\n\n\nAnderson, T. W. (2003). An Introduction to Multivariate Statistical Analysis (3rd ed). Wiley-Interscience.\n\n\nHotelling, H. (1931). The Generalization of Student’s Ratio. The Annals of Mathematical Statistics, 2(3), 360–378. https://doi.org/10.1214/aoms/1177732979",
    "crumbs": [
      "Multivariate Inferenz",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Einstichproben-T$^2$-Tests</span>"
    ]
  },
  {
    "objectID": "503-Einfaktorielle-Varianzanalyse.html",
    "href": "503-Einfaktorielle-Varianzanalyse.html",
    "title": "38  Einfaktorielle Varianzanalyse",
    "section": "",
    "text": "38.1 Anwendungsszenario\nDas Anwendungsszenario einer einfaktoriellen multivariaten Varianzanalyse ist durch das Vorliegen von multivariaten Datenpunkten von zwei oder mehr Gruppen randomisierter experimenteller Einheiten gekennzeichnet, die sich hinsichtlich der Level eines experimentellen Faktors unterscheiden. Ist die Anzahl an Datenpunkten in jeder Gruppe gleich, so spricht von einem balancierten einfaktoriellen multivariaten Varianzanalysedesign. Von den Datenpunkten der iten Gruppe bzw. des iten Faktorlevels wird angenommen, dass sie Realisierungen von jeweils \\(n_i\\) unabhängigen und identisch multivariat normalverteilten Zufallsvektoren sind, deren wahre, aber unbekannte, Erwartungswertparameter sich potentiell über die Gruppen hinweg unterscheiden und deren wahrer, aber unbekannter, Kovarianzmatrixparameter über die Gruppen hinweg identisch ist. In diesen Grundannahmen handelt es beim Anwendungsszenario der einfaktoriellen multivariaten Varianzanalyse also um die Generalisierung des Einstichproben-T\\(^2\\)-Test Szenarios zu zwei oder mehr Gruppen experimenteller Einheiten. Grundlegend wird voraussgesetzt, dass ein Interesse am einem inferentiellen Vergleich, der wahren, aber unbekannten, faktorlevelspezifischen Erwartungswertparameter besteht.",
    "crumbs": [
      "Multivariate Inferenz",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Einfaktorielle Varianzanalyse</span>"
    ]
  },
  {
    "objectID": "503-Einfaktorielle-Varianzanalyse.html#sec-anwendungsszenario",
    "href": "503-Einfaktorielle-Varianzanalyse.html#sec-anwendungsszenario",
    "title": "38  Einfaktorielle Varianzanalyse",
    "section": "",
    "text": "Anwendungsbeispiel\nAls konkretes Anwendungsbeispiel betrachten wir die Anlayse von Prä-Post-Interventions-BDI-Score und Prä-Post-Interventions-Glukokortikoidplasmalevel Differenzenwerten von drei Gruppen von jeweils 15 Patienti:innen, die unterschiedliche Psychotherapiesettings (Face-to-face und Online) bzw. eine Wartelistenkontrollbedingung durchlaufen haben. Wir stellen dazu in Tabelle 38.1 einen simulierten Beispieldatensatz dar. Die erste Spalte von Tabelle 38.1 (COND) listet das spezifische Therapiesetting (F2F: Face-to-face, ONL: online, WLC: waitlist control) der Patient:innen auf. Die zweite Spalte (dBDI) listet die entsprechenden BDI-Score-Differenzwerte und die dritte Spalte (dGLU) die entsprechenden Glukokortikoidplasmalevel-Differenzwerte auf. In beiden Fällen zeigen positive Werte eine Abnahme der Depressionssymptomatik, negative Werte dagegen einer Zunahme der Depressionssymptomatik an. Abbildung 38.1 visualisiert diesen Datensatz sowie die gruppenspezifischen Stichprobenmittel und Stichprobenkovarianzen als Normalverteilungsisokonturen.\n\n\n\n\nTabelle 38.1: Prä-Post-Interventions-BDI-II-Score und -Glukokortikoidplasmalevel Differenzenwerte von drei Studiengruppen (F2F: Face-to-face, ONL: online, WLC: waitlist control) jeweils 15 Patient:innen\n\n\n\n\n\n\nCOND\ndBDI\ndGLU\n\n\n\n\nF2F\n11\n4.3\n\n\nF2F\n10\n3.9\n\n\nF2F\n12\n3.5\n\n\nF2F\n7\n2.6\n\n\nF2F\n10\n3.3\n\n\nF2F\n12\n3.5\n\n\nF2F\n9\n3.1\n\n\nF2F\n9\n3.6\n\n\nF2F\n9\n5.6\n\n\nF2F\n11\n3.6\n\n\nF2F\n7\n3.4\n\n\nF2F\n9\n4.0\n\n\nF2F\n11\n5.6\n\n\nF2F\n14\n5.3\n\n\nF2F\n8\n3.2\n\n\nONL\n6\n3.1\n\n\nONL\n8\n2.7\n\n\nONL\n7\n2.1\n\n\nONL\n8\n3.1\n\n\nONL\n11\n2.8\n\n\nONL\n9\n2.8\n\n\nONL\n9\n3.7\n\n\nONL\n8\n2.7\n\n\nONL\n6\n3.6\n\n\nONL\n7\n1.4\n\n\nONL\n9\n1.3\n\n\nONL\n8\n3.4\n\n\nONL\n7\n3.7\n\n\nONL\n7\n2.3\n\n\nONL\n9\n3.4\n\n\nWLC\n-2\n0.7\n\n\nWLC\n2\n1.4\n\n\nWLC\n1\n1.0\n\n\nWLC\n2\n0.9\n\n\nWLC\n3\n1.6\n\n\nWLC\n2\n1.6\n\n\nWLC\n5\n1.1\n\n\nWLC\n-1\n-0.2\n\n\nWLC\n2\n2.5\n\n\nWLC\n-2\n0.4\n\n\nWLC\n1\n0.7\n\n\nWLC\n3\n-0.1\n\n\nWLC\n1\n1.3\n\n\nWLC\n4\n0.6\n\n\nWLC\n4\n0.7\n\n\n\n\n\n\n\n\nFolgender R Code demonstriert die Auswertung gruppenspezifischer Deskriptivstatistiken für diesen Datensatz.\n\n# Studiengruppenspezifische Deskriptivstatistiken\nD       = read.csv(\"./_data/503-einfaktorielle-varianzanalyse.csv\")             # Dateneinlesen\nm       = 2                                                                     # Datendimension von Interesse\np       = 3                                                                     # Anzahl Gruppen\nk       = 15                                                                    # Anzahl Datenpunkte pro Gruppe\nY       = array(dim = c(m,k,p))                                                 # Datenarrayinitialisierung\nY[,,1]  = rbind(D$dBDI[D$COND == \"F2F\"],                                        # F2F dBDI Werte\n                D$dGLU[D$COND == \"F2F\"])                                        # F2F dGLU Werte\nY[,,2]  = rbind(D$dBDI[D$COND == \"ONL\"],                                        # ONL dBDI Werte\n                D$dGLU[D$COND == \"ONL\"])                                        # ONL dGLU Werte\nY[,,3]  = rbind(D$dBDI[D$COND == \"WLC\"],                                        # WLC dBDI Werte\n                D$dGLU[D$COND == \"WLC\"])                                        # WLC dGLU Werte \ny_bar_i = array(dim = c(m,p))                                                   # Stichprobenmittelarray\nC_i     = array(dim = c(m,m,p))                                                 # Stichprobenkovarianzmatrizenarray\nj_k     = matrix(rep(1,k), nrow = k)                                            # 1_{l}\nI_k     = diag(k)                                                               # Einheitsmatrix I_l\nJ_k     = matrix(rep(1,k^2), nrow = k)                                          # 1_{ll}\nfor (i in 1:p){                                                                 # Gruppeniterationen\n    y_bar_i[,i] = (1/k)*(Y[,,i] %*% j_k)                                        # Stichprobenmittel \\bar{\\upsilon}_i\n    C_i[,,i]    = (1/(k-1))*(Y[,,i] %*% (I_k-(1/k)*J_k) %*% t(Y[,,i]))}         # Stichprobenkovarianzmatrix C_i\n\n\n\n\n\n\n\nAbbildung 38.1: Deskriptivstatistiken der dBDI, dGLU Daten des Beispieldatensatzes. Jeder Punkt visualisiert die Daten einer Patient:in. Die Stichprobenkovarianz ist durch die 0.7 Isokontur einer zweidimensionalen Normalverteilung mit Erwartungswertparameter und Kovarianzmatrixparameter entsprechend dem Stichprobenmittel und der Stichprobenkovarianz der jeweiligen Gruppe dargestellt.",
    "crumbs": [
      "Multivariate Inferenz",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Einfaktorielle Varianzanalyse</span>"
    ]
  },
  {
    "objectID": "503-Einfaktorielle-Varianzanalyse.html#sec-modellformulierung-und-modellschätzung",
    "href": "503-Einfaktorielle-Varianzanalyse.html#sec-modellformulierung-und-modellschätzung",
    "title": "38  Einfaktorielle Varianzanalyse",
    "section": "38.2 Modellformulierung und Modellschätzung",
    "text": "38.2 Modellformulierung und Modellschätzung\nWir definieren das Modell einfaktoriellen multivariaten Varianzanalyse wie folgt.\n\nDefinition 38.1 (Modell der einfaktoriellen multivariaten Varianzanalyse) Für \\(i = 1,...,p\\) und \\(j = 1,...,n_i\\) seien \\(\\upsilon_{ij}\\) \\(m\\)-dimensionale Zufallsvektoren, die die \\(n := \\sum_{i=1}^p n_i\\) \\(m\\)-dimensionalen Datenpunkte eines einfaktoriellen multivariaten Varianzanalyseszenarios modellieren. Dann hat das Modell der einfaktoriellen multivariaten Varianzanalyse die strukturelle Form \\[\\begin{equation}\n\\upsilon_{ij} = \\mu_i + \\varepsilon_{ij}\n\\mbox{ mit } \\varepsilon_{ij} \\sim N(0_m,\\Sigma) \\mbox{ u.i.v. }\n\\mbox{ mit } \\mu_i \\in \\mathbb{R}^m \\mbox{ und } \\Sigma \\in \\mathbb{R}^{m \\times m} \\mbox{ pd }\n\\end{equation}\\] und die Datenverteilungsform \\[\\begin{equation}\n\\upsilon_{ij} \\sim N(\\mu_i,\\Sigma) \\mbox{ u.v.  mit }  \\mu_i \\in \\mathbb{R}^m  \\mbox{ und } \\Sigma \\in \\mathbb{R}^{m \\times m} \\mbox{ pd}.\n\\end{equation}\\]\n\nIn Definition 38.1 bezeichnet \\(n_i\\) die Anzahl der Zufallsvektoren \\(\\upsilon_{ij}\\) der \\(i\\)ten von \\(p\\) Gruppen experimenteller Einheiten. Im Falle eines balancierten Designs gilt offenbar \\(n_1 = \\cdots = n_p\\). In diesem Fall setzen wir der Einfachheit halber \\(k := n_i\\) für \\(i = 1,...,p\\). In diesem Fall gilt für die Gesamtanzahl an Zufallsvektoren dann \\(n = pk\\). Die Äquivalenz von struktureller Form und Datenverteilungsform der einfaktoriellen multivariaten Varianzanalyse ergibt sich mit Theorem 20.4 durch Transformation der \\(\\varepsilon_{ij}\\) unter Multiplikation mit der Einheitsmatrix und unter Addition der jeweiligen gruppenspezifischen Erwartungswertparameter. Die wahren, aber unbekannten, Parameter \\(\\mu_i, i = 1,...,p\\) und \\(\\Sigma\\) des einfaktoriellen multivariaten Varianzanalysemodells können anhand der in folgendem Theorem definierten Schätzer geschätzt werden.\n\nTheorem 38.1 (Parameterschätzer der einfaktoriellen multivariaten Varianzanalyse) Gegeben sei das Modell der einfaktoriellen multivariaten Varianzanalyse. Dann ist für \\(i = 1,...,p\\) \\[\\begin{equation}\n\\hat{\\mu}_i := \\frac{1}{n_i}\\sum_{j = 1}^{n_i} \\upsilon_{ij}\n\\end{equation}\\] ein unverzerrte Schätzer des gruppenspezifischen Erwartungswertparameters \\(\\mu_i\\) und \\[\\begin{equation}\n\\hat{\\Sigma} := \\frac{1}{n-p}\\sum_{i = 1}^p \\sum_{j = 1}^{n_i} \\left(\\upsilon_{ij} - \\hat{\\mu}_i\\right)\\left(\\upsilon_{ij} - \\hat{\\mu}_i\\right)^T\n\\end{equation}\\] ein unverzerrter Schätzer des Kovarianzmatrixparameters \\(\\Sigma\\).\n\nIn Theorem 38.1 ist \\(\\hat{\\mu}_i\\) offenbar das Stichprobenmittel der Zufallsvektoren der \\(i\\)ten Gruppe. \\(\\hat{\\Sigma}\\) ist die gruppenunspezifische Stichprobenkovarianzmatrix aller Zufallsvektoren und entspricht der mit \\(1/(n-p)\\) skalierten Within-Group Sum-of-Squares Matrix, die wir in Theorem 38.2 einführen werden. Folgender R demonstriert die Evaluation der Parameterschätzer mithilfe einer R Funktion.\n\nestimate = function(Y){\n\n  # Diese Funktion evaluiert die Parameterschätzer einer einfaktoriellen\n  # multivariaten Varianzanalyse basierend auf einen m x k x p Datensatz Y.\n  #\n  # Input\n  #     Y          : m x k x p Datenarray\n  #\n  # Output\n  #     $mu_hat    : m x p \\mu_i Parameterschätzer\n  #     $Sigma_hat : m x m \\Sigma Parametschätzer\n  # ---------------------------------------------------------------------------\n  # Dimensionsparameter\n  d         = dim(Y)                                                            # Datensatzdimensionen\n  m         = d[1]                                                              # Datendimension\n  k         = d[2]                                                              # Anzahl Datenpunkte pro Gruppe\n  p         = d[3]                                                              # Anzahl Gruppen\n\n  # Erwartungswertparameterschätzer\n  mu_hat_i  = matrix(apply(Y,3,rowMeans), nrow = m)\n\n  # Kovarianzmatrixparameterschätzer\n  Sigma_hat = matrix(rep(0,m*m), nrow = m)\n  for(i in 1:p){\n      for(j in 1:k){\n          Sigma_hat = Sigma_hat + (1/(k*p-p))*(Y[,j,i] - mu_hat_i[,i]) %*% t(Y[,j,i] - mu_hat_i[,i])\n    }\n  }\n\n  # Outputspezifikation\n  return(list(mu_hat_i = mu_hat_i, Sigma_hat = Sigma_hat))}\n\nAnstelle eines Beweises validieren wir die Aussage von Theorem 38.1 beispielhaft mithilfe folgender R Simulation, in der wir die Erwartungswerte der Schätzer durch ihre Stichprobenmittelwerte über Datensatzrealisierungen hinweg approximieren.\n\n# Modellparameter\nlibrary(MASS)                                                                   # multivariate Normalverteilungen\np          = 3                                                                  # Anzahl Gruppen\nk          = 15                                                                 # Anzahl Datenpunkte pro Gruppe\nm          = 2                                                                  # Datendimension\nmu_i       = matrix(c(1,2,2,1,3,2.5), ncol = p)                                 # Erwartungswertparameter\nSigma      = matrix(c(1,.5,.5,1)    , ncol = m)                                 # Kovarianzmatrixparameter\n\n# Simulationsparameter und Arrays\nnsm        = 1e2                                                                # Anzahl Simulation\nmu_hat_is  = array(dim = c(m,p,nsm))                                            # \\hat{\\mu}_i Array\nSigma_hats = array(dim = c(m,m,nsm))                                            # \\hat{\\Sigma} Array\n\n# Simulationen\nfor(s in 1:nsm){\n\n    # Datengeneration\n    Y               = array(dim = c(m,k,p))                                     # Datenarray\n    for(i in 1:p){\n        Y[,,i] = t(mvrnorm(k,mu_i[,i],Sigma))                                   # Datengeneration\n    }\n    S               = estimate(Y)                                               # Parameterschätzung\n    mu_hat_is[,,s]  = S$mu_hat_i                                                # \\hat{\\mu}_i\n    Sigma_hats[,,s] = S$Sigma_hat                                               # \\hat{\\Sigma}\n}\n\n# Schätzererwartungswertschätzung\nE_hat_mu_i_hat  = apply(mu_hat_is , c(1,2), mean)\nE_hat_Sigma_hat = apply(Sigma_hats, c(1,2), mean)\n\nWie in Abbildung 38.2 ergibt sich hier auch schon bei einem recht geringen Simulationsaufwand von 100 Datensatzrealisierungen eine gute Korrespondenz zwischen wahren, aber unbekannten, Parameterwerten \\(\\mu_i, i = 1,...,p\\) und \\(\\Sigma\\) und den approximierten Erwartungswertparametern \\(\\mathbb{E}(\\hat{\\mu}_i)\\) für \\(i = 1,...,p\\) und \\(\\mathbb{E}(\\hat{\\Sigma})\\).\n\n\n\n\n\n\nAbbildung 38.2: Simulationsbasierte Validierung des Theorems zu den Parameterschätzern der einfaktoriellen multivariaten Varianzanalyse am Beispiel von \\(m := 2,p := 3, k := 15\\) und 100 Realisierungen der entsprechenden multivariat normalverteilten Zufallsvektoren\n\n\n\nDie Anwendung der Parameterschätzung auf die Daten des Beispieldatensatzes in Tabelle 38.1 ergibt folgende Resultate. \n\nS = estimate(Y)                                                                 # Parameterschätzung\n\n\n\n         [,1]     [,2]      [,3]\n[1,] 9.933333 7.933333 1.6666667\n[2,] 3.900525 2.803179 0.9594878\n\n\n          [,1]      [,2]\n[1,] 3.3142857 0.4022937\n[2,] 0.4022937 0.6381336",
    "crumbs": [
      "Multivariate Inferenz",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Einfaktorielle Varianzanalyse</span>"
    ]
  },
  {
    "objectID": "503-Einfaktorielle-Varianzanalyse.html#sec-modellevaluation",
    "href": "503-Einfaktorielle-Varianzanalyse.html#sec-modellevaluation",
    "title": "38  Einfaktorielle Varianzanalyse",
    "section": "38.3 Modellevaluation",
    "text": "38.3 Modellevaluation\nPrimäres Ziel einer einfaktoriellen multifaktoriellen Varianzanalyse ist meist das Testen der Nullhypothese \\[\\begin{equation}\nH_0 : \\mu_1 =  \\cdots = \\mu_p.\n\\end{equation}\\] Diese Nullhypothese besagt, dass keine Unterschiede zwischen den wahren, aber unbekannten, Erwartungswertparametern der Stichprobengruppen bestehen. Die Alternativhypothese lautet somit \\[\\begin{align}\n\\begin{split}\nH_1 : \\mu_{i_l} \\neq \\mu_{j_l}\n&\n\\mbox{ für mindestens ein Paar } i,j\n\\mbox{ mit } i \\neq j,  1 \\le i,j \\le p\n\\\\\n&\n\\mbox{ und mindestens ein } l\n\\mbox{ mit } 1 \\le l \\le m.\n\\end{split}\n\\end{align}\\] Die Alternativhypothese besagt also, dass sich mindestens zwei wahre, aber unbekannten, Erwartungswertparameter in mindestens einer ihrer Komponenten unterscheiden. Wie aus dem Kontext der univariaten einfaktorielle Varianzanalyse bekannt impliziert das Ablehnen der Nullhypothese auch hier keine Aussage über die genaue Form des inferierten Erwartungswertparameterunterschiedes.\nIm Rahmen einfaktoriellen multivariaten Varianzanalyse können Tests der Nullhypothese mit verschiedenen Teststatistiken konstruiert werden. Diesen Teststatistiken ist gemein, dass sie auf eine Generalisierung der aus dem univariaten Fall bekannten Quadratsummenzerlegung der einfaktoriellen Varianzanalyse zurückgehen. Wir führen im nächsten Abschnitt zunächst diese sogenannte Kreuzproduktsummenmatrizenzerlegung der einfaktoriellen multivariaten Varianzanalyse ein. Nachfolgend betrachten wir dann die Modellevaluation mithilfe der Wilks’-\\(\\Lambda\\)-Statistik.\n\nKreuzproduktsummenmatrizenzerlegung\nFolgendes Theorem generalisiert die Quadratsummenzerlegung der einfaktoriellen Varianzanalyse auf das multivariate Anwendungsszenario.\n\nTheorem 38.2 (Kreuzproduktsummenmatrizenzerlegung)  \nGegeben sei das Modell der einfaktoriellen multivariaten Varianzanalyse. Weiterhin seien \\[\\begin{equation}\n\\bar{\\upsilon}    := \\frac{1}{n}\\sum_{i = 1}^p \\sum_{j = 1}^{n_i} \\upsilon_{ij}\n\\mbox{ und }\n\\bar{\\upsilon}_i  := \\frac{1}{n_i} \\sum_{j = 1}^{n_i} \\upsilon_{ij}\n\\end{equation}\\] das und das , respektive. Schließlich seien\nDann gilt \\[\\begin{equation}\nT = B + W.\n\\end{equation}\\]\n\n\nBeweis. Es gilt \\[\\begin{align}\n\\begin{split}\nT\n& = \\sum_{i=1}^p \\sum_{j=1}^{n_i}(\\upsilon_{ij}-\\bar{\\upsilon})(\\upsilon_{ij}-\\bar{\\upsilon})^T\n\\\\\n& = \\sum_{i=1}^p \\sum_{j=1}^{n_i} (\\upsilon_{ij}-\\bar{\\upsilon}_i+\\bar{\\upsilon}_i-\\bar{\\upsilon})(\\upsilon_{ij}-\\bar{\\upsilon}_i+\\bar{\\upsilon}_i-\\bar{\\upsilon})^T\n\\\\\n& = \\sum_{i=1}^p \\sum_{j=1}^{n_i} \\left((\\upsilon_{ij}-\\bar{\\upsilon}_i)+(\\bar{\\upsilon}_i-\\bar{\\upsilon})\\right)\\left((\\upsilon_{ij}-\\bar{\\upsilon}_i)+(\\bar{\\upsilon}_i-\\bar{\\upsilon})\\right)^T\n\\\\\n& = \\sum_{i=1}^p \\sum_{j=1}^{n_i} \\left(\n                                      (\\upsilon_{ij}-\\bar{\\upsilon}_i)(\\upsilon_{ij}-\\bar{\\upsilon}_i)^T\n                                    +2(\\upsilon_{ij}-\\bar{\\upsilon}_i)(\\bar{\\upsilon}_i-\\bar{\\upsilon})^T\n                                    + (\\bar{\\upsilon}_i-\\bar{\\upsilon})(\\bar{\\upsilon}_i-\\bar{\\upsilon})^T\n                                \\right)\n\\\\\n& = \\sum_{i=1}^p \\left(\n                       \\sum_{j=1}^{n_i} (\\upsilon_{ij}-\\bar{\\upsilon}_i)(\\upsilon_{ij}-\\bar{\\upsilon}_i)^T\n                      +\\sum_{j=1}^{n_i}2(\\upsilon_{ij}-\\bar{\\upsilon}_i)(\\bar{\\upsilon}_i-\\bar{\\upsilon})^T\n                      +\\sum_{j=1}^{n_i}(\\bar{\\upsilon}_i-\\bar{\\upsilon})(\\bar{\\upsilon}_i-\\bar{\\upsilon})^T\n                 \\right)\n\\\\\n& = \\sum_{i=1}^p \\left(\n                       \\sum_{j=1}^{n_i} (\\upsilon_{ij}-\\bar{\\upsilon}_i)(\\upsilon_{ij}-\\bar{\\upsilon}_i)^T\n                      +2\\left(\\sum_{j=1}^{n_i}(\\upsilon_{ij}-\\bar{\\upsilon}_i)\\right)(\\bar{\\upsilon}_i-\\bar{\\upsilon})^T\n                      +n_i(\\bar{\\upsilon}_i-\\bar{\\upsilon})(\\bar{\\upsilon}_i-\\bar{\\upsilon})^T\n                 \\right)\n\\\\\n& = \\sum_{i=1}^p \\left(\n                       \\sum_{j=1}^{n_i} (\\upsilon_{ij}-\\bar{\\upsilon}_i)(\\upsilon_{ij}-\\bar{\\upsilon}_i)^T\n                      +2\\left(\\sum_{j=1}^{n_i}\\left(\\upsilon_{ij}-\\frac{1}{n_i}\\sum_{j=1}^{n_i} \\upsilon_{ij}\\right)\\right)(\\bar{\\upsilon}_i-\\bar{\\upsilon})^T\n                      +n_i(\\bar{\\upsilon}_i-\\bar{\\upsilon})(\\bar{\\upsilon}_i-\\bar{\\upsilon})^T\n                 \\right)\n\\\\\n& = \\sum_{i=1}^p \\left(\n                       \\sum_{j=1}^{n_i} (\\upsilon_{ij}-\\bar{\\upsilon}_i)(\\upsilon_{ij}-\\bar{\\upsilon}_i)^T\n                      +2\\left(\\sum_{j=1}^{n_i} \\upsilon_{ij}-\\sum_{j=1}^{n_i} \\upsilon_{ij}\\right) (\\bar{\\upsilon}_i-\\bar{\\upsilon})^T\n                      +n_i(\\bar{\\upsilon}_i-\\bar{\\upsilon})(\\bar{\\upsilon}_i-\\bar{\\upsilon})^T\n                 \\right)\n\\\\\n& = \\sum_{i=1}^p \\left(\n                       \\sum_{j=1}^{n_i} (\\upsilon_{ij}-\\bar{\\upsilon}_i)(\\upsilon_{ij}-\\bar{\\upsilon}_i)^T\n                       +n_i(\\bar{\\upsilon}_i-\\bar{\\upsilon})(\\bar{\\upsilon}_i-\\bar{\\upsilon})^T\n                 \\right)\n\\\\\n& = \\sum_{i=1}^p n_i(\\bar{\\upsilon}_i-\\bar{\\upsilon})(\\bar{\\upsilon}_i-\\bar{\\upsilon})^T\n   +\\sum_{i=1}^p \\sum_{j=1}^{n_i} (\\upsilon_{ij}-\\bar{\\upsilon}_i)(\\upsilon_{ij}-\\bar{\\upsilon}_i)^T\n\\\\\n& = B + W.\n\\end{split}\n\\end{align}\\]\n\nDie intuitive Interpretation der Totalen, Between-Group, und Within-Group Sum-of-Squares Matrizen ist analog zu den aus dem univariaten Szenario bekannten Begriffen: Die Matrix \\(T\\) repräsentiert die totale Variabilität der Datenvektoren um das Gesamtstichprobenmittel, die Matrix \\(B\\) repräsentiert die Variabilität der Gruppenstichprobenmittel um das Gesamtstichprobenmittel und die Matrix \\(W\\) repräsentiert die Variabilität der Datenvektoren um ihre jeweiligen Gruppenstichprobenmittel. Wie im univariaten Fall wird auch hier also die Gesamtdatenvariabilität additiv in zwei unabhängige Beiträge zerlegt. Dabei kann die Matrix \\(W\\) auch als Maß für die Residualvariabiliät verstanden werden, weil sie die verbleibende Variabilität nach Schätzung der Gruppenerwartungswertparameter quantifiziert. Offenbar gilt für den Schätzer \\(\\hat{\\Sigma}\\) des gemeinsamen Stichprobenkovarianzmatrixparameters aus Theorem 38.1 \\[\\begin{equation}\nW = (n - p)\\hat{\\Sigma}.\n\\end{equation}\\] Folgender R demonstriert die Evaluation der in Theorem 38.2 definierten Matrizen mithilfe einer R Funktion.\n\nsos = function(Y){\n\n  # Diese Funktion evaluiert die Kreuzproduktsummenmatrizen T,B,W einer\n  # einfaktoriellen Varianzanalyse basierend auf einen m x k x p Datensatz Y.\n  #\n  # Input\n  #     Y        : m x k x p Datenarray\n  #\n  # Output\n  #     $y_bar   : m x 1 Gesamtmittelwert\n  #     $y_bar_i : m x p Gruppenmittelwerte\n  #     $T       : m x m Total Sum of Squares Matrix\n  #     $B       : m x m Between-Group Sum-of-Squares Matrix\n  #     $W       : m x m Within  Group  Sum of Squares Matrix\n  # ---------------------------------------------------------------------------\n  d        = dim(Y)                                                             # Datensatzdimensionen\n  m        = d[1]                                                               # Datendimension\n  k        = d[2]                                                               # Anzahl Datenpunkte pro Gruppe\n  p        = d[3]                                                               # Anzahl Gruppen\n\n  # Mittelwerte\n  y_bar_i  = matrix(apply(Y,3,rowMeans), nrow = m)                              # Gruppenstichprobenmittel\n  y_bar    = matrix(rowMeans(y_bar_i)  , nrow = m)                              # Gesamtstichprobenmittel\n\n  # Totale Sum-of-Squares Matrix\n  T = matrix(rep(0,m*m), nrow = m)\n  for(i in 1:p){\n      for(j in 1:k){\n          T = T + (Y[,j,i] - y_bar) %*% t(Y[,j,i] - y_bar)}}\n\n  # Between Sum of Squares Matrix\n  B = matrix(rep(0,m*m), nrow = m)\n  for(i in 1:p){\n    B = B + k*(y_bar_i[,i] - y_bar) %*% t(y_bar_i[,i] - y_bar)}\n\n  # Within Sum of Squares Matrix\n  W = matrix(rep(0,m*m), nrow = m)\n  for(i in 1:p){\n      for(j in 1:k){\n          W = W + (Y[,j,i] - y_bar_i[,i]) %*% t(Y[,j,i] - y_bar_i[,i])}}\n\n  # Outputspezifikation\n  return(list(y_bar_i = y_bar_i, y_bar = y_bar, T = T, B = B, W = W))}\n\n\n\nModellevaluation mit der Wilks’-\\(\\Lambda\\)-Statistik\nBasierend auf der Kreuzproduktsummenmatrizenzerlegung in Theorem 38.2 wurde eine Reihe von Teststatisken für die einfaktorielle multivariate Varianzanalyse vorgeschlagen (Wilks (1932), Pillai (1955), Roy & Bose (1953), Hotelling (1951)). Wir betrachten hier exemplarisch lediglich die Wilks’-\\(\\Lambda\\)-Statistik nach Wilks (1932). Im Gegensatz zur \\(F\\)-Teststatistik der univariaten einfaktoriellen Varianzanalyse sind die Frequentistischen Verteilungen von der Wilks’-\\(\\Lambda\\)-Statistik bei Zutreffen der Nullhypothese nur für bestimmte Anwendungsszenarien, insbesondere bei kleinen Werte der Datendimension \\(m\\) und der Gruppenanzahl \\(p\\), analytisch exakt zu bestimmen. In diesen Szenarien ist man auf \\(f\\)-Verteilungen mit \\(m\\)- und \\(p\\)-abhängigen Freiheitsgradparametern geführt. Für Anwendungsszenarien mit größeren Werten von \\(m\\) und/oder \\(p\\) existieren lediglich Approximationen der Frequentistischen Verteilungen der Wilks’-\\(\\Lambda\\)-Statistik, die nur asymptotisch für unendlich große Stichprobenumfänge \\(n \\to \\infty\\) exakt sind. Auch diese Approximationen sind wiederum durch \\(f\\)-Verteilungen mit \\(m\\)- und \\(p\\)-abhängigen Freiheitsgradparametern gegeben. In der Anwendung unterscheiden sich Testentscheidungen basierend auf exakten oder approximativen Verteilungen der verschiedenen Teststatistiken meist nicht. Zur Absicherung dieser Aussage mögen im konkreten Fall von Datendimension und Gruppengröße Simulationen helfen, mögliche Unterschiede zwischen der Wilks’-\\(\\Lambda\\)-Statistik und anderen Teststatistiken sowie der Approximation ihrer Verteilungen abzuschätzen.\nWir definieren zunächst die Wilks’-\\(\\Lambda\\)-Statistik.\n\nDefinition 38.2 (Wilks’-\\(\\Lambda\\)-Statistik) Gegeben sei das Modell der einfaktoriellen multivariaten Varianzanalyse sowie die Between-Group Sum-of-Squares Matrix \\(B\\) und die Within-Group Sum-of-Squares Matrix \\(W\\). Dann ist Wilks’-\\(\\Lambda\\)-Statistik definiert als \\[\\begin{equation}\n\\Lambda := \\frac{|W|}{|T|} = \\frac{|W|}{|B+W|}.\n\\end{equation}\\]\n\nIntuitiv misst \\(\\Lambda\\) das Verhältnis von Residualdatenvariabilität, repräsentiert durch die Determinante von \\(W\\), und Gesamtdatenvariabilität, repräsentiert durch die Determinante von \\(T = B+W\\). \\(\\Lambda\\) ist damit also analog zum Effektstärkenmaß \\(\\eta^2\\) der univariaten einfaktoriellen Varianzanalyse definiert. Im Falle der Gleichheit der gruppenspezifischen Stichprobenmittel gilt für \\(\\Lambda\\) insbesondere \\[\\begin{equation}\n\\bar{\\upsilon}_1 = \\cdots = \\bar{\\upsilon}_p = \\bar{\\upsilon}\n\\Rightarrow\nB = 0_{mm}\n\\Rightarrow\n\\Lambda = \\frac{|W|}{|T|} = \\frac{|W|}{|B+W|} = \\frac{|W|}{|0_{mm} + W|} = \\frac{|W|}{|W|} = 1.\n\\end{equation}\\] Für ansteigende Unterschiede zwischen den gruppenspezifischen Stichprobenmittel \\(\\bar{\\upsilon}_i\\) nimmt \\(|B+W|\\) gegenüber \\(|W|\\) zu und \\(\\Lambda\\) somit ab. Ohne Beweis wollen wir dabei festhalten, dass \\(0 \\le \\Lambda \\le 1\\). Im Gegensatz zu den meisten bekannten Teststatistiken sprechen hier also kleine Werte der Teststatistik \\(\\Lambda\\) für eine Abweichung von der Nullhypothese.\nÄquivalent kann \\(\\Lambda\\) auch in Form der Eigenwerte der Matrix \\(W^{-1}B\\) angegeben werden. Dabei ist die Matrix \\(W^{-1}B\\) das multivariate Analogon zum dem aus der univariaten einfaktoriellen Varianzanalyse bekanntem Verhältnis von Between-Group Sum-of-Squares und Within-Group Sum-of-Squares, welches die Grundlage für \\(F\\)-Teststatitik im univariaten Fall bildet. Es gilt folgendes Theorem.\n\nTheorem 38.3 (Eigenwertform von der Wilks’-\\(\\Lambda\\)-Statistik) Es seien das Modell der einfaktoriellen multivariaten Varianzanalyse, die Between-Group Sum-of-Squares Matrix \\(B\\), die Within-Group Sum-of-Squares Matrix \\(W\\) und die Wilks’-\\(\\Lambda\\)-Statistik definiert wie oben. Weiterhin seien \\(\\lambda_1,...,\\lambda_s\\) die Eigenwerte von \\(W^{-1}B\\). Dann gilt \\[\\begin{equation}\n\\Lambda = \\prod_{i=1}^s \\frac{1}{1 + \\lambda_i}.\n\\end{equation}\\]\n\nWir verzichten auf einen Beweis.\nFür spezielle, durch die Datendimension \\(m\\) und die Gruppenanzahl \\(p\\) charakterisierte Anwendungsszenarien stellt Wilks (1932) analytische Formen für die Verteilungen von Transformationen von \\(\\Lambda\\) unter der Nullhypothese bereit. Dabei entsprechen diese analytischen Formen den Verteilungen von \\(f\\)-Zufallsvariablen mit bestimmten, durch die Datendimension und Gruppenanzahl des Anwendungsszenario vorgegebenen Freiheitsgradparametern. Wir fassen die Anwendungsszenarien, die betreffende Transformation von \\(\\Lambda\\) und die analytischen Formen ihrer Frequentistischen Verteilungen bei Zutreffen der Nullhypothese in folgendem Theorem zusammen.\n\nTheorem 38.4 (Spezielle \\(H_0\\) Verteilungen von Transformationen der Wilks’-\\(\\Lambda\\)-Statistik)  \nEs seien das Modell der einfaktoriellen Varianzanalyse und die Wilks’-\\(\\Lambda\\)-Statistik definiert wie oben und es gelte außerdem die Nullhypothese \\[\\begin{equation}\nH_0 : \\mu_1 =  \\cdots = \\mu_p.\n\\end{equation}\\] Dann sind für die in den ersten beiden Tabellenspalten aufgeführten Spezialfällen die in der dritten Tabellenspalte aufgeführten Statistiken \\(f\\)-Zufallsvariablen und zwar mit den in der vierten Tabellenspalte aufgeführten Freiheitsgradparametern.\n\nIn Abbildung 38.3 visualisieren wir exemplarische simulationsbasierte Validierungen der in Theorem 38.4 angegebenen Verteilungen.\n\n\n\n\n\n\nAbbildung 38.3: Simulation spezieller Verteilungen der Wilks’-\\(\\Lambda\\)-Statistik Transformationen bei Zutreffen der Nullhypothese.\n\n\n\nRao (1951) hat für allgemeine Anwendungsszenarien die in folgendem Theorem angegebenen Approximationen von Verteilungen von Transformationen der Wilks’-\\(Lambda\\)-Statistik vorgeschlagen. Man beachte, dass sich die in diesem Theorem definierte Teststatistik \\(\\tau\\) wie in Abbildung 38.4 gezeigt zu \\(\\Lambda\\) reziprok verhält. Geringe Werte von \\(\\Lambda\\) als Evidenz gegen die Nullhypothese entsprechen also hohen Werten von \\(\\tau\\).\n\nTheorem 38.5 (Approximative \\(H_0\\) Verteilungen von Transformationen der Wilks’-\\(\\Lambda\\)-Statistik) Es seien das Modell der einfaktoriellen Varianzanalyse und die Wilks’-\\(\\Lambda\\)-Statistik definiert wie oben und es gelte außerdem die Nullhypothese \\[\\begin{equation}\nH_0 : \\mu_1 =  \\cdots = \\mu_p.\n\\end{equation}\\] Dann ist die Statistik \\[\\begin{equation}\n\\tau := \\frac{1 - \\Lambda^{1/t}}{\\Lambda^{1/t}} \\frac{\\nu_2}{\\nu_1}\n\\end{equation}\\] mit \\[\\begin{equation}\n\\nu_1 := m(p-1)\n\\mbox{ und }\n\\nu_2 := wt-\\frac{1}{2}(m(p-1)-2)\n\\end{equation}\\] sowie \\[\\begin{equation}\nw := n-1-\\frac{1}{2}(m+p)\n\\mbox{ und }\nt   := \\sqrt{\\frac{m^2(p-1)^2 - 4}{m^2 + (p-1)^2 - 5}}\n\\end{equation}\\] approximativ \\(f\\)-verteilt mit Freiheitsgradparametern \\(\\nu_1\\) und \\(\\nu_2\\).\n\n\n\n\n\n\n\nAbbildung 38.4: Beziehung der Wilks’-\\(\\Lambda\\)-Statistik und der von Rao (1951) betrachteten \\(\\tau\\) Statistik in exemplarischen Anwendungsszenarien.\n\n\n\nIn Abbildung 38.5 visualisieren wir exemplarische simulationsbasierte Validierungen der in Theorem 38.5 angegebenen Verteilungen.\n\n\n\n\n\n\nAbbildung 38.5: Simulation approximativer Verteilungen der Wilks’-\\(\\Lambda\\)-Statistik Transformationen bei Zutreffen der Nullhypothese.\n\n\n\nMithilfe der von Rao (1951) bestimmten approximativen Verteilungen der transformierten Wilks’-\\(\\Lambda\\)-Statistik \\(\\tau\\) bei Zutreffen der Nullhypothese können wir nun einen Hypothesentest für die einfaktorielle multivariate Varianzanalyse angeben.\n\nTheorem 38.6 (Wilks’-\\(\\Lambda\\)-basierter Test, Testumfangkontrolle, p-Wert) Gegeben seien Modell der einfaktoriellen multivariaten Varianzanalyse und die transfomierte Wilks’-\\(\\Lambda\\)-Statistik \\(\\tau\\) mit Verteilungsparametern \\(\\nu_1,\\nu_2\\) wie oben definiert. Weiterhin sei für \\(\\Upsilon = (\\upsilon_1,...,\\upsilon_n)\\) der kritische Wert-basierte Test \\[\\begin{equation}\n\\phi(\\Upsilon) := 1_{\\{\\tau &gt; k\\}} :=\n\\begin{cases}\n1 & \\tau &gt;   k \\\\\n0 & \\tau \\le k\n\\end{cases}\n\\end{equation}\\] definiert. Dann ist \\(\\phi\\) genau dann ein Level-\\(\\alpha_0\\)-Test mit Testumfang \\(\\alpha\\), wenn \\[\\begin{equation}\nk := k_{\\alpha_0} := F^{-1}(1-\\alpha_0; \\nu_1,\\nu_2)\n\\end{equation}\\] ist und der p-Wert einer realisiertern \\(\\tau\\)-Teststatistik \\(\\tilde{\\tau}\\) ergibt sich zu \\[\\begin{equation}\n\\mbox{p-Wert} = \\mathbb{P}(\\tau \\ge \\tilde{\\tau}) = 1 - F(\\tilde{\\tau}; \\nu_1,\\nu_2)\n\\end{equation}\\]\n\nWir verzichten auf einen Beweis von Theorem 38.6, welcher analog zu den entsprechenden Beweisen zum Beispiel beim Einstichproben-T\\(^2\\)-Test geführt werden kann. Die in Theorem 38.6 implizite Wahl eines kritischen Wertes zur Testumfangkontrolle im Anwendungsszenario \\(m := 3, p := 4\\) und \\(n_i = 15\\) für \\(i = 1,..,p\\) und damit \\(\\nu_1 = 9\\) und \\(\\nu_2 = 132\\) bei einem Signifikanzlevel von \\(\\alpha_0 = 0.05\\) visualisieren wir in Abbildung 38.6.\n\n\n\n\n\n\nAbbildung 38.6: Testumfangkontrolle durch Selektion eines \\(\\alpha_0\\)-abhängigen kritischen Wertes für den Wilks’\\(\\Lambda\\)-Statistik-basierten Test anhand von KVF unf WDF der transformierten Wilks’\\(\\Lambda\\)-Statistik \\(\\tau\\).\n\n\n\n\n\nPraktisches Vorgehen\nIn der Praxis entsprechen obige Ergebnisse dann folgendem Vorgehen bei der Durchführung einer einfaktoriellen multivariaten Varianzanalyse: Man unterstellt, dass ein vorliegender Datensatz von \\(i = 1,...,p\\) Gruppen von \\(m\\)-dimensionalen Datenvektoren für jeweils \\(j = 1,...,n_i\\) eine Realisation von Zufallsvektoren \\(\\upsilon_{ij} \\sim N(\\mu_i,\\Sigma)\\) mit unbekannten, gruppenspezifischen Erwartungswertparametern \\(\\mu_i \\in \\mathbb{R}^m\\) und gruppenunspezifischem Kovarianzmatrixparameter \\(\\Sigma \\in \\mathbb{R}^{m \\times m} \\mbox{ pd}\\) ist. Man möchte entscheiden, ob eher die Nullhypothese \\(H_0 : \\mu_1 = \\cdots = \\mu_p\\) identischer wahrer, aber unbekannter, Erwartungswertparameter zutrifft oder eher nicht. Zu diesem Zweck wählt man zunächst ein Signifikanzlevel \\(\\alpha_0\\) und bestimmt dann den zugehörigen Freiheitsgradparameter-abhängigen kritischen Wert \\(k_{\\alpha_0}\\). Zum Beispiel gilt bei Wahl von \\(\\alpha_0 := 0.05\\) im Szenario von dreidimensionalen Datenvektoren (\\(m = 3\\)), vier Gruppen (\\(p = 4\\)) und \\(n_i = 15\\) experimentellen Einheiten pro Gruppe und damit einer Gesamtzahl von \\(n = 60\\) Datenpunkten, dass \\(\\nu_1 = 9\\) und \\(\\nu_2 = 132\\) sind und sich der in Theorem 38.6 definierte kritische Wert zu \\(k_{\\alpha_0} = F^{-1}(1 - 0.05; 9, 132) \\approx 1.95\\) ergibt. Basierend auf den realisierten Datensatz berechnet man dann zunächst die Wilk’s-\\(\\Lambda\\)-Statistik und den resultierenden, \\(m,p,n\\)-abhängigen realisierten Wert der transformierten Wilk’s-\\(\\Lambda\\)-Statistik \\(\\tau\\). Wenn der berechnete Wert von \\(\\tau\\) größer als \\(k_{\\alpha_0}\\) ist, lehnt man die Nullhypothese ab, andernfalls nicht. Die oben entwickelte Theorie zur Testumfangkontrolle bei der einfaktoriellen multivariaten Varianzanalyse auf Grundlage von Wilks’-\\(\\Lambda\\)-Statistik garantiert dann, dass man im Mittel in höchstens \\(\\alpha_0 \\cdot 100\\) von 100 Fällen die Nullhypothese fälschlicherweise ablehnt.\nFolgender R Code demonstriert die Anwendung des in Theorem 38.6 definierten Hypothesentests auf bei Zutreffen der Nullhypothese unter dem Modell der einfaktoriellen multivariaten Varianzanalyse generierte Daten und validiert seine Kontrolle des Testumfangs für die in Abbildung 38.5 skizzierten Anwendungszenarien.\n\n# Szenarioparameter\nlibrary(MASS)                                                                   # Multivariate Normalverteilungen\nnsm       = 1e4                                                                 # Datensimulationsanzahl\nM         = c(3,3,4,9)                                                          # Datendimension\nP         = c(4,9,4,4)                                                          # Gruppenanzahl\nk         = 15                                                                  # Datenpunkte pro Gruppe\nN         = k*P                                                                 # Gesamtanzahl Datenpunkte\nalpha_0   = 0.05                                                                # \\alpha_0\nnsc       = length(M)                                                           # Szenarienanzahl\nTAU       = matrix(rep(NaN,nsm*nsc), ncol = nsc)                                # Statistik Array\nNU        = matrix(rep(NaN,2*nsc)  , ncol = nsc)                                # Parameter Array\nKA        = rep(NaN, nsc)                                                       # Kritische Werte\nPHI       = matrix(rep(0,nsm*nsc)  , ncol = nsc)                                # Testarray\n\n# Simulationen\nfor(sc in 1:nsc){                                                               # Szenarioiterationen\n\n    # Modellparameter\n    m         = M[sc]                                                           # Datendimension\n    p         = P[sc]                                                           # Gruppenanzahl\n    n         = N[sc]                                                           # Gesamtanzahl Datenpunkte\n    mu_i      = matrix(rep(0,m), nrow = m)                                      # Identische Gruppenerwartungswertparameter bei H_0\n    Sigma     = diag(m)                                                         # Identische Gruppenkovarianzmatrixparameter\n\n    # Varianzanalyse Parameter\n    w         = n-1-(1/2)*(m+p)                                                 # w\n    t         = sqrt((m^2*(p-1)^2-4)/(m^2+(p-1)^2-5))                           # t\n    nu_1      = m*(p-1)                                                         # \\nu_1\n    nu_2      = w*t-(1/2)*(m*(p-1)-2)                                           # \\nu_2\n    KA[sc]    = qf(1-alpha_0,nu_1,nu_2)                                         # kritischer Wert\n\n    # Datensimulationen\n    for(sm in 1:nsm){\n        Y     = array(dim = c(m,k,p))                                           # Datenanarrayinitialisierung\n        for(i in 1:p){                                                          # Gruppeniterationen\n            Y[,,i] =  t(mvrnorm(k,mu_i,Sigma))}                                 # Datensimulation\n        S          = sos(Y)                                                     # Stichprobenmittel und Sum of Squares Matrizen\n        L          = det(S$W)/det(S$W + S$B)                                    # Wilks' Lambda\n        tau        = ((1-L^(1/t))/L^(1/t))*(nu_2/nu_1)                          # Statistik\n        PHI[sm,sc] = tau &gt; KA[sc]}}                                             # Test\n\n\n\nKritische Werte       :  1.951729 1.547641 1.821661 1.565152 \nGeschätzte Testumfänge:  0.0472 0.0496 0.0522 0.0488",
    "crumbs": [
      "Multivariate Inferenz",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Einfaktorielle Varianzanalyse</span>"
    ]
  },
  {
    "objectID": "503-Einfaktorielle-Varianzanalyse.html#anwendungsbeispiel-1",
    "href": "503-Einfaktorielle-Varianzanalyse.html#anwendungsbeispiel-1",
    "title": "38  Einfaktorielle Varianzanalyse",
    "section": "38.4 Anwendungsbeispiel",
    "text": "38.4 Anwendungsbeispiel\nWir betrachten das eingangs diskutierte Anwendungsbeispiel eines simulierten zweidimensionalen Datensatzes dreier Therapiegruppen. Wir wollen nun mithilfe einer einfaktoriellen Varianzanalyse für diesen Datensatz die Nullhypothese identischer Gruppenerwartungswertparameter überprüfen. Folgender R Code implementiert das praktische Vorgehen für ein Signifikanzlevel von \\(\\alpha_0 := 0.05\\).\n\n# Einlesen und Präprozessierung des Datensatzes\nD           = read.csv(\"./_data/503-einfaktorielle-varianzanalyse.csv\")         # Dateneinlesen\nm           = 2                                                                 # Datendimension von Interesse\np           = 3                                                                 # Anzahl Gruppen\nk           = 15                                                                # Anzahl Datenpunkte pro Gruppe\nn           = p*k                                                               # Gesamtanzahl Datenpunkte\nY           = array(dim = c(m,k,p))                                             # Datenarrayinitialisierung\nY[,,1]      = rbind(D$dBDI[D$COND == \"F2F\"],                                    # F2F dBDI Werte\n                    D$dGLU[D$COND == \"F2F\"])                                    # F2F dGLU Werte\nY[,,2]      = rbind(D$dBDI[D$COND == \"ONL\"],                                    # ONL dBDI Werte\n                    D$dGLU[D$COND == \"ONL\"])                                    # ONL dGLU Werte\nY[,,3]      = rbind(D$dBDI[D$COND == \"WLC\"],                                    # WLC dBDI Werte\n                    D$dGLU[D$COND == \"WLC\"])                                    # WLC dGLU Werte         \n\n# Einfaktorielle Varianzanalyse\nalpha_0     = 0.05                                                              # Signifikanzlevel\nS           = sos(Y)                                                            # Sum of Squares Matrizen\nL           = det(S$W)/det(S$W + S$B)                                           # Wilks' Lambda\nw           = n-1-(1/2)*(m+p)                                                   # w\nt           = sqrt((m^2*(p-1)^2-4)/(m^2+(p-1)^2-5))                             # t\nnu_1        = m*(p-1)                                                           # \\nu_1\nnu_2        = w*t-(1/2)*(m*(p-1)-2)                                             # \\nu_2\ntau         = ((1-L^(1/t))/L^(1/t))*(nu_2/nu_1)                                 # Teststatistik\nk_alpha_0   = qf(1-alpha_0,nu_1,nu_2)                                           # kritischer Wert\nphi         = as.numeric(tau &gt; k_alpha_0)                                       # Nullhypothesentest \nP           = 1-pf(tau,nu_1,nu_2)                                               # Überschreitungswahrscheinlichkeit\n\n\n\nWilks' Lambda      0.1569049 \ntau                31.25301 \nnu_1               4 \nnu_2               82 \nphi                1 \nP(tau &gt; tau_tilde) 8.881784e-16\n\n\nIm vorliegenden Fall wird die Nullhypothese identischer Gruppenerwartungswertparameter also verworfen. Schließlich validieren wir obige Analyse im Sinne eines Black-Box-Verfahrens mithilfe der R lm() und Manova() Funktionen.\n\nlibrary(car)\nD        = read.csv(\"./_data/503-einfaktorielle-varianzanalyse.csv\")            # Dateneinlesen\nmodel    = lm(cbind(D$dBDI,D$dGLU) ~ D$COND, D)                                 # Modellspezifikation\nManova(model, test.statistic = \"Wilks\")                                         # Einfaktorielle Varianzanalyse\n\n\nType II MANOVA Tests: Wilks test statistic\n       Df test stat approx F num Df den Df    Pr(&gt;F)    \nD$COND  2    0.1569   31.253      4     82 8.346e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Multivariate Inferenz",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Einfaktorielle Varianzanalyse</span>"
    ]
  },
  {
    "objectID": "503-Einfaktorielle-Varianzanalyse.html#literaturhinweise",
    "href": "503-Einfaktorielle-Varianzanalyse.html#literaturhinweise",
    "title": "38  Einfaktorielle Varianzanalyse",
    "section": "38.5 Literaturhinweise",
    "text": "38.5 Literaturhinweise\nDie Theorie der einfaktoriellen multivariaten Varianzanalyse geht zurück auf Wilks (1932). Anderson (2003) gibt eine Einführung in die Approximationstheorie für multivariate Modelle, das Wissen um die exakten Verteilungen der Teststatistiken um 1970 wird von Rao (1972) zusammengefasst.",
    "crumbs": [
      "Multivariate Inferenz",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Einfaktorielle Varianzanalyse</span>"
    ]
  },
  {
    "objectID": "503-Einfaktorielle-Varianzanalyse.html#selbstkontrollfragen",
    "href": "503-Einfaktorielle-Varianzanalyse.html#selbstkontrollfragen",
    "title": "38  Einfaktorielle Varianzanalyse",
    "section": "38.6 Selbstkontrollfragen",
    "text": "38.6 Selbstkontrollfragen\n\nErläutern Sie das Anwendungsszenario einer einfaktoriellen multivariaten Varianzanalyse.\nGeben Sie die Definition des Modells der einfaktoriellen multivariaten Varianzanalyse wieder.\nGeben Sie das Theorem zu den Parameterschätzern der einfaktoriellen multivariaten Varianzanalyse wieder.\nErläutern Sie die Null- und Alternativhypothesen einer einfaktoriellen multivariaten Varianzanalyse.\nGeben Sie das Theorem zur Kreuzproduktsummenmatrizenzerlegung wieder.\nWas messen die Totale, Between-Group und die Within-Group Sum-of-Squares Matrizen, respektive?\nGeben Sie die Definition der Wilks’-\\(\\Lambda\\)-Statistik wieder.\nErläutern Sie Gemeinsamkeiten und Unterschiede zwischen speziellen und approximativen \\(H_0\\) Verteilungen von Wilks-\\(\\Lambda\\)-Transformationen bei der einfaktoriellen multivariaten Varianzanalyse.\nGeben Sie das Theorem zum Wilks-\\(\\Lambda\\)-Statistik-basierten Test im Rahmen der einfaktoriellen multivariaten Varianzanalyse wieder.\nErläutern Sie das praktische Vorgehen zur Durchführung eines Wilks-\\(\\Lambda\\)-Statistik-basierten Tes tim Rahmen der einfaktoriellen multivariaten Varianzanalyse.\n\n\n\n\n\nAnderson, T. W. (2003). An Introduction to Multivariate Statistical Analysis (3rd ed). Wiley-Interscience.\n\n\nHotelling, H. (1951). A Generalized T Test and Measure of Multivariate Dispersion.\n\n\nPillai, K. C. S. (1955). Some New Test Criteria in Multivariate Analysis. The Annals of Mathematical Statistics, 26(1), 117–121. https://doi.org/10.1214/aoms/1177728599\n\n\nRao, C. R. (1951). An Asymptotic Expansion of the Distribution of Wilk’s Criterion. Bulletin of the International Statistical Institute, 33(2), 177–180.\n\n\nRao, C. R. (1972). Recent Trends of Research Work in Multivariate Analysis. Biometrics, 28(1), 3. https://doi.org/10.2307/2528958\n\n\nRoy, S. N., & Bose, R. C. (1953). Simultaneous Confidence Interval Estimation. The Annals of Mathematical Statistics, 24(4), 513–536. https://doi.org/10.1214/aoms/1177728912\n\n\nWilks, S. S. (1932). Certain Generalizations in the Analysis of Variance. Biometrika, 24(3-4), 471–494. https://doi.org/10.1093/biomet/24.3-4.471",
    "crumbs": [
      "Multivariate Inferenz",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Einfaktorielle Varianzanalyse</span>"
    ]
  },
  {
    "objectID": "504-Kanonische-Korrelationsanalyse.html",
    "href": "504-Kanonische-Korrelationsanalyse.html",
    "title": "39  Kanonische Korrelationsanalyse",
    "section": "",
    "text": "39.1 Anwendungsszenario und Grundzüge\nAusgangspunkt einer kanonischen Korrelationsanalyse ist die exploratorische Frage nach der Stärke des Zusammenhangs einer multivariaten unabhängigen Variablen \\(x\\) und einer multivariaten abhängigen Variable \\(y\\). Im Kontext der kanonischen Korrelationsanalyse werden diese Variablen auch oft als Prädiktoren und Kriterien, respektive, bezeichnet. Dabei werden vorliegende Daten von sowohl der Prädiktoren als auch der Kriterien als Realisierungen von unabhängig und identisch verteilten \\(m_x\\)- bzw. \\(m_y\\)-dimensionalen Zufallsvektoren betrachtet. Um die Frage nach der Stärke des Zusammenhangs von Prädiktoren und Kriterien zu beantworten, betrachtet die kanonische Korrelationsanalyse Linearkombinationen der Komponenten der Prädiktoren und Kriterien. Wir bezeichnen die Linearkombinationen von \\(x\\) und \\(y\\) mit Parametervektoren \\(a \\in \\mathbb{R}^{m_x}\\) und \\(b \\in \\mathbb{R}^{m_y}\\) im Folgenden mit \\[\\begin{equation}\n\\xi  := a^Tx  \n\\mbox{ und }\n\\upsilon := b^Ty  \n\\end{equation}\\] \\(\\xi\\) und \\(\\upsilon\\) sind dann als Linearkombinationen von Zufallsvariablen selbst Zufallsvariablen, deren Korrelation \\(\\rho(\\xi,\\upsilon)\\) berechnet werden kann. Die kanonische Korrelationsanalyse bestimmt dann die Parametervektoren \\(a\\) und \\(b\\) so, dass die Korrelation von \\(\\xi\\) und \\(\\upsilon\\) so groß wie möglich wird. Ist eine solche Parametervektorkombination und ihre entsprechende Korrelation, die dann als kanonische Korrelation bezeichnet gefunden, so kann \\(\\xi\\) als bester Prädiktor und \\(\\upsilon\\) als “am besten prädizierbares Kriterium” interpretiert werden.\nFür Skalare \\(\\alpha\\) und \\(\\beta\\) sind die Korrelationen \\(\\rho(\\xi,\\upsilon)\\) und \\(\\rho(\\alpha\\xi,\\beta\\upsilon)\\) allerdings, wie im Theorem zu Kovarianz und Korrelation bei linear-affinen Transformationen gezeigt, identisch. Die kanonische Korrelationsanalyse sucht deshalb speziell Parametervektoren \\(a\\) und \\(b\\), für die \\(\\rho(\\xi,\\upsilon)\\) einerseits maximal ist und für die \\(\\xi\\) und \\(\\upsilon\\) gleichzeitig jeweils eine standardisierte Varianz von 1 haben. Da aufgrund des Theorem zu Kovarianz und Korrelation bei linear-affinen Transformationen die Varianzen zu verschiedenen skalaren Vielfachen von \\(\\xi\\) und \\(\\upsilon\\) verschieden sind, legt diese die Parametervektorwerte \\(a\\) und \\(b\\), für die \\(\\rho(\\xi,\\upsilon)\\) maximal ist, eindeutig fest. Zur Bestimmung einer kanonischen Korrelation und der Parametervektoren von \\(a\\) und \\(b\\) ist man also auf ein Optimierungsproblem mit Nebenbedingungen geführt.\nIn unser Darstellung kanonischen Korrelationsanalyse folgen wir Mardia et al. (1979). Dabei werden die Prädiktor- und Kriterienzufallsvektoren \\(x\\) und \\(y\\) in einem Zufallsvektor \\[\\begin{equation}\nz := \\begin{pmatrix} x \\\\ y \\end{pmatrix}\n\\end{equation}\\] zusammengefasst, für den wir durchgängig annehmen wollen, dass \\(\\mathbb{E}(z) = 0_{m}\\) mit \\(m = m_x + m_y\\) gilt. Im Anwendungskontext entspricht dies der Annahme, dass vor Durchführung der kanonischen Korrelationsanalyse die Stichprobenmittel des Stichprobenmittels von den beobachteten Daten vor Durchführung der kanonischen Korrelationsanalyse subtrahiert werden. Da, wie wir sehen werden, die Schätzung der kanonischen Korrelation allerdings lediglich auf den Stichprobenkovarianzmatrizen beruht ist dieser Schritt verzichtbar.\nDer mathematische Fokus der Entwicklung der kanonischen Korrelationsanalyse nach Mardia et al. (1979) liegt damit auf der Kovarianzmatrix \\(\\mathbb{C}(z)\\). Speziell ergeben sich die Kovarianzen von Linearkombinationen von \\(x\\) und \\(y\\) aus Matrixprodukten von \\(\\mathbb{C}(z)\\) und es können einige Matrixtheoreme, die in Kapitel 14 und Kapitel 10 dokumentiert sind angewendet werden. Generell wird in der Entwicklung nach Mardia et al. (1979) ein Optimierungsansatz mithilfe von Lagrangefunktionen, wie in den Originalarbeiten von Hotelling (1935) und Hotelling (1936) gewählt, zugunsten der Eigenanalyse von Matrixprodukten supprimiert. Für die Entwicklung mit einem Lagrangeansatz verweisen wir auf zum Beispiel Anderson (2003).\nIm Folgenden diskutieren wir in Kapitel 39.2 zunächst zwei Theoreme, die direkt durch die kanonische Korrelationsanalyse motiviert sind und die, zusammen mit den oben erwähnten Theoremen in Kapitel 14 und Kapitel 10 das analytische und probabilistische Fundament der kanonischen Korrelationsanalyse bilden. In Kapitel 39.3 führen wir mit den kanonischen Korrelationen, den kanonischen Variaten und den kanonischen Koeffizientenvektoren dann die zentralen Begriffe der kanonischen Korrelationsanalyse ein. In Kapitel 39.4 diskutieren wir dann, wie diese Größen auf Basis der Stichprobenkovarianzmatrix eines Datensatzes von Prädiktoren- und Kriterienrealisierungen geschätzt werden können. In Kapitel 39.5 schließlich wenden wir die kanonische Korrelationsanalyse auf das unten skizzierte Anwendungsbeispiel an.",
    "crumbs": [
      "Multivariate Inferenz",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Kanonische Korrelationsanalyse</span>"
    ]
  },
  {
    "objectID": "504-Kanonische-Korrelationsanalyse.html#sec-anwendungsszenario",
    "href": "504-Kanonische-Korrelationsanalyse.html#sec-anwendungsszenario",
    "title": "39  Kanonische Korrelationsanalyse",
    "section": "",
    "text": "Anwendungsbeispiel\nAls konkretes Anwendungsbeispiel für eine kanonische Korrelationanalyse betrachten wir einen simulierten Datensatz zur Effektivität einer Psychotherapie bei Depressionen. Dabei seien pro Patient:in vier Variablen verfügbar: Zum einen als Maße für die Therapiequalität die Dauer der Psychotherapie (DUR) und die klinische Erfahrung der behandelnden Psychotherapeut:in (EXP); zum anderen als Maße für die Reduktion der Depressionssymptomatik sowohl BDI Score und Glukokortikoidplasmalevel Differenzwerte zwischen Beginn und Ende der Therapie (dBDI und dGLU), wobei positive Werte eine Verringerung der Depressionssymptomatik anzeigen sollen. Wir stellen uns vor, dass man im Sinne einer exploratorischen Analyse daran interessiert ist, inwieweit die Variablen DUR (\\(x_1\\)) und EXP (\\(x_2\\)) als Prädiktoren (unabhängige Variablen) mit den Variablen dBDI (\\(y_1\\)) und dGLU (\\(y_2\\)) als Kriterien (abhängige Variablen) zusammenhängen. Im Sinne obiger Bezeichner gilt hier also \\(m_x = m_y = 2\\). Tabelle 39.1 zeigt dazu einen Beispieldatensatz für \\(n = 20\\) Patient:innen.\n\n\n\n\nTabelle 39.1: Beispielhafte Prädiktoren- und Kriterienrealisierungen im Kontext der Psychotherapie bei Depressionen\n\n\n\n\n\n\nDUR\nEXP\ndBDI\ndGLU\n\n\n\n\n27.9\n7.8\n35.5\n6.1\n\n\n15.3\n9.3\n25.0\n4.0\n\n\n17.4\n2.1\n19.7\n1.7\n\n\n21.5\n6.5\n28.8\n2.6\n\n\n28.2\n1.3\n29.4\n1.9\n\n\n14.0\n2.7\n17.2\n0.9\n\n\n28.0\n3.9\n32.9\n2.0\n\n\n28.9\n0.1\n28.3\n4.1\n\n\n23.2\n3.8\n25.8\n3.9\n\n\n22.6\n8.7\n31.3\n3.8\n\n\n11.2\n3.4\n14.4\n2.1\n\n\n14.1\n4.8\n18.4\n2.0\n\n\n13.5\n6.0\n19.1\n5.0\n\n\n23.7\n4.9\n28.0\n2.6\n\n\n17.7\n1.9\n20.3\n2.1\n\n\n25.4\n8.3\n34.8\n4.4\n\n\n20.0\n6.7\n27.6\n4.0\n\n\n24.4\n7.9\n31.9\n3.9\n\n\n29.8\n1.1\n32.2\n1.0\n\n\n17.6\n7.2\n24.6\n1.9",
    "crumbs": [
      "Multivariate Inferenz",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Kanonische Korrelationsanalyse</span>"
    ]
  },
  {
    "objectID": "504-Kanonische-Korrelationsanalyse.html#sec-mathematischer-hintergrund",
    "href": "504-Kanonische-Korrelationsanalyse.html#sec-mathematischer-hintergrund",
    "title": "39  Kanonische Korrelationsanalyse",
    "section": "39.2 Mathematischer Hintergrund",
    "text": "39.2 Mathematischer Hintergrund\nFolgendes Theorem bildet das analytische Fundament der kanonischen Korrelationsanalyse.\n\nTheorem 39.1 (Maximierung quadratischer Formen mit Nebenbedingungen) \\(A \\in \\mathbb{R}^{m \\times m}, B \\in \\mathbb{R}^{m \\times m}\\) pd seien symmetrische Matrizen und \\(\\lambda_1\\) sei der größte Eigenwert von \\(B^{-1}A\\) mit assoziertem Eigenvektor \\(v_1 \\in \\mathbb{R}^m\\). Dann ist \\(\\lambda_1\\) eine Lösung des Optimierungsproblems \\[\\begin{equation}\\label{eq:opt_1}\n\\max_{x} x^TAx \\mbox{ unter der Nebenbedingung } x^TBx = 1.\n\\end{equation}\\]\n\n\nBeweis. \\(B^{1/2}\\) sei die symmetrische Quadratwurzel von \\(B\\) und es sei \\[\\begin{equation}\ny := B^{1/2}x \\Leftrightarrow x = B^{-1/2}y\n\\end{equation}\\] Dann kann mit der symmetrischen Matrix \\[\\begin{equation}\nK := B^{-1/2}AB^{-1/2} \\in \\mathbb{R}^{m \\times m}\n\\end{equation}\\] das Optimierungsproblem \\(\\eqref{eq:opt_1}\\) geschrieben werden als \\[\\begin{equation}\\label{eq:opt_2}\n\\max_{y} y^T K y\n\\mbox{ unter der Nebenbedingung }\ny^Ty = 1.\n\\end{equation}\\] Dies gilt, weil \\[\\begin{equation}\n\\max_{x} x^TAx\n\\Leftrightarrow\n\\max_{y} \\left(B^{-1/2}y\\right)^TA\\left(B^{-1/2}y\\right)\n\\Leftrightarrow\n\\max_{y} y^TB^{-1/2}AB^{-1/2}y\n\\Leftrightarrow\n\\max_{y} y^TKy\n\\end{equation}\\] und \\[\\begin{equation}\nx^TBx = 1\n\\Leftrightarrow y^T B^{-1/2}BB^{-1/2}y = 1\n\\Leftrightarrow y^Ty = 1.\n\\end{equation}\\] Weil \\(K\\) eine symmetrische Matrix ist, existiert die Orthonormalzerlegung (vgl. Kapitel 10) \\[\\begin{equation}\nK = Q\\Lambda Q^T,\n\\end{equation}\\] wobei die Spalten der orthogonalen Matrix \\(Q\\) die Eigenvektoren von \\(K\\) und die Diagonalemente von \\(\\Lambda\\) die zugehörigen Eigenwerte von \\(K\\) sind. Mit der orthogonalen Matrix \\(Q\\) aus obiger Orthornomalzerlegung sei nun \\[\\begin{equation}\nz := Q^Ty \\Leftrightarrow y := Qz.\n\\end{equation}\\] Dann kann das Optimierungsproblem \\(\\eqref{eq:opt_2}\\) geschrieben werden als \\[\\begin{equation}\\label{eq:opt_3}\n\\max_{z} \\sum_{i = 1}^m \\lambda_i z_i^2 \\mbox{ unter der Nebenbedingung } z^Tz = 1,\n\\end{equation}\\] weil \\[\\begin{equation}\n\\max_{y} y^TKy\n\\Leftrightarrow\n\\max_{z} (Qz)^TK(Qz)\n\\Leftrightarrow\n\\max_{z} z^TQ^TQ\\Lambda Q^TQz\n\\Leftrightarrow\n\\max_{z} z^T\\Lambda z\n\\Leftrightarrow\n\\max_{z} \\sum_{i=1}^m \\lambda_i z_i^2\n\\end{equation}\\] und \\[\\begin{equation}\ny^Ty = 1\n\\Leftrightarrow\n(Qz)^T Qz = 1\n\\Leftrightarrow\nz^T Q^TQz = 1\n\\Leftrightarrow\nz^T z = 1.\n\\end{equation}\\] Die Eigenwerte von \\(K\\) seien nun absteigend sortiert, also \\(\\lambda_1 \\ge \\cdots \\ge \\lambda_m\\). Dann gilt für das Optimierungsproblem \\(\\eqref{eq:opt_3}\\), dass \\[\\begin{equation}\n\\max_{z} \\sum_{i = 1}^m \\lambda_i z_i^2 \\le \\lambda_1,\n\\end{equation}\\] weil \\[\\begin{equation}\n\\max_{z} \\sum_{i = 1}^m \\lambda_i z_i^2\n\\le\n\\max_{z} \\sum_{i = 1}^m \\lambda_1 z_i^2\n=\n\\lambda_1 \\max_{z} \\sum_{i = 1}^m z_i^2\n=\n\\lambda_1\n\\end{equation}\\] wobei sich die letzte Gleichung aus der Nebenbedingung \\(z^Tz=1\\) ergibt. Schließlich gilt \\[\\begin{equation}\n\\max_{z} \\sum_{i = 1}^m \\lambda_i z_i^2  = \\lambda_1,\n\\end{equation}\\] für \\(z := e_1 = (1,0,...,0)^T\\). Zusammenfassend heißt das, dass \\(z = e_1\\) eine Lösung des Optimierungsproblem \\(\\eqref{eq:opt_3}\\) ist und das \\(\\lambda_1\\) das entsprechende Maximum ist. Damit ergibt sich aber sofort, dass dann \\[\\begin{equation}\ny = Qz = Qe_1 =  q_1 \\mbox{ und } x = B^{-1/2}q_1\n\\end{equation}\\] Lösungen der äquivalenten Optimierungsprobleme \\(\\eqref{eq:opt_2}\\) und \\(\\eqref{eq:opt_1}\\), respektive, sind. Nach Konstruktion ist \\(q_1\\) ein Eigenvektor von \\(B^{-1/2}AB^{-1/2}\\) und nach obigem Theorem zu Eigenwerten und Eigenvektoren von Matrixprodukten damit auch ein Eigenvektor von \\[\\begin{equation}\nB^{-1/2}B^{-1/2}A = B^{-1}A\n\\end{equation}\\] und die zugehörigen Eigenwerte sind gleich. Damit aber folgt, dass der größte Eigenwert von \\(B^{-1}A\\) und sein assoziierter Eigenvektor eine Lösung von \\[\\begin{equation}\n\\max_{x} x^TAx \\mbox{ unter der Nebenbedingung } x^TBx = 1.\n\\end{equation}\\] ist.\n\nNach Wortlaut des Theorems gilt also für die Funktion \\[\\begin{equation}\nf : \\mathbb{R}^m \\to \\mathbb{R}, x \\mapsto f(x) := x^TAx,\n\\end{equation}\\] dass \\[\\begin{equation}\nv_1 = \\mbox{argmax}_{x} x^TAx \\mbox{ unter der Nebenbedingung } x^TBx = 1\n\\end{equation}\\] und dass \\[\\begin{equation}\n\\lambda_1 = \\mbox{max}_{x} x^TAx \\mbox{ unter der Nebenbedingung } x^TBx = 1.\n\\end{equation}\\]\nDas folgende Theorem setzt die für die kanonischen Korrelationsanalyse zentralen Größen der Varianzen von Linearkombinationen von Zufallsvektoren und der Korrelation von Linearkombinationen von Zufallsvektoren in Bezug zur Kovarianzmatrix der gemeinsamen Verteilung der Zufallsvektoren und bildet das probabilistische Fundament der kanonischen Korrelationsanalyse.\n\nTheorem 39.2 (Linearkombinationen von Zufallsvektorpartitionen)  \nEs sei \\[\\begin{equation}\nz = \\begin{pmatrix} x \\\\ y \\end{pmatrix}\n\\mbox{ mit }\n\\mathbb{E}(z) = 0_m\n\\mbox{ und }\n\\mathbb{C}(z) =\n\\begin{pmatrix}\n\\Sigma_{xx} & \\Sigma_{xy} \\\\\n\\Sigma_{yx} & \\Sigma_{yy} \\\\\n\\end{pmatrix}\n\\end{equation}\\] ein \\(m\\)-dimensionaler partitionierter Zufallsvektor sowie sein Erwartungswertvektor und seine Kovarianzmatrix, respektive. Weiterhin seien für \\(a \\in \\mathbb{R}^{m_x}\\) und \\(b\\in\\mathbb{R}^{m_y}\\) die Zufallsvariablen \\[\\begin{equation}\n\\xi := a^T x \\mbox{ und } \\upsilon := b^T y\n\\end{equation}\\] als Linearkombinationen der Komponenten von \\(x\\) und \\(y\\) definiert. Dann gelten\n\n\nBeweis. Wir betrachten zunächst die Varianz von \\(\\xi\\). Mit dem Varianzverschiebungssatz gilt \\[\\begin{align}\n\\begin{split}\n\\mathbb{V}(\\xi)\n& = \\mathbb{E}\\left(\\xi \\xi \\right) - \\mathbb{E}(\\xi)\\mathbb{E}(\\xi) \\\\\n& = \\mathbb{E}\\left((a^Tx) (a^Tx)\\right) - \\mathbb{E}\\left(a^Tx\\right)\\mathbb{E}\\left(a^Tx\\right) \\\\\n& = \\mathbb{E}\\left((a^Tx) (a^Tx)^T\\right) - \\mathbb{E}\\left(a^Tx\\right)\\mathbb{E}\\left(a^Tx\\right) \\\\\n& = \\mathbb{E}\\left(a^Txx^Ta\\right) - \\mathbb{E}\\left(a^Tx\\right)\\mathbb{E}\\left(a^Tx\\right) \\\\\n& = a^T\\mathbb{E}\\left(xx^T\\right)a - a^T\\mathbb{E}(x)a^T\\mathbb{E}(x) \\\\\n& = a^T\\mathbb{E}\\left(xx^T\\right)a - a^T0_{m_x}a^T0_{m_x} \\\\\n& = a^T\\Sigma_{xx}a. \\\\\n\\end{split}\n\\end{align}\\] Der Beweis zur Varianz von \\(\\upsilon\\) folgt dann analog. Mit der Definition der Korrelation von Zufallsvariablen und mit \\(\\mathbb{V}(\\xi) = \\mathbb{V}(\\upsilon) = 1\\) und dem Kovarianzverschiebungssatz gilt \\[\\begin{align}\n\\begin{split}\n\\rho(\\xi,\\upsilon)\n& = \\frac{\\mathbb{C}(\\xi,\\upsilon)}{\\sqrt{\\mathbb{V}(\\xi)}\\sqrt{\\mathbb{V}(\\upsilon)}} \\\\\n& = \\frac{\\mathbb{C}(\\xi,\\upsilon)}{\\sqrt{1}\\sqrt{1}} \\\\\n& = \\mathbb{C}(\\xi,\\upsilon) \\\\\n& = \\mathbb{E}(\\xi\\upsilon) - \\mathbb{E}(\\xi)\\mathbb{E}(\\upsilon) \\\\\n& = \\mathbb{E}\\left((a^Tx)(b^Ty)\\right) - \\mathbb{E}(a^Tx)\\mathbb{E}(b^Ty) \\\\\n& = \\mathbb{E}\\left((a^Tx)(b^Ty)^T\\right) - \\mathbb{E}(a^Tx)\\mathbb{E}(b^Ty) \\\\\n& = \\mathbb{E}\\left(a^T xy^Tb \\right) - \\mathbb{E}(a^Tx)\\mathbb{E}(b^Ty) \\\\\n& = a^T\\mathbb{E}\\left(xy^T \\right)b - a^T\\mathbb{E}(x)b^T\\mathbb{E}(y) \\\\\n& = a^T\\mathbb{E}\\left(xy^T \\right)b - a^T0_{m_x}b^T0_{m_y} \\\\\n& = a^T\\Sigma_{xy}b. \\\\\n\\end{split}\n\\end{align}\\]\n\nDie Varianz der Zufallsvariable \\(a^Tx\\) ergibt sich also als die “quadrierte” Linearkombination von \\(\\Sigma_{xx}\\) und die Varianz der Zufallsvariable \\(b^Ty\\) ergibt sich als die “quadrierte” Linearkombination von \\(\\Sigma_{yy}\\). Die Korrelation der Zufallsvariablen \\(a^Tx\\) und \\(b^Ty\\) schließlich ergibt sich “quadrierte” Linearkombination von \\(\\Sigma_{xy}\\).",
    "crumbs": [
      "Multivariate Inferenz",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Kanonische Korrelationsanalyse</span>"
    ]
  },
  {
    "objectID": "504-Kanonische-Korrelationsanalyse.html#sec-modellformulierung-kka",
    "href": "504-Kanonische-Korrelationsanalyse.html#sec-modellformulierung-kka",
    "title": "39  Kanonische Korrelationsanalyse",
    "section": "39.3 Modellformulierung",
    "text": "39.3 Modellformulierung\nMit den beiden in Kapitel 39.2 diskutierten Theoremen ist es nun möglich, die Begriffe der kanonischen Koeffezientenvektoren, der kanonischen Variate, und schließlich der kanonischen Korrelationen zu definieren.\n\nDefinition 39.1 (Kanonische Koeffizientenvektoren, Variate, Korrelationen)  \nEs seien \\[\\begin{equation}\nz = \\begin{pmatrix} x \\\\ y \\end{pmatrix}\n\\mbox{ mit }\n\\mathbb{E}(z) := 0_m\n\\mbox{ und }\n\\mathbb{C}(z) :=\n\\begin{pmatrix}\n\\Sigma_{xx} & \\Sigma_{xy} \\\\\n\\Sigma_{yx} & \\Sigma_{yy} \\\\\n\\end{pmatrix}\n\\in \\mathbb{R}^{m \\times m}\n\\end{equation}\\] ein \\(m\\)-dimensionaler partitionierter Zufallsvektor sowie sein Erwartungswert und seine Kovarianzmatrix, respektive. Weiterhin sei \\[\\begin{equation}\nK := \\Sigma_{xx}^{-1/2}\\Sigma_{xy}\\Sigma_{yy}^{-1/2} \\in \\mathbb{R}^{m_x \\times m_y}\n\\end{equation}\\] mit der Singulärwertzerlegung \\[\\begin{equation}\nK = A \\Lambda B^T,\n\\end{equation}\\] wobei \\[\\begin{equation}\nA       := \\begin{pmatrix} \\alpha_1 & \\cdots & \\alpha_k \\end{pmatrix} \\in \\mathbb{R}^{m_x \\times m_y}\n\\mbox{ und }\nB       := \\begin{pmatrix} \\beta_1  & \\cdots &  \\beta_k \\end{pmatrix} \\in \\mathbb{R}^{m_y \\times m_y}\n\\end{equation}\\] die orthogonalen Matrix der Eigenvektoren von \\(KK^T\\) und die orthogonale Matrix der Eigenvektoren von \\(K^TK\\), respektive, bezeichnen und \\[\\begin{equation}\n\\Lambda := \\mbox{diag}\\left(\\lambda^{1/2}_1,...,\\lambda_k^{1/2}\\right) \\in \\mathbb{R}^{m_y \\times m_y},\n\\end{equation}\\] die Diagonalmatrix der Quadratwurzeln der zugehörigen absteigend geordneten Eigenwerte bezeichnet. Schließlich seien für \\(i = 1,...,k\\) \\[\\begin{equation}\na_i := \\Sigma_{xx}^{-1/2}\\alpha_i  \\in \\mathbb{R}^{m_x} \\mbox{ und } b_i := \\Sigma_{yy}^{-1/2}\\beta_i \\in \\mathbb{R}^{m_y}.\n\\end{equation}\\] Dann heißen für \\(i = 1,...,k\\)\n\nIhre Bedeutsamkeit erlangen diese Begriffe durch ihre Eigenschaften, die wir im folgenden Theorem zusammenfassen.\n\nTheorem 39.3 (Eigenschaften kanonischer Korrelationen und Variaten) Es seien \\[\\begin{equation}\nz = \\begin{pmatrix} x \\\\ y \\end{pmatrix}\n\\mbox{ mit }\n\\mathbb{E}(z) := 0_m\n\\mbox{ und }\n\\mathbb{C}(z) :=\n\\begin{pmatrix}\n\\Sigma_{xx} & \\Sigma_{xy} \\\\\n\\Sigma_{yx} & \\Sigma_{yy} \\\\\n\\end{pmatrix}\n\\in \\mathbb{R}^{m \\times m}\n\\end{equation}\\] ein \\(m\\)-dimensionaler partitionierter Zufallsvektor sowie sein Erwartungswert und seine Kovarianzmatrix, respektive. Weiterhin seien für \\(i = 1,...,k\\) die kanonischen Koeffizientenvektoren \\(a_i, b_i\\), die kanonischen Variaten \\(\\xi,\\upsilon_i\\) und die kanonischen Korrelationen \\(\\rho_i\\) definiert wie oben. Dann gilt, dass für \\(1 \\le r \\le k\\) das Maximum des \\(r\\)ten restringierten Optimierungsproblems \\[\\begin{equation}\n\\phi_r = \\max_{a,b} a^T\\Sigma_{xy}b\n\\end{equation}\\] unter den Nebenbedingungen \\[\\begin{equation}\na^T\\Sigma_{xx}a   = 1,\n\\quad\nb^T\\Sigma_{yy}b   = 1,\n\\quad\na_i^T\\Sigma_{xx}a = 0 \\mbox{ für } i = 1,...,r-1\n\\end{equation}\\] (1) den Wert \\(\\phi_r = \\rho_r\\) hat und (2) bei \\(a = a_r\\) und \\(b = b_r\\) angenommen wird.\n\n\nBeweis. Wir betrachten das restringierte Optimierungsproblem \\[\\begin{equation}\n\\phi_r^2 = \\max_{a,b} \\left(a^T\\Sigma_{xy}b\\right)^2\\,\n\\mbox{ u.d.N. }\na^T\\Sigma_{xx}a   = 1,\\,\nb^T\\Sigma_{yy}b   = 1,\\,\na_i^T\\Sigma_{xx}a = 0, i = 1,...,r-1\n\\end{equation}\\] Wir folgen Mardia et al. (1979), S. 284 und gehen schrittweise vor, d.h. wir lösen das restringierte Optimierungsproblem \\[\\begin{equation}\n\\phi_r^2 = \\max_{a} \\left(\\max_{b} \\left(a^T\\Sigma_{xy}b\\right)^2 \\mbox{ u.d.N.} b^T\\Sigma_{yy}b   = 1\\right)\n\\mbox{ u.d.N. } a^T\\Sigma_{xx}a   = 1,\\, a_i^T\\Sigma_{xy}a = 0,  i = 1,...,r-1\n\\end{equation}\\] von innen nach außen.\nSchritt (1)\nWir wählen wir zunächst ein festes \\(a \\in \\mathbb{R}^m\\) und betrachten das restringierte Optimierungsproblem \\[\\begin{equation}\n\\max_{b} \\left(a^T\\Sigma_{xy}b\\right)^2\n\\mbox{ u.d.N. }\nb^T\\Sigma_{yy}b   = 1\n\\end{equation}\\] Dieses Optimierungsproblem kann geschrieben werden als \\[\\begin{equation}\\label{eq:kka_opt_1}\n\\max_{b} b^T\\Sigma_{yx}aa^T\\Sigma_{xy}b\n\\mbox{ u.d.N. }\nb^T\\Sigma_{yy}b = 1,\n\\end{equation}\\] weil gilt, dass \\[\\begin{equation}\n\\left(a^T\\Sigma_{xy}b\\right)^2\n= \\left(a^T\\Sigma_{xy}b\\right)\\left(a^T\\Sigma_{xy}b\\right)\n= \\left(a^T\\Sigma_{xy}b\\right)^T a^T\\Sigma_{xy}b\n= b^T\\Sigma_{yx}aa^T\\Sigma_{xy}b.\n\\end{equation}\\] Das Optimierungsproblem \\(\\eqref{eq:kka_opt_1}\\) kann nun mithilfe des Theorems zur Maximierung quadratischer Formen mit Nebenbedingen gelöst werden. Im Sinne dieses Theorems setzen wir dazu \\[\\begin{equation}\nA := \\Sigma_{yx}aa^T\\Sigma_{xy} \\mbox{ und } B := \\Sigma_{yy}.\n\\end{equation}\\] Dann hat \\(\\eqref{eq:kka_opt_1}\\) die Form \\[\\begin{equation}\\label{eq:kka_opt_2}\n\\max_{b} b^TAb\n\\mbox{ unter der Nebenbedingung }\nb^TBb = 1,\n\\end{equation}\\] Das Maximum von \\(\\eqref{eq:kka_opt_2}\\) entspricht nach dem Theorem zur Maximierung quadratischer Formen mit Nebenbedingungen dem größten Eigenwert von \\[\\begin{equation}\nB^{-1}A = \\Sigma_{yy}^{-1}\\Sigma_{yx}aa^T\\Sigma_{xy}\n\\end{equation}\\] Der größte Eigenwert von \\(\\Sigma_{yy}^{-1}\\Sigma_{yx}aa^T\\Sigma_{xy}\\) wiederum kann mithilfe des Theorems zum Eigenwert und Eigenvektor eines Matrixvektorprodukts bestimmt werden. Im Sinne dieses Theorems setzen wir dazu \\[\\begin{equation}\nA := \\Sigma_{yy}^{-1}\\Sigma_{yx},\\quad b := a,\\quad B := \\Sigma_{xy}\n\\end{equation}\\] und erhalten für den betreffenden Eigenwert \\[\\begin{equation}\n\\lambda_a = b^TBAa = a^T\\Sigma_{xy}\\Sigma_{yy}^{-1}\\Sigma_{yx}a.\n\\end{equation}\\] als Lösung (Maximum) des restringierten Optimierungsproblems \\[\\begin{equation}\n\\max_{b} \\left(a^T\\Sigma_{xy}b\\right)^2 \\mbox{ u.d.N. } b^T\\Sigma_{yy}b = 1\n\\end{equation}\\]\nSchritt (2)\nBasierend auf Schritt (1) verbleibt die Lösung des restringierten Optimierungsproblem \\[\\begin{equation}\\label{eq:kka_opt_3}\n\\phi_r^2 = \\max_{a} a^T\\Sigma_{xy}\\Sigma_{yy}^{-1}\\Sigma_{yx}a\n\\mbox{ u.d.N. }\na^T\\Sigma_{xx}a   = 1,\\,\na_i^T\\Sigma_{xx}a = 0,  i = 1,...,r-1\n\\end{equation}\\] Dazu halten wir zunächst fest, dass \\(\\eqref{eq:kka_opt_3}\\) mit den Definitionen von \\(\\alpha_i\\) und \\(K\\) in der Definition der Kanonischen Koeffizientenvektoren, Variaten, und Korrelationen geschrieben werden kann als \\[\\begin{equation}\\label{eq:kka_opt_4}\n\\phi_r^2 = \\max_{\\alpha} \\alpha^T KK^T \\alpha\n\\mbox{ u.d.N. }\n\\alpha^T \\alpha   = 1,\\,\n\\alpha_i^T\\alpha  = 0,  i = 1,...,r-1,\n\\end{equation}\\] denn \\[\\begin{align}\n\\begin{split}\n\\phi_r^2 & = \\max_{a} a^T\\Sigma_{xy}\\Sigma_{yy}^{-1}\\Sigma_{yx}a\n\\mbox{ u.d.N. }\na^T\\Sigma_{xx}a   = 1,\na_i^T\\Sigma_{xx}a = 0 \\Leftrightarrow\n\\\\\n\\phi_r^2 & = \\max_{\\alpha} a^T\\Sigma_{xy}\\Sigma_{yy}^{-1}\\Sigma_{yx}a\n\\mbox{ u.d.N. }\n\\alpha^T\\Sigma_{xx}^{-1/2}\\Sigma_{xx}\\Sigma_{xx}^{-1/2}\\alpha = 1,\n\\alpha^T_i\\Sigma_{xx}^{-1/2}\\Sigma_{xx}\\Sigma_{xx}^{-1/2}\\alpha = 0\n\\\\\n\\phi_r^2 & = \\max_{\\alpha} \\alpha^T\\Sigma_{xx}^{-1/2}\\Sigma_{xy}\\Sigma_{yy}^{-1}\\Sigma_{yx}\\Sigma_{xx}^{-1/2}\\alpha\n\\mbox{ u.d.N. }\n\\alpha^T\\alpha = 1,\n\\alpha^T_i\\alpha = 0\n\\\\\n\\phi_r^2 & = \\max_{\\alpha} \\alpha^T\\Sigma_{xx}^{-1/2}\\Sigma_{xy}\\Sigma_{yy}^{-1/2}\\Sigma_{yy}^{-1/2}\\Sigma_{yx}\\Sigma_{xx}^{-1/2}\\alpha\n\\mbox{ u.d.N. }\n\\alpha^T\\alpha = 1,\n\\alpha^T_i\\alpha = 0\n\\\\\n\\phi_r^2 & = \\max_{\\alpha} \\alpha^TKK^T\\alpha\n\\mbox{ u.d.N. }\n\\alpha^T\\alpha = 1,\n\\alpha^T_i\\alpha = 0\n\\end{split}\n\\end{align}\\]\nDabei sind nach der betreffenden Definition die \\(\\alpha_i\\) die Eigenvektoren von \\(KK^T\\) mit den \\(i = 1,...,r-1\\) größten Eigenwerten. Nach dem Theorem zur Maximierung quadratischer Formen mit Nebenbedingungen ist die Lösung von \\(\\eqref{eq:kka_opt_4}\\) der größte Eigenwert von \\(KK^T\\) mit seinem assoziierten Eigenvektor. Die Nebenbedingung \\(\\alpha_i^T\\alpha = 0\\) schränkt diese Wahl auf den \\(r\\)t-größten Eigenwert und seinen assoziierten Eigenvektor \\(\\alpha_r\\) ein. Mit der Definition von Eigenwerten und Eigenvektoren gilt also \\[\\begin{equation}\n\\phi_r^2 = \\alpha_r^T KK^T \\alpha_r = \\alpha_r^T \\lambda_r \\alpha_r = \\lambda_r \\alpha_r^T\\alpha_r = \\lambda_r.\n\\end{equation}\\] Wir haben also gezeigt, dass das restringierte Optimierungsproblem des Theorems den Maximumwert \\(\\phi_r = \\lambda_r^{1/2}\\) hat. Es bleibt zu zeigen, dass dieser Maximumwert für \\(a_r\\) und \\(b_r\\) angenommen wird.\nSchritt (3)\nEinsetzen von \\(a_r\\) und \\(b_r\\) in \\(a^T\\Sigma_{xy}b\\) ergibt mit \\[\\begin{equation}\nK = A\\Lambda B^T\n\\Leftrightarrow KB = A\\Lambda B^TB\n\\Leftrightarrow KB = A\\Lambda\n\\Leftrightarrow K\\beta_r = \\alpha_r\\lambda_r^{1/2}\n\\end{equation}\\] dass \\[\\begin{equation}\na^T_r\\Sigma_{xy}b_r\n= \\alpha_r^T\\Sigma_{xx}^{-1/2}\\Sigma_{xy}\\Sigma_{yy}^{-1/2}\\beta_r\n= \\alpha_r^TK\\beta_r\n= \\alpha_r^T\\alpha_r\\lambda_r^{1/2}\n= \\rho_r\n\\end{equation}\\] Also nimmt \\(a^T\\Sigma_{xy}b\\) bei \\(a_r\\) und \\(b_r\\) seinen restringierten Maximalwert \\(\\lambda_r\\) an.\n\n\\(\\phi_1\\) ist also die größtmögliche Korrelation von \\[\\begin{equation}\n\\xi = a^Tx \\mbox{ und } \\upsilon = b^Ty\n\\end{equation}\\] unter den Nebenbedingungen \\[\\begin{equation}\n\\mathbb{V}(\\xi) = 1 \\mbox{ und } \\mathbb{V}(\\upsilon) = 1\n\\end{equation}\\] und erfüllt damit die Forderungen an die kanonische Korrelatione. \\(\\phi_r\\) mit \\(r &gt; 1\\) ist die größtmögliche Korrelation von \\[\\begin{equation}\n\\xi = a^Tx \\mbox{ und } \\upsilon = b^Ty\n\\end{equation}\\] unter den Nebenbedingungen \\[\\begin{equation}\n\\mathbb{V}(\\xi)   = 1,\n\\mathbb{V}(\\upsilon)  = 1\n\\mbox{ und } \\mathbb{C}(\\xi_i,\\xi) = 0 \\mbox{ für die kanonischen Variate } \\xi_i \\mbox{ mit } i = 1,...,r-1.\n\\end{equation}\\]\n\nSimulationsbeispiel\nWir betrachten das Beispiel (vgl. Uurtio et al. (2018)) \\[\\begin{equation}\np(x) = N(x;0_4,I_4) \\mbox{ und } p(y|x) = N(y; Lx, G)\n\\end{equation}\\] mit \\[\\begin{equation}\nL := \\begin{pmatrix} 0.0 & 0.0 & 1.0 & 0.0 \\\\ 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & -1.0 \\end{pmatrix}\n\\mbox{ und }\nG := \\begin{pmatrix} 0.2 & 0.0 & 0.0 \\\\ 0.0 & 0.4 & 0.0 \\\\ 0.0 & 0.0 & 0.3 \\end{pmatrix}\n\\end{equation}\\] Hier gilt offenbar \\(m_x = 4, m_y = 3, m = 7\\) und \\[\\begin{align}\n\\begin{split}\ny_1 & = \\,\\,\\,\\, x_3 + \\varepsilon_1 \\\\\ny_2 & = \\,\\,\\,\\, x_1 + \\varepsilon_2 \\\\\ny_3 & =        - x_4 + \\varepsilon_3 \\\\\n\\end{split}\n\\end{align}\\] mit \\[\\begin{equation}\nx_1 \\sim N(0,1), x_3 \\sim N(0,1), x_4 \\sim N(0,1)\n\\end{equation}\\] und \\[\\begin{equation}\n\\varepsilon_1 \\sim N(0,0.2), \\varepsilon_2 \\sim N(0,0.4), \\varepsilon_3 \\sim N(0,0.3)\n\\end{equation}\\] Mit dem Theorem zu gemeinsamen Normalverteilungen (vgl. ?sec-normalverteilungen) ergibt sich, dass \\[\\begin{equation}\n\\begin{pmatrix} x \\\\ y \\end{pmatrix}\n\\sim N(0_7,\\Sigma)\n\\end{equation}\\] mit \\[\\begin{equation}\n\\Sigma\n=\n\\begin{pmatrix}\n\\Sigma_{xx} & \\Sigma_{xy} \\\\\n\\Sigma_{yx} & \\Sigma_{yy}\n\\end{pmatrix},\n\\end{equation}\\] wobei \\[\\begin{equation}\n\\Sigma_{xx} = I_4, \\quad\n\\Sigma_{xy} = L^T, \\quad\n\\Sigma_{yx} = L \\mbox{ und }\n\\Sigma_{yy} = G + LL^T.\n\\end{equation}\\] Explizit ergibt sich also \\[\\begin{equation}\n\\Sigma\n=\n\\begin{pmatrix}\nI_4 &  L^T \\\\\nL   &  G + LL^T\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1.0 & 0.0 & 0.0 &  0.0 & 0.0 & 1.0 &  0.0 \\\\\n0.0 & 1.0 & 0.0 &  0.0 & 0.0 & 0.0 &  0.0 \\\\\n0.0 & 0.0 & 1.0 &  0.0 & 1.0 & 0.0 &  0.0 \\\\\n0.0 & 0.0 & 0.0 &  1.0 & 0.0 & 0.0 & -1.0 \\\\\n0.0 & 0.0 & 1.0 &  0.0 & 1.2 & 0.0 &  0.0 \\\\\n1.0 & 0.0 & 0.0 &  0.0 & 0.0 & 1.4 &  0.0 \\\\\n0.0 & 0.0 & 0.0 & -1.0 & 0.0 & 0.0 &  1.3 \\\\\n\\end{pmatrix}\n\\end{equation}\\]\nFolgender R Code definiert zunächst den Kovarianzmatrixparameter der gemeinsamen Verteilung von \\(x\\) und \\(y\\).\n\n# R Pakete für Matrizenrechnung\nlibrary(expm)\n\n# Modellparameter\nL = matrix(c(0,0,1, 0,\n             1,0,0, 0,\n             0,0,0,-1),\n           nrow  = 3,\n           byrow = T)\nG = diag(c(0.2,0.4,0.3))\n\n# Kovarianzmatrixpartition\nSigma_xx = diag(4)\nSigma_xy = t(L)\nSigma_yx = L\nSigma_yy = G + L %*% t(L)\nSigma    = rbind(cbind(Sigma_xx, Sigma_xy), cbind(Sigma_yx, Sigma_yy))\nprint(Sigma)\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n[1,]    1    0    0    0  0.0  1.0  0.0\n[2,]    0    1    0    0  0.0  0.0  0.0\n[3,]    0    0    1    0  1.0  0.0  0.0\n[4,]    0    0    0    1  0.0  0.0 -1.0\n[5,]    0    0    1    0  1.2  0.0  0.0\n[6,]    1    0    0    0  0.0  1.4  0.0\n[7,]    0    0    0   -1  0.0  0.0  1.3\n\n\nAnhand von Definition 39.1 bestimmt folgender R Code dann basierend auf obigem Kovarianzmatrixparameter die kanonischen Korrelationen und kanonischen Koeffizientenvektoren.\n\n# Evaluation der iten kanonischen Koeffizientenvektoren und Korrelationen\nK      = sqrtm(solve(Sigma_xx)) %*% Sigma_xy %*% sqrtm(solve(Sigma_yy))         # K\nALB    = svd(K)                                                                 # K = A\\LambdaV\nA      = ALB$u                                                                  # A\nLambda = ALB$d                                                                  # Lambda\nB      = ALB$v                                                                  # B\nrho    = Lambda                                                                 # \\rho_i = \\lambda_i^{1/2}\na      = sqrtm(solve(Sigma_xx)) %*% A                                           # a_i = \\Sigma_{xx}^{-1/2}\\alpha_i\nb      = sqrtm(solve(Sigma_yy)) %*% B                                           # b_i = \\Sigma_{yy}^{-1/2}\\beta_i\n\n\n\nrho_1 =  0.9128709 , a_1^T = ( 0 0 -1 0 ), b_1^T = ( -0.9128709 0 0 ) \nrho_2 =  0.877058  , a_2^T = ( 0 0 0 1 ) , b_2^T = ( 0 0 -0.877058 ) \nrho_3 =  0.8451543 , a_3^T = ( -1 0 0 0 ), b_3^T = ( 0 -0.8451543 0 )",
    "crumbs": [
      "Multivariate Inferenz",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Kanonische Korrelationsanalyse</span>"
    ]
  },
  {
    "objectID": "504-Kanonische-Korrelationsanalyse.html#sec-modellschaetzung-kka",
    "href": "504-Kanonische-Korrelationsanalyse.html#sec-modellschaetzung-kka",
    "title": "39  Kanonische Korrelationsanalyse",
    "section": "39.4 Modellschätzung",
    "text": "39.4 Modellschätzung\nZur Schätzung einer kanonischen Korrelationsanalyse wird die Kovarianzmatrix \\(\\mathbb{C}(z)\\) des gemeinsamen Zufallsvektors \\(z\\) von Prädiktoren und Kriterien durch ihr Stichprobenäquivalent \\(C\\) ersetzt. Dies ist die Aussage folgender Definition.\n\nDefinition 39.2 (Schätzer der kanonischen Korrelationsanalyse) Für \\(i = 1,...,n\\) seien \\[\\begin{equation}\nz_i = \\begin{pmatrix} x_i \\\\ y_i \\end{pmatrix}\n\\mbox{ mit }\n\\mathbb{E}(z_i) := 0_m\n\\mbox{ und }\n\\mathbb{C}(z_i) :=\n\\begin{pmatrix}\n\\Sigma_{xx} & \\Sigma_{xy} \\\\\n\\Sigma_{yx} & \\Sigma_{yy} \\\\\n\\end{pmatrix}\n\\in \\mathbb{R}^{m \\times m}\n\\end{equation}\\] unabhängig und identisch verteilte \\(m\\)-dimensionale partitionierte Zufallsvektoren sowie ihr Erwartungswert und ihre Kovarianzmatrix, respektive, und \\[\\begin{equation}\nC :=\n\\begin{pmatrix}\nC_{xx} & C_{xy} \\\\\nC_{yx} & C_{yy} \\\\\n\\end{pmatrix}\n\\in \\mathbb{R}^{m \\times m}\n\\end{equation}\\] sei ihre Stichprobenkovarianzmatrix. Dann sind für \\(i = 1,...,k := \\min \\{m_x,m_y\\}\\) \\[\\begin{equation}\n\\hat{a}_i := C_{xx}^{-1/2}\\hat{\\alpha}_i \\in \\mathbb{R}^{m_x}, \\quad\n\\hat{b}_i := C_{yy}^{-1/2}\\hat{\\beta}_i \\in \\mathbb{R}^{m_y} \\mbox{ und }\n\\hat{\\rho}_i := \\hat{\\lambda}_i^{1/2}\n\\end{equation}\\] Schätzer der \\(i\\)ten kanonischen Koeffizientenvektoren und kanonischen Korrelationen, respektive. Dabei sind mit \\[\\begin{equation}\n\\hat{K} := C_{xx}^{-1/2}C_{xy}C_{yy}^{-1/2} \\in \\mathbb{R}^{m_x \\times m_y}\n\\end{equation}\\] \\(\\hat{\\alpha}_i\\) und \\(\\hat{\\lambda}_i\\) der \\(i\\)te Eigenvektor und sein zugehöriger Eigenwert von \\(\\hat{K}\\hat{K}^T\\) und \\(\\hat{\\beta}_i\\) der entsprechende Eigenkvektor von \\(\\hat{K}^T\\hat{K}\\).\n\nWir verzichten auf eine Diskussion der Güte dieser Schätzung.\n\nSimulationsbeispiel\nMithilfe folgenden R Codes verdeutlichen wir uns die Schätzung kanonischer Korrelationen und Koeffizientenvektoren in dem oben betrachteten Beispiel. Dazu generieren wir Realisierungen der Prädiktoren und Kriterien bei Stichprobenumfängen zwischen \\(n = 100\\) und \\(n = 1000\\). Abbildung 39.1 visualisiert die wahren, aber unbekannten, kanonischen Korrelationen \\(\\rho_1,\\rho_2,\\rho_3\\) und ihre für jede Simulation basierend auf der Stichprobenkovarianzmatrix des realisierten Datensatzes geschätztes Äquivalente \\(\\hat{\\rho}_1, \\hat{\\rho}_2,\\hat{\\rho}_3\\). Die Variabilität der Schätzung nimmt mit zunehmenden Stichprobenumfang ab. Abbildung 39.2 visualisiert die Absolutwerte des wahren, aber unbekannten, ersten kanonischen Koeffizientenvektors \\(a_1\\) sowie ihre entsprechenden Schätzungen als \\(\\hat{a}_1\\). Auch hier nimmt die Variabilität der Schätzung mit zunehmenden Stichprobenumfang ab. Allerdings ist zu beachten, dass es sich hierbei um die Absolutwerte des kanonischen Koeffizientenvektors handelt und das Vorzeichen abhängig von der Schätzung auch im Widerspruch zum wahren, aber unbekannten, Wert des kanonischen Koeffizientenvektors stehen kann.\n\n# R Pakete\nlibrary(MASS)\nlibrary(expm)\n\n# Modellparameter\nm_x      = 4\nm_y      = 3\nk        = min(m_x,m_y)\nL        = matrix(c(0,0,1,0,1,0,0,0,0,0,0,-1), nrow = 3,byrow = 3)\nG        = diag(c(0.2,0.4,0.3))\nSigma_xx = diag(4)\nSigma_xy = t(L)\nSigma_yx = L\nSigma_yy = G + L %*% t(L)\nSigma    = rbind(cbind(Sigma_xx, Sigma_xy), cbind(Sigma_yx, Sigma_yy))\nK        = sqrtm(solve(Sigma_xx)) %*% Sigma_xy %*% sqrtm(solve(Sigma_yy))\nALB      = svd(K)\nA        = ALB$u\nLambda   = ALB$d\nB        = ALB$v\nrho      = Lambda\na        = sqrtm(solve(Sigma_xx)) %*% A\nb        = sqrtm(solve(Sigma_yy)) %*% B\n\n\n# Simulationen\nn       = 1e1:1e3\nrho_hat = matrix(rep(NaN, length(n)*k)  , nrow = k)\na_1_hat = matrix(rep(NaN, length(n)*m_x), nrow = m_x)\nfor(i in 1:length(n)){\n\n    # Datengeneration\n    Y          = t(mvrnorm(n[i],rep(0, m_x+m_y),Sigma))\n    I_n        = diag(n[i])\n    J_n        = matrix(rep(1,n[i]^2), nrow = n[i])\n\n    # Stichprobenkovarianzmatrixpartition\n    C          = (1/(n[i]-1))*(Y %*% (I_n-(1/n[i])*J_n) %*% t(Y))\n    C_xx       = C[1:m_x,1:m_x]\n    C_xy       = C[1:m_x,(m_x+1):(m_x+m_y)]\n    C_yx       = C[(m_x+1):(m_x+m_y),1:m_x]\n    C_yy       = C[(m_x+1):(m_x+m_y),(m_x+1):(m_x+m_y)]\n\n    # Kanonische Korrelationsanalyse\n    K_hat        = sqrtm(solve(C_xx)) %*% C_xy %*% sqrtm(solve(C_yy))\n    ALB_hat      = svd(K_hat)\n    A_hat        = ALB_hat$u\n    Lambda_hat   = ALB_hat$d\n    B_hat        = ALB_hat$v\n    a_hat        = sqrtm(solve(C_xx)) %*% A_hat\n    b_hat        = sqrtm(solve(C_yy)) %*% B_hat\n    rho_hat[,i]  = as.matrix(Lambda_hat)\n    a_1_hat[,i]  = a_hat[,1]\n}\n\n\n\n\n\n\n\nAbbildung 39.1: Schätzung kanonischer Korrelationen im Simulationsbeispiel\n\n\n\n\n\n\n\n\n\nAbbildung 39.2: Schätzung der Absolutwerte des ersten kanonischen Koeffizientenvektors im Simulationsbeispiel",
    "crumbs": [
      "Multivariate Inferenz",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Kanonische Korrelationsanalyse</span>"
    ]
  },
  {
    "objectID": "504-Kanonische-Korrelationsanalyse.html#sec-anwendungsbeispiel-kka",
    "href": "504-Kanonische-Korrelationsanalyse.html#sec-anwendungsbeispiel-kka",
    "title": "39  Kanonische Korrelationsanalyse",
    "section": "39.5 Anwendungsbeispiel",
    "text": "39.5 Anwendungsbeispiel\nZuletzt wollen wir die konkrete Berechnung einer kanonischen Korrelationsanalyse im Kontext des in Kapitel 39.1 diskutierten Anwendungsbeispiels demonstrieren. Folgender R Code implementiert die Berechnung der kanonischen Korrelationen und kanonischen Koeffizientenvektoren für diesen Datensatz.\n\n# R Paket \nlibrary(expm)\n\n# Datenpräprozessierung\nfname      = \"./_data/504-kanonische-korrelationsanalyse.csv\"\nD          = read.table(fname, sep = \",\", header = TRUE)\nx          = as.matrix(cbind(D$DUR, D$EXP))\ny          = as.matrix(cbind(D$dBDI, D$dGLU))\nn          = nrow(x)\nm_x        = ncol(x)\nm_y        = ncol(y)\nY          = t(cbind(x,y))\n\n# Stichprobenkovarianzmatrixpartition\nI_n        = diag(n)\nJ_n        = matrix(rep(1,n^2), nrow = n)\nC          = (1/(n-1))*(Y %*% (I_n-(1/n)*J_n) %*% t(Y))\nC_xx       = C[1:m_x,1:m_x]\nC_xy       = C[1:m_x,(m_x+1):(m_x+m_y)]\nC_yx       = C[(m_x+1):(m_x+m_y),1:m_x]\nC_yy       = C[(m_x+1):(m_x+m_y),(m_x+1):(m_x+m_y)]\n\n# Kanonische Korrelationsanalyse\nK_hat      = sqrtm(solve(C_xx)) %*% C_xy %*% sqrtm(solve(C_yy))\nALB_hat    = svd(K_hat)\nA_hat      = ALB_hat$u\nLambda_hat = ALB_hat$d\nB_hat      = ALB_hat$v\na_hat      = sqrtm(solve(C_xx)) %*% A_hat\nb_hat      = sqrtm(solve(C_yy)) %*% B_hat\nrho_hat    = as.matrix(Lambda_hat)\n\n\n\nrho_hat_1 :  0.9950575 \na_hat_1   :  -0.1623409 -0.173979 \nb_hat_1   :  -0.1554175 -0.05025419 \nrho_hat_2 :  0.5010358 \na_hat_2   :  -0.06026274 0.3118808 \nb_hat_2   :  -0.08128072 0.7773036\n\n\nNeben der Implementation mithilfe einer Singulärwertzerlegung bietet R auch eine direkte Bestimmung mithilfe der cancor() Funktion an. Folgender R Code demonstriert das entsprechende Vorgehen.\n\n# Datenpräprozessierung\nfname      = \"./_data/504-kanonische-korrelationsanalyse.csv\"\nD          = read.table(fname, sep = \",\", header = TRUE)\nx          = as.matrix(cbind(D$DUR, D$EXP))\ny          = as.matrix(cbind(D$dBDI, D$dGLU))\ncca        = cancor(x,y)\n\n\n\nrho_hat_1 :  0.9950575 \nrho_hat_2 :  0.5010358\n\n\nMan findet also, dass die geschätzte maximale Korrelation einer Linearkombinationen der Prädiktorvariablen DUR und EXP mit einer Linearkombination der Kriterien dBDI und dGLU mit \\(\\hat{\\rho}_1 = 0.99\\) sehr hoch ist. Man kann daraus schließen, dass in diesem Fall die Prädiktorvariablen gemeinschaftlich hoch mit den Kriterien assoziiert sind. Es ergeben sich hier insbesondere die Linearkombinationen \\[\\begin{equation}\n\\xi = 0.16 \\mbox{ DUR} + 0.17 \\mbox{ EXP}\n\\mbox{ und }\n\\upsilon  = 0.15 \\mbox{ dBDI} + 0.05 \\mbox{ dGLU}\n\\end{equation}\\] als bester Prädiktor und als am besten prädizierbares Kriterium, respektive. Die Dauer der Psychotherapie und die Erfahrung der behandelnden Psychotherapeut:in scheinen, bei der aktuellen Datenskalierung zur bestmöglichen Prädiktion der Therapiegüte also in etwa gleichbedeutend, bei dem bestprädizierbarem Kriterium der Therapieeffizienz trägt bei der aktuellen Datenskalierung die BDI Score Reduktion etwas mehr bei als die Glukokortikoidplasmalevel Reduktion bei.",
    "crumbs": [
      "Multivariate Inferenz",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Kanonische Korrelationsanalyse</span>"
    ]
  },
  {
    "objectID": "504-Kanonische-Korrelationsanalyse.html#literaturhinweise",
    "href": "504-Kanonische-Korrelationsanalyse.html#literaturhinweise",
    "title": "39  Kanonische Korrelationsanalyse",
    "section": "39.6 Literaturhinweise",
    "text": "39.6 Literaturhinweise\nDie kanonische Korrelationsanalyse geht zurück auf Hotelling (1935) und Hotelling (1936).\n\n\n\n\nAnderson, T. W. (2003). An Introduction to Multivariate Statistical Analysis (3rd ed). Wiley-Interscience.\n\n\nHotelling, H. (1935). The Most Predictable Criterion. Journal of Educational Psychology, 26(2), 139–142. https://doi.org/10.1037/h0058165\n\n\nHotelling, H. (1936). Relations Between Two Sets of Variates. Biometrika, 28(3/4), 321. https://doi.org/10.2307/2333955\n\n\nMardia, K. V., Kent, J. T., & Bibby, J. M. (1979). Multivariate Analysis. Academic Press.\n\n\nUurtio, V., Monteiro, J. M., Kandola, J., Shawe-Taylor, J., Fernandez-Reyes, D., & Rousu, J. (2018). A Tutorial on Canonical Correlation Methods. ACM Computing Surveys, 50(6), 1–33. https://doi.org/10.1145/3136624",
    "crumbs": [
      "Multivariate Inferenz",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Kanonische Korrelationsanalyse</span>"
    ]
  }
]