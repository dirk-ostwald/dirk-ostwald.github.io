# Erwartungswerte  {#sec-erwartungswerte}
\normalsize
## Erwartungswert einer Zufallsvariable {#sec-erwartungswert-einer-zufallsvariable}

:::{#def-erwartungswert-einer-zufallsvariable}
## Erwartungswert einer Zufallsvariable
$(\Omega, \mathcal{A},\mathbb{P})$ sei ein Wahrscheinlichkeitsraum und $\xi$
sei eine  Zufallsvariable. Dann ist der *Erwartungswert von $\xi$* definiert als

* $\mathbb{E}(\xi) := \sum_{x \in \mathcal{X}} x\,p(x)$, wenn $\xi : \Omega \to \mathcal{X}$ diskret mit WMF $p$ ist.
* $\mathbb{E}(\xi) := \int_{-\infty}^\infty x \,p(x)\,dx$, wenn $\xi : \Omega \to \mathbb{R}$ kontinuierlich mit WDF $p$ ist.

:::

Der Erwartungswert ist also eine skalare Zusammenfassung der Verteilung einer 
Zufallsvariable. Eine Definition des Erwartungswertes, die ohne eine Fallunterscheidung 
in kontinuierliche und diskrete Zufallsvariablen auskommt, ist möglich, erfordert 
aber mit der Einführung des Lebesgue-Integrals einigen technischen Aufwand. 
Wir verweisen dafür auf die weiterführende Literatur (vgl. @schmidt2009, @meintrup2005). 
Intuitiv entspricht der Erwartungswert einer Zufallsvariable dem im langfristigen 
Mittel zu erwartenden Wert der Zufallsvariable. Wir werden diese Intuition
im Kontext der Gesetze der großen Zahl in @sec-grenzwerte weiter ausarbeiten. Wir
betrachten zunächst einige Beispiele für @def-erwartungswert-einer-zufallsvariable.

#### Beispiele {-}

:::{#thm-erwartungwert-einer-diskreten-zufallsvariable}
## Erwartungswert einer diskreten Zufallsvariable
$\xi$ sei eine Zufallsvariable mit Ergebnisraum $\mathcal{X} := \{-1,0,1\}$ 
und WMF  
\begin{equation}
p(-1) = \frac{1}{4}, \quad p(0) = \frac{1}{2},  \quad p(1) = \frac{1}{4}.
\end{equation}
Dann gilt
\begin{equation}
\mathbb{E}(\xi) = 0. 
\end{equation}
:::

:::{.proof}
Nach @def-erwartungswert-einer-zufallsvariable ergibt sich
\begin{align}
\begin{split}
\mathbb{E}(\xi) 
& = \sum_{x \in \mathcal{X}} x p(x)                                     \\
& = -1 \cdot p(-1) + 0 \cdot p(0) + 1 \cdot p(1)                        \\
& = -1 \cdot \frac{1}{4} + 0 \cdot \frac{1}{2} + 1 \cdot  \frac{1}{4}   \\ 
& = 0. 
\end{split}
\end{align}
:::

:::{#thm-erwartungwert-einer-bernoulli-zufallsvariable}
## Erwartungswert einer Bernoulli Zufallsvariable
Es sei $\xi \sim \mbox{Bern}(\mu)$. Dann gilt $\mathbb{E}(\xi) = \mu$.
:::

:::{.proof}
$\xi$ ist diskret mit $\mathcal{X} = \{0,1\}$. Also gilt nach @def-erwartungswert-einer-zufallsvariable
\begin{align}
\begin{split}
& = \sum_{x \in \{0,1\}} x\,\mbox{Bern}(x;\mu)                    \\
& = 0\cdot \mu^0 (1 - \mu)^{1-0} + 1\cdot \mu^1 (1 - \mu)^{1-1}   \\
& = 1\cdot \mu^1 (1 - \mu)^{0}                                    \\
& = \mu.
\end{split}
\end{align}
:::


:::{#thm-erwartungwert-einer-normalverteilten-zufallsvariable}
## Erwartungswert einer normalverteilten Zufallsvariable
Es sei $\xi \sim N(\mu,\sigma^2)$. Dann gilt $\mathbb{E}(\xi) = \mu$.
:::

:::{.proof}
Wir verzichten auf einen Beweis.
:::

In Verallgemeinerung von @def-erwartungswert-einer-zufallsvariable gilt folgende
Definition für den Erwartungswert einer Funktion einer Zufallsvariable.

:::{#def-erwartungswert-einer-funktion-einer-zufallsvariable}
## Erwartungswert einer Funktion einer Zufallsvariable
$(\Omega, \mathcal{A},\mathbb{P})$ sei ein Wahrscheinlichkeitsraum, $\xi$
sei eine  Zufallsvariable mit Ergebnisraum $\mathcal{X}$ und $f: \mathcal{X} \to \mathcal{Z}$
sei eine Funktion mit Zielmenge $\mathcal{Z}$. Dann ist der *Erwartungswert der
Funktion $f$ der Zufallsvariable $\xi$* definiert als

* $\mathbb{E}(f(\xi)) := \sum_{x \in \mathcal{X}} f(x)\,p(x)$,
wenn $\xi : \Omega \to \mathcal{X}$ diskret mit WMF $p$ ist,
* $\mathbb{E}(f(\xi)) := \int_{-\infty}^\infty f(x) \,p(x)\,dx$,
wenn $\xi : \Omega \to \mathbb{R}$ kontinuierlich mit WDF $p$ ist.
:::

Der Erwartungswert einer Zufallsvariable ergibt sich anhand von 
@def-erwartungswert-einer-funktion-einer-zufallsvariable als der Spezialfall,
in dem gilt, dass 
\begin{equation}
f : \mathcal{X} \to \mathcal{Z}, x \mapsto f(x) := x.
\end{equation}
In der englischsprachigen Literatur ist @def-erwartungswert-einer-funktion-einer-zufallsvariable 
auch als *law of the unconscious statistician* bekannt und wird oft auch direkt zur Definition des
Erwartungswertes herangezogen. Folgendes Theorem gibt ein erstes Beispiel für 
den Erwartungswert einer Funktion einer Zufallsvariable.

:::{#thm-erwartungswert-bei-linear-affiner-transformation-einer-zufallsvariable}
## Erwartungswert bei linear-affiner Transformation einer Zufallsvariable
$\xi$ sei eine Zufallsvariable mit Ergebnisraum $\mathcal{X}$ und es sei 
\begin{equation}
f : \mathcal{X} \to \mathcal{Z}, x \mapsto f(x) := ax + b \mbox{ für } a,b \in \mathbb{R}
\end{equation}
eine linear-affine Funktion. Dann gilt
\begin{equation}
\mathbb{E}(f(\xi)) = \mathbb{E}(a\xi + b) = a\mathbb{E}(\xi) + b.
\end{equation}
:::

:::{.proof}
Die Aussage des Theorems folgt mit den Linearitätseigenschaften von Summen 
und Integralen. Wir betrachten  den Fall einer diskreten Zufallsvariable $\xi$
mit endlichem Ergebnisraum $\mathcal{X}$ und WMF $p$. Es gilt
\begin{align}
\begin{split}
\mathbb{E}(f(\xi))
& = \mathbb{E}(a\xi + b) 						                \\
& = \sum_{x \in \mathcal{X}} (ax + b)p(x)  				                \\
& = \sum_{x \in \mathcal{X}}  axp(x)  + b p(x)   		            \\
& = a\sum_{x \in \mathcal{X}} xp(x) + b \sum_{x \in \mathcal{X}} p(x)          \\
& = a\mathbb{E}(\xi) + b.
\end{split}
\end{align}
:::

## Erwartungswert eines Zufallsvektors {#sec-erwartungswert-eines-Zufallsvektors}
:::{#def-erwartungswert-eines-zufallsvektors}
## Erwartungswert eines Zufallsvektors
$\xi$ sei ein $n$-dimensionaler Zufallvektor. Dann ist der *Erwartungwert* von 
$\xi$ definiert als der $n$-dimensionale reelle Vektor
\begin{equation}
\mathbb{E}(\xi) :=
\begin{pmatrix}
\mathbb{E}(\xi_1) \\
\vdots            \\
\mathbb{E}(\xi_n)
\end{pmatrix}
\end{equation}
:::

Der Erwartungswert eines Zufallsvektors $\xi$ ist also der Vektor der Erwartungswerte
der Komponenten $\xi_1, ...,\xi_n$ und ist damit direkt im Sinne von Erwartungswerten 
von Zufallsvariablen definiert. Man beachte bei @def-erwartungswert-eines-zufallsvektors
allerdings, dass $\mathbb{E}(\xi)$ als Erwartungswert bezüglich der multivariaten
Verteilung von $\xi$ und die $\mathbb{E}(\xi_i)$ für $i = 1,...,n$ als die Erwartungswerte
bezüglich der univariaten Marginalverteilungen von $\xi$ gedacht sind. 

\hl{BEISPIEL}

In Analogie zu @def-erwartungswert-einer-funktion-einer-zufallsvariable definiert man für die 
Funktion eines Zufallsvektors den Erwartungswert dieser Transformation wie folgt.

:::{#def-erwartungswert-einer-funktion-einer-zufallsvektors}
## Erwartungswert einer Funktion eines Zufallsvektors
$(\Omega, \mathcal{A},\mathbb{P})$ sei ein Wahrscheinlichkeitsraum, $\xi$
sei ein Zufallsvektor mit Ergebnisraum $\mathcal{X}$ und $f: \mathcal{X} \to \mathcal{Z}$
sei eine Funktion mit Zielmenge $\mathcal{Z}$. Dann ist der *Erwartungswert der
Funktion $f$ des Zufallsvektors $\xi$* definiert als

* $\mathbb{E}(f(\xi)) := \sum_{x \in \mathcal{X}} f(x)\,p(x)$,
wenn $\xi : \Omega \to \mathcal{X}$ diskret mit WMF $p$ ist,
* $\mathbb{E}(f(\xi)) := \int_{-\infty}^\infty f(x) \,p(x)\,dx$,
wenn $\xi : \Omega \to \mathbb{R}$ kontinuierlich mit WDF $p$ ist.
:::

:::{#thm-erwartungswert-bei-linear-affiner-kombination}
## Erwartungswert bei linear-affiner Kombination
$\xi$ sei ein $n$-dimensionaler Zufallsvektor mit Komponenten $\xi_1,...,\xi_n$
und Ergebnisraum  $\mathcal{X} := \mathcal{X}_1 \times \cdots \times \mathcal{X}_n$.
Weiterhin sei
\begin{equation}
f : \mathcal{X} \to \mathcal{Z}, x \mapsto f(x) := a_0 + \sum_{i=1}^n a_ix_i \mbox{ für }  a_0,...,a_n \in \mathbb{R}.
\end{equation}
eine linear-affine Kombination der Komponenten von $\xi$. Dann gilt 
\begin{equation}
\mathbb{E}(f(\xi)) = \mathbb{E}\left(a_0 +\sum_{i=1}^n a_i\xi_i \right) = a_0 + \sum_{i = 1}^n a_i \mathbb{E}(\xi_i).
\end{equation}
:::

:::{.proof}
Wie unten gezeigt, folgt das Theorem mit den Linearitätseigenschaften von Summen 
und Integralen, sowie Fubini's Theorem zur Vertauschung von Summations- bzw. 
Integrationsreihenfolge. Zur Vereinfachung der Notation schreiben wir untenstehend
$\sum_{x_i \in \mathcal{X}_i}$ als $\sum_{x_i}$. Es gilt dann 
\begin{align}
\begin{split}
\mathbb{E}(f(\xi))
& = \mathbb{E}\left(a_0 + \sum_{i=1}^{n} a_i\xi_i\right)                            \\
& = \mathbb{E}(a_0 + a_1\xi_1 + \cdots + a_{n}\xi_{n})                              \\
& = \sum_{x_1}\cdots\sum_{x_n} (a_0 + a_1x_1 + \cdots  a_{n}x_{n})p(x_1,...,x_{n})  \\
& = \sum_{x_1}\cdots\sum_{x_n} a_0p(x_1,...,x_n) + a_1x_1p(x_1,...,x_n) + \cdots + a_nx_np(x_1,...,x_n)   \\
& = \sum_{x_1}\cdots\sum_{x_n} a_0p(x_1,...,x_n)    + 
    \sum_{x_1}\cdots\sum_{x_n} a_1x_1p(x_1,...,x_n) + \cdots + 
    \sum_{x_1}\cdots\sum_{x_n} a_nx_np(x_1,...,x_n)   \\
& = a_0\sum_{x_1}\cdots\sum_{x_n}p(x_1,...,x_n)    + 
    a_1\sum_{x_1}\cdots\sum_{x_n} x_1p(x_1,...,x_n) + \cdots + 
    a_n\sum_{x_n}\cdots\sum_{x_1} x_np(x_1,...,x_n)   \\
& = a_0     + 
    a_1\sum_{x_1}x_1 \sum_{x_2}     \cdots\sum_{x_n} p(x_1,...,x_n) + \cdots + 
    a_n\sum_{x_n}x_n \sum_{x_{n-1}} \cdots\sum_{x_1} p(x_1,...,x_n)   \\
& = a_0 + a_1\sum_{x_1}x_1 p(x_1) + \cdots + a_n\sum_{x_n}x_n p(x_n)  \\
& = a_0 + a_1\mathbb{E}(\xi_1) + \cdots + a_n\mathbb{E}(\xi_n)  \\
& = a_0 + \sum_{i=1}^n a_i\mathbb{E}(\xi_i)  
\end{split}
\end{align}
:::

@thm-erwartungswert-bei-linear-affiner-kombination zeigt, wie man den Erwartungswert
einer gewichteten Summe von Zufallsvariablen mit beliebiger gemeinsamer Verteilung
bestimmen kann. Für den Fall eines Produktes von Zufallsvariablen gibt es zumindest
für den Fall unabhängiger Zufallsvariablen ein entsprechendes Theorem.  

:::{#thm-erwartungswert-bei-multiplikativer-kombination}
## Erwartungswert der multiplikativen Kombination unabhängiger Zufallsvariablen
$\xi$ sei ein $n$-dimensionaler Zufallsvektor mit unabhängigen 
Komponenten $\xi_1$,...,$\xi_n$ und Ergebnisraum $\mathcal{X}$. Weiterhin sei
\begin{equation}
f : \mathcal{X} \to \mathcal{Z}, x \mapsto f(x) := \prod_{i=1}^n x_i
\end{equation}
eine multiplikative Kombination der Komponenten von $\xi$. Dann gilt
\begin{equation}
\mathbb{E}(f(\xi)) = \mathbb{E}\left(\prod_{i=1}^n \xi_i \right) = \prod_{i = 1}^n \mathbb{E}(\xi_i).
\end{equation}
:::

:::{.proof}

Wir betrachten den Fall eines diskreten Zufallsvektors $\xi$ mit WNF $p$. 
Weil die Komponenten von $\xi$ als unabhängig vorausgesetzt sind, gilt für $p$, dass
\begin{equation}
p(x_1,...,x_n) = \prod_{i=1}^n p(x_i).
\end{equation}
Damit folgt dann
\begin{align}
\begin{split}
\mathbb{E}\left(f(\xi)\right)
& = \mathbb{E}\left(\prod_{i=1}^n \xi_i \right)                                         \\
& = \sum_{x_1}\cdots\sum_{x_n} \left(\prod_{i=1}^n x_ip(x_1,...,x_n)\right)             \\
& = \sum_{x_1}\cdots\sum_{x_n} \left(\prod_{i=1}^n x_i \prod_{i=1}^n p(x_i)\right)	    \\
& = \sum_{x_1}\cdots\sum_{x_n} \left(\prod_{i=1}^n x_i p(x_i)\right)	                  \\
& = \prod_{i=1}^n \left(\sum_{x_i} x_i p(x_i)\right)                                    \\
& = \prod_{i=1}^n \mathbb{E}(\xi_i).
\end{split}
\end{align}
:::

## Stichprobenmittel {#sec-stichprobenmittel}

Wie wir @sec-grundbegriffe-frequentistischer-inferenz noch ausführlich diskutieren, 
werden ist eine Charakteristikum der probabilistischen Modellierung, beobachtete 
Daten als Realisierungen von Zufallsvariablen zu verstehen. Hat meine Menge 
$\xi_1,...,\xi_n$ von Zufallsvariablen, so nennt man diese auch eine *Stichprobe*. 
Basierend auf einer Stichprobe kann man nun eine Kennzahl berechnen, die auf den 
ersten Blick dem Begriff des Erwartungswerts ähnelt, mit diesem aber keinesfalls 
zu verwechseln sind. Defacto dient die in folgender Definition aufgeführte 
Stichprobenkennzahl oft als *Schätzer* für den Erwartungswert von Zufallsvariablen, wie wir in @sec-parameterschätzung ausführlich darlegen wollen.  

:::{#def-stichprobenmittel}
## Stichprobenmittel 
$\xi_1,...,\xi_n$ sei eine Menge von Zufallsvariablen, genannt *Stichprobe*.
Dann ist das Stichprobenmittel von $\xi_1,...,\xi_n$ ist definiert als 
\begin{equation}
\bar{\xi} := \frac{1}{n}\sum_{i=1}^n \xi_i.
\end{equation}
:::

Das Stichprobenmittel kann also die Funktion 
\begin{equation}
f : \mathcal{X} \to \mathcal{Z}, x \mapsto f(x) := \frac{1}{n}\sum_{i=1}^n x_i
\end{equation}
des Zufallsvektors $\xi := (\xi_1,...,\xi_n)^T$ mit Ergebnisraum $\mathcal{X}$
aufgefasst werden. Wir wollen dies an einem Beispiel erläutern. Nehmen wir also an, 
wir haben für $n := 10$ die in @tbl-realisationen gezeigten Realisation des 
Zufallsvektors $\xi$ oder,  äquivalent, die Realisationen der Komponenten 
$\xi_1,...,\xi_n$ von $\xi$. 

| $x_1$  | $x_2$  | $x_3$  | $x_4$  | $x_5$  | $x_6$  | $x_7$  | $x_8$  | $x_9$  | $x_{10}$ |
|:------:|:------:|:------:|:------:|:------:|:------:|:------:|:------:|:------:|:--------:|
|  0.54  |  1.01  | -3.28  |  0.35  |  2.75  | -0.51  |  2.32  |  1.49  |  0.96  |   1.25   |

: Werte der Zufallsvariablen $x_1$ bis $x_{10}$ {#tbl-realisationen}

Nach @def-stichprobenmittel ist die Realisation des Stichprobenmittels dann gegeben durch
\begin{equation}
\bar{x}
= \frac{1}{10}\sum_{i = 1}^{10}x_i
= \frac{6.88}{10}
= 0.68. 
\end{equation}


:::{#def-stichprobenmittel-von-zufallsvektoren}
## Stichprobenmittel von Zufallsvektoren 
$\xi_1,...,\xi_n$ sei eine Menge von $n$-dimensionalen Zufallsvektoren, genannt *Stichprobe*.
Dann ist das Stichprobenmittel von $\xi_1,...,\xi_n$ ist definiert als 
\begin{equation}
\bar{\xi} := \frac{1}{n}\sum_{i=1}^n \xi_i.
\end{equation}
:::

\hl{MATRIXFORM UND BEISPIEL}

## Bedingte Erwartungswerte {#sec-bedingte-erwartungswerte}
:::{#def-bedingter-errwartungswert}
## Bedingter Erwartungswert
Gegeben sei ein Zufallsvektor $\xi := (\xi_1,\xi_2)$ mit Ergebnisraum 
$\mathcal{X} := \mathcal{X}_1 \times \mathcal{X}_2$, WMF oder WDF $p(x_1,x_2)$
und bedingter WMF oder WDF $p(x_1|x_2)$ für alle $x_2 \in \mathcal{X}_2$. 
Dann ist der *bedingte Erwartungswert* von $\xi_1$ gegeben $\xi_2 = x_2$ definiert als

*  $\mathbb{E}(\xi_1|\xi_2 = x_2) := \sum_{x_1 \in \mathcal{X}_1}x_1p(x_1|x_2)$, wenn $\xi$ ein diskreter Zufallsvektor ist. 
*  $\mathbb{E}(\xi_1|\xi_2 = x_2) := \int_{\mathcal{X}_1}x_1p(x_1|x_2)\,dx_1$, wenn $\xi$ ein kontinuierlicher Zufallsvektor ist.

:::

Der bedingte Erwartungswert einer Zufallsvariable ist also der Erwartungswert
einer Zufallsvariable in einer bedingten Verteilung. Man beachte, dass wir in 
@def-bedingter-errwartungswert den bedingten Erwartungswert für einen festen Wert 
$x_2$ von $\xi_2$ definiert habe. Bei einem festen Wert $x_2$ von $\xi_2$ ist 
$\mathbb{E}(\xi_1|\xi_2 = x_2)$ damit ein fester Wert und durch Austauschen der 
Subskripte erhält man entsprechend $\mathbb{E}(\xi_2|\xi_1 = x_1)$. 

Allgemein ist $\mathbb{E}(\xi_1|\xi_2)$ allerdings eine Zufallsvariable, 
da $\xi_2$ eine Zufallsvariable ist und nur mit einer gewissen Wahrscheinlichkeit
einen Wert $\xi_2 = x_2$ annimmt. Wir werden später sehen, dass analog zu @def-bedingter-errwartungswert 
bedingte Varianzen, bedingte Kovarianzen und bedingte Korrelationen definiert werden. 
In der Psychologie ist der Begriff des bedingten Erwartungswerts zentral, denn 
in der klassischen Testtheorie sind die True-Scores von Personen bei Tests oder 
Fragebögen als  bedingte Erwartungswerte definiert. Zur Verdeutlichung von 
@def-bedingter-errwartungswert betrachten wir ein Beispiel im Kontext eines diskreten Zufallsvektors.

**Beispiel**

Für einen zweidimensionalen Zufallsvektor $\xi:= (\xi_1,\xi_2)$, der 
Werte in $\mathcal{X} := \mathcal{X}_1 \times \mathcal{X}_2$ mit
$\mathcal{X}_1 := \{1,2,3\}$ und $\mathcal{X}_2 = \{1,2,3,4\}$ annehme seien
bedingte WMFen für $p(x_2|x_1)$ für alle $x_1 \in \mathcal{X}_1$ wie in @tbl-bedingte-wkt
spezifiziert 


| $p(x_2|x_1)$       | $x_2 = 1$      | $x_2 = 2$      | $x_2 = 3$      | $x_2 = 4$      |
|:------------------:|:--------------:|:--------------:|:--------------:|:--------------:|
| $p(x_2|x_1 = 1)$  | $\frac{1}{4}$  | $0$            | $\frac{1}{2}$  | $\frac{1}{4}$  |
| $p(x_2|x_1 = 2)$  | $\frac{1}{3}$  | $\frac{2}{3}$  | $0$            | $0$            |
| $p(x_2|x_1 = 3)$  | $0$            | $\frac{1}{3}$  | $\frac{1}{3}$  | $\frac{1}{3}$  |

: Bedingte Wahrscheinlichkeitsverteilung von $x_2$ gegeben $x_1$ {#tbl-bedingte-wkt}

Dann ergeben sich die bedingten Erwartungswerte
\begin{align}
\begin{split}
\mathbb{E}(\xi_2|\xi_1 = 1) 
&
= \sum_{x_2 \in \mathcal{X}_2}x_2p(x_2|x_1 = 1) 
= 1 \cdot \frac{1}{4} + 2 \cdot 0 + 3 \cdot \frac{1}{2} + 4 \cdot \frac{1}{4}
= \frac{11}{4},
\\
\mathbb{E}(\xi_2|\xi_1 = 2) 
&
= \sum_{x_2 \in \mathcal{X}_2}x_2p(x_2|x_1 = 2) 
= 1 \cdot \frac{1}{3} + 2 \cdot \frac{2}{3} + 3 \cdot 0 + 4 \cdot 0
= \frac{5}{3},
\\
\mathbb{E}(\xi_2|\xi_1 = 3) 
&
= \sum_{x_2 \in \mathcal{X}_2}x_2p(x_2|x_1 = 3) 
= 1 \cdot 0 + 2 \cdot \frac{1}{3} + 3 \cdot \frac{1}{3} + 4 \cdot \frac{1}{3}
= \frac{8}{3}.
\end{split}
\end{align}



## Selbstkontrollfragen  

