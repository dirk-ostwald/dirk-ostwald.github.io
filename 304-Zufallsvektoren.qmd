# Zufallsvektoren {#sec-zufallsvektoren}
\normalsize

Zufallsvektoren sind Tupel von Zufallsvariablen. Da jede Zufallsvariable einer
Wahrscheinlichkeitsverteilung unterliegt, unterliegt auch ein Zufallsvektor
einer Wahrscheinlichkeitsverteilung. Da ein Zufallsvektor aus zwei oder mehr
Zufallsvariablen besteht, beschreibt die Verteilung eines Zufallsvektors die
*gemeinsame Verteilung* von zwei oder mehr Zufallsvariablen. In diesem Kapitel
wollen wir zunächst den Begriff des Zufallsvektors und seiner assoziierten 
Wahrscheinlichkeitsverteilung, die oft einfach als eine *multivariate Verteilung* 
bezeichnet wird, einführen. Wir wollen dann diskutieren, wie die Begriffe der 
WMF, WDF und KVF von Zufallsvariablen auf Zufallsvektoren übertragen werden können 
(@sec-definition-und-multivariate-verteilungen). Mit den *marginalen Verteilungen* 
und den *bedingten Verteilungen* führen wir dann nachfolgend Begriffe ein, die in der Betrachtung 
von Zufallsvariablen nicht auftreten. Schließlich führen wir mit dem Begriff der 
*unabhängigen Zufallsvariablen* das probabilistische Standardmodell für univariate 
Datensätze ein. Dies stellt einen Spezialfall der gemeinsamen Verteilung der 
Zufallsvariablen eines Zufallsvektors dar.

## Definition und multivariate Verteilungen {#sec-definition-und-multivariate-verteilungen}

Die Konstruktion und Definition eines Zufallsvektors ist analog zu der einer 
Zufallsvariable, mit dem Unterschied, dass es sich bei einer Zufallsvariable 
um eine skalarwertige, bei einem Zufallsvektor dagegen um eine vektorwertige
Abbildung auf dem Ergebnisraum eines Wahrscheinlichkeitsraums handelt.

:::{#def-zufallsvektor}
## Zufallsvektor
$(\Omega, \mathcal{A}, \mathbb{P})$ sei ein Wahrscheinlichkeitsraum und 
$(\mathcal{X},\mathcal{S})$ sei ein $n$-dimensionaler Messraum. 
Ein $n$-dimensionaler *Zufallsvektor* ist definiert als eine Abbildung
\begin{equation}
\xi:\Omega \to \mathcal{X}, \omega \mapsto \xi(\omega) :=
\begin{pmatrix}
\xi_1(\omega) \\
\vdots  	\\
\xi_n(\omega)
\end{pmatrix}
\end{equation}
mit der *Messbarkeitseigenschaft*
\begin{equation}
\{\omega \in \Omega|\xi(\omega) \in S \} \in \mathcal{A} \mbox{ für alle } S \in \mathcal{S}.
\end{equation}
:::
Das Standardbeispiel für den Ergebisraum eines Zufallsvektors ist $\mathbb{R}^n$,
das Standardbeispiel für die auf ihm definierte $\sigma$-Algebra ist die 
$n$-dimensionale Borelsche $\sigma$-Algebra $\mathcal{B}(\mathbb{R}^n)$. Für 
eine explizite und formale Einführung der $n$-dimensionalen Borelschen $\sigma$-Algebra
verweisen wir auf die weiterführende Literatur (z.B. @schmidt2009). Wir begnügen 
uns hier wieder mit der (weiterhin formal falschen) Intuition der $n$-dimensionale Borelschen 
$\sigma$-Algebra als Menge aller Teilmengen des $\mathbb{R}^n$. Das Standardbeispiel 
für einen $n$-dimensionalen Messraum ist damit $(\mathbb{R}^n, \mathcal{B}(\mathbb{R}^n))$.

Wie bei allen vektorwertigen Funktionen nennen wir die den Zufallsvektor konstituierenden
Funktionen $\xi_i$ die *Komponentenfunktionen* von $\xi$. Legen wir den $n$-dimensionalen 
Messraum $(\mathbb{R}^n, \mathcal{B}(\mathbb{R}^n))$ dem Zufallsvektor zugrunde, 
so haben diese die Form
\begin{equation}
\xi_i : \Omega \to \mathbb{R}, \omega \mapsto \xi_i(\omega).
\end{equation}
Ohne Beweis halten wir fest, dass der Zufallsvektor $\xi$ messbar ist, wenn für alle 
$i = 1,...,n$ die Funktionen $\xi_i$  messbar sind und umgekehrt. Damit sind die 
Komponentenfunktionen eines Zufallsvektors (letztlich nach Definition) Zufallsvariablen.
Ein $n$-dimensionaler Zufallsvektor wird also als eine Konkatenation von $n$ 
Zufallsvariablen betrachtet, für $n := 1$ entspricht ein Zufallsvektor einer 
Zufallsvariable.

Zufallsvektoren werden manchmal auch als "multivariate Zufallsvariablen" bezeichnet.
Tatsächlich stehen bei der Betrachtung von Zufallsvektoren auch zunächst primär
wahrscheinlichkeitstheoretische Aspekte und nicht etwa Aspekte der geometrischen 
Vektorraumtheorie im Vordergrund. Die Betrachtung von Vektorraumstrukturen ist im 
Kontext probabilistischer Standardmodelle wie dem Allgemeinem Linearen Modell aber
durchaus üblich, so dass wir hier den Begriff des Zufallsvektors bevorzugen 
(vgl. @christensen2011). Trotzdem werden wir Zufallsvektoren, wie in vielen 
Texten der Probabilistik üblich, auch oft in Zeilenform, also etwa als
 $\xi:= (\xi_1,...,\xi_n)$, notieren.

![Konstruktion von Zufallsvektor und multivariater Verteilung.](./_figures/204-zufallsvektor){#fig-zufallsvektor fig-align="center" width=80%}

Das durch die Konstruktion eines Zufallsvektors definierte Bildmaß heißt die
*multivariate Verteilung des Zufallsvektors*, wie in folgender Definition ausgeführt
(vgl. @fig-zufallsvektor).

:::{#def-multivariate-verteilung}
## Multivariate Verteilung
$(\Omega, \mathcal{A}, \mathbb{P})$ sei ein Wahrscheinlichkeitsraum, 
$(\mathcal{X},\mathcal{S})$ sei ein $n$-dimensionaler Messraum und
\begin{equation}
\xi : \Omega \to \mathcal{X}, \omega \mapsto \xi(\omega)
\end{equation}
sei ein Zufallsvektor. Dann heißt das Wahrscheinlichkeitsmaß $\mathbb{P}_\xi$, 
definiert durch
\begin{equation}
\mathbb{P}_\xi : \mathcal{S} \to [0,1], S \mapsto \mathbb{P}_\xi(S)
:= \mathbb{P}(\xi^{-1}(S))
 = \mathbb{P}\left(\{\omega \in \Omega|\xi(\omega) \in S\}\right)
\end{equation}
die *multivariate Verteilung des Zufallsvektor $\xi$*.
:::

Der Einfachheit halber spricht man oft auch nur von *der Verteilung des Zufallsvektors $\xi$*
oder einer *multivariaten Verteilung*. Die Notationskonventionen für Zufallsvariablen 
@def-notation-für-zufallsvariablen gelten für Zufallsvektoren analog. Zum Beispiel
gelten
\begin{align}
\begin{split}
\mathbb{P}_\xi(\xi \in S)
& := \mathbb{P}\left(\{\xi \in S\} \right)
= \mathbb{P}\left(\{\omega \in \Omega|\xi(\omega) \in S\} \right)
\\
\mathbb{P}_\xi(\xi = x)
& := \mathbb{P}\left(\{\xi = x\} \right)
= \mathbb{P}\left(\{\omega \in \Omega|\xi(\omega) = x\} \right)
\\
\mathbb{P}_\xi(\xi \le x)
& := \mathbb{P}\left(\{\xi \le x\} \right)
= \mathbb{P}\left(\{\omega \in \Omega|\xi(\omega) \le x\} \right)
\\
\mathbb{P}_\xi(x_1 \le \xi \le x_2)
& := \mathbb{P}\left(\{x_1 \le \xi \le x_2\} \right)
   = \mathbb{P}\left(\{\omega \in \Omega|x_1 \le \xi(\omega) \le x_2\} \right)
\end{split}
\end{align}
wobei die Relationsoperatoren $<, \le, >, \ge$ hier *komponentenweise* 
verstanden werden. So heißt beispielsweise $x \le y$ für $x,y \in \mathbb{R}^n$, 
dass für alle Komponenten $x_i,y_i, i = 1,...,n$ gilt, dass $x_i \le y_i$. Eben
dieser Konvention folgt auch die Definition der *multivariaten kumulativen Verteilungsfunktion*
in Generalisierung von @def-kumulative-verteilungsfunktion. Wie bei den Zufallsvariablen
werden die qualifizierenden Subskripte bezüglich eines speziellen Zufallsvektors
im Sinne der Multimethode meist weggelassen. Wir nutzen diese Konvention schon 
in folgender Definition der multivariaten kumulativen Verteilungsfunktion eines
Zufallsvektors $\xi$.

:::{#def-multivariate-kumulative-verteilungsfunktionen}
## Multivariate kumulative Verteilungsfunktionen
$\xi$ sei ein Zufallsvektor mit Ergebnisraum $\mathcal{X}$. Dann heißt eine Funktion
der Form
\begin{equation}
P : \mathcal{X} \to [0,1],\, x \mapsto P(x) := \mathbb{P}(\xi \le x)
\end{equation}
multivariate kumulative Verteilungsfunktion von $\xi$.
:::

Wie kumulative Verteilungsfunktionen können auch multivariate kumulative 
Verteilungsfunktionen zur Definition von multivariaten Verteilungen genutzt werden.
Häufiger ist allerdings, wie im univariaten Fall, die Definition multivariater 
Verteilungen durch *multivariate Wahrscheinlichkeitsmasse* - oder 
*Wahrscheinlichkeitsdichtefunktionen*. Wir generalisieren die Definitionen 
diskreter und kontinuierlicher Zufallsvariablen und ihren assoziierten 
Wahrscheinlichkeitsmasse- und Wahrscheinlichkeitsdichtefunktionen wie folgt
(vgl. @def-diskrete-zufallsvariable-und-wahrscheinlichkeitsmassefunktion und 
@def-kontinuierliche-zufallsvariable-und-wahrscheinlichkeitsdichtefunktion).

:::{#def-diskreter-zufallsvektor-und-multivariate-wahrscheinlichkeitsmassefunktion}
## Diskreter Zufallsvektor und multivariate Wahrscheinlichkeitsmassefunktion
Ein Zufallsvektor $\xi$ heißt *diskret*, wenn sein Ergebnisraum $\mathcal{X}$ 
endlich oder abzählbar ist und eine Funktion 
\begin{equation}
p : \mathcal{X} \to [0,1], x \mapsto p(x)  
\end{equation}
existiert, für die gilt

(1) $\sum_{x \in \mathcal{X}}p(x) = 1$
(2) $\mathbb{P}(\xi = x) = p(x)$ für alle $x \in \mathcal{X}$.

Ein entsprechende Funktion $p$ heißt *multivariate Wahrscheinlichkeitsmassefunktion (WMF)* von $\xi$.
:::

Der Begriff der multivariaten WMF ist offenbar direkt analog zum Begriff der WMF.
Wie univariate WMFen sind multivariate WMFen nicht-negativ und normiert. 
Der Einfachheit halber spricht man oft einfach von *der WMF eines Zufallsvektors*
und verzichtet bei ihrer Bezeichnung, wenn der betreffende Zufallsvektor aus dem 
Kontext klar ist, auf das $\xi$ Subskript, schreibt also oft einfach $p$ anstelle
von $p$. 

### Beispiel {-}

Zur Illustration des Begriffs des diskreten Zufallsvektors und seiner WMF wollen wir 
ein Beispiel betrachten. Dazu sei $\xi:= (\xi_1,\xi_2)$  ein Zufallsvektor, der 
Werte in $\mathcal{X} := \mathcal{X}_1 \times \mathcal{X}_2$ annimmt, wobei
$\mathcal{X}_1 := \{1,2,3\}$ und $\mathcal{X}_2 = \{1,2,3,4\}$ seien. Dann entspricht 
der Ergebnisraum von $\xi$ der in @tbl-ergebnisraum spezifizierten Menge an 
Tupeln $(x_1,x_2)$.

\footnotesize

| $(x_1,x_2)$  | $x_2 = 1$  | $x_2 = 2$  | $x_2 = 3$  | $x_2 = 4$ |
|:------------:|:----------:|:----------:|:----------:|:---------:|
| $x_1 = 1$    | $(1,1)$    | $(1,2)$    | $(1,3)$    | $(1,4)$   |
| $x_1 = 2$    | $(2,1)$    | $(2,2)$    | $(2,3)$    | $(2,4)$   |
| $x_1 = 3$    | $(3,1)$    | $(3,2)$    | $(3,3)$    | $(3,4)$   |

: Stichprobenrealisation {#tbl-ergebnisraum}

\normalsize

Eine exemplarische bivariate WMF der Form
\begin{equation}
p: \{1,2,3\} \times \{1,2,3,4\} \to [0,1], (x_1,x_2) \mapsto p(x_1,x_2)
\end{equation}
ist dann durch @tbl-gemeinsame-wkt definiert.

\footnotesize

| $p(x_1,x_2)$ | $x_2 = 1$ | $x_2 = 2$ | $x_2 = 3$ | $x_2 = 4$ |
|:------------:|:---------:|:---------:|:---------:|:---------:|
| $x_1 = 1$    | $0.1$     | $0.0$     | $0.2$     | $0.1$     |
| $x_1 = 2$    | $0.1$     | $0.2$     | $0.0$     | $0.0$     |
| $x_1 = 3$    | $0.0$     | $0.1$     | $0.1$     | $0.1$     |

: Gemeinsame Wahrscheinlichkeitsverteilung $p(x_1,x_2)$ {#tbl-gemeinsame-wkt}

\normalsize

Man beachte, dass die so spezifierte Funktion $p$ den Normiertheits- und 
Nichtnegativitätsansprüchen an eine WMF genügt. Insbesondere gilt hier
\begin{equation}
\sum_{x \in \mathcal{X}} p(x) = \sum_{x_1 = 1}^3 \sum_{x_2 = 1}^4 p(x_1,x_2) = 1.
\end{equation}

Den Begriff des kontinuierlichen Zufallsvektors und der multivariaten Wahrscheinlichkeitsdichtefunktion
definieren wir wie folgt.

:::{#def-kontinuierlicher-zufallsvektor-und-multivariate-wahrscheinlichkeitdichtefunktion}
## Kontinuierlicher Zufallsvektor und multivariate Wahrscheinlichkeitdichtefunktion
Ein Zufallsvektor $\xi$ heißt *kontinuierlich*, wenn sein Ergebnisraum durch 
$\mathbb{R}^n$ gegeben ist und eine Funktion  
\begin{equation}
p : \mathbb{R}^n \to \mathbb{R}_{\ge 0}, x \mapsto p(x),
\end{equation}
existiert, für die gilt, dass

(1) $\int_{\mathbb{R}^n} p(x)\,dx = 1$ und
(2) $\mathbb{P}(x_1 \le \xi \le x_2)  = \int_{x_{1_1}}^{x_{2_1}} \cdots \int_{x_{1_n}}^{x_{2_n}} p(s_1,...,s_n)\,ds_1 \cdots ds_n$.

Eine entsprechende Funktion $p$ heißt *multivariate Wahrscheinlichkeitsdichtefunktion (WDF) von $\xi$*.
:::

Offenbar ist der der Begriff der multivariaten WDF eines kontinuierlichen Zufallsvektors 
analog zum Begriff der WDF einer kontinuierlichen Zufallsvariable und wie univariate
WDFen sind multivariate WDFen nicht-negativ und normiert. Der Einfachheit
halber spricht man auch hier oft einfach von *multivariaten WDFen* und verzichtet
auf die den Zufallsvektor identifizierenden Subskripte. Wie für kontinuierliche 
Zufallsvariablen gilt für kontinuierliche Zufallsvektoren
\begin{equation}
\mathbb{P}(\xi = x)
= \mathbb{P}(x \le \xi \le x)
= \int_{x_1}^{x_1} \cdots \int_{x_n}^{x_n} p(s_1,...,s_n)\,ds_1 \cdots ds_n
= 0.
\end{equation}

Das Standardbeispiel für eine multivariate WDF ist die *multivariate Normalverteilung*,
welcher wir ein eigenes Kapitel widmen.

## Marginalverteilungen {#sec-marginalverteilungen}

Hat man die Verteilung eines Zufallsvektors spezifiziert, so kann man sich fragen,
welche Verteilungen daraus für die einzelnen Komponenten des Zufallsvektors, also
die Zufallsvariablen, die zusammen den Zufallsvektor bilden, folgen. Im Kontext
eines Zufallsvektors nennt man diese die *univariaten Marginalverteilungen* des
Zufallsvektors. Folgende Definition ist grundlegend.

:::{#def-univariate-marginalverteilung}
## Univariate Marginalverteilung
$(\Omega, \mathcal{A}, \mathbb{P})$ sei ein Wahrscheinlichkeitsraum, 
$(\mathcal{X}, \mathcal{S})$ sei ein $n$-dimensionaler Messraum, 
$\xi:\Omega \to \mathcal{X}$ sei ein Zufallsvektor, $\mathbb{P}$ sei die 
Verteilung von $\xi$, $\mathcal{X}_i \subset \mathcal{X}$ sei der Ergebnisraum der 
$i$ten Komponente $\xi_i$ von $\xi$, und $\mathcal{S}_i$ sei eine 
$\sigma$-Algebra auf $\xi_i$. Dann heißt die durch
\begin{equation}
\mathbb{P}_{\xi_i} : \mathcal{S}_i \to [0,1],
S \mapsto  \mathbb{P}\left(\mathcal{X}_1
                     \times
                     \cdots
                     \times
                     \mathcal{X}_{i-1}
                     \times S
                     \times \mathcal{X}_{i+1}
                     \times \cdots
                     \times \mathcal{X}_n\right)
\mbox{ für } S \in \mathcal{S}_i
\end{equation}
definierte Verteilung die *$i$te univariate Marginalverteilung von $\xi$*.
:::

Konkret kann man sowohl für diskrete als auch für kontinuierliche Zufallsvektoren
die WMFen bzw. WDFen ihrer Komponenten direkt aus der entsprechenden multivariaten
WMF bzw. WDF bestimmen. Dies ist die Aussage folgenden Theorems.

:::{#thm-marginale-wahrscheinlichkeitsmasse-und-dichtefunktionen}
## Marginale Wahrscheinlichkeitsmasse und Wahrscheinlichkeitsdichtefunktionen

\noindent (1) $\xi= (\xi_1,...,\xi_n)$ sei ein $n$-dimensionaler diskreter Zufallsvektor
mit WMF $p$ und Komponentenergebnisräumen $\mathcal{X}_1, ..., \mathcal{X}_n$. 
Dann ergibt sich die WMF der $i$ten Komponente $\xi_i$ von $\xi$ als

\footnotesize
\begin{equation}
p : \mathcal{X}_i \to [0,1], 
x_i \mapsto p(x_i) := \sum_{x_1} \cdots \sum_{x_{i-1}} \sum_{x_{i+1}} \cdots \sum_{x_n} p(x_1,...,x_{i-1},x_i,x_{i+1}, ...,x_n).
\end{equation}

\normalsize
\noindent (2) $\xi= (\xi_1,...,\xi_n)$ sei ein $n$-dimensionaler kontinuierlicher Zufallsvektor 
mit WDF $p$ und Komponentenergebnisraum $\mathbb{R}$. Dann ergibt sich die 
WDF der $i$ten Komponente $\xi_i$ von $\xi$ als

\footnotesize
\begin{equation}
p : \mathbb{R} \to \mathbb{R}_{\ge 0},  
x_i \mapsto p(x_i) :=  
\int_{x_1} \cdots \int_{x_{i-1}} \int_{x_{i+1}} \cdots \int_{x_n}
p(x_1,..., x_{i-1},x_i,x_{i+1}, ...,x_n)
\,dx_1...\,dx_{i-1}\,dx_{i+1}...\,dx_n.
\end{equation}
\normalsize
:::

Die WMFen der univariaten Marginalverteilungen diskreter Zufallsvektoren ergeben 
sich also durch Summation über alle Werte der zu der jeweils betrachteten Zufallsvariable
komplementären Zufallsvariablen und die WDFen der univariaten Marginalverteilungen 
kontinuierlicher Zufallsvektoren ergeben sich analog durch Integration über alle 
Werte der zu der jeweils betrachteten Zufallsvariable komplementären Zufallsvariablen.
Für einen Beweis von @thm-marginale-wahrscheinlichkeitsmasse-und-dichtefunktionen 
verweisen wir auf die weiterführende Literatur. 

**Beispiel**

In Fortführung des in @sec-definition-und-multivariate-verteilungen betrachteten
Beispiels eines zweidimensionalen Zufallsvektor $\xi:= (\xi_1,\xi_2)$ ergeben
sich für die dort definierte WMF für die marginalen WMFen die an den Rändern der 
von @tbl-gemeinsame-wkt-rand aufgelisteten WMFen anhand von 
\begin{equation}
p(x_1) = \sum_{x_2 = 1}^{4} p(x_1,x_2) \mbox{ und }
p(x_2) = \sum_{x_1 = 1}^{3} p(x_1,x_2). 
\end{equation}

\footnotesize

| $p(x_1,x_2)$  | $x_2 = 1$  | $x_2 = 2$  | $x_2 = 3$  | $x_2 = 4$  | **$p(x_1)$**  |
|:-------------:|:----------:|:----------:|:----------:|:----------:|:-------------:|
| $x_1 = 1$     | $0.1$     | $0.0$       | $0.2$      | $0.1$      | **$0.4$**     |
| $x_1 = 2$     | $0.1$     | $0.2$       | $0.0$      | $0.0$      | **$0.3$**     |
| $x_1 = 3$     | $0.0$     | $0.1$       | $0.1$      | $0.1$      | **$0.3$**     |
| **$p(x_2)$**  | **$0.2$** | **$0.3$**   | **$0.3$**  | **$0.2$**  |               |

: Gemeinsame Wahrscheinlichkeitsverteilung $p(x_1, x_2)$ mit Randverteilungen {#tbl-gemeinsame-wkt-rand}

\normalsize

Für die Werte $p(x_1)$ werden die entsprechenden Werte von $p(x_1,x_2)$
also zeilenweise und für die Werte von $p(x_2)$ spaltenweise addiert.
Man beachte, dass aus der Normiertheit von $p(x_1,x_2)$ die Normiertheit von 
$p(x_1)$ und $p(x_2)$ direkt folgt, da sich die Gesamtsumme an 
Wahrscheinlichkeitsmasse nicht ändert:
\begin{equation}
1 
= \sum_{x_1=1}^{3}\sum_{x_2 = 1}^{4} p(x_1,x_2) 
= \sum_{x_1=1}^{3} p(x_1) 
= \sum_{x_2=1}^{4} p(x_2). 
\end{equation}


Ein Realisierungsbeispiel mithilfe relativer Häufigkeiten mag den Begriff der 
marginalen WMF intuitiv verdeutlichen. Nehmen wir an, wir hätten $n = 100$ 
unabhängige  Realisierungen von $\xi$ vorliegen. Um die Wahrscheinlichkeiten 
$p(x_1,x_2)$ zu schätzen, würden wir die Anzahl der Realisierungen von 
$(x_1,x_2)$ zählen und durch $n$ teilen. Hätten wir beispielsweise 12 Realisierungen 
von $(3,2)$ vorliegen, so würden wir $p(3,2) \approx 12/100 = 0.12$ schätzen. 
Die Frage nach der marginalen Wahrscheinlichkeit von $x_2 = 2$ entspräche dann 
der Frage, wie oft unter den Realisierungen solche zu finden sind, bei denen 
$x_2 = 2$ ist, irrespektive des Wertes von $x_1$. Dies wäre gerade die Anzahl 
der Realisierungen der Form $(1,2), (2,2)$ und $(3,2)$. Gäbe es von diesen 
beispielsweise $0, 22$ und $12$ respektive, so würde man die Wahrscheinlichkeit 
$p(2)$ 
natürlicherweise durch 
\begin{equation}
\frac{0 + 22 + 12}{100} = \frac{0}{100} + \frac{22}{100} + \frac{12}{100} = 0.00 + 0.22 + 0.12 = 0.34
\end{equation}
schätzen. Anstelle der Wahrscheinlichkeiten $p(1,2)$, $p(2,2)$, $p(3,2)$ 
addiert man hier also die entsprechenden relativen Häufigkeiten.

Marginale Verteilungen im Fall von kontinuierlichen Zufallsvektoren behandeln wir
am Standardbeispiel der multivariaten Normalverteilung in @sec-multivariate-normalverteilungen.

## Bedingte Verteilungen {#sec-bedingte-verteilungen}

Hat man die Verteilung eines Zufallsvektors spezifiziert, so kann man sich fragen,
welche Verteilung daraus für eine einzelne Komponenten des Zufallsvektors folgt, 
wenn man den Wert einer anderen Komponente als bekannt annimmt. Dies führt auf 
den Begriff der *bedingten Verteilung*, welcher sich natürlicherweise aus dem
Begriff der bedingten Wahrscheinlichkeit (vgl. @sec-bedingte-wahrscheinlichkeiten)
ergibt. Wir erinnern uns zunächst, dass für einen Wahrscheinlichkeitsraum 
$(\Omega, \mathcal{A}, \mathbb{P})$ und zwei Ereignisse $A,B \in \mathcal{A}$ 
mit $\mathbb{P}(B) > 0$ die bedingte Wahrscheinlichkeit von $A$ gegeben $B$ 
definiert ist als
\begin{equation}
\mathbb{P}(A|B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}.
\end{equation}
Analog wird für zwei Zufallsvariablen $\xi_1,\xi_2$ mit Ereignisräumen 
$\mathcal{X}_1,\mathcal{X}_2$ und (messbaren) Mengen $S_1 \in \mathcal{X}_1, S_2 \in \mathcal{X}_2$ 
die bedingte Verteilung von $\xi_1$ gegeben $\xi_2$ mithilfe der Ereignisse
\begin{equation}
A := \{\xi_1 \in S_1\} \mbox{ und } B := \{\xi_2 \in S_2\}
\end{equation}
definiert. So ergibt sich zum Beispiel die bedingte Wahrscheinlichkeit, dass $\xi_1 \in S_1$ 
gegeben, dass $\xi_2 \in S_2$ und unter der Annahme, dass $\mathbb{P}(\{\xi_2 \in S_2\}) > 0$, zu
\begin{equation}
\mathbb{P}( \{\xi_1 \in S_1\}|\{\xi_2 \in S_2\})
= \frac{\mathbb{P}(\{\xi_1 \in S_1\} \cap \{\xi_2 \in S_2\})}{\mathbb{P}(\{\xi_2 \in S_2\})}.
\end{equation}

Wir betrachten zunächst die Definition der bedingten Verteilungen von diskreten 
Zufallsvektoren, die lediglich aus zwei Zufallsvariablen bestehen.  


:::{#def-bedingte-wahrscheinlichkeitsmassefunktion-und-diskrete-bedingte-verteilung}
## Bedingte Wahrscheinlichkeitsmassefunktion und diskrete bedingte Verteilung

$\xi:= (\xi_1,\xi_2)$ sei ein diskreter Zufallsvektor mit Ergebnisraum 
$\mathcal{X} := \mathcal{X}_1 \times \mathcal{X}_2$, WMF $p(x_1,x_2)$ und 
marginalen WMFen $p(x_1)$ und $p(x_2)$. Die bedingte WMF von $\xi_1$ gegeben 
$\xi_2 = x_2$ ist dann für $p(x_2) > 0$ definiert als
\begin{equation}
p : \mathcal{X}_1 \to [0,1],
x_1 \mapsto p(x_1|x_2) := \frac{p(x_1,x_2)}{p(x_2)}
\end{equation}
Analog ist für $p(x_1) > 0$ die bedingte WMF von $\xi_2$ gegeben $\xi_1 = x_1$ 
definiert als
\begin{equation}
p : \mathcal{X}_2 \to [0,1],
x_2 \mapsto p(x_2|x_1) := \frac{p(x_1,x_2)}{p(x_1)}
\end{equation}
Die bedingten Verteilungen mit WMFen $p(x_1|x_2)$ und $p(x_2|x:1)$ heißen dann 
die *diskreten bedingten Verteilungen von $\xi_1$ gegeben $\xi_2 = x_2$ bzw. $\xi_2$ gegeben $\xi_1 = x_1$*.
:::

In Analogie zur Definition der bedingten Wahrscheinlichkeit von Ereignissen gilt also
\begin{equation}
p(x_1|x_2)
= \frac{p(x_1,x_2)}{p(x_2)}
= \frac{\mathbb{P}(\{\xi_1 = x_1\} \cap \{\xi_2 = x_2\})}{\mathbb{P}(\{\xi_2 = x_2\})}.
\end{equation}
Es ist dabei entscheidend zu erkennen, dass bedingte Verteilungen lediglich 
normalisierte gemeinsame Verteilungen sind. 

**Beispiel**

In Fortführung des in @sec-definition-und-multivariate-verteilungen betrachteten
Beispiels eines zweidimensionalen Zufallsvektors $\xi:= (\xi_1,\xi_2)$ und seiner
in @sec-marginalverteilungen bestimmten Marginalverteilungen ergeben sich die 
in @tbl-bedingte-wkt-x1-g-x2 dargestellten bedingte WMFen für $p(x_1|x_2)$  und die
in @tbl-bedingte-wkt-x2-g-x1 dargestellten bedingte WMFen für $p(x_2|x_1)$. 

\renewcommand{\arraystretch}{1.3}
\footnotesize

| $p(x_1|x_2)$       | $x_2 = 1$                | $x_2 = 2$                      | $x_2 = 3$                      | $x_2 = 4$               |
|:------------------:|:------------------------:|:------------------------------:|:------------------------------:|:-----------------------:|
| $p(x_1 = 1|x_2)$   | $\frac{0.1}{0.2} = 0.5$  | $\frac{0.0}{0.3} = 0.0$        | $\frac{0.2}{0.3} = 0.6\bar{6}$ | $\frac{0.1}{0.2} = 0.5$ |
| $p(x_1 = 2|x_2)$   | $\frac{0.1}{0.2} = 0.5$  | $\frac{0.2}{0.3} = 0.6\bar{6}$ | $\frac{0.0}{0.3} = 0.0$        | $\frac{0.0}{0.2} = 0.0$ |
| $p(x_1 = 3|x_2)$   | $\frac{0.0}{0.2} = 0.0$  | $\frac{0.1}{0.3} = 0.3\bar{3}$ | $\frac{0.1}{0.3} = 0.3\bar{3}$ | $\frac{0.1}{0.2} = 0.5$ |

: Bedingte Wahrscheinlichkeitsverteilungen $p(x_1|x_2)${#tbl-bedingte-wkt-x1-g-x2}


| $p(x_2|x_1)$      | $p(x_2 = 1|x_1)$               | $p(x_2 = 2|x_1)$               | $p(x_2 = 3|x_1)$               | $p(x_2 = 4|x_1)$                |
|:-----------------:|:------------------------------:|:------------------------------:|:------------------------------:|:-------------------------------:|
| $x_1 = 1$         | $\frac{0.1}{0.4} = 0.25$       | $\frac{0.0}{0.4} = 0.00$       | $\frac{0.2}{0.4} = 0.50$       | $\frac{0.1}{0.4} = 0.25$        |
| $x_1 = 2$         | $\frac{0.1}{0.3} = 0.3\bar{3}$ | $\frac{0.2}{0.3} = 0.6\bar{6}$ | $\frac{0.0}{0.3} = 0.00$       | $\frac{0.0}{0.3} = 0.00$        |
| $x_1 = 3$         | $\frac{0.0}{0.3} = 0.00$       | $\frac{0.1}{0.3} = 0.3\bar{3}$ | $\frac{0.1}{0.3} = 0.3\bar{3}$ | $\frac{0.1}{0.3} = 0.3\bar{3}$  |

: Bedingte Wahrscheinlichkeitsverteilungen $p(x_2 | x_1)$ {#tbl-bedingte-wkt-x2-g-x1}

\normalsize

Man beachte, dass zum einen gilt, dass
\begin{equation}
\sum_{x_1 = 1}^3 p(x_1|x_2) = 1 \mbox{ für alle } x_2 \in \mathcal{X}_2 \mbox{ und }
\sum_{x_2 = 1}^4 p(x_2|x_1) = 1 \mbox{ für alle } x_1 \in \mathcal{X}_1,
\end{equation}
die bedingten WMFen sind also normiert. Zum anderen beachte man die qualitative 
Ähnlichkeit der WMFen $p(x_1,x_2)$ und $p(x_1|x_2)$ bzw. $p(x_2|x_1)$,
die sich einfach daraus ergibt, dass einerseits $p(x_1,x_2)$ und $p(x_1|x_2)$
für alle $x_2 \in \mathcal{X}_2$ bis auf den gemeinsamen Skalierungsfaktor $1/p(x_2)$ und andererseits $p(x_1,x_2)$ und $p(x_2|x_1)$
für alle $x_1 \in \mathcal{X}_1$ bis auf den gemeinsamen Skalierungsfaktor $1/p(x_1)$
identisch sind. Im Fall eines kontinuierlichen Zufallsvektors sind die analogen 
bedingten WDFen definiert wie folgt.

:::{#def-bedingte-wahrscheinlichkeitsdichtefunktion-und-kontinuierliche-bedingte-verteilung}
## Bedingte Wahrscheinlichkeitsdichtefunktion und kontinuierliche bedingte Verteilung

$\xi:= (\xi_1,\xi_2)$ sei ein kontinuierlicher Zufallsvektor mit Ergebnisraum 
$\mathbb{R}^2$, WDF $p(x_1,x_2)$ und marginalen WDFen $p(x_1)$ und $p(x_2)$.
Die bedingte WDF von $\xi_1$ gegeben $\xi_2 = x_2$ ist dann für $p(x_2) > 0$ definiert als
\begin{equation}
p : \mathbb{R} \to \mathbb{R}_{\ge 0},
x_1 \mapsto p(x_1|x_2) := \frac{p(x_1,x_2)}{p(x_2)}
\end{equation}
Analog ist für $p(x_1) > 0$ die bedingte WMF von $\xi_2$ gegeben $\xi_1 = x_1$ definiert als
\begin{equation}
p : \mathbb{R} \to \mathbb{R}_{\ge 0},
x_2 \mapsto p(x_2|x_1) := \frac{p(x_1,x_2)}{p(x_1)}
\end{equation}

Die Verteilungen mit WDFen $p$ und $p$ heißen dann
die *kontinuierlichen bedingten Verteilungen von $\xi_1$ gegeben $\xi_2 = x_2$ 
und $\xi_2$ gegeben $\xi_1 = x_1$*, respektive.
:::

Man beachte, dass im kontinuierlichen Fall  zwar $\mathbb{P}(\xi = x) = 0$, aber 
nicht notwendig auch $p(x) = 0$ gilt. Die bedingten Verteilungen multivariater
Normalverteilungen diskutieren wir in einem eigenen Kapitel.

## Unabhängige Zufallsvariablen {#sec-unabhängige-zufallsvariablen}

Ähnlich wie die bedingte Wahrscheinlichkeiten von Ereignissen lässt sich auch
das Konzept der unabhängigen Ereignisse auf Zufallsvektoren übertragen. Wir 
definieren zunächst den Begriff der unabhängigen Zufallsvariablen.

:::{#def-unabhängige-zufallsvariablen}
## Unabhängige Zufallsvariablen
$(\Omega, \mathcal{A}, \mathbb{P})$ sei ein Wahrscheinlichkeitsraum und 
$\xi: = (\xi_1,\xi_2)$ ein zweidimensionaler Zufallsvektor. Die Zufallsvariablen 
$\xi_1,\xi_2$ mit Ergebnisräumen $\mathcal{X}_1, \mathcal{X}_2$ 
heißen *unabhängig*, wenn für alle $S_1 \subseteq \mathcal{X}_1$ und 
$S_2 \subseteq \mathcal{X}_2$ gilt, dass
\begin{equation}
\mathbb{P}(\xi_1 \in S_1, \xi_2 \in S_2) =
\mathbb{P}(\xi_1 \in S_1)\mathbb{P}(\xi_2 \in S_2).
\end{equation}
:::

@def-unabhängige-zufallsvariablen besagt, dass die Ereignisse $\{\xi_1 \in S_1\}$ 
und $\{\xi_2 \in S_2\}$ unabhängig sind. Es gilt also auch, dass 
\begin{equation}
\mathbb{P}(\{\xi_1 \in S_1\})|\{\xi_2 \in S_2\}) = \mathbb{P}(\{\xi_1 \in S_1\})
\end{equation}
und das Wissen um das Eintreten des Ereignisses $\{\xi_2 \in S_2\}$ verändert 
die Wahrscheinlichkeit des Ereignisses $\{\xi_1 \in S_1\}$ nicht. Das Faktorisierungsprinzip
zur Modellierung probabilistischer Unabhängigkeit überträgt sich auf WMFen und
WDFen von Zufallsvektoren. Dies ist die Aussagen folgenden Theorems. 

:::{#thm-unabhängigkeit-und-faktorisierung}
## Unabhängigkeit und Faktorisierung

(1) $\xi:= (\xi_1,\xi_2)$ sei ein diskreter Zufallsvektor mit Ergebnisraum
$\mathcal{X}_1 \times \mathcal{X}_2$, WMF $p(x_1,x_2)$ und marginalen WMFen 
$p(x_1), p(x_2)$. Dann gilt
\begin{multline}
\xi_1 \mbox{ und } \xi_2 \mbox{ sind unabhängige Zufallsvariablen} \Leftrightarrow \\
p(x_1,x_2) = p(x_1)p(x_2) \mbox{ für alle } (x_1,x_2) \in \mathcal{X}_1 \times \mathcal{X}_2.
\end{multline}
(2) $\xi:= (\xi_1,\xi_2)$ sei ein kontinuierlicher Zufallsvektor mit Ergebnisraum
$\mathbb{R}^2$, WDF $p$ und marginalen WDFen $p, p$. Dann gilt
\begin{multline}
\xi_1 \mbox{ und } \xi_2 \mbox{ sind unabhängige Zufallsvariablen } \Leftrightarrow \\
p(x_1,x_2) = p(x_1)p(x_2) \mbox{ für alle } (x_1,x_2) \in \mathbb{R}^2.
\end{multline}
:::
Generell ist die Unabhängigkeit zweier Zufallsvariablen also äquivalent zur 
Faktorisierung ihrer gemeinsamen WMF oder WDF. Für einen Beweis von 
@thm-unabhängigkeit-und-faktorisierung verweisen wir auf die weiterführende Literatur. 
@thm-unabhängigkeit-und-faktorisierung für weite Aspekte der probabilistischen 
Modellierung grundlegend.

**Beispiel**

Wir betrachten erneut den zweidimensionalen Zufallsvektor $\xi:= (\xi_1, \xi_2)$
aus @sec-definition-und-multivariate-verteilungen, dessen gemeinsame und
marginale WMFen die in @tbl-gemeinsame-wkt-rand spezifierten Formen haben
Wir fragen zunächst, ob $\xi_1$ und $\xi_2$ unabhängig sind. Dies ist nicht 
der Fall, da hier gilt, dass
\begin{equation}
p(1,1) = 0.10 \neq 0.08 = 0.40\cdot 0.20 =  p(1)p(1).
\end{equation}
Möchten wir basierend auf den Marginalverteilungen von $\xi$ eine gemeinsame
Verteilung erzeugen, in der $\xi_1$ und $\xi_2$ unabhängig sind, so muss sich
jeder Eintrag der gemeinsamen Verteilung $p(\xi_1,\xi_2)$ aus dem jeweiligen
Produkt der Marginalwahrscheinlichkeiten ergeben. Die gemeinsame Verteilung von 
$\xi_1$ und $\xi_2$ unter dieser Annahme der Unabhängigkeit von $\xi_1$ und $\xi_2$ 
bei gleichen Marginalverteilungen wie in @tbl-gemeinsame-wkt-rand ergibt sich
zu der in  @tbl-gemeinsame-wkt-rand-unabhaengig gemeinsamen WMF.

\footnotesize

| $p(x_1,x_2)$  | $x_2 = 1$  | $x_2 = 2$  | $x_2 = 3$  | $x_2 = 4$  | $p(x_1)$  |
|:-------------:|:----------:|:----------:|:----------:|:----------:|:---------:|
| $x_1 = 1$     | $0.08$     | $0.12$     | $0.12$     | $0.08$     | $0.40$    |
| $x_1 = 2$     | $0.06$     | $0.09$     | $0.09$     | $0.06$     | $0.30$    |
| $x_1 = 3$     | $0.06$     | $0.09$     | $0.09$     | $0.06$     | $0.30$    |
| $p(x_2)$      | $0.20$     | $0.30$     | $0.30$     | $0.20$     |           |

: Gemeinsame WMF bei Annahme der Unabhängigkeit von $\xi_1$ und $\xi_2$ {#tbl-gemeinsame-wkt-rand-unabhaengig}

\normalsize


Weiterhin ergeben sich im Falle der Unabhängigkeit von $\xi_1$ und $\xi_2$ beispielsweise
die bedingten WMFen $p(x_2|x_1)$ wie in @tbl-bedingte-wkt-unabhaengig dargestellt.
Im Falle der Unabhängigkeit von $\xi_1$ und $\xi_2$ ändert sich die Verteilung von 
$\xi_2$ gegeben (oder im Wissen um) den Wert von $\xi_1$ also nicht und entspricht 
jeweils der Marginalverteilung von $\xi_2$. Dies entspricht natürlich der Intuition 
der Unabhängigkeit von Ereignissen im Kontext elementarer Wahrscheinlichkeiten.


\footnotesize

| $p(x_2|x_1)$       | $x_2 = 1$                   | $x_2 = 2$                   | $x_2 = 3$                   | $x_2 = 4$                  |
|:------------------:|:---------------------------:|:---------------------------:|:---------------------------:|:--------------------------:|
| $p(x_2|x_1 = 1)$   | $\frac{0.08}{0.40} = 0.2$   | $\frac{0.12}{0.40} = 0.3$   | $\frac{0.12}{0.40} = 0.3$   | $\frac{0.08}{0.40} = 0.2$  |
| $p(x_2|x_1 = 2)$   | $\frac{0.06}{0.30} = 0.2$   | $\frac{0.09}{0.30} = 0.3$   | $\frac{0.09}{0.30} = 0.3$   | $\frac{0.06}{0.30} = 0.2$  |
| $p(x_2|x_1 = 3)$   | $\frac{0.06}{0.30} = 0.2$   | $\frac{0.09}{0.30} = 0.3$   | $\frac{0.09}{0.30} = 0.3$   | $\frac{0.06}{0.30} = 0.2$  |

: Bedingte WMFen bei Annahme der Unabhängigkeit von $\xi_1$ und $\xi_2$ {#tbl-bedingte-wkt-unabhaengig}

\normalsize

Wir wollen abschließend den Begriff der unabhängigen Zufallsvariablen für mehr 
als zwei Zufallsvariablen definieren.

:::{#def-n-unabhängige-zufallsvariablen}
## $n$ unabhängige Zufallsvariablen
$\xi:= (\xi_1,...,\xi_n)$ sei ein $n$-dimensionaler Zufallsvektor mit Ergebnisraum 
$\mathcal{X}  = \times_{i=1}^n \mathcal{X}_i$. Die $n$ Zufallsvariablen $\xi_1,...,\xi_n$ 
heißen *unabhängig*, wenn für alle $S_i \subseteq \mathcal{X}_i, i = 1,...,n$ gilt, dass
\begin{equation}
\mathbb{P}(\xi_1 \in S_1, ...,\xi_n \in S_n) = \prod_{i=1}^n \mathbb{P}(\xi_i \in S_i).
\end{equation}
Wenn der Zufallsvektor eine $n$-dimensionale WMF oder WDF $p$ mit marginalen
WMFen oder WDFen $p_{\xi_i}, i = 1,...,n$ besitzt, dann ist die Unabhängigkeit von 
$\xi_1,...,\xi_n$ gleichbedeutend mit der Faktorisierung der gemeinsamen WMF oder WDF, also mit
\begin{equation}
p(\xi_1,...,\xi_n) = \prod_{i=1}^n p(x_i).
\end{equation}
:::

Es handelt bei @def-n-unabhängige-zufallsvariablen also  um eine direkte 
Generalisierung des zweidimensionalen Falls. 

Sind $n$ Zufallsvariablen nicht nur unabhängig, sondern haben sie auch alle die
gleiche Verteilung, so nennt man sie *unabhängig und identisch verteilt (u.i.v)*.

:::{#def-unabhängig-und-identisch-verteilte-zufallsvariablen}
## Unabhängig und identisch verteilte Zufallsvariablen
$n$ Zufallsvariablen $\xi_1,...,\xi_n$ heißen *unabhängig und identisch verteilt (u.i.v.)*, wenn

(1) $\xi_1,...,\xi_n$ unabhängige Zufallsvariablen sind, und
(2) die Marginalverteilungen der $\xi_i$ übereinstimmen, also gilt, dass
\begin{equation}
\mathbb{P}(\xi_i) = \mathbb{P}(\xi_j) \mbox{ für alle } 1 \le i,j \le n.
\end{equation}

Wenn die Zufallsvariablen $\xi_1,...,\xi_n$ unabhängig und identisch verteilt sind
und die $i$te Marginalverteilung $\mathbb{P}$ ist, so 
schreibt man auch
\begin{equation}
\xi_1,...,\xi_n \sim \mathbb{P}.
\end{equation}
:::

Man sagt kurz, dass *$\xi_1,...,\xi_n$ u.i.v.* sind. Im Englischen spricht man 
von *independent and identically distributed (i.i.d)* Zufallsvariablen. U.i.v.
Zufallsvariablen spielen an vielen Stellen der probabilistischen Modellierung eine
wichtige Rolle. So werden, wie wir an späterer Stelle sehen werden, additive
Fehlerterme in probabilistischen Modellen meist durch u.i.v. Zufallsvariablen
modelliert. 

Schließlich halten wir fest, dass $n$ u.i.v. normalverteilte Zufallsvariablen als
\begin{equation}
\xi_1,...,\xi_n \sim N(\mu,\sigma^2)
\end{equation}
geschrieben werden. In einem eigenen Kapiteil zeigen wir, wie genau die gemeinsame 
Verteilung von $n$ u.i.v. normalverteilten Zufallsvariablen im Sinne eines 
Zufallsvektors beschaffen ist.

## Selbstkontrollfragen
\footnotesize
1. Geben Sie die Definition des Begriffs des Zufallsvektors wieder.
1. Geben Sie die Definition des Begriffs der multivariaten Verteilung eines Zufallsvektors wieder.
1. Geben Sie die Definition des Begriffs der multivariaten WMF wieder.
1. Geben Sie die Definition des Begriffs der multivariaten WDF wieder.
1. Geben Sie die Definition des Begriffs der univariaten Marginalverteilung eines Zufallsvektors wieder
1. Wie berechnet man die WMF der $i$ten Komponente eines diskreten Zufallsvektors?
1. Wie berechnet man die WDF der $i$ten Komponente eines kontinuierlichen Zufallsvektors?
1. Geben Sie die Definition des Begriffs der  Unabhängigkeit zweier Zufallsvariablen wieder.
1. Wie erkennt man an der gemeinsamen WMF oder WDF eines zweidimensionalen Zufallsvektors, ob die Komponenten des Zufallsvektors unabhängig sind oder nicht?
1. Geben Sie die Definition des Begriffs der Unabhängigkeit von $n$ Zufallsvariablen wieder.
1. Geben Sie die Definition des Begriffs der $n$ unabhängig und identisch verteilten Zufallsvariablen wieder.

\normalsize

