# T-Statistiken
\normalsize

In diesem Abschnitt führen wir T-Statistiken als Maße zur Evaluation von 
Betaparameterschätzern im ALM ein. T-Statistiken quantifizieren dabei die 
geschätzten Effekte des Betaparameterschätzers in Bezug zur durch den 
Varianzparameterschätzer geschätzten Residualvariabilität. Der Wert einer 
T-Statistik ist also zunächst einmal einfach als Signalzu-Rauschen Verhältnis 
(Signal-to-Noise Ratio) zu verstehen.

T-Statistiken erlauben weiterhin die Evaluation von Linearkombinationen der 
Komponenten des Betaparameterschätzers im Sinne Frequentistischer 
Konfidenzinteralle und Hypothesentests. Wir betrachten hier zunächst nur die 
funktionale Form von T-Statistiken und ihre Frequentistische Verteilung zum 
Zwecke der Konfidenzintervallbestimmung. Der Einsatz von T-Teststatistiken 
im Rahmen von Einstich- und Zweistichproben T-Tests ist das Thema von @sec-t-tests.

## Definition und Beispiele

Vor dem Hintergrund des ALMs und seiner Parameterpunktschätzer definieren wir 
die T-Statistik wie folgt.

:::{#def-t-statistik} 
## T-Statistik
Es sei
\begin{equation}
\upsilon = X \beta+\varepsilon \mbox{ mit } \varepsilon \sim N\left(0_{n}, \sigma^{2} I_{n}\right)
\end{equation}
das ALM. Weiterhin seien
\begin{equation}
\hat{\beta}:=\left(X^{T} X\right)^{-1}X^{T}\upsilon 
\mbox{ und } 
\hat{\sigma}^{2}:=\frac{(\upsilon-X\hat{\beta})^{T}(\upsilon-X\hat{\beta})}{n-p}
\end{equation}
die Betaparameter- und Varianzparameterschätzer, respektive. Dann ist für einen Kontrastgewichtsvektor $c \in \mathbb{R}^{p}$ und einen Parameter $\beta_{0} \in \mathbb{R}^{p}$ die T-Statistik definiert als
\begin{equation}
T:=\frac{c^{T} \hat{\beta}-c^{T} \beta_{0}}{\sqrt{\hat{\sigma}^{2} c^{T}\left(X^{T} X\right)^{-1} c}} .
\end{equation}
:::

Geeignete Wahlen des Kontrastgewichtvektors $c \in \mathbb{R}^{p}$ und des 
Parameters $\beta_{0} \in \mathbb{R}^{p}$ erlauben eine Vielzahl von Einsatzmöglichkeiten 
der T-Statistik. Betrachten wir zunächst den Kontrastgewichtsvektor. Offenbar 
dient der Konstrastgewichtsvektor dazu, den Zufallsvektor $\hat{\beta} \in \mathbb{R}^{p}$ 
in die Zufallsvariable $c^{T} \hat{\beta}$ zu transfomieren und sichert damit 
die Skalarität der T-Statistik. Weiterhin erlaubt die Wahl von $p$-dimensionalen 
Einheitsvektoren für den Kontrastgewichtsvektor die Auswahl einzelner Komponenten
des Betaparameters zur Evaluation mithilfe der T-Statistik. Schließlich erlaubt 
eine generelle Wahl des Kontrastgewichtsvektors die Evaluation beliebiger 
Linearkombination der Betaparameterkomponenten, wie zum Beispiel Differenzen 
einzelner Komponenten. Beispielhaft seien für $\hat{\beta} \in \mathbb{R}^{2}$ 
hier folgende Möglichkeiten für die Wahl von $c \in \mathbb{R}^{2}$ hinsichtlich 
des Skalarproduktes $c^{T} \hat{\beta}$ aufgeführt:
\begin{equation}
c:=
\begin{pmatrix}
1 \\
0
\end{pmatrix} 
\Rightarrow 
c^{T}\hat{\beta} = \hat{\beta}_{1}, 
\quad 
c := 
\begin{pmatrix}
0 \\
1
\end{pmatrix} 
\Rightarrow 
c^{T}\hat{\beta}
= 
\hat{\beta}_{2}, \quad 
c
:=\begin{pmatrix}
1 \\
-1
\end{pmatrix} 
\Rightarrow 
c^{T}\hat{\beta} = \hat{\beta}_{1} - \hat{\beta}_{2}.
\end{equation}
Die Wahl des Parameters $\beta_{0} \in \mathbb{R}^{p}$ eröffnet die Möglichkeit, 
die T-Statistik unterschiedlich einzusetzen. Wählt man zum Beispiel $\beta_{0}:=0_{p}$, 
so erhält man mit der T-Statistik eine Deskriptivstatistik, die es erlaubt, 
geschätzte Regressoreffekte, also Komponenten oder Linearkombinationen von 
$\hat{\beta}$, im Sinne eines Signal-zu-Rauschen Verhältnisses in Bezug zu der 
durch $\hat{\sigma}^{2}$ quantifizierten Residualdatenvariabilität zu setzen. 
Der Nenner der TStatistik stellt dabei sicher, dass insbesondere die adäquate
(Ko)Standardabweichung der entsprechenden Betaparameterkomponentenkombination 
als Bezugsgröße dient, da es sich bei $\hat{\sigma}^{2}\left(X^{T} X\right)^{-1}$ 
bekanntlich um die Kovarianz des Betaparameterschätzers handelt (vgl. @thm-frequentistische-verteilung-des-betaparameterschaetzers).

Wählt man für $\beta_{0}$ dagegen $\beta$, also den wahren, aber unbekannten, 
Betaparameterwert, so eröffnet die T-Statistik die Möglichkeit, für einzelne 
Komponenten des Betaparametervektors Konfidenzintervalle zu bestimmen. Wir 
vertiefen diesen Aspekt der T-Statistik in @sec-konfidenzintervalle-fuer-betaparameterkomponenten. 
Deklariert man schließlich $\beta_{0}$ im Kontext eines Testszenarios als das Element einer 
Nullhypothese, so eröffnet die T-Statistik die hypothesentestbasierte Inferenz 
über Betaparameterkomponenten und ihrer Linearkombinationen. Anwendungsfälle 
dieser Art diskutieren wir ausführlich in @sec-t-tests.

Die Anwendung der T-Statistik zum Zwecke der Frequentistischen Inferenz im Sinne 
von Konfidenzintervallen und Hypothesentests basiert dabei natürlich auf der 
Frequentistischen Verteilung der T-Statistik vor dem Hintergrund des ALMs. 
Diese ist der zentrale Inhalt folgenden Theorems, auf dessen Beweis wir verzichten.

:::{#thm-frequentistische-verteilung-der-t-statistik}  
## Frequentistische Verteilung der T-Statistik
Es sei
\begin{equation}
\upsilon = X \beta+\varepsilon \mbox{ mit } \varepsilon \sim N\left(0_{n}, \sigma^{2} I_{n}\right)
\end{equation}
das ALM. Weiterhin seien
\begin{equation}
\hat{\beta} := 
\left(X^{T} X\right)^{-1}X^{T}\upsilon 
\mbox{ und } 
\hat{\sigma}^{2}:=\frac{(\upsilon-X\hat{\beta})^{T}(\upsilon-X\hat{\beta})}{n-p}
\end{equation}
die Betaparameter- und Varianzparameterschätzer, respektive. Schließlich sei für 
einen Kontrastgewichtsvektor $c \in \mathbb{R}^{p}$ und einen Parameter 
$\beta_{0} \in \mathbb{R}^{p}$
\begin{equation}
T := \frac{c^{T}\hat{\beta} - c^{T}\beta_{0}}{\sqrt{\hat{\sigma}^{2} c^{T}\left(X^{T} X\right)^{-1} c}}
\end{equation}
die T-Statistik. Dann gilt
\begin{equation}
T \sim t(\delta, n-p) 
\mbox{ mit } 
\delta:=\frac{c^{T} \beta-c^{T} \beta_{0}}{\sqrt{\sigma^{2} c^{T}\left(X^{T} X\right)^{-1} c}} .
\end{equation}
:::

Im Allgemeinen ist die T-Statistik also nichtzentral t-verteilt. Gilt dabei, 
wie bei der Bestimmung von Konfidenzintervallen (vgl. @sec-konfidenzintervalle-fuer-betaparameterkomponenten) $\beta_{0}:=\beta$ oder gilt in einem Testszenario bei Zutreffen der Nullhypothese 
$\beta := \beta_{0}$ (vgl. @sec-t-tests), so ist die T-Statistik sogar $t$-verteilt, 
jeweils mit Freiheitsgradparameter $n-p$. Gilt in einem Testszenario dagegen, 
dass die Nullhypothese nicht zutrifft, so kann die Verteilung der T-Statistik aus 
@thm-frequentistische-verteilung-der-t-statistik zur Herleitung der Testgütefunktion 
und damit zur Bestimmung der Power des Tests genutzt werden (vgl. Kapitel @sec-t-tests). 
Wir werden diese Aspekte an gegebener Stelle vertiefen. An dieser Stelle wollen 
wir die T-Statistik und ihre Verteilung zunächst nur an den Beispielen der 
unabhängigen identisch normalverteilten Zufallsvariablen und der einfachen 
linearen Regression illustrieren.

### Beispiel (1) Unabhängige und identisch normalverteilte Zufallsvariablen {-}

Es sei
\begin{equation}
\upsilon \sim N\left(X \beta, \sigma^{2} I_{n}\right) 
\mbox{ mit } 
X := 1_{n} \in \mathbb{R}^{n \times 1}, \beta:=\mu \in \mathbb{R} \mbox{ und } \sigma^{2}>0 
\end{equation}
das ALM Szenario unabhängiger und identisch normalverteilter Zufallsvariablen und 
es seien $c:=1$ und $\beta_{0}:=\mu_{0} \in \mathbb{R}$. Dann gilt für die T-Statistik
\begin{equation}
T 
= \frac{c^{T} \hat{\beta}-c^{T} \mu_{0}}{\sqrt{\hat{\sigma}^{2} c^{T}\left(X^{T} X\right)^{-1} c}}
= \frac{1^{T} \bar{v}-1^{T} \mu_{0}}{\sqrt{s_{v}^{2} 1^{T}\left(1_{n}^{T} 1_{n}\right)^{-1} 1}}
= \sqrt{n}\left(\frac{\bar{v}-\mu_{0}}{s_{v}}\right)
\end{equation}
Dies entspricht offenbar der bekannten Einstichproben-T-Teststatistik (vgl. @sec-hypothesentests). 
Wie diese nimmt die hier betrachtete T-Statistik, bei Konstanz der jeweils komplementären 
Terme, große absolute Werte für eine große absolute Differenz von $\bar{v}-\mu_{0}$ 
(oft als Effekt bezeichnet), sowie für kleine Werte von $s_{v}^{2}$ (also eine geringe Datenvariabilität) 
und einen großen Wert von $n$ (also einen großen Stichprobenumfang) an. Folgender **R**
Code simuliert die Frequentistische Verteilung dieser T-Statistik für die Fälle
$\beta=\beta_{0}$ und $\beta \neq \beta_{0}$.

\tiny
```{r}
# Libraries
library(MASS)                                                                   # multivariate Normalverteilung

# Modellformulierung
n          = 12                                                                 # Anzahl von Datenpunkten
p          = 1                                                                  # Anzahl von Betaparametern
X          = matrix(c(rep(1,n)), nrow = n)                                      # Designmatrix
I_n        = diag(n)                                                            # Einheitsmatrix
beta       = c(0,1)                                                             # wahre , aber unbekannte , Betaparameter
nscn       = length(beta)                                                       # Anzahl wahrer, aber unbekannter, Hypothesenszenarien
sigsqr     = 1                                                                  # wahrer, aber unbekannter, Varianzparameter
c          = 1                                                                  # Kontrastvektor von Interessse
beta_0     = 0                                                                  # Nullhypothesenbetaparameter

# Frequentistische Simulation
nsim       = 1e4                                                                # Anzahl Simulationen
delta      = rep(NaN, nscn)                                                     # Anzahl Nichtzentralitätsparameter
Tee        = matrix(rep(NaN, nscn*nsim), ncol = nscn)                           # T-Teststatistik Realisierungsarray
for(s in 1:nscn){                                                               # Hypothesenszenarien
  delta[s]    = ((t(c) %*% beta[s] - t(c) %*% beta_0)/                          # Nichtzentralitätsparameter
                sqrt(sigsqr*t(c)%*%solve(t(X)%*%X)%*%c))
  for(i in 1:nsim){                                                             # Simulationsiterationen
    y          = mvrnorm(1, X %*% beta[s], sigsqr*I_n)                          # y
    beta_hat   = solve(t(X) %*% X) %*% t(X) %*% y                               # \hat{\beta}
    eps_hat    = y - X %*% beta_hat                                             # \hat{\eps}
    sigsqr_hat = (t(eps_hat) %*% eps_hat)/(n-p)                                 # \hat{\sigma}^2
    Tee[i,s]   = ((t(c) %*% beta_hat - t(c) %*% beta_0)/                        # T
                  sqrt(sigsqr_hat*t(c)%*%solve(t(X)%*%X)%*%c))
  }
}
```
\normalsize

```{r, eval = F, echo = F}
# Visualisierung
library(latex2exp)
pdf(
file        = "./_figures/405-t-statistik-uinv.pdf",
width       = 8,
height      = 4)
par(
family      = "sans",
mfcol       = c(1,2),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2.5,1,0),
xaxs        = "i",
yaxs        = "i",
xpd         = TRUE,
font.main   = 1,
cex         = 1,
cex.main    = 1.2)

# T-Teststatistik Ergebnisraum
xlims  = c(-5,12)
t_min  = xlims[1]
t_max  = xlims[2]
t_res  = 1e3
t      = seq(t_min, t_max, len = t_res)
lab    = c(TeX("$\\c^T beta = \\c^T beta_0$"), TeX("$\\c^T beta \\neq \\c^T beta_0$") )

# T-Teststatistiken
for(s in 1:nscn){
  p_t    = dt(t,n-p, delta[s])
  hist(
  Tee[,s],
  breaks = 50,
  col    = "gray90",
  prob   = TRUE,
  xlab   = TeX("$T$"),
  ylab   = "",
  xlim   = xlims,
  ylim   = c(0,.4),
  main   = lab[s])
  lines(
  t,
  p_t,
  type  = "l",
  lwd   = 2,
  col   = "darkorange")
  mtext(LETTERS[s], adj=0, line=2, cex = 1.5, at = -10) 
}
dev.off()
```


@fig-t-statistik-uinv A und B zeigen die resultierenden simulierten und 
analytischen Verteilungen der T-Statistik.

![Verteilungen der T-Statistik bei unabhängig und identisch normalverteilten Zufallsvariablen.](./_figures/405-t-statistik-uinv){#fig-t-statistik-uinv fig-align="center" width=100%}

### Beispiel (2) Einfache lineare Regression {-}

In diesem Beispiel wollen wir nicht auf die spezifische Form der T-Statistik eingehen, 
aber anhand einer Simulation demonstrieren, wie sich das Prinzip der T-Statistik im Kontext
der einfachen linearen Regression darstellt. Dazu betrachten wir das bekannte 
Beispielmodell der einfachen linearen Regression (vgl. @sec-modellformulierung), 
in diesem Fall mit den wahren, aber unbekannten, Parameterwerten $\beta_{A}:=(1,0)$ 
und $\beta_{B}:=(1,1)$. Weiterhin betrachten wir den Kontrastgewichtsvektor $c:=(0,1)$, 
so dass die T-Statistik zu Evaluation des Steigungsparameters der einfachen linearen 
Regression genutzt werden kann. Schließlich betrachten wir in beiden Fällen den 
Parameter $\beta_{0}:=(0,0)^{T}$, so dass im Fall von $\beta_{A}$ gilt, dass 
$c^{T} \beta=c^{T} \beta_{0}$ und im Fall von $\beta_{B}$ gilt, dass $c^{T} \beta \neq c^{T} \beta_{0}$. 
Folgender **R** Code implementiert die skizzierten Szenarien, @fig-t-statistik-elr A 
und B zeigen die resultierenden simulierten und analytischen Verteilungen der T-Statistik.

\tiny
```{r}
# Modellformulierung
library(MASS)                                                                   # multivariate Normalverteilung
n          = 10                                                                 # Anzahl von Datenpunkten
p          = 2                                                                  # Anzahl von Betaparametern
x          = 1:n                                                                # Prädiktorwerte
X          = matrix(c(rep(1,n),x), ncol = p)                                    # Designmatrix
I_n        = diag(n)                                                            # Einheitsmatrix
beta       = matrix(c(1,0,1,1), nrow = 2)                                       # wahre, aber unbekannte, Betaparameter
nscn       = ncol(beta)                                                         # Anzahl wahrer, aber unbekannter, Hypothesenszenarien
sigsqr     = 1                                                                  # wahrer, aber unbekannter, Varianzparameter
c          = matrix(c(0,1), nrow = 2)                                           # Kontrastvektor von Interessse
beta_0     = matrix(c(0,0), nrow = 2)                                           # Nullhypothesenbetaparameter

# Frequentistische Simulation
nsim       = 1e4                                                                # Anzahl Simulationen
delta      = rep(NaN, nscn)                                                     # Anzahl Nichtzentralitätsparameter
Tee        = matrix(rep(NaN, nscn*nsim), ncol = nscn)                           # T-Teststatistik Realisierungsarray
for(s in 1:nscn){                                                               # Hypothesenszenarien
  delta[s]    = ((t(c) %*% beta[,s] - t(c) %*% beta_0)/                         # Nichtzentralitätsparameter
                sqrt(sigsqr*t(c)%*%solve(t(X)%*%X)%*%c))
  for(i in 1:nsim){                                                             # Simulationsiterationen
    y          = mvrnorm(1, X %*% beta[,s], sigsqr*I_n)                         # y
    beta_hat   = solve(t(X) %*% X) %*% t(X) %*% y                               # \hat{\beta}
    eps_hat    = y - X %*% beta_hat                                             # \hat{\eps}
    sigsqr_hat = (t(eps_hat) %*% eps_hat)/(n-p)                                 # \hat{\sigma}^2
    Tee[i,s]   = ((t(c) %*% beta_hat - t(c) %*% beta_0)/                        # T
                  sqrt(sigsqr_hat*t(c)%*%solve(t(X)%*%X)%*%c))
  }
}
```
\normalsize

```{r, eval = F, echo = F}
# Visualisierung
library(latex2exp)
pdf(
file        = "./_figures/405-t-statistik-elr.pdf",
width       = 8,
height      = 4)
par(
family      = "sans",
mfcol       = c(1,2),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2.5,1,0),
xaxs        = "i",
yaxs        = "i",
xpd         = TRUE,
font.main   = 1,
cex         = 1,
cex.main    = 1.2)

# T-Teststatistik Ergebnisraum
xlims  = c(-5,20)
t_min  = xlims[1]
t_max  = xlims[2]
t_res  = 1e3
t      = seq(t_min, t_max, len = t_res)
lab    = c(TeX("$\\c^T beta = \\c^T beta_0$"), TeX("$\\c^T beta \\neq \\c^T beta_0$") )

# T-Teststatistiken
for(s in 1:nscn){
  p_t    = dt(t,n-p, delta[s])
  hist(
  Tee[,s],
  breaks = 50,
  col    = "gray90",
  prob   = TRUE,
  xlab   = TeX("$T$"),
  ylab   = "",
  xlim   = xlims,
  ylim   = c(0,.4),
  main   = lab[s])
  lines(
  t,
  p_t,
  type  = "l",
  lwd   = 2,
  col   = "darkorange")
  mtext(LETTERS[s], adj=0, line=2, cex = 1.5, at = -12)
}
dev.off()
```

![Verteilungen der T-Statistik bei einfacher linearer Regression.](./_figures/405-t-statistik-elr){#fig-t-statistik-elr fig-align="center" width=100%}

## Konfidenzintervalle für Betaparameterkomponenten {#sec-konfidenzintervalle-fuer-betaparameterkomponenten}

Mithilfe der T-Statistik können Konfidenzintervalle für die Komponenten des 
Betaparametervektors bestimmt werden. Das folgende Theorem ist die zentrale Aussage dieses Abschnitts.

:::{#thm-konfidenzintervalle-fuer-betaparameterkomponenten}
## Konfidenzintervalle für Betaparameterkomponenten 
Es sei
\begin{equation}
\upsilon = X \beta+\varepsilon \mbox{ mit } \varepsilon \sim N\left(0_{n}, \sigma^{2} I_{n}\right)
\end{equation}
das ALM,
\begin{equation}
\hat{\beta}:=\left(X^{T} X\right)^{-1}X^{T}\upsilon 
\mbox{ und } 
\hat{\sigma}^{2}:=\frac{(\upsilon-X\hat{\beta})^{T}(\upsilon-X\hat{\beta})}{n-p}
\end{equation}
seien die Betaparameter- und Varianzparameterschätzer, respektive und für ein 
$\delta \in] 0,1[$ sei
\begin{equation}
t_{\delta}:=\Psi^{-1}\left(\frac{1+\delta}{2}; n-p\right).
\end{equation}
Schließlich sei für $j=1, \ldots, p$
\begin{equation}
\lambda_{j}:=\left(\left(X^{T} X\right)^{-1}\right)_{j j} 
\mbox{ das $j$te Diagonalelement von }
\left(X^{T} X\right)^{-1}
\end{equation}
Dann ist für $j=1, \ldots, p$
\begin{equation}
\kappa_{j} := 
\left[
\hat{\beta}_{j}-\hat{\sigma} \sqrt{\lambda_{j}} t_{\delta}, 
\hat{\beta}_{j}+\hat{\sigma} \sqrt{\lambda_{j}} t_{\delta}
\right]
\end{equation}
ein $\delta$-Konfidenzintervall für die $j$te Komponente $\beta_{j}$ des 
Betaparameters $\beta=\left(\beta_{1}, \ldots, \beta_{p}\right)^{T}$.
:::

:::{.proof}
Wir müssen zeigen, dass
\begin{equation}
\mathbb{P}\left(\kappa_{j} \ni \beta_{j}\right) = \delta.
\end{equation}
Dazu halten wir zunächst fest, dass für alle $j=1, \ldots, p$ bei Wahl von 
$\beta_{0}=\beta$ und $c:=e_{j}$ nach @thm-frequentistische-verteilung-der-t-statistik 
für $T \sim t(\delta, n-p)$ gilt, dass
\begin{equation}
T 
= \frac{e_{j}^{T} \hat{\beta}-e_{j}^{T} \beta}{\sqrt{\hat{\sigma}^{2} e_{j}^{T}\left(X^{T} X\right)^{-1} e_{j}}}
= \frac{\hat{\beta}_{j}-\beta_{j}}{\sqrt{\hat{\sigma}^{2}\left(\left(X^{T} X\right)^{-1}\right)_{j j}}} 
= \frac{\hat{\beta}_{j}-\beta_{j}}{\hat{\sigma}\sqrt{\lambda_{j}}} 
=: T_{j}
\end{equation}
und
\begin{equation}
\delta 
= \frac{e_{j}^{T} \beta-e_{j}^{T} \beta}{\sqrt{\hat{\sigma}^{2} e_{j}^{T}\left(X^{T} X\right)^{-1} e_{j}}}
= 0
\end{equation}
Damit gilt dann auch sofort, dass $T_{j} \sim t(n-p)$. Weiterhin erinnern erinnern 
wir daran (vgl. @sec-konfidenzintervalle), dass per Definition von $t_{\delta}$ gilt, 
dass
\begin{equation}
\mathbb{P}\left(-t_{\delta} \leq T_{j} \leq t_{\delta}\right).
\end{equation}
Aus der Definition eines $\delta$-Konfidenzintervalls folgt dann
\begin{equation}
\begin{aligned}
\delta 
& = \mathbb{P}\left(-t_{\delta} \leq T_{j} \leq t_{\delta}\right) \\
& = \mathbb{P}\left(-t_{\delta} \leq \frac{\hat{\beta}_{j}-\beta_{j}}{\hat{\sigma} \sqrt{\lambda_{j}}} \leq t_{\delta}\right) \\
& = \mathbb{P}\left(-t_{\delta} \hat{\sigma} \sqrt{\lambda_{j}} \leq \hat{\beta}_{j}-\beta_{j} \leq t_{\delta} \hat{\sigma} \sqrt{\lambda_{j}}\right) \\
& = \mathbb{P}\left(-\hat{\beta}_{j}-t_{\delta} \hat{\sigma} \sqrt{\lambda_{j}} \leq-\beta_{j} \leq-\hat{\beta}_{j}+t_{\delta} \hat{\sigma} \sqrt{\lambda_{j}}\right) \\
& =\mathbb{P}\left(\hat{\beta}_{j}+t_{\delta} \hat{\sigma} \sqrt{\lambda_{j}} \geq \beta_{j} \geq \hat{\beta}_{j}-t_{\delta} \hat{\sigma} \sqrt{\lambda_{j}}\right) \\
& =\mathbb{P}\left(\hat{\beta}_{j}-t_{\delta} \hat{\sigma} \sqrt{\lambda_{j}} \leq \beta_{j} \leq \hat{\beta}_{j}+t_{\delta} \hat{\sigma} \sqrt{\lambda_{j}}\right) \\
& =\mathbb{P}\left(\left[\hat{\beta}_{j}-\hat{\sigma} \sqrt{\lambda_{j}} t_{\delta}, \hat{\beta}_{j}+\hat{\sigma} \sqrt{\lambda_{j}} t_{\delta}\right]\right) \\
& =\mathbb{P}\left(\kappa_{j} \ni \beta_{j}\right)
\end{aligned}
\end{equation}
und damit ist alles gezeigt.
:::

### Beispiel (1) Unabhängige und identisch normalverteilte Zufallsvariablen {-}

Wie gewohnt betrachten wir als erstes Beispiel die ALM Form des Szenarios unabhängig 
und identisch normalverteilter Zufallsvariablen
\begin{equation}
\upsilon \sim N\left(X \beta, \sigma^{2} I_{n}\right) \mbox{ mit } 
X := 1_{n} \in \mathbb{R}^{n}, \beta:=\mu \in \mathbb{R}, \sigma^{2}>0
\end{equation}
Dann gelten, wie bereits gesehen
\begin{equation}
\hat{\beta} = \frac{1}{n} \sum_{i=1}^{n} v_{i} =: \bar{v}, 
\hat{\sigma}^{2}=\frac{1}{n-1} \sum_{i=1}^{n}\left(v_{i}-\bar{v}\right)^{2}=: s_{v}^{2} 
\mbox{ und } \lambda_{1}=\left(1_{n}^{T} 1_{n}\right)^{-1}=\frac{1}{n}.
\end{equation}
Nach @thm-konfidenzintervalle-fuer-betaparameterkomponenten gilt dann, dass
\begin{equation}
\kappa:=\left[\bar{v}-\frac{s}{\sqrt{n}} t_{\delta}, \bar{v}+\frac{s}{\sqrt{n}} t_{\delta}\right]
\end{equation}
ein $\delta$-Konfidenzintervall für $\beta$ ist und dieses ist offenbar identisch 
mit dem bekannten $\delta$-Konfidenzintervall für den Erwartungsparameter der Normalverteilung.

### Beispiel (2) Einfache lineare Regression {-}

In diesem Beispiel wollen wir nicht auf die spezifische Form der Konfidenzintervalle 
für den Offset- und Steigungsparameter eingehen, sondern lediglich anhand folgender 
Simulation an die Frequentistische Bedeutung eines $\delta$-Konfidenzintervalls erinnern: 
Realisierungen von $\delta$-Konfidenzintervallen überdecken den wahren, aber 
unbekannten, Parameterwert mit einer frequentistischen Wahrscheinlichkeit von 
$\delta$. @fig-elr-konfidenzintervalle zeigt, basierend auf folgendem **R** Code, dass dies 
in der konkreten Simulation mit $\delta=0.95$ für den Offsetparameter in 94 
von 100 und für den Steigungsparameter in 93 von 100 Fällen der Fall ist.

\tiny
```{r}
# Modellformulierung
library(MASS)                                                                   # multivariate Normalverteilung
set.seed(0)                                                                     # Random number generator seed
ns         = 1e2                                                                # Anzahl Simulationen
n          = 10                                                                 # Anzahl von Datenpunkten
p          = 2                                                                  # Anzahl von Betaparametern
x          = 1:n                                                                # Prädiktorwerte
X          = matrix(c(rep(1,n),x), ncol = p)                                    # Designmatrix
I_n        = diag(n)                                                            # Einheitsmatrix
beta       = matrix(c(1,2), nrow = 2)                                           # wahre, aber unbekannte, Betaparameter
sigsqr     = 1                                                                  # wahrer, aber unbekannter, Varianzparameter
delta      = 0.95                                                               # Konfidenzbedingung  
t_delta    = qt((1+delta)/2,n-1)                                                # \Psi^{-1}((1+\delta)/2,n-1)
lambda     = diag(solve(t(X) %*% X))                                            # \lambda_j Werte

# Simulation
kappa      = array(rep(NaN, ns*p*p), dim=c(ns,2,2))                             # Konfidenzintervallarray  
beta_hat   = matrix(rep(NaN,p*ns), nrow = p)                                    # Betaparameterschätzer
for(i in 1:ns){                                                                 # Iteration über Realisierungen
  y              = mvrnorm(1, X %*% beta, sigsqr*I_n)                           # Datenrealisierung
  beta_hat[,i]   = solve(t(X) %*% X) %*% t(X) %*% y                             # \hat{\beta}
  eps_hat        = y - X %*% beta_hat[,i]                                       # \hat{\varepsilon}
  sigsqr_hat     = (t(eps_hat) %*% eps_hat)/(n-p)                               # \hat{\sigma}^2
  for(j in 1:p){                                                                # Iteration über Betaarraykomponenten
    kappa[i,1,j] = beta_hat[j,i]-sqrt(sigsqr_hat*lambda[j])*t_delta             # untere KI Grenze
    kappa[i,2,j] = beta_hat[j,i]+sqrt(sigsqr_hat*lambda[j])*t_delta             # obere  KI Grenze
  }
}
```
\normalsize

```{r, eval = F, echo = F}
library(latex2exp)
pdf(
file        = "./_figures/405-elr-konfidenzintervalle.pdf",
width       = 9,
height      = 7)
labels      = c(TeX("Offsetparameter $\\beta_1 = 1,\\,\\, \\sigma^2 = 1, \\, n = 10,\\, \\delta = 0.95"),
                TeX("Slopeparameter  $\\beta_2 = 2,\\,\\, \\sigma^2 = 1, \\, n = 10,\\, \\delta = 0.95"))
ylimits     = list(c(-3,5), c(1,3.5))
mp          = c(4,3)
par(
family      = "sans",
mfcol       = c(2,1),
pty         = "m",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(3,2,0),
xaxs        = "i",
yaxs        = "i",
xpd         = TRUE,
font.main   = 1,
cex         = 1,
cex.main    = 1)
for(j in 1:p){
  P_idx      = rep(NaN,ns)                                                       # Nicht überdeckende KIs für Visualisierung
  P_idx[beta[j] < kappa[,1,j] | beta[j] > kappa[,2,j]]  = mp[j]                     # Markerpositions
  plot(
  1:ns,
  beta_hat[j,],
  type    = "p",
  ylim    = ylimits[[j]],
  xlim    = c(0,102),
  xlab    = "Simulationen",
  ylab    = "",
  pch     =  19,
  cex     =  .5,
  main    = labels[j])
  arrows(
  x0      = 1:ns,
  y0      = kappa[,1,j],
  x1      = 1:ns,
  y1      = kappa[,2,j],
  code    = 3,
  angle   = 90,
  length  = 0.01,
  lwd     = .7)
  lines(
  1:ns,
  rep(beta[j],ns),
  col      = "gray80",
  lty      = 1)
  lines(
  1:ns,
  P_idx,
  type    = "p",
  pch     = 13,
  col     = "darkorange")
  mtext(LETTERS[j], adj=0, line=2, cex = 1.5, at = -10)
}
dev.off()
```
![Konfidenzintervalle für Betaparameterkomponenten bei einfacher linearer Regression.](./_figures/405-elr-konfidenzintervalle){#fig-elr-konfidenzintervalle fig-align="center" width=100%}


## Literaturhinweise
@box1981 und @zabell2008 geben einen historischen Überblick zur Entwicklung der 
T-Statistik und ihrer Verteilung im Kontext der Arbeiten von @student1908 und 
@fisher1925, @fisher1925a und @fisher1925b. Die Theorie der Konfidenzintervalle 
geht auf @neyman1935 und @neyman1937 zurück.

## Selbstkontrollfragen
\footnotesize
1. Geben Sie die Definition der T-Statistik wieder.
1. Erläutern Sie für die T-Statistik die Bedeutung der Wahl von $c \in \mathbb{R}^{p}$.
1. Erläutern Sie für die T-Statistik die Bedeutung der Wahl von $\beta_{0} \in \mathbb{R}^{p}$.
1. Wann kann die T-Statistik als Signal-zu-Rauschen Verhältnis interpretiert werden?
1. Geben Sie das Theorem zur T-Statistik wieder.
1. Geben Sie die Form der T-Statistik im Szenario von $n$ u.i. normalverteilten Zufallsvariablen wieder.
1. Geben Sie das Theorem zu Konfidenzintervallen für Betaparameterkomponenten wieder.
\normalsize


