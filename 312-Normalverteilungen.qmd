# Multivariate Normalverteilungen {#sec-multivariate-normalverteilungen}
\normalsize

Die multivariate Normalverteilung ist die multivariate Generalisierung der 
univariaten Normalverteilung. Die Motivation für die verbreiteten 
Normalverteilungsannahmen in der probabilistischen Modellierung liegt bekanntermaßen
im Zentralen Grenzwertsatz: In probabilistischen Modellen repräsentieren probabilistische 
Terme die Summation sehr vieler Zufallsvorgänge, die durch die deterministischen Bestandteile 
des jeweiligen Modells, also eine formalisierte wissenschaftliche Theorie, nicht 
erklärt werden. Nach dem Zentralen Grenzwertsatz ist die Summe dieser nicht erklärten
Zufallsvorgänge dann aber gerade normalverteilt. 

Über diesen grundlegenden Aspekt hinaus hat die Normalverteilung viele günstige 
mathematische Eigenschaften, die ihren Einsatz in vielen Bereichen der probabilistischen 
Modellierung ermöglichen. Anwendungen multivariater Normalverteilungen finden 
sich damit im Kontext des Allgemeinen Linearen Modells, den Generalisierungen 
des Allgemeinen Modells zu Hierarchischen Linearen Modellen oder Multivariaten 
Allgemeinen Linearen Modellen, der Bayesianischen Inferenz bei Normalverteilungsannahmen
und nicht zuletzt der Theorie probabilistischer Filter, wie zum Beispiel 
dem Kalman-Bucy Filter. 

## Konstruktion

In diesem Abschnitt wollen wir zeigen, wie ein bivariat normalverteilter Zufallsvektor 
durch Transformation und Konkatenation zweier univariat normalverteilter 
Zufallsvariablen konstruiert werden kann. Dazu erinnern wir zunächst an den 
Begriff der normalverteilten Zufallsvariable

:::{#def-normalverteilte-zufallsvariable}
## Normalverteilte Zufallsvariable
$\xi$ sei eine Zufallsvariable mit Ergebnisraum $\mathbb{R}$ und WDF
\begin{equation}
p : \mathbb{R} \to \mathbb{R}_{>0}, x\mapsto p(x)
:= \frac{1}{\sqrt{2\pi \sigma^2}}\exp\left(-\frac{1}{2\sigma^2}(x - \mu)^2\right).
\end{equation}
Dann sagen wir, dass $\xi$ einer *Normalverteilung (oder  Gauß-Verteilung)* mit 
*Erwartungswertparameter $\mu \in \mathbb{R}$* und *Varianzparameter $\sigma^2 > 0$* 
unterliegt und nennen $\xi$ eine *normalverteilte Zufallsvariable*. Wir kürzen dies 
mit $\xi \sim N(\mu,\sigma^2)$ ab. Die WDF einer normalverteilten Zufallsvariable 
bezeichnen wir mit
\begin{equation}
N\left(x;\mu,\sigma^2\right) := \frac{1}{\sqrt{2\pi \sigma^2}}\exp\left(-\frac{1}{2\sigma^2}(x - \mu)^2\right).
\end{equation}
:::

```{r, echo = F, eval = F}
library(latex2exp)
pdf(
file        = file.path("./_figures/210-univariate-normal-wdf.pdf"),
width       = 9,
height      = 3)
par(                                                                    
mfcol      = c(1,3),                                           
family     = "sans",                                                   
pty        = "s",                                                       
bty        = "l",                                                       
lwd        = 1,                                                         
las        = 1,                                                         
mgp        = c(2,1,0),                                                  
font.main  = 1,                                                         
cex.main   = 1.4)                                                        
 
# model formulation
x_min   = -10                                                                    # minimum x-value
x_max   = 10                                                                     # maximum x-value
x_res   = 1e3                                                                    # x space resolution
x       = seq(x_min,x_max,len = x_res)                                           # x space
mu      = c(0,-2.5,3)                                                            # expectation parameters
sigsqr  = c(1,10,0.5)                                                            # variance parameters

# parameter iterations
for (i in 1:length(mu)){

  # Gaussian PDFs
  plot(
       x,                                                                        # x values
       dnorm(x,mu[i],sqrt(sigsqr[i])),                                           # y values
       type         = "l",                                                       # line style
       lwd          =  1.5,                                                      # line width
       col          = "Black",                                                   # line color
       ylab         = " ",                                                       # no y-axis label
       xlab         = "x",                                                       # x-axis label
       ylim         = c(0,0.6))                                                  # y-axis limits
  title(sprintf("N(x; %g,%g)", mu[i], sigsqr[i]))                                # plot title, print formatted data to string
}
dev.off()
```

![Wahrscheinlichkeitsdichtefunktionen univariater Normalverteilungen.](./_figures/210-univariate-normal-wdf){#fig-univariate-normal-wdf fig-align="center" width=100%} 

Visuell entspricht der Parameter $\mu$ einer normalverteilten Zufallsvariable 
dem Wert höchster Wahrscheinlichkeitsdichte und der Parameter $\sigma^2$ spezifiziert 
die Breite der WDF (@fig-univariate-normal-wdf). Weiterhin gelten für den 
Erwartungswert und die Varianz einer normalverteilten Zufallsvariable bekanntlich
\begin{equation}
\mathbb{E}(\xi) = \mu \mbox{ und } \mathbb{V}(\xi) = \sigma^2.
\end{equation}
Eine normalverteilte Zufallsvariable der Form $\xi \sim N(0,1)$ schließlich 
heißt auch *standardnormalverteilt*.

Folgendes Theorem zeigt, wie zwei unabhängige, univariat standardnormalverteilte Zufallsvariablen
kombiniert werden können, um einen bivariat verteilten Zufallsvektor zu konstruieren.
Die Verteilung eines ebensolchen Zufallsvektors wird dann als *bivariate Normalverteilung*
bezeichnet. 
    
:::{#thm-konstruktion-bivariater-normalverteilungen}
## Konstruktion bivariater Normalverteilungen
$\zeta_1 \sim N(0,1)$ und $\zeta_2 \sim N(0,1)$ seien zwei unabhängige
standardnormalverteilte  Zufallsvariablen. Weiterhin seien $\mu_1,\mu_2\in \mathbb{R}$,
$\sigma_1,\sigma_2>0$ und $\rho \in ]-1,1[$. Schließlich seien
\begin{align}
\begin{split}
\xi_1 & := \sigma_1\zeta_1 + \mu_1                                            \\
\xi_2 & := \sigma_2\left(\rho\zeta_1 + (1 -\rho^2)^{1/2}\zeta_2\right) + \mu_2.
\end{split}
\end{align}
Dann hat die WDF des Zufallsvektors $\xi := (\xi_1,\xi_2)^T$, also der gemeinsamen
Verteilung von $\xi_1$ und $\xi_2$, die Form
\begin{equation}
p : \mathbb{R}^2 \to \mathbb{R}_{>0},\, x \mapsto p(x) := (2\pi)^{-\frac{n}{2}}|\Sigma|^{-\frac{1}{2}}\exp\left(-\frac{1}{2}(x-\mu)^T \Sigma^{-1} (x-\mu)\right),
\end{equation}
wobei $n:=2$ und $\mu \in \mathbb{R}^{2}$ und  $\Sigma \in \mathbb{R}^{2\times 2}$ durch
\begin{equation}
\mu = 
\begin{pmatrix}
\mu_1 \\
\mu_2
\end{pmatrix}
\mbox{ und }
\Sigma =
\begin{pmatrix}
\sigma_1^2           & \rho\sigma_1\sigma_2 \\
\rho\sigma_2\sigma_1 & \sigma_2^2           \\
\end{pmatrix}
\end{equation}
gegeben sind.
:::
Für einen Beweis des Theorems verweisen wir auf @degroot2012.

**Beispiel**

Folgender **R** Code zeichnet das obige Theorem anhand konkreter Beispielwerte
für $\mu_1,\mu_2,\sigma_1,\sigma_2$ und $\rho$ nach und gibt die Parameter
$\mu$ und $\Sigma$ der resultierenden bivariaten WDF aus.

\tiny
```{r}
# Parameterdefinitionen
mu_1   = 5.0                                              # \mu_1
mu_2   = 4.0                                              # \mu_2
sig_1  = 1.5                                              # \sigma_1
sig_2  = 1.0                                              # \sigma_2
rho    = 0.9                                              # \rho

# Realisierungen der standardnormalverteilten ZVen
n      = 100                                              # Anzahl Realisierungen
zeta_1 = rnorm(n)                                         # \zeta_1 \sim N(0,1)
zeta_2 = rnorm(n)                                         # \zeta_1 \sim N(0,1)

# Evaluation von Realisierungen von \xi_1 und \xi_2
xi_1   = sig_1*zeta_1 + mu_1                              # Realsierungen von zeta_1
xi_2   = sig_2*(rho*zeta_1 + sqrt(1-rho^2)*zeta_2) + mu_2 # Realsierungen von zeta_2

# Parameter der gemeinsamen Verteilung von \xi_1 und \xi_2
mu     = matrix(c(mu_1,                                   # \mu \in \mathbb{R}^2
                  mu_2),
                 nrow = 2, byrow = TRUE)
Sigma  = matrix(c(sig_1^2        , rho*sig_1*sig_2,       # \Sigma \in \mathbb{R}^{2 x 2}
                 rho*sig_1*sig_2, sig_2^2),
                 nrow = 2, byrow = TRUE)
print(mu)
print(Sigma)
```
\normalsize

Die durch obigen **R** Code generierten Realisierungen von $\xi = (\xi_1,\xi_2)^T$,
sowie die Isokonturen der durch @thm-konstruktion-bivariater-normalverteilungen
postulierten WDF sind in @fig-konstruktion dargestellt.

```{r echo = F, eval = F}
library(latex2exp)
library(mvtnorm)
pdf(
file        = file.path("./_figures/210-konstruktion.pdf"),
width       = 3.5,
height      = 3.5)
par(
family      = "sans",
mfcol       = c(1,1),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2,1,0),
xaxs        = "i",
yaxs        = "i",
font.main   = 1,
cex         = .8,
cex.main    = 1)

# Visualisierung
x_min       = 0
x_max       = 10
x_res       = 1e3
x_1         = seq(x_min, x_max, length.out = x_res)
x_2         = seq(x_min, x_max, length.out = x_res)
X           = expand.grid(x_1,x_2)
p           = matrix(dmvnorm(as.matrix(X), mu, Sigma), nrow = x_res)
contour(
x_1,
x_2,
p,
xlim        = c(x_min,x_max),
ylim        = c(x_min,x_max),
xlab        = TeX("$x_1$"),
ylab        = TeX("$x_2$"),
nlevels     = 5)
points(
xi_1,
xi_2,
pch         = 21,
col         = "white",
bg          = "gray60",
cex         = .9)
dev.off()
```
![Konstruktion bivariater Normalverteilungen.](./_figures/210-konstruktion){#fig-konstruktion fig-align="center" width=50%}


```{r echo = F, eval = F}
library(latex2exp)
library(mvtnorm)
pdf(
file        = file.path("./_figures/210-bivariate-normal-wdf.pdf"),
width       = 9,
height      = 3)
par(
family      = "sans",
mfcol       = c(1,3),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2,1,0),
xaxs        = "i",
yaxs        = "i",
font.main   = 1)

# Ergebnisraumdefintion
x_min  = 0                                           # x_i Minimum
x_max  = 2                                           # x_i Maxim
x_res  = 1e3                                         # x_i Auflösung
x_1    = seq(x_min, x_max, length.out = x_res)       # x_1 Raum
x_2    = seq(x_min, x_max, length.out = x_res)       # x_2 Raum
X      = expand.grid(x_1,x_2)                        # X = (x_1,x_2)^T Raum

# Parameterdefinition
mu     = c(1,1)                                      # \mu \in \mathbb{R}^2
S      = list(matrix(c(0.2,  0.15,  0.15, 0.2), 2),  # \Sigma in \mathbb{R}^{2 \times 2}
              matrix(c(0.2,  0.00,  0.00, 0.2), 2),  # \Sigma in \mathbb{R}^{2 \times 2}
              matrix(c(0.2, -0.15, -0.15, 0.2), 2))  # \Sigma in \mathbb{R}^{2 \times 2}

# Kovarianzparametervariantenschleife
i = 1
for (Sigma in S){

  # Wahrscheinlichkeitsdichtefunktionauswertung
  p      = matrix(                                   # Matrixkonversion des von
                  dmvnorm(as.matrix(X), mu, Sigma),  # dmvnorm() ausgegebenen Vektors
                  nrow = x_res)

  # Visualisierung
  contour(
  x_1,
  x_2,
  p,
  xlim  = c(x_min,x_max),
  ylim  = c(x_min,x_max),
  xlab  = TeX("$x_1$"),
  ylab  = TeX("$x_2$"),
  nlevels   = 5)
  mtext(LETTERS[i], adj=1, line=1, cex = 1.2, at = -.5)
  i = i + 1
}
dev.off()
```
![Wahrscheinlichkeitsdichtefunktionen bivariater Normalverteilungen.](./_figures/210-bivariate-normal-wdf){#fig-bivariate-normal-wdf fig-align="center" width=100%}


```{r, eval = F, echo = F}
library(mvtnorm)
library(latex2exp)
pdf(
file        = file.path("./_figures/210-gemeinsame-normalverteilungen.pdf"),
width       = 9,
height      = 4.5)
par(
family      = "sans",
mfcol       = c(1,3),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2,1,0),
xaxs        = "i",
yaxs        = "i",
font.main   = 1,
cex         = .9,
cex.main    = 1)

# Ergebnisraumdefintion
x_min  = -1                                          # x_i Minimum
x_max  = 4                                           # x_i Maxim
x_res  = 1e3                                         # x_i Auflösung
x      = seq(x_min, x_max, length.out = x_res)       # x_1 Raum
y      = seq(x_min, x_max, length.out = x_res)       # x_2 Raum
xy     = expand.grid(x,y)                        # X = (x_1,x_2)^T Raum

# Parameterdefinition
mu_x      = 1                                        # \mu \in \mathbb{R}
Sigma_xx  = 0.2                                      # \Sigma_xx in \mathbb{R}
A         = 1                                        # A
b         = 1                                        # b
Sigma_yy  = 0.1                                      # \Sigma_yy

mu_xy     = c(mu_x, A*mu_x + b)
Sigma_xy  = matrix(c(Sigma_xx  , Sigma_xx*t(A),
                     A*Sigma_xx, Sigma_yy + A*Sigma_xx*t(A)),
                   nrow = 2,
                   byrow = TRUE)


# Visualisierung marginale Verteilung
plot(
x,
dnorm(x, mu_x, sqrt(Sigma_xx)),
type = "l",
xlab = TeX("$x$"),
ylim = c(0,2),
ylab = "",
main = TeX("$p(x) = N(x;\\mu_\\xi,\\Sigma_{\\xi\\xi})$"))
mtext(LETTERS[1], adj=1, line=2, cex = 1.5, at = -1.5)

# Visualisierung bedingte Verteilung
plot(
y,
dnorm(y, A*1 + b, sqrt(Sigma_yy)),
type = "l",
xlab = TeX("$y$"),
ylim = c(0,2),
ylab = "",
main = TeX("$p(y|x) = N(y; Ax + b, \\Sigma_{\\upsilon\\upsilon})$"))
text(0,1.75, TeX("$x = 1$"), cex = 1.2)
mtext(LETTERS[2], adj=1, line=2, cex = 1.5, at = -1.5)


# Visualisierung gemeinsame Verteilung
p_xy    = matrix(                                           # Matrixkonversion des von
                dmvnorm(as.matrix(xy), mu_xy, Sigma_xy),    # dmvnorm() ausgegebenen Vektors
                nrow = x_res)
contour(
x,
y,
p_xy,
xlim      = c(x_min,x_max),
ylim      = c(x_min,x_max),
xlab      = TeX("$x$"),
ylab      = TeX("$y$"),
main      = TeX("$p((x,y)^T) = N((x,y)^T; \\mu_{\\xi,\\upsilon},\\Sigma_{\\xi,\\upsilon})$"),
nlevels   = 5)
mtext(LETTERS[3], adj=1, line=2, cex = 1.5, at = -1.5)
dev.off()
```

```{r, eval = F, echo = F}
library(mvtnorm)
library(latex2exp())
pdf(
file        = file.path("./_figures/210-bedingte-normalverteilungen.pdf"),
width       = 10,
height      = 3.33)
par(
family      = "sans",
mfcol       = c(1,3),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2,1,0),
xaxs        = "i",
yaxs        = "i",
font.main   = 1,
cex         = .8,
cex.main    = 1)

# Ergebnisraumdefintion
x_min  = 0                                           # x_i Minimum
x_max  = 4                                           # x_i Maxim
x_res  = 1e3                                         # x_i Auflösung
x      = seq(x_min, x_max, length.out = x_res)       # x Raum
y      = seq(x_min, x_max, length.out = x_res)        # y Raum
X      = expand.grid(x,y)                            # (x,y)^T Raum

# Parameterdefinition
mu     = c(1,2)                                      # \mu \in \mathbb{R}^2
Sigma  = matrix(c(0.12, 0.09,
                  0.09, 0.12),
                nrow = 2,
                byrow = TRUE)                        # \Sigma in \mathbb{R}^{2 \times 2}

# Visualisierung gemeinsame Verteilung
p      = matrix(                                     # Matrixkonversion des von
                dmvnorm(as.matrix(X), mu, Sigma),    # dmvnorm() ausgegebenen Vektors
                nrow = x_res)
contour(
x,
y,
p,
xlim      = c(x_min,x_max),
ylim      = c(x_min,x_max),
xlab      = TeX("$x$"),
ylab      = TeX("$y$"),
main      = TeX("$p(x,y) = N((x,y)^T; \\mu_{\\xi,\\upsilon},\\Sigma_{\\xi,\\upsilon})$"),
nlevels   = 8)
abline(1.5,0)
abline(2.5,0)
text(3.5,1, TeX("$y = 1.5$"))
text(3.5,3, TeX("$y = 2.8$"))
mtext(LETTERS[1], adj=1, line=2, cex = 1.5, at = -.8)

# Visualsierung bedingter Verteilung
plot(
x,
dnorm(x, mu_x + Sigma[1,2]*(1/Sigma[2,2])*(1.5 - mu[2]),sqrt(Sigma[1,1] - Sigma[1,2]*(1/Sigma[2,2]*Sigma[2,1]))),
type = "l",
xlim = c(x_min,x_max),
ylim = c(0,2),
xlab = TeX("$x$"),
ylab = "",
main = TeX("$p(x|y) = N(x; \\mu_{\\xi|\\upsilon},\\Sigma_{\\xi|\\upsilon})$"))
text(3.5,1.8, TeX("$y = 1.5$"))
mtext(LETTERS[2], adj=1, line=2, cex = 1.5, at = -.8)

# Visualsierung bedingter Verteilung
plot(
x,
dnorm(x, mu_x + Sigma[1,2]*(1/Sigma[2,2])*(2.8 - mu[2]),sqrt(Sigma[1,1] - Sigma[1,2]*(1/Sigma[2,2]*Sigma[2,1]))),
type = "l",
xlim = c(x_min,x_max),
ylim = c(0,2),
xlab = TeX("$x$"),
ylab = "",
main = TeX("$p(x|y) = N(x; \\mu_{\\xi|\\upsilon},\\Sigma_{\\xi|\\upsilon})$"))
text(3.5,1.8, TeX("$y = 2.8$"))
mtext(LETTERS[3], adj=1, line=2, cex = 1.5, at = -.8)
dev.off()
```

## Definition 

Wir wollen die multivariate Normalverteilung nun formal einführen und erste 
Eigenschaften angeben. Wir nutzen dazu folgende Definition.

:::{#def-multivariate-normalverteilung}
$\xi$ sei ein $n$-dimensionaler Zufallsvektor mit Ergebnisraum $\mathbb{R}^n$ und WDF
\begin{equation}
p : \mathbb{R}^n \to \mathbb{R}_{>0},\, x \mapsto p(x) := (2\pi)^{-\frac{n}{2}}|\Sigma|^{-\frac{1}{2}}\exp\left(-\frac{1}{2}(x-\mu)^T \Sigma^{-1} (x-\mu)\right).
\end{equation}
Dann sagen wird, dass $\xi$ einer *multivariaten (oder $n$-dimensionalen)
Normalverteilung* mit *Erwartungswertparameter* $\mu \in \mathbb{R}^n$ und
positiv-definitem *Kovarianzmatrixparameter* $\Sigma \in \mathbb{R}^{n \times n}$
unterliegt und nennen $\xi$ einen *(multivariat) normalverteilten Zufallsvektor*.
Wir kürzen dies mit $\xi \sim N(\mu,\Sigma)$ ab. Die WDF eines multivariat
normalverteilten Zufallsvektors bezeichnen wir mit
\begin{equation}
N(x;\mu,\Sigma):= (2\pi)^{-\frac{n}{2}}|\Sigma|^{-\frac{1}{2}}\exp\left(-\frac{1}{2}(x-\mu)^T \Sigma^{-1} (x-\mu)\right).
\end{equation}
:::

@fig-bivariate-normal-wdf A,B,C zeigen die Isokonturen der WDFen bivariat normlaverteilter
Zufallsvektoren für $\mu = (1,1)^T$ und
\begin{equation}
\Sigma_A := \begin{pmatrix} 0.20 &  0.15  \\   0.15 & 0.20 \end{pmatrix}, \quad
\Sigma_B := \begin{pmatrix} 0.20 &  0.00  \\   0.00 & 0.20 \end{pmatrix}, \quad
\Sigma_C := \begin{pmatrix} 0.20 &  -0.15 \\  -0.15 & 0.20 \end{pmatrix}
\end{equation}
respektive. @fig-bivariate-normal-realisierungen A,B und C zeigen jeweils 200 
Realisierungen der entsprechenden Zufallsvektoren.

```{r echo = F, eval = F}
library(latex2exp)
library(mvtnorm)

# Modellformulierung
mu     = c(1,1)                                # \mu \in \mathbb{R}^2
Sigma  = matrix(c(0.2,  0.15,  0.15, 0.2), 2)  # \Sigma in \mathbb{R}^{2 \times 2}

# Abbildungsparameter
pdf(
file        = file.path("./_figures/210-bivariate-normal-realisierungen.pdf"),
width       = 9,
height      = 3)
par(
family      = "sans",
mfcol       = c(1,3),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2,1,0),
xaxs        = "i",
yaxs        = "i",
font.main   = 1,
cex         = .7,
cex.main    = 1.2)

# Parameterdefinition
mu     = c(1,1)                                      # \mu \in \mathbb{R}^2
S      = list(matrix(c(0.2,  0.15,  0.15, 0.2), 2),  # \Sigma in \mathbb{R}^{2 \times 2}
              matrix(c(0.2,  0.00,  0.00, 0.2), 2),  # \Sigma in \mathbb{R}^{2 \times 2}
              matrix(c(0.2, -0.15, -0.15, 0.2), 2))  # \Sigma in \mathbb{R}^{2 \times 2}

# Kovarianzparametervariantenschleife
i = 1
for (Sigma in S){

  # Zufallsvektorrealisierungen
  samples = rmvnorm(n = 200, mu, Sigma)

  # Visualisierung
  plot(
  samples,
  xlim  = c(0,2),
  ylim  = c(0,2),
  xlab  = TeX("$x_1$"),
  ylab  = TeX("$x_2$"),
  pch   = 21,
  col   = "white",
  bg    = "gray60")
  mtext(LETTERS[i], adj=1, line=1, cex = 1.2, at = -.5)
  i = i + 1
}
dev.off()
```

![Realisierungen bivariat normalverteilter Zufallsvektoren.](./_figures/210-bivariate-normal-realisierungen){#fig-bivariate-normal-realisierungen fig-align="center" width=100%}

Ohne Beweis halten wir fest, dass wie im Fall einer univariat normalverteilten
Zufallsvariable, der Erwartungswert und die Kovarianzmatrix eines normalverteilten
Zufallsvektors durch die entsprechenden Parameter gegeben sind.

:::{#thm-erwartungswert-und-kovarianzmatrix-normalverteilter-zufallsvektoren}
## Erwartungswert und Kovarianzmatrix normalverteilter Zufallsvektoren
$\xi \sim N(\mu,\Sigma)$ sei ein multivariat normalverteilter Zufallsvektor mit
Erwartungswertparameter $\mu \in \mathbb{R}^n$ und Kovarianzmatrixparameter 
$\Sigma \in \mathbb{R}^{n \times n} \mbox{ pd}$.
Dann gelten
\begin{equation}
\mathbb{E}(\xi) = \mu \mbox{ und } \mathbb{C}(\xi) = \Sigma.
\end{equation}
:::

Wie im Falle der univariat normalverteilten Zufallsvariable entspricht der 
Parameter $\mu \in \mathbb{R}^n$  dem Wert höchster Wahrscheinlichkeitsdichte
der multivariaten Normalverteilung. Analog zum Varianzparameter der univariat 
normalverteilten Zufallsvariable spezifizieren die  Diagonalelemente von 
$\Sigma \in \mathbb{R}^{n \times n} \mbox{ pd}$ die Breite der WDF bezüglich der
Zufallsvektorkomponenten $\xi_1,...,\xi_n$. Allgemein spezifiziert im Falle des 
multivariat normalverteilten Zufallsvektors das $i,j$te Element von 
$\Sigma \in \mathbb{R}^{n \times n} \mbox{ pd}$ hier nun die Kovarianz der 
Zufallsvektorkomponenten $\xi_i$ und $\xi_j$.

## Transformationen

In diesem Abschnitt stellen wir einige Resultate zu den Verteilungen
transformierter normalverteilter Zufallsvektoren zusammen. Wir verzichten
dabei auf Beweise.
    
:::{#thm-invertierbare-lineare-transformation}
## Invertierbare lineare Transformation eines normalverteilten Zufallsvektors
$\xi \sim N(\mu_\xi,\Sigma_\xi)$ sei ein normalverteilter $n$-dimensionaler Zufallsvektor
und es sei $\zeta := A\xi$ mit einer invertierbaren Matrix $A \in \mathbb{R}^{n \times n}$.
Dann gilt
\begin{equation}
\zeta \sim N\left(\mu_\zeta, \Sigma_\zeta\right) 
\mbox{ mit } 
\mu_\zeta = A\mu_\xi \mbox{ und } 
\Sigma_\zeta = A\Sigma_\xi A^T.
\end{equation}
:::

Nach @thm-invertierbare-lineare-transformation ergibt die invertierbare lineare 
Transformation eines multivariat normalverteilten Zuallsvektors also wiederum einen
multivariat normalverteilten Zufallsvektor und die Parameter der Verteilung
dieses normalverteilten Zufallsvektors ergeben sich aus den Parametern der Verteilung
des ursprünglichen Zufallsvektors und der Transformationsmatrix.

\newpage
**Beispiel**

Als Beispiel betrachten wir die invertierbare lineare Transformation eines
bivariaten normalverteilten Zufallsvektors $\xi$. Es seien 
\begin{equation}
\mu_\xi := \begin{pmatrix} 1 \\ 1 \end{pmatrix}
\mbox{ und }
\Sigma_\xi := \begin{pmatrix} 0.20 & 0.15 \\ 0.15 & 0.20 \end{pmatrix}
\end{equation}
der Erwartungswert- und Kovarianzmatrixparameter von $\xi$, respektive, und es sei
\begin{equation}
A := \begin{pmatrix} -2 & 1 \\ - 1 & 2 \end{pmatrix}
\end{equation}
die Transformationsmatrix. Da $|A| = -3 \neq 0$ ist $A$ invertierbar und es 
gilt nach @thm-invertierbare-lineare-transformation, dass
\begin{equation}
\zeta \sim N(\mu_\zeta,\Sigma_\zeta) \mbox{ mit }
\mu_\zeta = A\mu_\xi = \begin{pmatrix} -1 \\ 1 \end{pmatrix}
\mbox{ und }
A\Sigma_\xi A^T = \begin{pmatrix} 0.40 & 0.05 \\ 0.05 & 0.40 \end{pmatrix}.
\end{equation}
@fig-lineare-transformation A zeigt Isokonturen der WDF von $\xi$ und Realisierungen 
$x^{(i)} \in \mathbb{R}^2$ von $\xi$ für $i = 1,...,50$. @fig-lineare-transformation B
zeigt die transfomierten Realisierungen $z^{(i)} = Ax^{(i)} \in \mathbb{R}^{2}$
von $\zeta$ sowie die Isokonturen der WDF von $\zeta$ nach @thm-invertierbare-lineare-transformation.

```{r, eval = F, echo = F}
library(latex2exp)
library(mvtnorm)
pdf(
file        = file.path("./_figures/210-lineare-transformation.pdf"),
width       = 8,
height      = 4)
par(
family      = "sans",
mfcol       = c(1,2),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2,1,0),
xaxs        = "i",
yaxs        = "i",
font.main   = 1,
cex         = 1,
cex.main    = 1)

# x-Modell
z_nin  = -3                                                                     # x_i Minimum
z_nax  = 3                                                                      # x_i Maximum
x_res  = 1e3                                                                    # x_i Auflösung
x_1    = seq(z_nin, z_nax, length.out = x_res)                                  # x_1 Raum
x_2    = seq(z_nin, z_nax, length.out = x_res)                                  # x_2 Raum
x      = expand.grid(x_1,x_2)                                                   # x = (x_1,x_2)^T Raum
mu     = c(1,1)                                                                 # \mu \in \mathbb{R}^2
Sigma  = matrix(c(0.2,  0.15,  0.15, 0.2), 2)                                   # \Sigma in \mathbb{R}^{2 \times 2}
px     = matrix(dmvnorm(as.matrix(x), mu, Sigma), nrow = x_res)                 # x \sim N(\mu,\Sigma)
xi     = rmvnorm(n = 50, mu, Sigma)                                             # Realisierungen
contour(
x_1,
x_2,
px,
xlim      = c(z_nin,z_nax),
ylim      = c(z_nin,z_nax),
xlab      = TeX("$x_1$"),
ylab      = TeX("$x_2$"),
levels    = c(0.9,0.7,0.5,0.3,0.1),
main      = TeX("$\\xi \\sim N(\\mu_\\xi,\\Sigma_\\xi)$"))
points(
xi,
pch   = 21,
col   = "white",
bg    = "gray60",
cex   = 1)
mtext(LETTERS[1], adj=1, line=2, cex = 1.5, at = -4)

# y-Modell
A      = matrix(c(-2,1,-1,2), nrow = 2, byrow = TRUE)                           # Transformationsmatrix
z_nin  = -3                                                                     # z_i Minimum
z_nax  =  3                                                                     # z_i Maximum
z_res  = 1e3                                                                    # z_i Auflösung
z_1    = seq(z_nin, z_nax, length.out = z_res)                                  # z_1 Raum
z_2    = seq(z_nin, z_nax, length.out = z_res)                                  # z_2 Raum
z      = expand.grid(z_1,z_2)                                                   # z = (z_1,z_2)^T Raum
pz     = matrix(dmvnorm(as.matrix(z), A%*%mu, A%*%Sigma%*%t(A)), nrow = z_res)  # \zeta \sim N(A\mu,A\SigmaA^T)
ups    = t(A %*% t(xi))                                                         # Realisierungen
contour(
z_1,
z_2,
pz,
xlim      = c(z_nin,z_nax),
ylim      = c(z_nin,z_nax),
xlab      = TeX("$z_1$"),
ylab      = TeX("$z_2$"),
levels    = c(0.9,0.7,0.5,0.3,0.1),
main      = TeX("$\\zeta \\sim N(A\\mu_\\xi,A\\Sigma_\\xi A^T)$"))
points(
ups,
pch   = 21,
col   = "white",
bg    = "gray60",
cex   = 1)
mtext(LETTERS[2], adj=1, line=2, cex = 1.5, at = -4)
dev.off()
print(A)
print(A%*%mu)
print(A%*%Sigma%*%t(A))
```

![Invertierbare lineare Transformation eines normalverteilten Zufallsvektors](./_figures/210-lineare-transformation){#fig-lineare-transformation fig-align="center" width=100%}

Die Tatsache, dass ein linear transfomierter normalverteilter Zufallsvektor wiederum
normalverteilt ist und dass sich die Parameter der Verteilung des transformierten Zufallsvektors
aus den Parametern der Verteilung des ursprünglichen Zufallsvektors sowie den Transformationsparametern
bestimmen lassen, bleibt auch im Falle einer nicht notwendigerweise invertierbaren
linearen Transformation und auch im Falle einer nicht notwendigerweise invertierbaren
linear-affinen Transformation wahr. Dies ist die Aussage folgenden zentralen Theorems.
Für einen Beweis verweisen wir auf @anderson2003. 

:::{#thm-linear-affine-transformation}
## Linear-affine Transformation eines normalverteilten Zufallsvektors
$\xi \sim N(\mu_\xi,\Sigma_\xi)$ sei ein normalverteilter $n$-dimensionaler Zufallsvektor
und es sei 
\begin{equation}
\zeta := Ax + b \mbox{ mit } A \in \mathbb{R}^{m \times n} \mbox{ und } b \in \mathbb{R}^m.
\end{equation}
Dann gilt
\begin{equation}
\zeta \sim N(\mu_\zeta, \Sigma_\zeta) 
\mbox{ mit } 
\mu_\zeta    = A\mu + b \in \mathbb{R}^m \mbox{ und } 
\Sigma_\zeta = A\Sigma A^T \in \mathbb{R}^{m \times m}.
\end{equation}
:::

## Sphärizität

Folgendes Theorem ist für die grundlegende Theorie des Allgemeinen Linearen Modells zentral.

:::{#thm-sphärizität}
## Sphärische multivariate Normalverteilung
Für $i = 1,...,n$ seien $N(x_i; \mu_i,\sigma^2)$ die WDFen von $n$ unabhängigen
univariaten normalverteilten Zufallsvariablen $\xi_1,...,\xi_n$ mit $\mu_1,...,\mu_n \in \mathbb{R}$
und $\sigma^2 > 0$. Weiterhin sei $N(x;\mu,\sigma^2I_n)$ die WDF eines $n$-variaten
normalverteilten Zufallsvektors $\xi$ mit Erwartungswertparameter $\mu := (\mu_1,...,\mu_n)^T \in \mathbb{R}^n$.
Dann gilt 
\begin{equation}
p_\xi(x) = p_{\xi_1,...,\xi_n}(x_1,...,x_n) = \prod_{i=1}^n p_{\xi_i}(x_i)
\end{equation}
und insbesondere
\begin{equation}
N\left(x;\mu,\sigma^2I_n\right) = \prod_{i=1}^n N\left(x_i;\mu_i,\sigma^2\right) 
\end{equation}
für alle $x = (x_1,...,x_n)^T \in \mathbb{R}^n$.
:::

:::{.proof}
Wir zeigen die Identität der multivariaten WDF $N(x;\mu,\sigma^2 I_n)$ mit dem
Produkt von $n$ univariaten WDFen $N(x_i;\mu_i,\sigma^2 I_n)$, wobei $\mu_i$
der $i$te Eintrag von $\mu \in \mathbb{R}^n$ ist. Es ergibt sich
\begin{align}
\begin{split}
N\left(x;\mu,\sigma^{2}I_{n} \right)
& = \left(2\pi \right)^{-\frac{n}{2}}
	\left|\sigma^2 I_n \right|^{-\frac{1}{2}}
	\exp\left(-\frac{1}{2}(x-\mu)^{T}(\sigma^2 I_n)^{-1}(x-\mu)\right)\\
& = \left(\prod_{i=1}^n 2\pi ^{-\frac{1}{2}} \right)
	\left(\sigma^2\right)^{-\frac{n}{2}}
	\exp\left(-\frac{1}{2\sigma^2}(x-\mu)^{T}(x-\mu)\right) \\
& = \left(\prod_{i=1}^n \left(2\pi\sigma^2 \right) ^{-\frac{1}{2}} \right)
	\exp\left(-\frac{1}{2\sigma^2} \sum_{i=1}^n (x_i - \mu_i)^2\right) \\
& = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}
	\prod_{i=1}^n \exp\left(-\frac{1}{2\sigma^2} (x_i - \mu_i)^2\right) \\
& = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}
	 				\exp\left(-\frac{1}{2\sigma^2} (x_i - \mu_i)^2\right) \\
& = \prod_{i=1}^n N\left(x_i; \mu_i,\sigma^2\right).
\end{split}
\end{align}
:::

Einen Kovarianzmatrixparameter der Form $\Sigma = \sigma^2 I_n$ nennt man auch *sphärisch*,
da die Isokonturen der WDF eines normalverteilten Zufallsvektors mit einem solchen
Kovarianzmatrixparameter *Sphären* bilden (zum Beispiel Kreise bei $n = 2$ und
Kugeln bei $n = 3$). Eine multivariate Normalverteilung mit sphärische Kovarianzmatrixparameter
nennt man entsprechend eine *sphärische Normalverteilung*. @thm-sphärizität besagt,
dass die WDF eines $n$-dimensionalen normalverteilten Zufallskvektors mit 
sphärischem Kovarianzparameter der gemeinsamen WDF von $n$ unabhängigen 
univariat normalverteilten Zufallsvariablen entspricht und umgekehrt. Eine 
Realisierung eines $n$-dimensionalen normalverteilten Zufallsvektors entspricht 
also den Realisierungen von $n$ unabhängigen univariat normalverteilten Zufallsvariablen 
und umgekehrt. Man beachte, dass die Identität der Verteilungen der $\xi_i, i = 1,...,n$ hier 
nicht voraussgesetzt ist, insbesondere können sich ihre Erwartungswertparameter 
$\mu_i, i = 1,...,n$ explizit unterscheiden. 


## Marginale und bedingte Verteilungen 

Multivariate Normalverteilungen haben die Eigenschaft, dass auch alle anderen
assoziierten Verteilungen Normalverteilungen sind und deren Erwartungswert- und
Kovarianzmatrixparameter aus den Parametern der jeweils komplementären Verteilung
errechnet werden können. Insbesondere gilt zum einen, dass die uni- und multivariaten 
Marginalverteilungen multivariater Normalverteilungen wiederum Normalverteilungen sind.
Zum anderen lassen sich wie alle multivariaten Verteilungen multivariate Normalverteilungen 
multiplikativ in eine marginale und eine bedingte Verteilung zerlegen. Insbesondere sind 
nun aber bei multivariaten Normalverteilungen diese Verteilungen wiederum (multivariate) 
Normalverteilungen, deren Parameter aus den Parametern der gemeinsame Verteilung errechnet 
werden können und umgekehrt. Wir fassen obige Erkenntnisse formal in den 
folgenden drei Theoremen zusammen.

:::{#thm-marginale-normalverteilungen}
## Marginale Normalverteilungen
Es sei $n := k + l$ und $\xi = (\xi_1,...,\xi_n)^T$ sei ein $n$-dimensionaler
normalverteilter Zufallsvektor mit Erwartungswertparameter
\begin{equation}
\mu =
\left(\begin{matrix}
\mu_\upsilon \\
\mu_\zeta
\end{matrix}\right) \in \mathbb{R}^n,
\end{equation}
mit $\mu_\upsilon \in \mathbb{R}^k$ and $\mu_\zeta \in \mathbb{R}^l$ und Kovarianzmatrixparameter
\begin{equation}
\Sigma =
\left(\begin{matrix}
\Sigma_{\upsilon\upsilon} 	& \Sigma_{\upsilon\zeta} \\
\Sigma_{\zeta\upsilon} 	& \Sigma_{\zeta\zeta}
\end{matrix}\right) \in \mathbb{R}^{n \times n},
\end{equation}
mit $\Sigma_{\upsilon\upsilon}  \in \mathbb{R}^{k \times k}$,
    $\Sigma_{\upsilon\zeta}     \in \mathbb{R}^{k \times l}$,
    $\Sigma_{\zeta\upsilon}     \in \mathbb{R}^{l \times k}$,
und $\Sigma_{\zeta\zeta}        \in \mathbb{R}^{l \times l}$.
Dann sind $\upsilon := (\xi_1,...,\xi_k)^T$ und $\zeta := (\xi_{k+1}, ...,\xi_n)^T$
$k$- und $l$-dimensionale normalverteilte Zufallsvektoren, respektive, und es gilt
\begin{equation}
\upsilon \sim N(\mu_\upsilon,\Sigma_{\upsilon\upsilon}) \mbox{ und } \zeta \sim N(\mu_\zeta,\Sigma_{\zeta\zeta}).
\end{equation}
:::

Die Marginalverteilungen einer multivariaten Normalverteilung sind also auch Normalverteilungen
und die Parameter der Marginalverteilungen ergeben sich aus den Parametern der gemeinsamen Verteilung.
Für Beweise dieses Theorems verweisen wir auf @mardia1979 und @anderson2003.
@fig-marginale-normalverteilungen visualisiert @thm-marginale-normalverteilungen für den
Fall $n := 2, k := 1, l := 1$,
\begin{equation}
\mu := \begin{pmatrix} 1 \\ 2 \end{pmatrix} \in \mathbb{R}^2 
\mbox{ und } 
\Sigma := \begin{pmatrix} 0.10 & 0.08 \\ 0.08 & 0.15 \end{pmatrix}  \in \mathbb{R}^{2 \times 2}.
\end{equation}
@fig-marginale-normalverteilungen A zeigt dabei die WDF des bivariaten Zufallsvektors
$\xi$ und @fig-marginale-normalverteilungen B und C die WDFen der entsprechenden
marginalen Zufallsvariablen $\upsilon$ und $\zeta$.

```{r, eval = F, echo = F}
library(mvtnorm)
library(latex2exp())
pdf(
file        = file.path("./_figures/210-marginale-normalverteilungen.pdf"),
width       = 9,
height      = 4.5)
par(
family      = "sans",
mfcol       = c(1,3),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2,1,0),
xaxs        = "i",
yaxs        = "i",
font.main   = 1,
cex         = 1,
cex.main    = 1.2)

# Ergebnisraumdefintion
x_min  = 0                                           # x_i Minimum
x_max  = 3                                           # x_i Maxim
x_res  = 1e3                                         # x_i Auflösung
x_1    = seq(x_min, x_max, length.out = x_res)       # x_1 Raum
x_2    = seq(x_min, x_max, length.out = x_res)       # x_2 Raum
X      = expand.grid(x_1,x_2)                        # X = (x_1,x_2)^T Raum

# Parameterdefinition
mu     = c(1,2)                                      # \mu \in \mathbb{R}^2
Sigma  = matrix(c(0.10, 0.08,
                  0.08, 0.15),
                nrow = 2,
                byrow = TRUE)                        # \Sigma in \mathbb{R}^{2 \times 2}


# Visualisierung gemeinsame Verteilung
p      = matrix(                                     # Matrixkonversion des von
                dmvnorm(as.matrix(X), mu, Sigma),    # dmvnorm() ausgegebenen Vektors
                nrow = x_res)
contour(
x_1,
x_2,
p,
xlim      =  c(x_min,x_max),
ylim      =  c(x_min,x_max),
xlab      = TeX("$y$"),
ylab      = TeX("$z$"),
main      = TeX("$N(x;\\mu,\\Sigma)$"),
nlevels   = 10)
mtext(LETTERS[1], adj=1, line=2, cex = 1.5, at = -.3)

# Visualisierung Marginalverteilungen
p_marg   = list(dnorm(x_1, mu[1],Sigma[1,1]), dnorm(x_1, mu[2],Sigma[2,2]))
l_marg   = c(TeX("$y$"), TeX("$z$"))
L_marg   = c(TeX("$N(y;\\mu_\\upsilon,\\Sigma_{\\upsilon\\upsilon})$"), TeX("$N(z; \\mu_\\zeta,\\Sigma_{\\zeta\\zeta})$"))
i        = 1
for(i in 1:length(p_marg)){
  plot(x_1,
      p_marg[[i]],
      type = "l",
      xlab = l_marg[[i]],
      ylim = c(0,5),
      ylab = "",
      main = L_marg[[i]])
  mtext(LETTERS[i+1], adj=1, line=2, cex = 1.5, at = -.3)
}
dev.off()
```

![Marginale Verteilungen eines bivariaten normalverteilten Zufallsvektor.](./_figures/210-marginale-normalverteilungen){#fig-marginale-normalverteilungen fig-align="center" width=100%}

Mithilfe einer marginalen und einer bedingten multivariaten Normalverteilung
lässt sich eine gemeinsame multivariate Normalverteilung konstruieren, deren
Parameter sich aus den Parametern der marginalen und bedingten Verteilung 
ergeben. Dies ist die zentrale Aussage folgenden Theorems.

:::{#thm-gemeinsame-normalverteilungen}
## Gemeinsame Normalverteilungen
$\xi$ sei ein $m$-dimensionaler normalverteilter Zufallsvektor mit WDF
\begin{equation}
p_\xi : \mathbb{R}^m \to \mathbb{R}_{>0},\,x\mapsto
p_\xi(x) := N(x;\mu_\xi,\Sigma_{\xi\xi}) \mbox{ mit }
\mu_\xi \in \mathbb{R}^m,
\Sigma_{\xi\xi} \in \mathbb{R}^{m\times m},
\end{equation}
$A\in\mathbb{R}^{n\times m}$ sei eine Matrix, $b\in\mathbb{R}^n$ sei ein Vektor
und $\upsilon$ sei ein $n$-dimensionaler bedingt normalverteilter Zufallsvektor mit
bedingter WDF
\begin{equation}
p_{\upsilon|\xi}(\cdot|x) : \mathbb{R}^n \to \mathbb{R}_{>0},\, y\mapsto
p_{\upsilon|\xi}(y|x) := N(y;A\xi+b,\Sigma_{\upsilon\upsilon}) \mbox{ mit }
\Sigma_{\upsilon\upsilon} \in \mathbb{R}^{n\times n}.
\end{equation}
Dann ist der $m+n$-dimensionale Zufallsvektor $(\xi,\upsilon)^T$ normalverteilt mit (gemeinsamer) WDF
\begin{equation}\label{eq:gauss_joint}
p_{\xi,\upsilon} : \mathbb{R}^{m+n} \to \mathbb{R}_{>0},\, \begin{pmatrix} x \\ y \end{pmatrix} \mapsto
p_{\xi,\upsilon}\left(\begin{pmatrix} x \\ y \end{pmatrix}\right) = N\left(\begin{pmatrix} x \\ y \end{pmatrix};
\mu_{\xi,\upsilon}, \Sigma_{\xi,\upsilon} \right),
\end{equation}
mit $\mu_{\xi,\upsilon} \in \mathbb{R}^{m+n}$ und $\Sigma_{\xi,\upsilon} \in \mathbb{R}^{m+n \times m+n}$ und insbesondere
\begin{equation}
\mu_{\xi,\upsilon} = \left( \begin{matrix} \mu_\xi \\ A\mu_\xi + b \end{matrix} \right)
\mbox{ und }
\Sigma_{\xi,\upsilon} = \left(\begin{matrix} \Sigma_{\xi\xi} & \Sigma_{\xi\xi}A^T \\ A\Sigma_{\xi\xi} & \Sigma_{\upsilon\upsilon} + A\Sigma_{\xi\xi}A^T \end{matrix} \right).
\end{equation}
:::

Insbesondere ergeben sich die Parameter der gemeinsamen Verteilung also als
linear-affine Transformation der Parameter der induzierenden Verteilungen.
@fig-gemeinsame-normalverteilungen visualisiert @thm-gemeinsame-normalverteilungen für den
Fall $m := 1, n := 1, \mu_\xi := 1, \Sigma_{\xi\xi} := 0.2, A := 1, b := 1$ und 
$\Sigma_{\upsilon\upsilon} := 0.1$. @fig-gemeinsame-normalverteilungen A zeigt dabei 
die WDF der Zufallsvariable $\xi$,  @fig-gemeinsame-normalverteilungen B zeigt die
WDF der bedingten Verteilung der Zufallsvariable  $\upsilon$ gegeben $\xi$ und 
@fig-marginale-normalverteilungen C schließlich zeigt die WDFen des induzierten bivariaten
Zufallsvektors $(\xi,\upsilon)$.

![Gemeinsame Verteilungen einer marginalen und einer auf dieser bedingten normalverteilten Zufallsvariable.](./_figures/210-gemeinsame-normalverteilungen){#fig-gemeinsame-normalverteilungen fig-align="center" width=100%}

Die Definition einer multivariaten Normalverteilung erlaubt es weiterhin, die
bedingten Verteilungen aller Komponenten des entsprechenden Zufallsvektors direkt
mithilfe der Parameter der multivariaten Normalverteilung zu bestimmen. Dies
ist die zentrale Aussage folgenden Theorems.

:::{#thm-bedingte-normalverteilungen}
## Bedingte Normalverteilungen
$(\xi,\upsilon)$ sei ein $m+n$-dimensionaler normalverteilter Zufallsvektor mit WDF
\begin{equation}
p_{\xi,\upsilon} : \mathbb{R}^{m + n} \to \mathbb{R}_{>0}, \begin{pmatrix} x \\ y \end{pmatrix}
\mapsto p_{\xi,\upsilon}\left(\begin{pmatrix} x \\ y \end{pmatrix} \right)
:= N\left(\begin{pmatrix} x \\ y \end{pmatrix}; \mu_{\xi,\upsilon}, \Sigma_{\xi,\upsilon}\right),
\end{equation}
mit
\begin{equation}
\mu_{\xi,\upsilon} 
= \left(\begin{matrix} \mu_\xi \\ \mu_\upsilon \end{matrix} \right), 
\Sigma_{\xi,\upsilon} = \left(\begin{matrix} \Sigma_{\xi\xi} & \Sigma_{\xi\upsilon} \\ \Sigma_{\upsilon\xi} & \Sigma_{\upsilon\upsilon} \end{matrix} \right),
\end{equation}
mit $x,\mu_\xi \in \mathbb{R}^m, y,\mu_\upsilon\in\mathbb{R}^n$ und $\Sigma_{\xi\xi} \in \mathbb{R}^{m\times m}, \Sigma_{\xi\upsilon} \in \mathbb{R}^{m\times n}, \Sigma_{\upsilon\upsilon} \in \mathbb{R}^{n \times n}$. Dann ist die
bedingte Verteilung von $\xi$ gegeben $\upsilon$ eine $m$-dimensionale Normalverteilung
mit bedingter WDF
\begin{equation}
p_{\xi|\upsilon}(\cdot|y) : \mathbb{R}^m \to \mathbb{R}_{>0}, x \mapsto p_{\xi|\upsilon}(x|y) :=
N(x;\mu_{\xi|\upsilon},\Sigma_{\xi|\upsilon})
\end{equation}
mit
\begin{equation}\label{eq:gauss_cond_exp}
\mu_{\xi|\upsilon} = \mu_\xi  + \Sigma_{\xi\upsilon}\Sigma_{\upsilon\upsilon}^{-1}(y-\mu_\upsilon) \in \mathbb{R}^m
\end{equation}
und
\begin{equation}\label{eq:gauss_cond_var}
\Sigma_{\xi|\upsilon} = \Sigma_{\xi\xi}  - \Sigma_{\xi\upsilon}\Sigma_{\upsilon\upsilon}^{-1}\Sigma_{\upsilon\xi} \in \mathbb{R}^{m\times m}.
\end{equation}
:::

Im Zusammenspiel mit @thm-gemeinsame-normalverteilungen und @thm-marginale-normalverteilungen 
können die Parameter bedingter und marginaler Normalverteilungen also aus den 
Parametern der komplementären bedingten und marginalen Normalverteilungen bestimmt werden.
@fig-bedingte-normalverteilungen visualisiert @thm-bedingte-normalverteilungen für den Fall
$m := 2, n := 1$, 
\begin{equation}
\mu := \begin{pmatrix} 1 \\ 2 \end{pmatrix} 
\mbox{ und } 
\Sigma := \begin{pmatrix} 0.12 & 0.09 \\ 0.09 & 0.12 \end{pmatrix}
\end{equation}
Dabei zeigt @fig-bedingte-normalverteilungen A die WDF des bivariaten Zufallsvektors $(\xi,\upsilon)^T$
und @fig-bedingte-normalverteilungen B und C zeigen die WDF der bedingten Verteilung
der Zufallsvariable $\xi$ gegeben $\upsilon = 1.5$ und $\upsilon = 2.8$, respektive.

![Bedingte Normalverteilungen](./_figures/210-bedingte-normalverteilungen){#fig-bedingte-normalverteilungen fig-align="center" width=110%}

# Normalverteilungstransformationen {#sec-normalverteilungstransformationen}
\normalsize 

In diesem Kapitel diskutieren wir sechs Transformationen normalverteilter 
Zufallsvariablen, die in der Frequentistischen Inferenz zentrale Rollen spielen
und bei denen es sich um Anwendungen von den in @sec-transformationstheoreme eingeführten
Transformationtheore handelt. Unsere Aussagen sind dabei von der allgemeinen Form 
"Wenn $\xi_i, i = 1,...,n$  unabhängig und identisch normalverteilte Zufallsvariablen 
sind und $\upsilon := f(\xi_1,...,\xi_n)$ eine Transformation dieser Zufallsvariablen ist, 
dann ist die WDF von $\upsilon$ durch die Formel $p_\upsilon := \{\mbox{Formel}\}$ gegeben und 
man nennt die Verteilung von $\upsilon$ *Verteilungsname*". Aussagen dieser Form sind 
für die Frequentistische Inferenz zentral, weil

(1)  die Zentralen Grenzwertsätze die Annahme additiv unabhängig normalverteilter
Störvariablen, und damit normalverteilter Daten, begründet,
(2) wie wir in @sec-grundbegriffe-frequentistischer-inferenz sehen werden, es 
sich bei Schätzern und Statistiken um Transformationen von Zufallsvariablen handelt, und
(3) Parameterschätzergütekriterien, Konfidenzintervalle und Hypothesentests der 
Frequentistischen Inferenz durch die Verteilungen der jeweiligen Schätzer und 
Statistiken charakterisiert und begründet sind.

## Summentransformation und Mittelwertstransformation {#sec-summentransformation-und-mittelwerttransformation}

In diesem Abschnitt betrachten wir die resultierenden Verteilung bei Summation
und Mittelwertbildung von unabhängig und identisch normalverteilten Zufallsvariablen.
Speziell besagt das @thm-summationstransformation besagt, dass die Summe
unabhängig normalverteilter Zufallsvariablen wiederum normalverteilt ist und gibt die
Parameter dieser Verteilung an, während @thm-mittelwerttransformation
besagt, dass das Stichprobenmittel unabhängig normalverteilter Zufallsvariablen wiederum normalverteilt ist und
gibt die Parameter dieser Verteilung an.

:::{#thm-summationstransformation}
## Summationstransformation

Für $i = 1,...,n$ seien $\xi_i \sim N(\mu_i,\sigma^2_i)$ unabhängige normalverteilte
Zufallsvariablen. Dann gilt für die Summe $\upsilon := \sum_{i=1}^n \xi_i$ , dass
\begin{equation}
\upsilon \sim N\left(\sum_{i=1}^n \mu_i, \sum_{i=1}^n \sigma^2_i\right)
\end{equation}
Für unabhängige und identisch normalverteilte Zufallsvariablen
$\xi_i \sim N(\mu,\sigma^2)$ gilt folglich
\begin{equation}
\upsilon \sim N(n\mu, n \sigma^2).
\end{equation}
:::

:::{.proof}
Wir skizzieren mithilfe von @thm-summe-unabhängiger-zufallsvariablen-konvolution, dass für $\xi_1 \sim N(\mu_1,\sigma^2_1)$,
$\xi_2 \sim N(\mu_2,\sigma^2_2)$, und $\upsilon := \xi_1 + \xi_2$ gilt, dass
$\upsilon \sim N(\mu_1 + \mu_2,\sigma_1^2 + \sigma_2^2)$. Für $n > 2$ folgt das Theorem
dann durch Iteration. Mit der Definition der WDF der Normalverteilung erhalten
wir zunächst
\begin{align}
\begin{split}
p_\upsilon(y)
& = \int_{-\infty}^\infty p_{\xi_1}(x_1)p_{\xi_2}(y - x_1)\,dx_1
\\
& = \int_{-\infty}^\infty
    \frac{1}{\sqrt{2 \pi} \sigma_1} \exp\left(-\frac{1}{2}\left(\frac{x_1 - \mu_1}{\sigma_1}\right)^2\right)
	\frac{1}{\sqrt{2 \pi} \sigma_2} \exp\left(-\frac{1}{2}\left(\frac{y - x_1 - \mu_2}{\sigma_2}\right)^2\right)
	\,dx_1
\\
& = \int_{-\infty}^\infty
    \frac{1}{2 \pi \sigma_1\sigma_2}\exp
    \left(
    -\frac{1}{2}\left(\frac{x_1 - \mu_1}{\sigma_1}\right)^2
    -\frac{1}{2}\left(\frac{y - x_1 - \mu_2}{\sigma_2}\right)^2
    \right)
	\,dx_1 .
\\
\end{split}
\end{align}
Mit einigem algebraischen Aufwand erhält man die Identität
\begin{multline}
-\frac{1}{2}\left(\frac{x_1 - \mu_1}{\sigma_1}\right)^2
-\frac{1}{2}\left(\frac{y - x_1 - \mu_2}{\sigma_2}\right)^2
=
-\frac{(y - \mu_1 - \mu_2)^2}
      {2(\sigma_1^2 + \sigma_2^2)}
-\frac{((\sigma_1^2 + \sigma_2^2)x_1 -\sigma_1^2y + \mu_2 \sigma_1^2 - \mu_1 \sigma_2^2)^2}
      {2\sigma_1^2\sigma_2^2(\sigma_1^2 + \sigma_2^2)},
\end{multline}
so dass weiterhin gilt, dass
\begin{align}
\begin{split}
p_\upsilon(y)
& = \int_{-\infty}^\infty
	\frac{1}{2 \pi \sigma_1\sigma_2}
	\exp\left(
 	-\frac{(y - \mu_1 - \mu_2)^2}
      {2(\sigma_1^2 + \sigma_2^2)}
	-\frac{((\sigma_1^2 + \sigma_2^2)x_1 -\sigma_1^2y + \mu_2 \sigma_1^2 - \mu_1 \sigma_2^2)^2}
      {2\sigma_1^2\sigma_2^2(\sigma_1^2 + \sigma_2^2)}
    \right)
	\,dx_1
\\
& = \int_{-\infty}^\infty
	\frac{1}{2 \pi \sigma_1\sigma_2}
	\exp\left(
 	-\frac{(y - \mu_1 - \mu_2)^2}
      {2(\sigma_1^2 + \sigma_2^2)}
    \right)
	\exp\left(
	-\frac{((\sigma_1^2 + \sigma_2^2)x_1 -\sigma_1^2y + \mu_2 \sigma_1^2 - \mu_1 \sigma_2^2)^2}
      {2\sigma_1^2\sigma_2^2(\sigma_1^2 + \sigma_2^2)}
    \right)
	\,dx_1
\\
& = \frac{1}{2 \pi \sigma_1\sigma_2}
	\exp\left(
 	-\frac{(y - \mu_1 - \mu_2)^2}
      {2(\sigma_1^2 + \sigma_2^2)}
    \right)
	\int_{-\infty}^\infty
	\exp\left(
	-\frac{((\sigma_1^2 + \sigma_2^2)x_1 -\sigma_1^2y + \mu_2 \sigma_1^2 - \mu_1 \sigma_2^2)^2}
      {2\sigma_1^2\sigma_2^2(\sigma_1^2 + \sigma_2^2)}
    \right)
	\,dx_1.
\end{split}
\end{align}
Für das verbleibende Integral zeigt man mithilfe der Integration durch Substitution, dass
\begin{equation}
\int_{-\infty}^\infty
	\exp\left(
	-\frac{((\sigma_1^2 + \sigma_2^2)x_1 -\sigma_1^2y + \mu_2 \sigma_1^2 - \mu_1 \sigma_2^2)^2}
      {2\sigma_1^2\sigma_2^2(\sigma_1^2 + \sigma_2^2)}
    \right)
	\,dx_1
= \frac{\sqrt{2\pi}\sigma_1\sigma_2}{\sqrt{\sigma_1^2 + \sigma_2^2}}.
\end{equation}
Es ergibt sich also
\begin{align}
\begin{split}
p_\upsilon(y)
& = \frac{1}{2 \pi \sigma_1\sigma_2}
	\frac{\sqrt{2\pi}\sigma_1\sigma_2}{\sqrt{\sigma_1^2 + \sigma_2^2}}
	\exp\left(
 	-\frac{(y - \mu_1 - \mu_2)^2}
      {2(\sigma_1^2 + \sigma_2^2)}
    \right)
\\
& = \frac{(2\pi)^{-1}(2\pi)^2}{\sqrt{\sigma_1^2 + \sigma_2^2}}
	\exp\left(
 	-\frac{(y - \mu_1 - \mu_2)^2}
      {2(\sigma_1^2 + \sigma_2^2)}
    \right)
\\
& = \frac{1}{\sqrt{2\pi}\sqrt{\sigma_1^2 + \sigma_2^2}}
	\exp\left(
 	-\frac{(y - \mu_1 - \mu_2)^2}
      {2(\sigma_1^2 + \sigma_2^2)}
    \right).
\end{split}
\end{align}
Schließlich folgt, dass
\begin{align}
\begin{split}
p_\upsilon(y)
& = \frac{1}{\sqrt{2\pi(\sigma_1^2 + \sigma_2^2)}}
    \exp\left(-\frac{1}{2(\sigma_1^2 + \sigma_2^2)}\left(y - (\mu_1 + \mu_2)\right)^2\right) 
  = N(y; \mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2)
\end{split}
\end{align}
Ein einfacheres Vorgehen ergibt sich vermutlich nach Fouriertransformation der
WDF im Sinne der sogenannten charakteristischen Funktion einer Zufallsvariable.
In diesem Fall würde die Faltung der WDFen der Multiplikation der charakteristischen
Funktionen entsprechen.
:::

Ein wichtiger Anwendungsfall von @thm-summationstransformation
ist das nachfolgende @thm-mittelwerttransformation
sowie die in @eq-generalisierung-zentraler-grenzwertsatz-lindenberg-levy und 
@eq-generalisierung-zentraler-grenzwertsatz-liapunov erwähnten Generalisierungen
der Zentralen Grenzwertsätze. Wir visualisieren @thm-summationstransformation
exemplarisch in @fig-summation.

```{r, echo = F, eval = F}
library(latex2exp)
pdf(file =  "./_figures/209-summation.pdf", width = 12, height = 5)                                                                      
par(                                                                   
family     = "sans",                                                 
mfcol      = c(1,3),                                                  
pty        = "m",                                                      
bty        = "l",                                                     
lwd        = 1,                                                       
las        = 1,                                                      
mgp        = c(2,1,0),                                                
xaxs       = "i",                                                    
yaxs       = "i",                                                     
cex        = 1.1,                                                     
font.main  = 1,                                                       
cex.main   = 1.2)

# outcome space of interest
x_min       = -5                                                                 
x_max       = 5                                                                   
x_res       = 1e3                                                                 
x           = seq(x_min, x_max, len = x_res)                                     

# \xi_1 sample
mu_1        = -2                                                                # \mu_1
sigsqr_1    = .5                                                                # \sigma_1^2
n           = 1e4                                                               # number of samples
X_1         = rnorm(n, mu_1, sqrt(sigsqr_1))                                    # n samples of \xi_1 \sim N(\mu_1,\sigma_1^2)
p_X_1       = dnorm(x, mu_1, sqrt(sigsqr_1))                                    # density
hist(
X_1,                                                                         
breaks = 50,                                                                
col   = "gray90",                                                           
prob  = TRUE,                                                               
xlim  = c(-5, 5),                                                           
ylim  = c(0,.7),                                                             
xlab  = expression("x"[1]),                                                                
ylab  = "",                                                                 
main  = TeX("$\\xi_1 \\sim N(-2.0,0.5)$"))                                      

lines(
x,                                                                          
p_X_1,                                                                      
lwd   = 2,                                                                  
col   = "darkorange")                                                       

# \xi_2 sample
mu_2        = 1                                                                 # \mu_2
sigsqr_2    = .7                                                                # \sigma_2^2
n           = 1e4                                                               # number of samples
X_2         = rnorm(n, mu_2, sqrt(sigsqr_2))                                    # n samples of \xi_2 \sim N(\mu_2,\sigma_2^2)
p_X_2       = dnorm(x, mu_2, sqrt(sigsqr_2))                                    # density
hist(
X_2,                                                                        
breaks = 50,                                                                
col   = "gray90",                                                           
prob  = TRUE,                                                               
xlim  = c(-5, 5),                                                           
ylim  = c(0,.7),                                                           
xlab  = expression("x"[2]),                                                 
ylab  = "",                                                                
main  = TeX("$\\xi_2 \\sim N(1.0,0.7)$"))
lines(
x,                                                                          
p_X_2,                                                                      
lwd   = 2,                                                                  
col   = "darkorange")                                                       

# Y = X_1 + \xi_2 sample
Y           = X_1 + X_2                                                           
p_Y        = dnorm(x, mu_1+mu_2, sqrt(sigsqr_1+sigsqr_2))                         
hist(
Y,                                                                          
breaks = 50,                                                                
col   = "gray90",                                                           
prob  = TRUE,                                                              
xlim  = c(-5, 5),                                                          
ylim  = c(0,.7),                                                           
xlab  = TeX("$x_1 + x_2$"),                                                
ylab  = "",                                                                 
main  = TeX("$\\xi_1 + \\xi_2 \\sim N(-1.0,1.2)$"))
lines(
x,                                                                          
p_Y,                                                                        
lwd   = 2,                                                                  
col   = "darkorange")
dev.off()
```

![Summation normalverteilter Zufallsvariablen.](./_figures/209-summation){#fig-summation fig-align="center" width=100%}

:::{#thm-mittelwerttransformation}
## Mittelwertstransformation

Für $i = 1,...,n$ seien $\xi_i \sim N(\mu,\sigma^2)$ unabhängig und identisch
normalverteilte Zufallsvariablen. Dann gilt für das Stichprobenmittel
$\bar{\xi}_n := \frac{1}{n}\sum_{i=1}^n \xi_i$ , dass
\begin{equation}
\bar{\xi}_n \sim N\left(\mu, \frac{\sigma^2}{n}\right).
\end{equation}
:::

:::{.proof}
Wir halten zunächst fest, dass mit dem Theorem zur Summe von unabhängig
normalverteilten Zufallsvariablen gilt, dass $\bar{\xi}_n = \frac{1}{n}\upsilon$ mit
$\upsilon := \sum_{i=1}^n \xi_i \sim N(n\mu,n\sigma^2)$. Einsetzen in @thm-univariates-wdf-transformationstheorem-bei-linear-affinen-abbildungen ergibt dann
\begin{align}
\begin{split}
p_{\bar{\xi}_n}(\bar{x}_n)
& = \frac{1}{|1/n|}N\left(n\bar{x}_n; n\mu , n\sigma^2 \right) \\
& = \frac{n}{\sqrt{2\pi n\sigma^2}}\exp\left(-\frac{1}{2n\sigma^2}
\left(n\bar{x}_n - n\mu\right)^2 \right) \\
& = \frac{n}{\sqrt{2\pi n\sigma^2}}\exp\left(-\frac{1}{2n\sigma^2}
\left(n\bar{x}_n - n\mu\right)^2 \right) \\
& = nn^{-\frac{1}{2}}\frac{1}{\sqrt{2\pi\sigma^2}}
\exp\left(
			-\frac{(n\bar{x}_n)^2}{2n\sigma^2}
		  	+ \frac{2(n\bar{x}_n)(n\mu)}{2n\sigma^2}
		  	- \frac{(n\mu)^2}{2n\sigma^2}
		 \right) \\
& = \sqrt{n}\frac{1}{\sqrt{2\pi\sigma^2}}
\exp\left(
			-\frac{n\bar{x}_n^2}{2\sigma^2}
		  	+ \frac{2n\bar{x}_n\mu}{2\sigma^2}
		  	- \frac{n\mu^2}{2\sigma^2}
		 \right) \\
& = \frac{1}{1/\sqrt{n}}\frac{1}{\sqrt{2\pi\sigma^2}}
\exp\left(
			-\frac{\bar{x}_n^2}{2(\sigma^2/n)}
		  	+ \frac{2\bar{x}_n\mu}{2(\sigma^2/n)}
		  	- \frac{\mu^2}{2(\sigma^2/n)}
		 \right) \\
& = \frac{1}{\sqrt{2\pi(\sigma^2/n)}}
\exp\left(-\frac{1}{2(\sigma^2/n)}
			(\bar{x}_n - \mu)^2
		 \right) \\
& = N\left(\bar{x}_n;\mu,\sigma^2/n \right)
\end{split}
\end{align}
:::

Wichtige Anwendungsfälle von @thm-mittelwerttransformation
sind die Analyse von Erwartungswertschätzern in @sec-punktschaetzung sowie die
sowie die in @eq-generalisierung-zentraler-grenzwertsatz-lindenberg-levy erwähnte Generalisierung
des Zentralen Grenzwertsatzes nach Lindenberg-Lévy. Wir visualisieren 
@thm-mittelwerttransformation exemplarisch in @fig-mittelwert.


```{r, echo = F, eval = F}
# figure setup
library(latex2exp)
pdf(file = "./_figures/209-mittelwert.pdf", width = 12, height = 5)                                                                      
par(                                                                   
family      = "sans",                                                 
mfcol       = c(1,2),                                                
pty         = "m",                                                   
bty         = "l",                                                   
lwd         = 1,                                                      
las         = 1,                                                      
mgp         = c(2,1,0),                                               
xaxs        = "i",                                                    
yaxs        = "i",                                                    
font.main   = 1,                                                      
cex         = 1.2,                                                   
cex.main    = 1.2)                                                     

# N(\mu,\sigma^2) sample
mu          = 2                                                                  
sigsqr      = 4                                                                 
sigma       = sqrt(sigsqr)                                                       
n           = 1e1                                                                
n_sim       = 1e4                                                                
x_min       = -5                                                                 
x_max       = 10                                                                  
x_res       = 1e3                                                                 
x           = seq(x_min, x_max, len = x_res)                                      
p_X         = dnorm(x,mu, sigma)                                                  
X           = matrix(rep(NaN,n*n_sim), nrow = n) 

# n_sim simulations of n samples of \xi \sim N(\mu,\sigma^2)
for(i in 1:n_sim){
    X[,i] = rnorm(n, mu, sqrt(sigsqr))                                           
}

# histogram
hist(
X,                                                                          
breaks = 50,                                                               
col   = "gray90",                                                          
prob  = TRUE,                                                               
xlim  = c(-5, 10),                                                          
ylim  = c(0,.7),                                                           
xlab  = "x",                                                               
ylab  = "",                                                                
main  = TeX("$\\xi_i \\sim N(2,4),\\, i = 1,...,10$"))

# density
lines(
x,                                                                          
p_X,                                                                        
lwd   = 2,                                                                 
col   = "darkorange")                                                       

X_bar       = colMeans(X)                                                         
p_X_bar     = dnorm(x, mu, sqrt(sigsqr/n))                                        

# histogram
hist(
X_bar,                                                                      
breaks = 50,                                                                
col   = "gray90",                                                           
prob  = TRUE,                                                               
xlim  = c(-5, 10),                                                          
ylim  = c(0,.7),                                                           
xlab  = TeX("$\\bar{x}_n$"),                                               
ylab  = "",                                                                
main  = TeX("$\\bar{\\xi}_n \\sim N(2,4/10)$"))

# density
lines(
x,                                                                        
p_X_bar,                                                                  
lwd   = 2,                                                                
col   = "darkorange")                                                      
dev.off()                                                                    
```

![Mittelwertbildung bei normalverteilten Zufallsvariablen.](./_figures/209-mittelwert){#fig-mittelwert fig-align="center" width=100%}

## $Z$-Transformation

Das @thm-z-transformation besagt, dass Subtraktion des Erwartungswertparameters 
und gleichzeitige Division mit der Wurzel des Varianzsparameters die Verteilung 
einer normalverteilten Zufallsvariable in eine Standardnormalverteilung transformiert.

:::{#thm-z-transformation}
## $Z$-Transformation
Es sei $\upsilon \sim N(\mu,\sigma^2)$ eine normalverteilte Zufallsvariable. Dann ist
die Zufallsvariable
\begin{equation}
Z := \frac{\upsilon - \mu}{\sigma}
\end{equation}
eine standardnormalverteilte Zufallsvariable, es gilt also $Z \sim N(0,1)$.
:::

:::{.proof}
Wir nutzen @thm-univariates-wdf-transformationstheorem-bei-linear-affinen-abbildungen.
Dazu halten wir zunächst fest, dass die $Z$-Transformation einer Funktion der Form
\begin{equation}
f(\upsilon) := \frac{\upsilon - \mu}{\sigma} =: Z
\end{equation}
entspricht. Wir stellen weiterhin fest, dass die Umkehrfunktion von $f$ durch
\begin{equation}
f^{-1}(Z) := \sigma Z + \mu
\end{equation}
gegeben ist, da für alle $z \in \mathbb{R}$ mit $z = {y - \mu}{\sigma}$ gilt, dass
\begin{equation}
\zeta^{-1}(z)
= \zeta^{-1}\left(\frac{y - \mu}{\sigma}\right)
= \frac{\sigma(y- \mu)}{\sigma} + \mu
= y - \mu + \mu
= y.
\end{equation}
Schließlich stellen wir fest, dass für die Ableitung $f'$ von $f$ gilt, dass
\begin{equation}
f'(y)
= \frac{d}{dy}\left(\frac{y - \mu}{\sigma} \right)
= \frac{d}{dy}\left(\frac{y}{\sigma} -\frac{\mu}{\sigma} \right)
= \frac{1}{\sigma}.
\end{equation}
Einsetzen in das univariate WDF Transformationstheorem für lineare Funktionen ergibt dann
\begin{align}
\begin{split}
p_Z(z)
& = \frac{1}{|1/\sigma|}N\left(\sigma z + \mu; \mu , \sigma^2 \right) \\
& = \frac{1}{1/\sqrt{\sigma^2}}\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{1}{2\sigma^2}\left(\sigma z + \mu - \mu\right)^2 \right) \\
& = \frac{\sqrt{\sigma^2}}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{1}{2\sigma^2}\sigma^2 z^2\right)\\
& = \frac{1}{\sqrt{2\pi}}\exp\left(-\frac{1}{2} z^2\right)\\
& = N(z;0,1)
\end{split}
\end{align}
also, dass $Z \sim N(0,1)$. 
:::
Wichtige Anwendungsfälle von @thm-z-transformation sind neben der häufig angewandten 
Standardisierung von normalverteilten Zufallsvariablen im Sinne der sogenannten 
*$Z$-Werte ($Z$-Scores)* die $Z$-Konfidenzintervallstatistik und die $Z$-Teststatistik.
Wir visualisieren @thm-z-transformation exemplarisch in @fig-z-transformation.

```{r, echo = F, eval = F}
# figure setup
library(latex2exp)
pdf(file = "./_figures/209-z-transformation.pdf", width = 5, height = 3)                                                                        
par(                                                                    
family     = "sans",                                                  
mfcol      = c(1,2),                                                  
pty        = "m",                                                     
bty        = "l",                                                     
lwd        = 1,                                                       
las        = 1,                                                       
mgp        = c(2,1,0),                                               
xaxs       = "i",                                                     
yaxs       = "i",                                                     
font.main  = 1,                                                      
cex        = .7,
cex.main   = 1)

# N(\mu,\sigma^2) sample
mu          = 2                                                                  # expectation parameter
sigsqr      = 3                                                                  # variance parameter
sigma       = sqrt(sigsqr)                                                       # standard deviation
n           = 1e5                                                                # number of samples
X           = rnorm(n ,mu, sqrt(sigsqr))                                         # n samples of \xi \sim N(\mu,\sigma^2)
x_min       = -5                                                                 # minimum x-value
x_max       = 10                                                                 # maximum x-value
x_res       = 1e3                                                                # x-space resolution
x           = seq(x_min, x_max, len = x_res)                                     # x-space
p_X         = dnorm(x,mu, sigma)                                                 # X density

# histogram
hist(
X,                                                                          
breaks = 50,                                                                
col   = "gray90",                                                           
prob  = TRUE,                                                               
xlim  = c(-5, 10),                                                          
ylim  = c(0,.42),                                                           
xlab  = TeX("y"),                                                               
ylab  = "",                                                                 
main  = TeX("$\\upsilon \\sim N(2,3)$"))

# density
lines(x,                                                                          
p_X,                                                                        
lwd   = 2,                                                                  
col   = "darkorange")                                                      

# Transformation der  N(\mu,\sigma^2) sample
Z           = (X - mu)/sigma                                                     # vector arithmetic
z_min       = -5                                                                 # minimum x-value
z_max       = 10                                                                 # maximum x-value
z_res       = 1e3                                                                # x-space resolution
z           = seq(z_min, z_max, len = z_res)                                     # x-space
p_Z         = dnorm(z, 0, 1)                                                     # Z density N(z;0,1)


# histogram
hist(
Z,                                                                          
breaks = 50,                                                                
col   = "gray90",                                                           
prob  = TRUE,                                                               
xlim  = c(-5, 10),                                                          
ylim  = c(0,.42),                                                          
xlab  = "z",                                                                
ylab  = "",                                                                 
main  = TeX("$Z = \\frac{\\upsilon - \\mu}{\\sigma} \\sim N(0,1)$"))

# density
lines(
z,                                                                          
p_Z,                                                                        
lwd   = 2,                                                                  
col   = "darkorange")   
dev.off()                                                                                                                        
```

![$Z$-Transformation normalverteilter Zufallsvariablen.](./_figures/209-z-transformation){#fig-z-transformation fig-align="center" width=100%}

## $\chi^2$-Transformation {#sec-chi-quadrat-transformation}

Mit der $\chi^2$-Transformation führen wir nun eine erste Transformation unabhängig
und identisch normalverteilter Zufallsvariablen ein, die *nicht* wiederrum auf
eine Normalverteilung führt. Speziell besagt @thm-chi2-transformation, dass 
die Summe quadrierter unabhängiger standardnormalverteilter Zufallsvariablen 
eine $\chi^2$-verteilte Zufallsvariable ist. Dazu erinnern wir zunächst an den
Begriff der $\chi^2$-Zufallsvariable als Spezialfall der in @sec-zufallsvariablen
betrachteten Gammazufallsvariablen (vgl. @def-gamma-zufallsvariable).

:::{#def-chi-quadrat-zufallsvariable}
## $\chi^2$-Zufallsvariable
$U$ sei eine Zufallsvariable mit Ergebnisraum $\mathbb{R}_{>0}$ und WDF
\begin{equation}
p : \mathbb{R}_{>0} \to \mathbb{R}_{>0},
u \mapsto p(u)
:= \frac{1}{\Gamma\left(\frac{n}{2}\right)2^{\frac{n}{2}}}
u^{\frac{n}{2}-1}\exp\left(-\frac{1}{2}u\right),
\end{equation}
wobei $\Gamma$ die Gammafunktion bezeichne. Dann sagen wir, dass $U$ einer
$\chi^2$-Verteilung mit Freiheitsgradparameter $n$ unterliegt und nennen $U$ eine
$\chi^2$-Zufallsvariable mit Freiheitsgradparameter $n$. Wir kürzen dies mit
$U \sim \chi^2(n)$ ab. Die WDF einer $\chi^2$-Zufallsvariable bezeichnen wir mit
\begin{equation}
\chi^2(u;n) :=
\frac{1}{\Gamma\left(\frac{n}{2}\right)2^{\frac{n}{2}}}
u^{\frac{n}{2}-1}\exp\left(-\frac{1}{2}u\right).
\end{equation}
:::
Wir erinnern daran, dass die WDF der $\chi^2$-Verteilung der WDF $G\left(u;\frac{n}{2},2\right)$ 
einer Gammaverteilung entspricht. In @fig-chi2-wdf visualisieren wir exemplarisch
einige WDFen von $\chi^2$-Zufallsvariablen. Wir beobachten, dass mit ansteigendem
$n$ sich  $\chi^2(u;n)$ verbreiter und  Wahrscheinlichkeitsmasse zur größeren Werten
von $u$ verschoben wird.

```{r, eval = F, echo = F}
library(latex2exp)
pdf(file = "./_figures/209-chi2-wdf.pdf", width = 6,height = 4)                                                                        
par(                                                                    
family     = "sans",                                                  
pty        = "m",                                                     
bty        = "l",                                                     
lwd        = 1,                                                       
las        = 1,                                                       
mgp        = c(2,1,0),                                                
xaxs         = "i",                                                   
yaxs         = "i",                                                   
font.main  = 1,                                                       
cex.main   = 1.2)

# chi^2 space
u_min   = 1e-5                                                                   # minimum z-value
u_max   = 10                                                                     # maximum z-value
u_res   = 1e3                                                                    # z-space resolution
u       = seq(u_min,u_max, len = u_res)                                          # z-space

# parameters of interest
n       = c(1,2,3,5,10)                                                          # degrees of freedom

# plotting
matplot(
u,
matrix(
c(dchisq(u,n[1]),
dchisq(u,n[2]),
dchisq(u,n[3]),
dchisq(u,n[4]),
dchisq(u,n[5])),
ncol  = 5),
type  = "l",                                                       
lty   = 1,                                                         
lwd   = 2,                                                         
col   = c("gray90","gray70", "gray50", "gray30", "gray10"),        
ylim  = c(0,.6),                                                    
xlim  = c(u_min,u_max),                                            
ylab  = " ",                                                       
xlab  = "u",                                                       
main  = TeX("$\\chi^2(u;n)$"))                                     

legend(
7,                                                                        
.6,                                                                       
c("n = 1", "n = 2", "n = 3", "n = 5", "n = 10"),                          
lty         = 1,                                                          
lwd         = 2,                                                          
col         =  c("gray90","gray70", "gray50", "gray30", "gray10"),        
bty         = "n",                                                         
cex         = 1.1,                                                        
y.intersp   = 1)                                                         
dev.off()                                                                     
```

![WDFen von $\chi^2$ Zufallsvariablen.](./_figures/209-chi2-wdf){#fig-chi2-wdf fig-align="center" width=80%}

:::{#thm-chi2-transformation}
## $\chi^2$-Transformation
$Z_1,...,Z_n \sim N(0,1)$ seien unabhängig und identisch standardnormalverteilte 
Zufallsvariablen. Dann ist die Zufallsvariable
\begin{equation}
U := \sum_{i=1}^n Z_i^2
\end{equation}
eine $\chi^2$-verteilte Zufallsvariable mit Freiheitsgradparameter $n$, es gilt also
$U \sim \chi^2(n)$. Insbesondere gilt für $Z \sim N(0,1)$ und $U := Z^2$, dass
$U \sim \chi^2(1)$.
:::

:::{.proof}
Wir zeigen das Theorem nur für den Fall $n := 1$ mithilfe von  @thm-univariate-wdf-transformation-bei-stückweise-bijektiven-abbildungen. 
Danach ist die WDF einer Zufallsvariable $U := f(Z)$, welche aus der Transformation einer
Zufallsvariable $Z$ mit WDF $p_\zeta$ durch eine stückweise bijektive Abbildung
hervorgeht, gegeben durch
\begin{equation}\label{eq:piecewise_pdf_transform}
p_U(u) = \sum_{i=1}^k 1_{\mathcal{U}_i} \frac{1}{|f'_i(f_i^{-1}(u))|}p_\zeta\left(f_i^{-1} (u)\right).
\end{equation}
Wir definieren
\begin{equation}
\mathcal{U}_1 := ]-\infty,0[,
\mathcal{U}_2 := ]0,\infty[, \mbox{ und }
\mathcal{U}_i := \mathbb{R}_{>0} \mbox{ für } i = 1,2,
\end{equation}
sowie
\begin{equation}
f_i : \mathcal{Z}_i \to \mathcal{U}_i, x \mapsto f_i(z) := z^2 =: u \mbox{ für } i = 1,2.
\end{equation}
Die Ableitung und die Umkehrfunktion der $f_i$ ergeben sich zu
\begin{equation}
f_i' : \mathcal{Z}_i \to \mathcal{Z}_i, x \mapsto f_i'(z) = 2z \mbox{ für } i = 1,2,
\end{equation}
und
\begin{equation}
f_1^{-1} : \mathcal{U}_1 \to \mathcal{U}_1, u \mapsto f_1^{-1}(u) = - \sqrt{u}
\mbox{ und }
f_2^{-1} : \mathcal{U}_2 \to \mathcal{U}_2, u \mapsto f_2^{-1}(u) = \sqrt{u},
\end{equation}
respektive.
Einsetzen in Gleichung \eqref{eq:piecewise_pdf_transform} ergibt dann
\begin{align}
\begin{split}
p_U(u)
& = 1_{\mathcal{U}_1}(u) \frac{1}{|f'_1(f_1^{-1}(u))|}p_\zeta\left(f_1^{-1} (u)\right)
  + 1_{\mathcal{U}_2}(u) \frac{1}{|f'_2(f_2^{-1}(u))|}p_\zeta\left(f_2^{-1} (u)\right) \\
& = \frac{1}{|2(-\sqrt{u})|}\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{1}{2}(-\sqrt{u})^2\right)
  + \frac{1}{|2( \sqrt{u})|}\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{1}{2}( \sqrt{u})^2\right) \\
& = \frac{1}{2\sqrt{u}}\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{1}{2}u\right)
  + \frac{1}{2\sqrt{u}}\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{1}{2}u\right)\\
& = \frac{1}{\sqrt{2\pi}}\frac{1}{\sqrt{u}}\exp\left(-\frac{1}{2}u\right).
\end{split}
\end{align}
Andererseits gilt, dass mit $\Gamma\left(\frac{1}{2}\right) = \sqrt{\pi}$, die PDF einer $\chi^2$-Zufallsvariable $U$ mit $n = 1$ durch
\begin{equation}
\frac{1}{\Gamma\left(\frac{1}{2}\right)2^{\frac{1}{2}}} u^{\frac{1}{2}-1}\exp\left(-\frac{1}{2}u\right)
= \frac{1}{\sqrt{2\pi}}\frac{1}{\sqrt{u}}\exp\left(-\frac{1}{2}u\right)
\end{equation}
gegeben ist. Also gilt, dass wenn $Z \sim N(0,1)$ ist, dann ist $U := Z^2 \sim \chi^2(1)$.
:::

Wichtige Anwendungsfälle sind die $U$-Konfidenzintervallstatistik sowie die
im folgenden eingeführten $t$- und $f$-Zufallsvariablen. Wir visualisieren 
@thm-chi2-transformation exemplarisch in @fig-chi2-transform.

```{r, echo = F, eval = F}
pdf(file = "./_figures/209-chi2-transform.pdf", width = 10, height = 5)                                                                         
library(latex2exp)
par(                                                                    
family     = "sans",                                                  
mfcol      = c(1,2),                                                 
pty        = "m",                                                     
bty        = "l",                                                     
lwd        = 1,                                                       
las        = 1,                                                      
mgp        = c(2,1,0),                                               
xaxs         = "i",                                                   
yaxs         = "i",                                                   
font.main  = 1,                                                       
cex        = 1.2,
cex.main   = 1.3)                                                      

# Z sample
n           = 1e5                                                                # number of samples
Z           = rnorm(n, 0 ,1)                                                     # n samples of Z \sim N(0,1)
z_min       = -5                                                                 # minimum z-value
z_max       = 5                                                                  # maximum z-value
z_res       = 1e3                                                                # z-space resolution
z           = seq(z_min, z_max, len = z_res)                                     # z-space
p_Z         = dnorm(z, 0, 1)                                                     # z density

# histogram
hist(
Z,                                                                         
breaks = 50,                                                                
col   = "gray90",                                                           
prob  = TRUE,                                                               
xlim  = c(-5, 10),                                                         
ylim  = c(0,.42),                                                          
xlab  = "z",                                                                
ylab  = "",                                                                 
main  = TeX("$Z \\sim N(0,1)$"))

# density
lines(
z,                                                                          
p_Z,                                                                        
lwd   = 2,                                                                 
col   = "darkorange")                                                      

# Transformation der N(0,1) sample
U           = Z^2                                                                # vector arithmetic
u_min       = -5                                                                 # minimum u-value
u_max       = 10                                                                 # maximum u-value
u_res       = 1e3                                                                # u-space resolution
u           = seq(u_min, u_max, len = u_res)                                     # u-space
p_U         = dchisq(u,1)                                                        # Chi^2 density chi^2(u;1)


# histogram
hist(
U,                                                                         
breaks= 100,                                                                
col   = "gray90",                                                           
prob  = TRUE,                                                               
xlim  = c(-5, 10),                                                         
ylim  = c(0,1),                                                           
xlab  = "x",                                                                
ylab  = "",                                                                 
main  = TeX("$U = Z^2 \\sim \\chi^2(1)$"))                                 

# density
lines(
u,                                                                         
p_U,                                                                        
lwd   = 2,                                                                  
col   = "darkorange")                                                       
dev.off()                                                                     
```
![$\chi^2$-Transformation normalverteilter Zufallsvariablen.](./_figures/209-chi2-transform){#fig-chi2-transform fig-align="center" width=100%}


## $T$-Transformation {#sec-t-transformation}

Das in diesem Abschnitt betrachtete Theorem geht auf @student1908 zurück und ist das 
zentrale und stilprägende Resultat der Entwicklung der Frequentistischen Inferenz 
in der ersten Hälfte der 20. Jahrhunderts. @hald2007 und @zabell2008 und geben 
hierzu einen historischen Überblick. Das zentrale @thm-t-transformation besagt dabei, 
dass die Zufallsvariable, die sich durch Division einer standardnormalverteilten 
Zufallsvariable durch die Quadratwurzel einer $\chi^2$-verteilten Zufallsvariable 
geteilt durch ein $n$, ergibt, eine $t$-verteilte Zufallsvariable ist. Dabei ist
eine $t$-verteilte Zufallsvariable wie folgt definiert. 

:::{#def-t-zufallsvariable}
## $t$-Zufallsvariable
$T$ sei eine Zufallsvariable mit Ergebnisraum $\mathbb{R}$ und WDF
\begin{equation}
p : \mathbb{R} \to \mathbb{R}_{>0}, t \mapsto p(t)
:= \frac{\Gamma\left(\frac{n+1}{2}\right)}{\sqrt{n\pi}\Gamma\left(\frac{n}{2}\right)}
\left(1 + \frac{t^2}{n} \right)^{-\frac{n+1}{2}},
\end{equation}
wobei $\Gamma$ die Gammafunktion bezeichne. Dann sagen wir, dass $T$ einer
$t$-Verteilung mit Freiheitsgradparameter $n$ unterliegt und nennen $T$ eine $t$-Zufallsvariable
mit Freiheitsgradparameter $n$. Wir kürzen dies mit $T \sim t(n)$ ab. Die WDF einer
$t$-Zufallsvariable bezeichnen wir mit
\begin{equation}
T(t;n) := \frac{\Gamma\left(\frac{n+1}{2}\right)}{\sqrt{n\pi}\Gamma\left(\frac{n}{2}\right)}
\left(1 + \frac{t^2}{n} \right)^{-\frac{n+1}{2}}.
\end{equation}
:::

In @fig-t-wdf visualisieren wir exemplarisch einige WDFen von $t$-Zufallsvariablen. 
Wir beobachten, dass die $t$-Verteilung immer um $0$ symmetrisch ist und ein 
steigendes $n$ Wahrscheinlichkeitsmasse aus den Ausläufen zum Zentrum verschiebt.
Wir merken an, dass ab etwa $n = 30$ gilt, dass $T(t;n) \approx N(0,1)$.

```{r, echo = F, eval = F}
pdf(file  = "./_figures/209-t-wdf.pdf", width = 6, height = 5)                                                                         
par(                                                                    
family     = "sans",                                                  
pty        = "m",                                                     
bty        = "l",                                                     
lwd        = 1,                                                       
las        = 1,                                                       
mgp        = c(2,1,0),                                                
xaxs       = "i",                                                    
yaxs       = "i",                                                     
font.main  = 1,                                                       
cex        = 1.1,
cex.main   = 1.4)

# t space
t_min   = -5                                                                     # minimum t-value
t_max   = 5                                                                      # maximum t-value
t_res   = 1e3                                                                    # t-space resolution
t       = seq(t_min,t_max, len = t_res)                                          # t-space

# parameters of interest
n       = c(2,3,5,10,30)                                                        # degrees of freedom

# plotting
matplot(
t,
matrix(
c(dt(t,n[1]),
dt(t,n[2]),
dt(t,n[3]),
dt(t,n[4]),
dt(t,n[5])),
ncol = 5),
type         = "l",                                                      
lty          = 1,                                                          
lwd          = 2,                                                         
col          = c("gray90","gray70", "gray50", "gray30", "gray10"),       
ylim         = c(0,.4),                                                   
xlim         = c(t_min,t_max),                                            
ylab         = " ",                                                       
xlab         = "t",                                                       
main         = TeX("$\\T(t;n)$"))                                         

legend(
2,                                                                         
.4,                                                                       
c("n = 2", "n = 3", "n = 5", "n = 10", "n = 30"),                         
lty         = 1,                                                          
lwd         = 2,                                                          
col         =  c("gray90","gray70", "gray50", "gray30", "gray10"),        
bty         = "n",                                                       
cex         = 1,                                                        
y.intersp   = 1)                                                        
dev.off()                                                                   
```

![WDFen von $T$-Zufallsvariablen.](./_figures/209-t-wdf){#fig-t-wdf fig-align="center" width=60%}

:::{#thm-t-transformation}
## $T$-Transformation
$Z \sim N(0,1)$ sei eine standarnormalverteilte Zufallsvariable, $U \sim \chi^2(n)$ 
sei eine $\chi^2$-Zufallsvariable mit Freiheitsgradparameter $n$, und $Z$ und $U$ seien
unabhängig. Dann ist die Zufallsvariable
\begin{equation}
T := \frac{Z}{\sqrt{U/n}}
\end{equation}
eine $t$-verteilte Zufallsvariable mit Freiheitsgradparameter $n$, es gilt also $T \sim t(n)$.
:::

:::{.proof}
Wir halten zunächst fest, dass die zweidimensionale WDF der gemeinsamen
(unabhängigen) Verteilung von $Z$ und $U$ durch
\begin{equation}
p_{Z,U}(z,u)
=
\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{1}{2}z^2\right)
\frac{1}{\Gamma(\frac{n}{2})2^{\frac{n}{2}}}u^{\frac{n}{2}-1} \exp\left(-\frac{1}{2}u\right).
\end{equation}
gegeben ist. Wir betrachten dann die multivariate vektorwertige Abbildung
\begin{equation}
f : \mathbb{R}^2 \to \mathbb{R}^2,
(z,u)
\mapsto
f(z,u)
:=
\left(\frac{z}{\sqrt{u/n}},u\right)
=:
(t,w)
\end{equation}
und benutzen das multivariate WDF Transformationstheorem für bijektive Abbildungen
um die WDF von $(t,w)$ herzuleiten. Dazu erinnern wir uns, dass wenn $\xi$ ein
$n$-dimensionaler Zufallsvektor mit WDF $p_\xi$ und $\upsilon := f(\xi)$ für eine
differenzierbare und bijektive Abbildung $f : \mathbb{R}^n \to \mathbb{R}^n$ ist,
die WDF des Zufallsvektors $\upsilon$ durch
\begin{equation}\label{eq:pdftmv}
p_\upsilon : \mathbb{R}^n \to \mathbb{R}_{\ge 0},
y \mapsto p_\upsilon(y) :=
\frac{1}{|J^f\left(f^{-1}(y)\right)|}p_\xi\left(f^{-1}(y)\right)
\end{equation}
gegeben ist. Für die im vorliegenden Fall betrachtete Abbildung halten wir zunächst fest, dass
\begin{equation}
f^{-1}:\mathbb{R}^2 \to \mathbb{R}^2,
(t,w)
\mapsto
f^{-1}
(t,w)
:=\left(\sqrt{w/n}t, w\right).
\end{equation}
Dies ergibt sich direkt aus
\begin{equation}
f^{-1}(f(z,u))
=
f^{-1}\left(\frac{z}{\sqrt{u/n}},u\right)
=
\left(\frac{\sqrt{u/n}z}{\sqrt{u/n}}, u \right)
=
(z,u)
\mbox{ für alle }
(z,u)
\in \mathbb{R}^2.
\end{equation}
Wir halten dann fest, dass die Determinante der Jacobi-Matrix von $f$ an der Stelle $(z,u)$ durch
\begin{equation}
|J^f(z,u)|
=
\begin{vmatrix}
  \frac{\partial}{\partial z} \left(\frac{z}{\sqrt{u/n}}\right)
& \frac{\partial}{\partial u} \left(\frac{z}{\sqrt{u/n}}\right) \\
  \frac{\partial}{\partial z} u
& \frac{\partial}{\partial u} u\\
\end{vmatrix}
= \left(\frac{v}{n}\right)^{-1/2},
\end{equation}
gegeben ist, sodass folgt, dass
\begin{equation}
\frac{1}{|J^f\left(f^{-1}(z,u)\right)|}
= \left(\frac{w}{n}\right)^{1/2}.
\end{equation}
Einsetzen in Gleichung \eqref{eq:pdftmv} ergibt dann
\begin{equation}
p_{T,W}(t,w) = \left(\frac{w}{n}\right)^{1/2}p_{Z,V}\left(\sqrt{w/n}t,w\right),
\end{equation}
Es folgt also
\begin{align}
\begin{split}
p_T(t)
& =
\int_0^\infty  p_{T,W}(t,w)
\,dw 													\\
& =
\int_0^\infty
\left(\frac{w}{n}\right)^{1/2}
p_{Z,V}\left(\sqrt{w/n}t,w\right)
\,dw  \\
& =
\int_0^\infty
\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{1}{2}(\sqrt{w/n}t)^2\right)
\frac{1}{\Gamma(\frac{n}{2})2^{\frac{n}{2}}}w^{\frac{n}{2}-1} \exp\left(-\frac{1}{2}w\right)
\left(\frac{w}{n}\right)^{1/2}
\,dw \\
& =
\frac{1}{\sqrt{2\pi}}\frac{1}{\Gamma(\frac{n}{2})2^{\frac{n}{2}}n^{\frac{1}{2}}}
\int_0^\infty
\exp\left(-\frac{1}{2}\frac{w}{n}t^2\right)
w^{\frac{n}{2}-1} \exp\left(-\frac{1}{2}w\right)w^{1/2}
\,dw \\
& =
\frac{1}{\sqrt{2\pi}}\frac{1}{\Gamma(\frac{n}{2})2^{\frac{n}{2}}n^{\frac{1}{2}}}
\int_0^\infty
\exp\left(-\frac{1}{2}\frac{w}{n}t^2 -\frac{1}{2}w\right)
w^{\frac{n}{2}-1} w^{\frac{1}{2}}
\,dw \\
& =
\frac{1}{\sqrt{2\pi}}\frac{1}{\Gamma(\frac{n}{2})2^{\frac{n}{2}}n^{\frac{1}{2}}}
\int_0^\infty
\exp\left(-\frac{1}{2}\left(\frac{w}{n}t^2 + w\right)\right)
w^{\frac{n + 1}{2}-1}
\,dw \\
& =
\frac{1}{\sqrt{2\pi}}\frac{1}{\Gamma(\frac{n}{2})2^{\frac{n}{2}}n^{\frac{1}{2}}}
\int_0^\infty
\exp\left(-\frac{1}{2}\left(1 + \frac{t^2}{n}\right)\right)
w^{\frac{n + 1}{2}-1}
\,dw \\
\end{split}
\end{align}
Wir stellen dann fest, dass der Integrand auf der linken Seite der obigen Gleichung
dem Kern einer Gamma WDF mit Parametern  $\alpha = \frac{n+1}{2}$ und
$\beta = \frac{2}{1+\frac{t^2}{n}}$ entspricht, wie man leicht einsieht:
\begin{align*}
\Gamma(w;\alpha,\beta)
= \frac{1}{\Gamma(\alpha)\beta^{\alpha}}w^{\alpha-1}\exp\left(-\frac{w}{\beta}\right) & \\
\Rightarrow
\Gamma\left(w;\frac{n+1}{2},\frac{2}{1+\frac{t^2}{n}}\right)
& = \frac{1}{\Gamma(\frac{n+1}{2})\left(\frac{2}{1+\frac{t^2}{n}}\right)^{\frac{n+1}{2}}}
w^{\frac{n+1}{2}-1}\exp\left(-\frac{w}{\frac{2}{1+\frac{t^2}{n}}}\right) \\
& = \frac{1}{\Gamma( \frac{n+1}{2})\left(\frac{2}{1+\frac{t^2}{n}}\right)^{ \frac{n+1}{2}}}
\exp\left(-\frac{1}{2}\left(1 + \frac{t^2}{n}\right)\right) w^{\frac{n+1}{2}-1}.
\end{align*}
Es ergibt sich also
\begin{equation}
p_T(t)
=
\frac{1}{\sqrt{2\pi}}\frac{1}{\Gamma(\frac{n}{2})2^{\frac{n}{2}}n^{\frac{1}{2}}}
\int_0^\infty
\Gamma\left(w;\frac{n+1}{2},\frac{2}{1+\frac{t^2}{n}}\right)
\,dw .
\end{equation}
Schließlich stellen wir fest, dass der Integralterm in obiger Gleichung dem
Normalisierungsterm einer Gamma WDF entspricht. Abschließend ergibt sich also
\begin{equation}
p_T(t) =
\frac{1}{\sqrt{2\pi}}\frac{1}{\Gamma(\frac{n}{2})2^{\frac{n}{2}}n^{\frac{1}{2}}}
\Gamma\left(\frac{n+1}{2}\right)\left(\frac{2}{1 + \frac{t^2}{n}} \right)^{\frac{n+1}{2}}.
\end{equation}
Die Verteilung von $Z/\sqrt{U/n}$ hat also die WDF einer $t$-Zufallsvariable.
:::

Wichtige Anwendungsfälle sind die $T$-Konfidenzintervallstatistik sowie die 
$T$-Teststatistiken der Theorie von Hypothesentests im Kontext des Allgemeinen 
Linearen Modells. Wir visualisieren @thm-t-transformation exemplarisch in @fig-t-transform.

```{r, eval = F, echo = F}
pdf(file = "./_figures/209-t-transform.pdf", width = 6, height = 2)                                                                        
library(latex2exp)
par(                                                                    
family     = "sans",                                                  
mfcol      = c(1,3),                                                  
pty        = "m",                                                     
bty        = "l",                                                     
lwd        = .5,                                                       
las        = 1,                                                       
mgp        = c(2,1,0),                                                
xaxs       = "i",                                                     
yaxs       = "i",                                                     
font.main  = 1,                                                      
cex        = 0.5,
cex.main   = 1.1)

# Z sample
n           = 1e4                                                                # number of samples
Z           = rnorm(n, 0 ,1)                                                     # n samples of Z \sim N(0,1)
z_min       = -5                                                                 # minimum z-value
z_max       = 5                                                                  # maximum z-value
z_res       = 1e3                                                                # z-space resolution
z           = seq(z_min, z_max, len = z_res)                                     # z-space
p_Z         = dnorm(z, 0, 1)                                                     # z density

# histogram
hist(
Z,                                                                         
breaks = 50,                                                               
col   = "gray90",                                                           
prob  = TRUE,                                                               
xlim  = c(-5, 5),                                                           
ylim  = c(0,.42),                                                           
xlab  = "z",                                                                
ylab  = "",                                                                 
main  = TeX("$Z \\sim N(0,1)$"))                                             

# density
lines(
z,                                                                          
p_Z,                                                                       
lwd   = 2,                                                                  
col   = "darkorange")                                                       

# chi^2 sample
df          = 3                                                                  # degrees of freedom
U           = rchisq(n,df)                                                       # n samples of U \sim \chi^2(3)
u_min       = 1e-5                                                               # minimum z-value
u_max       = 10                                                                 # maximum z-value
u_res       = 1e3                                                                # u-space resolution
u           = seq(u_min, u_max, len = u_res)                                     # u-space
p_U         = dchisq(u,df)                                                       # u density

# histogram
hist(
U,                                                                          
breaks= 100,                                                                
col   = "gray90",                                                           
prob  = TRUE,                                                               
xlim  = c(0, 10),                                                           
ylim  = c(0,0.3),                                                           
xlab  = "u",                                                                
ylab  = "",                                                                 
main  = TeX("$U \\sim \\chi^2(3)$"))                                         

# density
lines(
u,                                                                          
p_U,                                                                       
lwd   = 2,                                                                  
col   = "darkorange")                                                       

# T-transformation
Tee         = Z/(sqrt(U/df))                                                     # element-wise vector arithmetic
Tee         = Tee[!abs(Tee) > 10]                                                # mildly censored sample for good histogram performance
t_min       = -4                                                                 # minimum t-value
t_max       = 4                                                                  # maximum t-value
t_res       = 1e3                                                                # t-space resolution
t           = seq(z_min, z_max, len = z_res)                                     # t-space
p_T         = dt(z,df)                                                           # t density

# histogram
hist(
Tee,                                                                        
breaks      = 100,                                                                
col         = "gray90",                                                           
prob        = TRUE,                                                               
xlim        = c(t_min, t_max),                                                     
ylim        = c(0,0.4),                                                           
xlab        = "t",                                                                
ylab        = "",                                                                 
main        = TeX("$T = Z/\\sqrt{U/n}\\sim \\t(3)$"))                        
 
# density
lines(
t,                                                                          
p_T,                                                                       
lwd         = 2,                                                                  
col         = "darkorange")                                                       
dev.off()                                                                 
```

![$T$-Transformation normalverteilter Zufallsvariablen.](./_figures/209-t-transform){#fig-t-transform fig-align="center" width=100%}

## Nichtzentrale $T$-Transformation {#sec-nichtzentrale-t-transformation}

In diesem Abschnitt betrachten wir den Fall, dass der Erwartungswertparameter
der Zählervariable der in @thm-t-transformation betrachteten Zufallsvariable $T$
von Null verschieden ist, dass es sich bei der Zählervariable also nicht um eine
nach $N(0,1)$, sondern eine nach $N(\mu,1)$ verteilte Zufallsvariable für ein 
beliebiges $\mu \in \mathbb{R}$ handelt. Die so entstehende Zufallsvariable $T$
folgt dann einer sogenannten *nichtzentralen-t-Verteilung*. Eine frühe ausführliche
Diskussion dieser Verteilung findet sich zum Beispiel in @johnson1940. Eine entsprechende
nichtzentralen-$t$-verteilte Zufallsvariable ist wie folgt definiert (vgl. @lehmann1986).

:::{#def-nichtzentrale-$t$-zufallsvariable}
## Nichtzentrale $t$-Zufallsvariable
$T$ sei eine Zufallsvariable mit Ergebnisraum $\mathbb{R}$ und WDF
\begin{multline}
p : \mathbb{R} \to \mathbb{R}_{>0}, t \mapsto p(t) :=
\frac{1}{2^{\frac{n-1}{2}}\Gamma\left(\frac{n}{2} \right)(n \pi)^{\frac{1}{2}}} \\
\times \int_{0}^\infty \tau^{\frac{n-1}{2}} \exp\left(-\frac{\tau}{2}\right)
\exp\left(-\frac{1}{2}\left(t \left(\frac{\tau}{n}\right)^{\frac{1}{2}} - \delta \right)^2 \right)\,d\tau.
\end{multline}
Dann sagen wir, dass $T$ einer nichtzentralen $t$-Verteilung mit 
Nichtzentralitätsparameter $\delta$ und Freiheitsgradparameter $n$ unterliegt 
und nennen $T$ eine *nichtzentrale $t$-Zufallsvariable mit Nichtzentralitätsparameter $\delta$ und Freiheitsgradparameter $n$*. Wir kürzen dies mit $t(\delta, n)$ ab. 
Die WDF einer nichtzentralen $t$-Zufallsvariable bezeichnen wir mit
$t(T;\delta,n)$. Die KVF und inverse KVF einer nichtzentralen $t$-Zufallsvariable
bezeichnen wir mit $\Psi(\cdot; \delta, n)$ und $\Psi^{-1}(\cdot; \delta, n)$, respektive.
:::

Ohne Beweis merken wir an, dass eine nichtzentrale $t$-Zufallsvariable mit 
$\delta = 0$ einer $t$-Zufallsvariable enstpricht, es gelten also
\begin{equation}
t(T;0,n) = t(T;n)
\end{equation}
sowie 
\begin{equation}
\Psi(T;0,n) = \Psi(T;n)  \mbox{ und } \Psi^{-1}(T;0,n) = \Psi^{-1}(T;n).
\end{equation}

In @fig-nichtzentrale-t-wdf visualisieren wir exemplarisch einige WDFen von nichtzentralen 
$t$-Zufallsvariablen. Wir beobachten, dass ein positiver Nichtzentralitätsparameter
$\delta$ die Verteilung nach rechts verschiebt und die Verteilungen mit steigendem
Freiheitsgradparameter $\delta$ sich entsprechend lokalisierten Normalverteilungen mit
Varianzparameter 1 annähern. Man beachte auch die Nichtsymmetrie der WDFen für 
kleine Freiheitsgradparameter bei von Null verschiedenem positivem Nichtzentralitätsparameter. 

```{r, echo = F, eval = F}
options(warn=-1)                                                                # warning off
t_min     = -5                                                                  # Minimum T-Wert
t_max     = 30                                                                  # Maximum T-Wert
t_res     = 1e3                                                                 # T-Wert Auflösung
t         = seq(t_min, t_max, len = t_res)                                      # T-Raum
delta     = c(0,5,15)                                                           # Nichtzentralitätsparameter
n         = c(5, 30)                                                            # Freiheitsgrade
p         = cbind(
            matrix(dt(t, n[1], delta[1]),nrow=length(t)),
            matrix(dt(t, n[2], delta[1]),nrow=length(t)),
            matrix(dt(t, n[1], delta[2]),nrow=length(t)),
            matrix(dt(t, n[2], delta[2]),nrow=length(t)),
            matrix(dt(t, n[1], delta[3]),nrow=length(t)),
            matrix(dt(t, n[2], delta[3]),nrow=length(t)))

# Visualisierung
pdf(
file        = "./_figures/209-nichtzentrale-t-wdf.pdf",
width       = 7,
height      = 4.5)
library(latex2exp)
par(
family      = "sans",
pty         = "m",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2,1,0),
xaxs        = "i",
yaxs        = "i",
font.main   = 1,
cex         = 1,
cex.main    = 1.2)
matplot(
t,
p,
type        = "l",
lty         = c(1,2,1,2,1,2),
col         = c("gray10", "gray10", "gray50", "gray50", "gray70", "gray70"),
lwd         = 2,
xlab        = "T",
ylab        = "",
ylim        = c(0,.4),
main        = TeX("$t(T;\\delta,n)$"))
legend(
18,
.4,
c(TeX("$\\delta = 0 , \\,\\,\\, n = 5$"),
  TeX("$\\delta = 0 , \\,\\,\\, n = 30$"),
  TeX("$\\delta = 5 , \\,\\,\\, n = 5$"),
  TeX("$\\delta = 5 , \\,\\,\\, n = 30$"),
  TeX("$\\delta = 15, \\, n = 5$"),
  TeX("$\\delta = 15, \\, n = 30$")),
lty         = c(1,2,1,2,1,2),
col         = c("gray10", "gray10", "gray50", "gray50", "gray70", "gray70"),
lwd         = 2,
bty         = "n",
seg.len     = 1.75)
dev.off()
```

![WDFen von Nichtzentralen-$T$-Zufallsvariablen.](./_figures/209-nichtzentrale-t-wdf){#fig-nichtzentrale-t-wdf fig-align="center" width=80%}

Eine nichtzentrale $t$-Zufallsvariable ist das Resultat einer nichtzentralen $T$-Transformation, wie folgendes Theorem besagt.

:::{#thm-nichtzentrale-t-transformation}
## Nichtzentrale T-Transformation
$\upsilon \sim N(\mu,1)$ sei eine normalverteilte Zufallsvariable, $U \sim \chi^2(n)$
sei eine $\chi^2$ Zufallsvariable mit Freiheitsgradparameter $n$, und $\upsilon$ und
$U$ seien unabhängige Zufallsvariablen. Dann ist die Zufallsvariable
\begin{equation}
T := \frac{\upsilon}{\sqrt{U/n}}
\end{equation}
eine nichtzentrale $t$-Zufallsvariable mit Nichtzentralitätsparameter $\mu$ und
Freiheitsgradparameter $n$, also $T \sim t(\mu,n)$.
:::

Wir verzichten auf einen Beweis. Wichtige Anwendungsfälle sind die Testgütefunktionen
der T-Test Varianten im Kontext des Allgemeinen Linearen Modells. Wir visualisieren @thm-nichtzentrale-t-transformation exemplarisch in @fig-nichtzentrale-t-transformation.

```{r, echo = F, eval = F}
library(latex2exp)
pdf(
file        = "./_figures/209-nichtzentrale-t-transformation.pdf",
width       = 7,
height      = 3)
par(
family      = "sans",
mfcol       = c(1,3),
pty         = "m",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2,1,0),
xaxs        = "i",
yaxs        = "i",
font.main   = 1,
cex.main    = 1.2)

# simulation parameters
mu          = 5                                                                  # expectation parameter
n           = 5                                                                  # degrees of freedom parameter
ns          = 1e5                                                                # number of samples


# X sample
X           = rnorm(ns,mu,1)                                                     # ns samples of X \sim N(0,1)
x_min       = 0                                                                  # minimum x-value
x_max       = 10                                                                 # maximum x-value
x_res       = 1e3                                                                # x-space resolution
x           = seq(x_min, x_max, len = x_res)                                     # x-space
p_X         = dnorm(x,mu, 1)                                                     # x density

# histogram and density
hist(
X,
breaks      = 50,
col         = "gray90",
prob        = TRUE,
xlim        = c(x_min,x_max),
ylim        = c(0,.5),
xlab        = "y",
ylab        =  "",
main        = TeX("$y \\sim N(5,1)$"))
lines(
x,
p_X,
lwd         = 2,
col         = "darkorange")

# chi^2 sample
U           = rchisq(ns,n)                                                       # ns samples of U \sim \chi^2(n)
u_min       = 1e-5                                                               # minimum z-value
u_max       = 20                                                                 # maximum z-value
u_res       = 1e3                                                                # u-space resolution
u           = seq(u_min, u_max, len = u_res)                                     # u-space
p_U         = dchisq(u,n)                                                        # u density

# histogram and density
hist(
U,
breaks      = 100,
col         = "gray90",
prob        = TRUE,
xlim        = c(u_min, u_max),
ylim        = c(0,0.2),
xlab        = "u",
ylab        = "",
main        = TeX("$U \\sim \\chi^2(5)$"))
lines(
u,
p_U,
lwd   = 2,
col   = "darkorange")

# Noncentral T-transformation
Tee         = X/(sqrt(U/n))                                                      # element-wise vector arithmetic
Tee         = Tee[!abs(Tee) > 30]                                                # mildly censored sample for good histogram performance
t_min       = 0                                                                  # minimum t-value
t_max       = 20                                                                 # maximum t-value
t_res       = 1e3                                                                # t-space resolution
t           = seq(t_min,t_max,len = t_res)                                       # t-space
p_T         = dt(t,n,mu)                                                         # t density


# histogram and density
hist(
Tee,
breaks      = 100,
col         = "gray90",
prob        = TRUE,
xlim        = c(t_min, t_max),
ylim        = c(0,0.3),
xlab        = "t",
ylab        = "",
main        = TeX("$T = y/\\sqrt{U/n} \\sim \\t(5,5)$"))
lines(
t,
p_T,
lwd         = 2,
col         = "darkorange")
dev.off()
```

![Nichtzentrale $T$-Transformation normalverteilter Zufallsvariablen.](./_figures/209-nichtzentrale-t-transformation){#fig-nichtzentrale-t-transformation fig-align="center" width=100%}

## $F$-Transformation


Das in diesem Abschnitt zentrale @thm-f-transformation besagt, dass die Zufallsvariable, 
die sich durch Division zweier $\chi^2$ verteilter Zufallsvariablen, jeweils geteilt
durch ihre jeweiligen Freiheitsgradparameter, eine $F$-verteilte Zufallsvariable ist.
Dabei ist eine $F$-verteilte Zufallsvariable wie folgt definiert.

:::{#def-f-zufallsvariable}
## $f$-Zufallsvariable
$F$ sei eine Zufallsvariable mit Ergebnisraum $\mathbb{R}_{>0}$ und WDF
\begin{equation}
p_F : \mathbb{R} \to \mathbb{R}_{>0}, f \mapsto p_F(f)
:= m^{\frac{m}{2}}n^{\frac{n}{2}}
   \frac{\Gamma\left(\frac{m+n}{2}\right)}{\Gamma\left(\frac{m}{2}\right)\Gamma\left(\frac{n}{2}\right)}
   \frac{f^{\frac{m}{2}-1}}{\left(1 + \frac{m}{n}f \right)^{\frac{m+n}{2}}},
\end{equation}
wobei $\Gamma$ die Gammafunktion bezeichne. Dann sagen wir, dass $F$ einer
$f$-Verteilung mit Freiheitsgradparametern $n,m$ unterliegt und nennen $F$ eine
$f$-Zufallsvariable mit Freiheitsgradparametern $n,m$. Wir kürzen dies mit $F \sim f(n,m)$ ab.
Die WDF einer $f$-Zufallsvariable bezeichnen wir mit
\begin{equation}
F(f;n,m)
:= m^{\frac{m}{2}}n^{\frac{n}{2}}
   \frac{\Gamma\left(\frac{m+n}{2}\right)}{\Gamma\left(\frac{m}{2}\right)\Gamma\left(\frac{n}{2}\right)}
   \frac{f^{\frac{m}{2}-1}}{\left(1 + \frac{m}{n}f \right)^{\frac{m+n}{2}}}.
\end{equation}
:::

In @fig-f-wdf visualisieren wir exemplarisch einige WDFen von $f$-Zufallsvariablen.
Wir beobachten, dass die Form der WDFen zunächt primär durch den Freiheitsgradparameter
$n$ und dann sekundär durch den Freiheitsgradparameter $m$ bestimmt werden.

```{r, echo = F, eval = F}
pdf(file = "./_figures/209-f-wdf.pdf", width = 6, height = 5) 
par(                                                                    
family     = "sans",                                                 
pty        = "m",                                                     
bty        = "l",                                                    
lwd        = 1,                                                       
las        = 1,                                                       
mgp        = c(2,1,0),                                                
xaxs       = "i",                                                   
yaxs       = "i",                                      
font.main  = 1,                                                       
cex.main   = 1.4)

# f space
f_min   = 0                                                                     # minimum f-value
f_max   = 4                                                                     # maximum f-value
f_res   = 1e3                                                                   # f-space resolution
f        = seq(f_min, f_max, len = f_res)                                        # f-space

# parameters of interest
n       = c(2,2 ,5,5 ,10,10)                                                        # degrees of freedom
m       = c(2,10,2,10,2 ,10)                                                        # degrees of freedom

# plotting
matplot(
f,
matrix(
c(df(f,n[1],m[1]),
df(f,n[2],m[2]),
df(f,n[3],m[3]),
df(f,n[4],m[4]),
df(f,n[5],m[5]),
df(f,n[6],m[6])),
ncol = 6),
type         = "l",                                                      
lty          = c(1,2,1,2,1,2),                                            
lwd          = 2,                                                         
col          = c("gray90","gray70", "gray50", "gray30", "gray10"),      
ylim         = c(0,1),                                                    
xlim         = c(f_min,f_max),                                            
ylab         = " ",                                                       
xlab         = "f",                                                       
main         = TeX("$F(f;n,m)$"))                                        

legend(
2.40,                                                                        
1,                                                                       
c("n = 2,  m = 2",
"n = 2,  m = 10",
"n = 5,  m = 2",
"n = 5,  m = 10",
"n = 10, m = 5",
"n = 10, m = 10"),                                                      
lty         = c(1,2,1,2,1,2),                                             
lwd         = 2,                                                          
col         = c("gray90","gray70", "gray50", "gray30", "gray10"),       
bty         = "n",                                                        
cex         = 1,                                                        
y.intersp   = 1)                                                        
dev.off()                                                                  
```


![WDFen von $f$-verteilten Zufallsvariablen.](./_figures/209-f-wdf){#fig-f-wdf fig-align="center" width=60%}

:::{#thm-f-transformation}
## $F$-Transformation
$V \sim \chi^2(n)$ und $W \sim \chi^2(m)$ seien zwei unabhängige
$\chi^2$-Zufallfsvariablen mit Freiheitsgradparametern $n$ und $m$, respektive.
Dann ist die Zufallsvariable
\begin{equation}
F := \frac{V/n}{W/m}
\end{equation}
eine $f$-verteilte Zufallsvariable mit Freiheitsgradparametern $n,m$, es gilt also $F \sim f(n,m)$.
:::

Das Theorem kann bewiesen werden, in dem man zunächst ein Transformationstheorem
für Quotienten von Zufallsvariablen mithilfe von @thm-transformation-eines-zufallsvektors
und Marginalisierung herleitet und dieses Theorem dann auf die WDF von $\chi^2$-verteilten
Zufallsvariablen anwendet. Wir visualisieren @thm-f-transformation exemplarisch in @fig-f-transform.
Wichtige Anwendungsfälle von @thm-f-transformation sind die im Rahmen der
Theorie des Allgemeinen Linearen Modells betrachteten $F$-Statistiken.


```{r, echo = F, eval = F}
pdf(file = "./_figures/209-f-transform.pdf", width = 12, height = 4)                                                                       
par(                                                                    
family     = "sans",                                                  
mfcol      = c(1,3),                                                  
pty        = "m",                                                     
bty        = "l",                                                     
lwd        = 1,                                                       
las        = 1,                                                       
mgp        = c(2,1,0),                                                
xaxs         = "i",                                                   
yaxs         = "i",                                                  
font.main  = 1,                                                       
cex        = 1.2,
cex.main   = 1)

# chi^2 samples
n           = 5
m           = 10
nsamp       = 1e4                                                                # number of samples
V           = rchisq(nsamp,n)                                                    # nsamp samples of V \sim \chi^2(n)
W           = rchisq(nsamp,m)                                                    # nsamp samples of W \sim \chi^2(m)
v_min       = 0                                                                  # minimum chi^2-value
v_max       = 25                                                                 # maximum chi^2-value
v_res       = 1e3                                                                # chi^2-space resolution
v           = seq(v_min, v_max, len = v_res)                                     # chi^2-space
w           = v                                                                  # chi^2-space
p_V         = dchisq(v,n)                                                        # chi^2(n) density
p_W         = dchisq(w,m)                                                        # chi^2(m) density

# V histogram
hist(
V,                                                                          
breaks = 50,                                                                
col   = "gray90",                                                           
prob  = TRUE,                                                               
xlim  = c(v_min,v_max),                                                     
ylim  = c(0,.2),                                                            
xlab  = "v",                                                                
ylab  = "",                                                                
main  = TeX("$V \\sim \\chi^2(5)$")) 

# density
lines(
v,                                                                          
p_V,                                                                        
lwd   = 2,                                                                  
col   = "darkorange")                                                       

# W histogram
hist(
W,                                                                          
breaks = 50,                                                                
col   = "gray90",                                                           
prob  = TRUE,                                                               
xlim  = c(v_min,v_max),                                                     
ylim  = c(0,.2),                                                            
xlab  = "w",                                                                
ylab  = "",                                                                  
main  = TeX("$W \\sim \\chi^2(10)$"))                                       

# density
lines(
w,                                                                         
p_W,                                                                        
lwd   = 2,                                                                  
col   = "darkorange")                                                       

# F-transformation
Eff         = (V/n)/(W/m)                                                        # element-wise vector arithmetic
f_min       = 0                                                                  # minimum t-value
f_max       = 6                                                                  # maximum t-value
f_res       = 1e3                                                                # t-space resolution
f           = seq(f_min, f_max, len = f_res)                                     # t-space
p_F         = df(f,n,m)                                                          # t density

# histogram
hist(
Eff,                                                                       
breaks= 100,                                                                
col   = "gray90",                                                           
prob  = TRUE,                                                               
xlim  = c(f_min, f_max),                                                    
ylim  = c(0,.8),                                                            
xlab  = "f",                                                                
ylab  = "",                                                                 
main  = TeX("$F = \\frac{V/n}{W/m}\\sim \\F(5,10)$"))

# density
lines(
f,                                                                          
p_F,                                                                       
lwd   = 2,                                                                  
col   = "darkorange")                                                       
dev.off()                                                                
```

![$F$-Transformation normalverteilter Zufallsvariablen.](./_figures/209-f-transform){#fig-f-transform fig-align="center" width=100%}

##  Selbstkontrollfragen
\footnotesize
1. Erläutern Sie die Bedeutung der in diesem Abschnitt betrachteten Transformationen von normalverteilten Zufallsvariablen für die Frequentistische Inferenz.
1. Geben Sie das Theorem zur Summentransformation wieder.
1. Geben Sie das Theorem zur Mittelwerttransformation wieder.
1. Geben Sie das Theorem zur $Z$-Transformation wieder.
1. Geben Sie das Theorem zur $\chi^2$-Transformation wieder.
1. Beschreiben Sie die WDF der $t$-Verteilung in Abhängigkeit ihrer Freiheitsgradparameter.
1. Geben Sie das Theorem zur $T$-Transformation wieder.
1. Geben Sie das Theorem zur $F$-Transformation wieder.





## Literaturhinweise

Die Entwicklung der bivariaten Normalverteilung hat ihre Ursprünge in der statistischen
Literatur zur Mitte des 19. Jahrhunderts, insbesondere in den Arbeiten von 
Francis Galton (1822-1911). Die mathematische Formalisierung der bivariaten Normalverteilung 
geht dabei wohl insbesondere auf @pearson1896 zurück (@seal1967). Die
ursprüngliche Formulierung der multivariaten Normalverteilung wird bei @edgeworth1892
verortet. @tong1990 gibt eine umfassenden Überblick zur Theorie und Anwendung
der multivariaten Normalverteilung.

## Selbstkontrollfragen
\footnotesize
1. Geben Sie Definition eines Zufallsvektors wieder.
1. Geben Sie Definition der multivariaten Verteilung eines Zufallsvektors wieder.
1. Geben Sie Definition einer multivariaten WMF wieder.
1. Geben Sie Definition einer multivariaten WDF wieder.
1. Geben Sie die Definition des Erwartungswerts eines Zufallsvektors wieder.
1. Geben Sie die Definition der Kovarianzmatrix eines Zufallsvektors wieder.
1. Was repräsentieren die Diagonalelemente der Kovarianzmatrix eines Zufallsvektors?
1. Was repräsentieren die Nichtdiagonalelemente der Kovarianzmatrix eines Zufallsvektors?
1. Geben Sie die Definition der Korrelationsmatrix eines Zufallsvektors wieder.
1. Geben Sie die Definition der univariaten Marginalverteilung eines Zufallsvektors wieder.
1. Wie berechnet man die WMF der $i$ten Komponente eines diskreten Zufallsvektors?
1. Wie berechnet man die WDF der $i$ten Komponente eines kontinuierlichen Zufallsvektors?
1. Geben Sie Definition der bedingten WMF und der diskreten bedingten Verteilung wieder.
1. Geben Sie Definition der bedingten WDF und der kontinuierlichen bedingten Verteilung wieder.
1. Geben Sie die Definition der WDF eines multivariaten normalverteilten Zufallsvektors wieder.
1. Erläutern Sie die Komponenten der WDF eines multivariaten normalverteilten Zufallsvektors.
1. Geben Sie den Erwartungswert und die Kovarianzmatrix eines normalverteilten Zufallsvektors an.
1. Geben Sie das Theorem zu marginalen Normalverteilungen wieder.
1. Geben Sie das Theorem zu gemeinsamen Normalverteilungen wieder.
1. Geben Sie das Theorem zu bedingten Normalverteilungen wieder.
\normalsize