<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="de" xml:lang="de"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Probabilistische Datenwissenschaft für die Psychologie - 19&nbsp; Punktschätzung</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./303-Konfidenzintervalle.html" rel="next">
<link href="./301-Grundbegriffe-Frequentistischer-Inferenz.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "Keine Treffer",
    "search-matching-documents-text": "Treffer",
    "search-copy-link-title": "Link in die Suche kopieren",
    "search-hide-matches-text": "Zusätzliche Treffer verbergen",
    "search-more-match-text": "weitere Treffer in diesem Dokument",
    "search-more-matches-text": "weitere Treffer in diesem Dokument",
    "search-clear-button-title": "Zurücksetzen",
    "search-detached-cancel-button-title": "Abbrechen",
    "search-submit-button-title": "Abschicken",
    "search-label": "Suchen"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Seitenleiste umschalten" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./301-Grundbegriffe-Frequentistischer-Inferenz.html">Frequentistische Inferenz</a></li><li class="breadcrumb-item"><a href="./302-Punktschätzung.html"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Punktschätzung</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Seitenleiste umschalten" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Probabilistische Datenwissenschaft für die Psychologie</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/dirk-ostwald/dirk-ostwald.github.io/tree/gh-pages" rel="" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./Probabilistische-Datenwissenschaft-für-die-Psychologie.pdf" rel="" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Suchen"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Willkommen</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Vorwort.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Vorwort</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Mathematische Grundlagen</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Abschnitt umschalten">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./101-Sprache-und-Logik.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Sprache und Logik</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./102-Mengen.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Mengen</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./103-Summen-Produkte-Potenzen.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Summen, Produkte, Potenzen</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./104-Funktionen.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Funktionen</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./105-Differentialrechnung.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Differentialrechnung</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./106-Folgen-Grenzwerte-Stetigkeit.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Folgen, Grenzwerte, Stetigkeit</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./107-Integralrechnung.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Integralrechnung</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./108-Vektoren.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Vektoren</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./200-Wahrscheinlichkeitstheorie.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Wahrscheinlichkeitstheorie</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Abschnitt umschalten">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./201-Wahrscheinlichkeitsräume.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Wahrscheinlichkeitsräume</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./202-Elementare-Wahrscheinlichkeiten.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Elementare Wahrscheinlichkeiten</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./203-Zufallsvariablen.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Zufallsvariablen</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./204-Zufallsvektoren.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Zufallsvektoren</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./205-Erwartungswerte.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Erwartungswerte</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./206-Ungleichungen.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Ungleichungen</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./207-Grenzwerte.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Grenzwerte</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./208-Transformationstheoreme.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Transformationstheoreme</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./209-Transformationen-der-Normalverteilung.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Transformationen der Normalverteilung</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Frequentistische Inferenz</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Abschnitt umschalten">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./301-Grundbegriffe-Frequentistischer-Inferenz.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Grundbegriffe Frequentistischer Inferenz</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./302-Punktschätzung.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Punktschätzung</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./303-Konfidenzintervalle.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Konfidenzintervalle</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./304-Hypothesentests.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Hypothesentests</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Referenzen.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Referenzen</span></a>
  </div>
</li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Inhaltsverzeichnis</h2>
   
  <ul>
  <li><a href="#sec-maximum-likelihood-schätzung" id="toc-sec-maximum-likelihood-schätzung" class="nav-link active" data-scroll-target="#sec-maximum-likelihood-schätzung"><span class="header-section-number">19.1</span> Maximum-Likelihood Schätzung</a>
  <ul class="collapse">
  <li><a href="#beispiele" id="toc-beispiele" class="nav-link" data-scroll-target="#beispiele">Beispiele</a></li>
  <li><a href="#anwendungsbeispiel" id="toc-anwendungsbeispiel" class="nav-link" data-scroll-target="#anwendungsbeispiel">Anwendungsbeispiel</a></li>
  </ul></li>
  <li><a href="#sec-schaetzereigenschaften-bei-endlichen-stichproben" id="toc-sec-schaetzereigenschaften-bei-endlichen-stichproben" class="nav-link" data-scroll-target="#sec-schaetzereigenschaften-bei-endlichen-stichproben"><span class="header-section-number">19.2</span> Schätzereigenschaften bei endlichen Stichproben</a>
  <ul class="collapse">
  <li><a href="#sec-erwartungstreue" id="toc-sec-erwartungstreue" class="nav-link" data-scroll-target="#sec-erwartungstreue"><span class="header-section-number">19.2.1</span> Erwartungstreue</a></li>
  <li><a href="#sec-varianz-und-standardfehler" id="toc-sec-varianz-und-standardfehler" class="nav-link" data-scroll-target="#sec-varianz-und-standardfehler"><span class="header-section-number">19.2.2</span> Varianz und Standardfehler</a></li>
  <li><a href="#sec-mittlerer-quadratischer-fehler" id="toc-sec-mittlerer-quadratischer-fehler" class="nav-link" data-scroll-target="#sec-mittlerer-quadratischer-fehler"><span class="header-section-number">19.2.3</span> Mittlerer quadratischer Fehler</a></li>
  <li><a href="#sec-cramer-rao-ungleichung" id="toc-sec-cramer-rao-ungleichung" class="nav-link" data-scroll-target="#sec-cramer-rao-ungleichung"><span class="header-section-number">19.2.4</span> Cramér-Rao-Ungleichung</a></li>
  </ul></li>
  <li><a href="#sec-asymptotische-schaetzereigenschaften" id="toc-sec-asymptotische-schaetzereigenschaften" class="nav-link" data-scroll-target="#sec-asymptotische-schaetzereigenschaften"><span class="header-section-number">19.3</span> Asymptotische Schätzereigenschaften</a>
  <ul class="collapse">
  <li><a href="#sec-asymptotische-erwartungstreue" id="toc-sec-asymptotische-erwartungstreue" class="nav-link" data-scroll-target="#sec-asymptotische-erwartungstreue"><span class="header-section-number">19.3.1</span> Asymptotische Erwartungstreue</a></li>
  <li><a href="#sec-konsistenz" id="toc-sec-konsistenz" class="nav-link" data-scroll-target="#sec-konsistenz"><span class="header-section-number">19.3.2</span> Konsistenz</a></li>
  <li><a href="#sec-asymptotische-normalitaet" id="toc-sec-asymptotische-normalitaet" class="nav-link" data-scroll-target="#sec-asymptotische-normalitaet"><span class="header-section-number">19.3.3</span> Asymptotische Normalität</a></li>
  <li><a href="#sec-asymptotische-effizienz" id="toc-sec-asymptotische-effizienz" class="nav-link" data-scroll-target="#sec-asymptotische-effizienz"><span class="header-section-number">19.3.4</span> Asymptotische Effizienz</a></li>
  </ul></li>
  <li><a href="#sec-eigenschaften-von-maximum-likelihood-schaetzern" id="toc-sec-eigenschaften-von-maximum-likelihood-schaetzern" class="nav-link" data-scroll-target="#sec-eigenschaften-von-maximum-likelihood-schaetzern"><span class="header-section-number">19.4</span> Eigenschaften von Maximum-Likelihood Schätzern</a></li>
  <li><a href="#literaturhinweise" id="toc-literaturhinweise" class="nav-link" data-scroll-target="#literaturhinweise"><span class="header-section-number">19.5</span> Literaturhinweise</a></li>
  <li><a href="#selbstkontrollfragen" id="toc-selbstkontrollfragen" class="nav-link" data-scroll-target="#selbstkontrollfragen"><span class="header-section-number">19.6</span> Selbstkontrollfragen</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/dirk-ostwald/dirk-ostwald.github.io/tree/gh-pages/edit/main/302-Punktschätzung.qmd" class="toc-action">Seite editieren</a></p></div></div></nav>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-punktschaetzung" class="quarto-section-identifier"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Punktschätzung</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>In diesem Kapitel gehen wir immer im Sinne der in <a href="301-Grundbegriffe-Frequentistischer-Inferenz.html"><span>Kapitel&nbsp;18</span></a> eingeführten Begrifflichkeiten immer von einem parametrischem Produktmodell <span class="math display">\[\begin{equation}
\mathcal{M} := \{\mathcal{Y},\mathcal{A}, \{\mathbb{P}_\theta| \theta \in \Theta\}\}
\end{equation}\]</span> mit <span class="math inline">\(n\)</span>-dimensionalen Stichprobenraum (z.B. <span class="math inline">\(\mathcal{Y} := \mathbb{R}^n\)</span>), <span class="math inline">\(d\)</span>-dimensionalen Parameteraum <span class="math inline">\(\Theta \subset \mathbb{R}^d\)</span> und gegebener WMF oder WDF <span class="math inline">\(p_\theta\)</span> für alle <span class="math inline">\(\theta \in \Theta\)</span> aus. <span class="math inline">\(\upsilon := (\upsilon_1,...,\upsilon_n)\)</span> bezeichnet die zu <span class="math inline">\(\mathcal{M}\)</span> gehörende Stichprobe unabhängig und identisch verteilter Zufallsvariablen, es gilt also durchgängig <span class="math display">\[\begin{equation}
\upsilon_1,...,\upsilon_n \sim \mathbb{P}_\theta.
\end{equation}\]</span> Wesen und Ziel der hier behandelten <em>Punkt</em>schätzung ist es, basierend auf der Stichprobe einen möglichst guten Tipp für eine interessierende Kennzahl der Verteilung <span class="math inline">\(\mathbb{P}_\theta\)</span> einer Stichprobenvariable anzugeben. Dabei ist der Tipp von der gleichen mathematischen Wesensart wie die entsprechende Kennzahl, also zum Beispiel ein skalarer Wert für einen skalaren Parameter. Dies ist nicht die einzige Möglichkeit der Schätzung, mit den Konfidenzintervallen werden wir in <a href="303-Konfidenzintervalle.html"><span>Kapitel&nbsp;20</span></a> eine Möglichkeit der Schätzung von skalaren Werten durch Intervalle kennenlernen und die Bayesianische Inferenz nutzt zur Schätzung von skalaren Werten in aller Regel Wahrscheinlichkeitsverteilungen. Die zu schätzenden Kennzahlen von <span class="math inline">\(\mathbb{P}_\theta\)</span> sind oft schlicht die wahren, aber unbekannten, Parameter selbst. Wir widmen uns diesem Fall ausführlich in <a href="#sec-maximum-likelihood-schätzung"><span>Kapitel&nbsp;19.1</span></a>. Allerdings sind viele grundlegende Resultate der Frequentistischen Punktschätzung auch dann valide, wenn es sich bei zu schätzenden Kennzahlen nicht um die Parameter selbst, sondern, bei parameterischen Produktmodellen, Funktionen von ihnen handelt, wie zum Beispiel die Schätzung des Erwartungswerts, der Varianz, oder der Standardabweichung von <span class="math inline">\(\mathbb{P}_\theta\)</span>. Beginnen wollen wir allerdings mit der <em>Parameterschätzung</em>. Um den wahren, aber unbekannten, Parametert eines parametrischen Produktmodells oder auch allgemein eines Frequentistischen Inferenzmodells zu schätzen, nutzt man in der Frequentistischen Inferenz sogenannte <em>Parameterpunktschätzer</em>.</p>
<div id="def-parameterpunktschätzer" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 19.1 </strong></span><span class="math inline">\(\mathcal{M} := (\mathcal{Y}, \mathcal{A}, \{\mathbb{P}_\theta|\theta \in \Theta\})\)</span> sei ein Frequentistisches Inferenzmodell, <span class="math inline">\((\Theta,\mathcal{S})\)</span> sei ein Messraum und <span class="math inline">\(\hat{\theta} : \mathcal{Y} \to \Theta\)</span> sei eine Abbildung. Dann nennt man <span class="math inline">\(\hat{\theta}\)</span> einen <em>Parameterpunktschätzer</em> für <span class="math inline">\(\theta\)</span>.</p>
</div>
<p>Parameterpunktschätzer werden meist auch einfach als <em>Parameterschätzer</em> bezeichnet. Im Sinne von <a href="301-Grundbegriffe-Frequentistischer-Inferenz.html#def-schätzer">Definition&nbsp;<span>18.5</span></a> sind Parameterpunktschätzer Schätzer mit <span class="math inline">\(\tau := \mbox{id}_\Theta\)</span>. Parameterpunktschätzer sind also Funktionen von Daten und nehmen Zahlwerte im Parameterraum an. Als als Funktionen von Zufallsvariablen sind Parameterschätzer natürlich auch Zufallsvariablen. Oft wird dabei notationell nicht zwischen <span class="math inline">\(\hat{\theta}\)</span> als Zufallsvariable und <span class="math inline">\(\hat{\theta}(y)\)</span> als Wert dieser Zufallsvariable unterschieden.</p>
<p><a href="#def-parameterpunktschätzer">Definition&nbsp;<span>19.1</span></a> macht offenbar keine Angabe darüber, wie ein Parameterpunktschätzer zu konstruieren ist oder inwieweit er dann ein sinnvoller Schätzer sein mag. Im Folgenden werden wir mit der <em>Maximum-Likelihood Schätzung</em> zunächst ein allgemeines Prinzip diskutieren, das es erlaubt, für ein gegebenenes Frequentistisches Inferenzmodell Parameterschätzer zu bestimmen, die, wie wir an späterer Stelle sehen werden, garantiert bestimmte wünschenswerte Eigenschaften haben (<a href="#sec-eigenschaften-von-maximum-likelihood-schaetzern"><span>Kapitel&nbsp;19.4</span></a>). Dabei beziehen sich diese Eigenschaften allgemein auf sein qualitatives Verteilungsverhalten bei festen Stichprobenumfang bzw. im Grenzübergang zu einem unendlich großen Stichprobenumfang. Wir führen diese Eigenschaften allgemein und insbesondere auch in der Schätzung auf andere Kennzahlen von <span class="math inline">\(\mathbb{P}_\theta\)</span> in <a href="#sec-schaetzereigenschaften-bei-endlichen-stichproben"><span>Kapitel&nbsp;19.2</span></a> und <a href="#sec-asymptotische-schaetzereigenschaften"><span>Kapitel&nbsp;19.3</span></a> ein.</p>
<section id="sec-maximum-likelihood-schätzung" class="level2" data-number="19.1">
<h2 data-number="19.1" class="anchored" data-anchor-id="sec-maximum-likelihood-schätzung"><span class="header-section-number">19.1</span> Maximum-Likelihood Schätzung</h2>
<p>Die Grundidee der Maximum-Likelihood Schätzung ist es, als Tipp für einen wahren, aber unbekannten, Parameterwert denjenigen Parameterwert zu wählen, für den die Wahrscheinlichkeit der beobachteten Daten maximal ist. Dafür ist es zunächst nötig, die Wahrscheinlichkeit beobachteter Daten eines Frequentistischen Inferenzmodells als Funktion des betreffenden Parameters zu betrachten. Dies ermöglichen und formalisieren die <em>Likelihood-Funktion</em> und ihr Logarithmus, die <em>Log-Likelihood-Funktion</em>. Wir definieren diese Begriffe hier für parametrische Produktmodelle.</p>
<div id="def-likelihood-funktion-und-log-likelihood-funktion" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 19.2 (Likelihood-Funktion und Log-Likelihood-Funktion) </strong></span><span class="math inline">\(\mathcal{M}\)</span> sei ein parametrisches Produktmodell mit WMF oder WDF <span class="math inline">\(p_\theta\)</span>. Dann ist die <em>Likelihood-Funktion</em> definiert als <span class="math display">\[\begin{equation}
L : \Theta \to [0,\infty[, \theta \mapsto L(\theta) := \prod_{i=1}^n p_\theta(y_i)
\end{equation}\]</span> und die <em>Log-Likelihood-Funktion</em> ist definiert als <span class="math display">\[\begin{equation}
\ell_n : \Theta \to \mathbb{R}, \theta \mapsto \ell(\theta) := \ln L(\theta).
\end{equation}\]</span></p>
</div>
<p>Die Likelihood-Funktion ist also eine Funktion des Parameters und ihre Funktionswerte sind die Werte der gemeinsamen WMF bzw. WDF beobachteter Datenwerte <span class="math inline">\(y_1,...,y_n\)</span>. Generell gibt es keinen Grund anzunehmen, dass eine Likelihood-Funktion über dem Parameterraum zu 1 integriert, die Likelihood-Funktion ist also im Allgemeinen keine WMF oder WDF. Die Log-Likelihood Funktion ist schlicht die logarithmierte Likelihood-Funktion. Ein nach dem Prinzip der Maximum-Likelihood Schätzung gewonnener Parameterschätzer soll nun die Likelihood-Funktion bzw. die Log-Likelihood-Funktion maximieren. Dies führt auf folgende Definition des Begriffs des <em>Maximum-Likelihood Schätzers</em>.</p>
<div id="def-maximum-likelihood-schaetzer" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 19.3 (Maximum-Likelihood Schätzer) </strong></span><span class="math inline">\(\mathcal{M}\)</span> sei ein parametrisches Produktmodell mit Parameter <span class="math inline">\(\theta \in \Theta\)</span>. Ein <em>Maximum-Likelihood Schätzer</em> von <span class="math inline">\(\theta\)</span> ist definiert als <span class="math display">\[\begin{equation}
\hat{\theta}^{\mbox{\tiny ML}} : \mathcal{Y} \to \Theta,
y \mapsto \hat{\theta}^{\mbox{\tiny ML}}(y)
:= \mbox{argmax}_{\theta \in \Theta} L(\theta)
= \mbox{argmax}_{\theta \in \Theta} \ell(\theta)
\end{equation}\]</span></p>
</div>
<p>Man beachte bei <a href="#def-maximum-likelihood-schaetzer">Definition&nbsp;<span>19.3</span></a>, dass eine Maximumstelle der Log-Likelihood-Funktion der Maximumstelle der Likelihood-Funktion entspricht, weil die Logarithmusfunktion eine monoton steigende Funktion ist. Das Arbeiten mit der Log-Likelihood-Funktion ist allerdings oft einfacher als das direkte Arbeiten mit der Likelihood-Funktion, zum Beispiel, wenn in der WMF oder WDF des Modells Exponentialfunktionen auftauchen. Weiterhin beachte man bei <a href="#def-maximum-likelihood-schaetzer">Definition&nbsp;<span>19.3</span></a>, dass <a href="#def-likelihood-funktion-und-log-likelihood-funktion">Definition&nbsp;<span>19.2</span></a> impliziert, dass <span class="math display">\[\begin{equation}
\hat{\theta}^{\mbox{\tiny ML}}(y)
= \mbox{argmax}_{\theta \in \Theta} \prod_{i=1}^n p_\theta(y_i)
= \mbox{argmax}_{\theta \in \Theta} \sum_{i=1}^n \ln p_\theta(y_i)
\end{equation}\]</span> was die Abhängigkeit eines Maximum-Likelihood Schätzers von den Daten verdeutlicht.</p>
<p>Mit <a href="#def-maximum-likelihood-schaetzer">Definition&nbsp;<span>19.3</span></a> handelt es sich bei der Maximum-Likelihood Schätzung also um das Problem, Extremalstellen einer Funktion zu bestimmen. Für diese Extremalstellen stellt die Differentialrechnung bekanntlich notwendige und hinreichende Bedingungen bereit (vgl. <a href="105-Differentialrechnung.html#sec-analytische-optimierung"><span>Kapitel&nbsp;5.2</span></a>). In ihrer Anwendung auf die Gewinnung von Maximum-Likelihood Schätzern begnügt man sich zumeist aufgrund der funktionellen Form der betrachteten Funktionen mit dem Erfülltsein der notwendigen Bedingung. Je nach Beschaffenheit der Log-likelihood Funktion bieten sich dann Methoden entweder der analytischen Optimierung oder der numerischen Optimierung an. In den folgenden klassischen Beispielen nutzen wir einen analytischen Zugang anhand folgendem standardisierten Vorgehen:</p>
<ol type="1">
<li>Formulierung der Log-Likelihood-Funktion.</li>
<li>Bestimmung der ersten Ableitung der Log-Likelihood-Funktion und Nullsetzen.</li>
<li>Auflösen nach potentiellen Maximumstellen.</li>
</ol>
<p>In <a href="#thm-maximum-likelihood-schaetzer-des-bernoullimodells">Theorem&nbsp;<span>19.1</span></a> zeigen wir, dass der Maximum-Likelihood Schätzer für den Parameter des Bernoullimodells aus <a href="301-Grundbegriffe-Frequentistischer-Inferenz.html#def-bernoullimodell">Definition&nbsp;<span>18.3</span></a> durch das entsprechende Stichprobenmittel gegeben ist und in <a href="#thm-maximum-likelihood-schaetzer-des-normalverteilungsmodells">Theorem&nbsp;<span>19.2</span></a> zeigen wir, dass die Maximum-Likelihood Schätzer für den Erwartungswert- und Varianzparameter des Normalverteilungsmodells aus <a href="301-Grundbegriffe-Frequentistischer-Inferenz.html#def-normalverteilungsmodell">Definition&nbsp;<span>18.2</span></a> durch das Stichprobenmittel und eine modifizierte Stichprobenvarianz, respektive, gegeben sind.</p>
<section id="beispiele" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="beispiele">Beispiele</h3>
<div id="thm-maximum-likelihood-schaetzer-des-bernoullimodells" class="theorem">
<p><span class="theorem-title"><strong>Theorem 19.1 (Maximum-Likelihood Schätzer des Bernoullimodells) </strong></span><span class="math inline">\(\mathcal{M}\)</span> sei das Bernoullimodell, es gelte also <span class="math inline">\(\upsilon_1,...,\upsilon_n \sim \mbox{Bern}(\mu)\)</span>. Dann ist <span class="math display">\[\begin{equation}
\hat{\mu}^{\mbox{\tiny ML}} : \{0,1\}^n \to [0,1],
y \mapsto \hat{\mu}^{\mbox{\tiny ML}}(y):= \frac{1}{n}\sum_{i=1}^n y_i
\end{equation}\]</span> ein Maximum-Likelihood Schätzer von <span class="math inline">\(\mu\)</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Beweis</em>. </span>Wir formulieren zunächst die Log-Likelihood-Funktion. Für die Likelihood-Funktion gilt <span class="math display">\[\begin{equation}
L : ]0,1[ \to ]0,1[,
\mu \mapsto L(\mu)
:= \prod_{i=1}^n \mu^{y_i}(1 - \mu)^{1-y_i}
= \mu^{\sum_{i=1}^n y_i}(1 - \mu)^{n - \sum_{i=1}^n y_i}.
\end{equation}\]</span> Logarithmieren ergibt <span class="math display">\[\begin{equation}
\ell : ]0,1[ \to \mathbb{R}, \mu \mapsto  \ell(\mu)
= \ln \mu \sum_{i=1}^n y_i + \ln (1- \mu) \left(n - \sum_{i=1}^n y_i \right).
\end{equation}\]</span> Wir werten dann die Ableitung der Log-Likelihood-Funktion aus. Es gilt <span class="math display">\[\begin{align}
\begin{split}
\frac{d}{d\mu} \ell(\mu)
&amp; = \frac{d}{d\mu}\left(\ln \mu \sum_{i=1}^n y_i + \ln (1- \mu) \left(n - \sum_{i=1}^n y_i \right)\right)  \\
&amp; = \frac{d}{d\mu} \ln \mu \sum_{i=1}^n y_i  + \frac{d}{d\mu} \ln (1 - \mu) \left(n - \sum_{i=1}^n y_i \right)  \\
&amp; = \frac{1}{\mu}\sum_{i=1}^n y_i  -  \frac{1}{1-\mu} \left(n - \sum_{i=1}^n y_i \right).
\end{split}
\end{align}\]</span> Nullsetzen ergibt dann folgende als notwendige Bedingung für einen Maximum-Likelihood Schätzer im Bernoullimodell: <span class="math display">\[\begin{equation}
\frac{1}{\hat{\mu}^{\mbox{\tiny ML}}}\sum_{i=1}^n y_i - \frac{1}{1-\hat{\mu}^{\mbox{\tiny ML}}} \left(n - \sum_{i=1}^n y_i \right) = 0.
\end{equation}\]</span> Auflösen der Maximum-Likelihood-Gleichung nach <span class="math inline">\(\hat{\mu}^{\mbox{\tiny ML}}\)</span> ergibt dann <span class="math display">\[\begin{align}
\begin{split}
\frac{1}{\hat{\mu}^{\mbox{\tiny ML}}}\sum_{i=1}^n y_i - \frac{1}{1-\hat{\mu}^{\mbox{\tiny ML}}} \left(n - \sum_{i=1}^n y_i \right) &amp; = 0 \\
\Leftrightarrow
\hat{\mu}^{\mbox{\tiny ML}}(1 - \hat{\mu}^{\mbox{\tiny ML}})\left(\frac{1}{\hat{\mu}^{\mbox{\tiny ML}}}\sum_{i=1}^n y_i - \frac{1}{1-\hat{\mu}^{\mbox{\tiny ML}}} \left(n - \sum_{i=1}^n y_i \right) \right) &amp; = 0 \\
\Leftrightarrow
\sum_{i=1}^n y_i - \hat{\mu}^{\mbox{\tiny ML}} \sum_{i=1}^n y_i - n \hat{\mu}^{\mbox{\tiny ML}}  + \hat{\mu}^{\mbox{\tiny ML}}\sum_{i=1}^n y_i &amp; = 0 \\
\Leftrightarrow
n \hat{\mu}^{\mbox{\tiny ML}}  &amp; = \sum_{i=1}^n y_i \\
\Leftrightarrow
\hat{\mu}^{\mbox{\tiny ML}}  &amp; = \frac{1}{n} \sum_{i=1}^n y_i. \\
\end{split}
\end{align}\]</span> <span class="math inline">\(\hat{\mu}^{\mbox{\tiny ML}} = \frac{1}{n}\sum_{i=1}^n y_i\)</span> ist also ein Kandidat für einen Maximum-Likelihood Schätzer von <span class="math inline">\(\mu\)</span>. Dies könnte durch Betrachten der zweiten Ableitung von <span class="math inline">\(\ell\)</span> verifiziert werden, worauf wir hier aber verzichten wollen.</p>
</div>
<div id="thm-maximum-likelihood-schaetzer-des-normalverteilungsmodells" class="theorem">
<p><span class="theorem-title"><strong>Theorem 19.2 (Maximum-Likelihood Schätzer des Normalverteilungsmodells) </strong></span><span class="math inline">\(\mathcal{M}\)</span> sei das Normalverteilungsmodell, es gelt also <span class="math inline">\(\upsilon_1,...,\upsilon_n \sim N\left(\mu,\sigma^2\right)\)</span>. Dann sind <span class="math display">\[\begin{equation}
\hat{\mu}^{\mbox{\tiny ML}} :
\mathbb{R}^n \to \mathbb{R}, y \mapsto \hat{\mu}^{\mbox{\tiny ML}}(y)
:= \frac{1}{n}\sum_{i=1}^n y_i
\end{equation}\]</span> und <span class="math display">\[\begin{equation}
\hat{\sigma}^{2^{\mbox{\tiny ML}}} :
\mathbb{R}^n \to \mathbb{R}_{\ge 0},
y \mapsto \hat{\sigma}^{2^{\mbox{\tiny ML}}}(y)
:= \frac{1}{n}\sum_{i=1}^n \left(y_i - \hat{\mu}^{\mbox{\tiny ML}}\right)^2.
\end{equation}\]</span> Maximum-Likelihood Schätzer für <span class="math inline">\(\mu\)</span> und <span class="math inline">\(\sigma^2\)</span>, respektive.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Beweis</em>. </span>Wir formulieren zunächst die Log-Likelihood-Funktion. Für die Likelihood-Funktion ergibt sich <span class="math display">\[\begin{align}
\begin{split}
L : \mathbb{R} \times \mathbb{R}_{&gt;0} \to \mathbb{R}_{&gt;0},
(\mu,\sigma^2) \mapsto L(\mu,\sigma^2)
:= &amp; \prod_{i=1}^n \frac{1}{\sqrt{2\pi \sigma^2}}\exp\left(-\frac{1}{2\sigma^2}(y_i-\mu)^2\right) \\
= &amp; \left(2 \pi \sigma^2\right)^{-\frac{n}{2}}\exp\left(-\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-\mu)^2\right). \\
\end{split}
\end{align}\]</span> Logarithmieren ergibt dann <span class="math display">\[\begin{equation}
\ell : \mathbb{R} \times \mathbb{R}_{&gt;0} \to \mathbb{R},
(\mu,\sigma^2) \mapsto \mathcal{\ell}_n(\mu,\sigma^2)
= -\frac{n}{2} \ln 2\pi - \frac{n}{2} \ln \sigma^2  -\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-\mu)^2.
\end{equation}\]</span> Die Auswertung der partiellen Ableitungen der Log-Likelihood-Funktion ergeben dann <span class="math display">\[\begin{equation}
\frac{\partial}{\partial{\mu}} \ell(\mu,\sigma^2)
= - \frac{\partial}{\partial{\mu}} \frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-\mu)^2
= - \frac{1}{2\sigma^2}\sum_{i=1}^n \frac{\partial}{\partial{\mu}} (y_i-\mu)^2
= \frac{1}{\sigma^2}\sum_{i=1}^n (y_i-\mu)
\end{equation}\]</span> und <span class="math display">\[\begin{align}
\begin{split}
\frac{\partial}{\partial\sigma^2} \ell(\mu,\sigma^2)
= - \frac{n}{2} \frac{\partial}{\partial\sigma^2} \ln \sigma^2  - \frac{\partial}{\partial\sigma^2} \frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-\mu)^2
= - \frac{n}{2 \sigma^2} + \frac{1}{2\sigma^4}\sum_{i=1}^n(y_i-\mu)^2.
\end{split}
\end{align}\]</span> Das System der Maximum-Likelihood Gleichungen als Ausdruck der notwendigen Bedingungen für Extremstellen der Log-Likelihood-Funktion hat in diesem Fall also die Form <span class="math display">\[\begin{equation}
\sum_{i=1}^n (y_i-\hat{\mu}^{\mbox{\tiny ML}}) = 0
\mbox{ und }
- \frac{n}{2 \hat{\sigma}^{2^{\mbox{\tiny ML}}}} + \frac{1}{2\hat{\sigma}^{4^{\mbox{\tiny ML}}}}\sum_{i=1}^n(y_i-\mu)^2  = 0.
\end{equation}\]</span> Lösen des Systems der Maximum-Likelihood Gleichungen ergibt dann zunächst <span class="math display">\[\begin{equation}
\sum_{i=1}^n (y_i-\hat{\mu}^{\mbox{\tiny ML}})  = 0
\Leftrightarrow \sum_{i=1}^n y_i  = n\hat{\mu}^{\mbox{\tiny ML}}
\Leftrightarrow \hat{\mu}^{\mbox{\tiny ML}} = \frac{1}{n}\sum_{i=1}^n y_i.
\end{equation}\]</span> Damit ist <span class="math display">\[\begin{equation}
\hat{\mu}^{\mbox{\tiny ML}} = \frac{1}{n}\sum_{i=1}^n y_i
\end{equation}\]</span> ein potentieller Maximum-Likelihood Schätzer von <span class="math inline">\(\mu\)</span>. Einsetzen dieses Schätzers in die zweite Maximum-Likelihood Gleichung ergibt dann<br>
<span class="math display">\[\begin{align}
\begin{split}
- \frac{n}{2 \hat{\sigma}^{2^{\mbox{\tiny ML}}}} + \frac{1}{2\hat{\sigma}^{4^{\mbox{\tiny ML}}}}\sum_{i=1}^n(y_i-\hat{\mu}^{\mbox{\tiny ML}})^2 &amp; = 0 \\
\Leftrightarrow
- n\hat{\sigma}^{2^{\mbox{\tiny ML}}} + \sum_{i=1}^n(y_i-\hat{\mu}^{\mbox{\tiny ML}})^2 &amp; = 0 \\
\Leftrightarrow
\hat{\sigma}^{2^{\mbox{\tiny ML}}} &amp; = \frac{1}{n} \sum_{i=1}^n(y_i-\hat{\mu}^{\mbox{\tiny ML}})^2.
\end{split}
\end{align}\]</span> Also ist <span class="math display">\[\begin{equation}
\hat{\sigma}^{2^{\mbox{\tiny ML}}} = \frac{1}{n}\sum_{i=1}^n\left(y_i-\hat{\mu}^{\mbox{\tiny ML}}\right)^2
\end{equation}\]</span> ein potentieller Maximum-Likelihood Schätzer von <span class="math inline">\(\sigma^2\)</span>. Beide potentiellen Maximum-Likelihood Schätzer können durch Betrachten der zweiten Ableitung von <span class="math inline">\(\ell\)</span> verifiziert werden, worauf wir hier verzichten wollen.</p>
</div>
<p>Man beachte bei <a href="#thm-maximum-likelihood-schaetzer-des-normalverteilungsmodells">Theorem&nbsp;<span>19.2</span></a>, dass <span class="math inline">\(\hat{\mu}^{\mbox{\tiny ML}}\)</span> mit dem Stichprobenmittel <span class="math inline">\(\bar{\upsilon}\)</span> identisch ist, aber <span class="math inline">\(\hat{\sigma}^{2^{\mbox{\tiny ML}}}\)</span> nicht mit der Stichprobenvarianz <span class="math inline">\(S^2\)</span> übereinstimmt. Im Gegensatz zur Stichprobenvarianz findet sich im Maximum-Likelihood Schätzer von <span class="math inline">\(\sigma^2\)</span> der multiplikative Faktor <span class="math inline">\(\frac{1}{n}\)</span>, nicht, wie in der Stichprobenvarianz, der multiplikative Faktor <span class="math inline">\(\frac{1}{n-1}\)</span>. Wir werden auf diesen Unterschied im Kontext der Schätzereigenschaften zurückkommen.</p>
</section>
<section id="anwendungsbeispiel" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="anwendungsbeispiel">Anwendungsbeispiel</h3>
<p>Zum Abschluss dieses Abschnitts wollen wir <a href="#thm-maximum-likelihood-schaetzer-des-normalverteilungsmodells">Theorem&nbsp;<span>19.2</span></a> im Kontext des Anwendungsbeispiels aus <a href="301-Grundbegriffe-Frequentistischer-Inferenz.html#sec-anwendungsbeispiel-frequentistische-inferenz"><span>Kapitel&nbsp;18.3.1</span></a> betrachten. Wir hatten dort den beobachteten <code>dBDI</code> Werten das Normalverteilungsmodell <span class="math display">\[\begin{equation}
\upsilon_1,...,\upsilon_n \sim N(\mu,\sigma^2)
\end{equation}\]</span> zugrundegelegt. Die Maximum-Likelihood Schätzer für die Parameter dieses Modells lassen sich dann anhand von <a href="#thm-maximum-likelihood-schaetzer-des-normalverteilungsmodells">Theorem&nbsp;<span>19.2</span></a> mithilfe der <strong>R</strong> Stichprobenmittel- und Stichprobenvarianzfunktionen <code>mean()</code> und <code>var()</code> und unter Beachtung der Identität <span class="math display">\[\begin{equation}
\frac{n-1}{n}s^2
= \frac{n-1}{n}\cdot\frac{1}{n-1}\sum_{i=1}^n (y_i - \bar{y})^2
= \frac{1}{n}\sum_{i=1}^n \left(y_i - \hat{\mu}^{\mbox{\tiny ML}}\right)^2
= \hat{\sigma}^{2^{\mbox{\tiny ML}}}
\end{equation}\]</span> wie in folgendem <strong>R</strong> Code auswerten.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a>D           <span class="ot">=</span> <span class="fu">read.csv</span>(<span class="st">"./_data/302-Punktschätzung.csv"</span>) <span class="co"># Datensatzeinlesen </span></span>
<span id="cb1-2"><a href="#cb1-2"></a>y           <span class="ot">=</span> D<span class="sc">$</span>dBDI                                     <span class="co"># Datenauswahl            </span></span>
<span id="cb1-3"><a href="#cb1-3"></a>mu_hat      <span class="ot">=</span> <span class="fu">mean</span>(y)                                    <span class="co"># Maximum-Likelihood Schätzung des Erwartungswertparameters</span></span>
<span id="cb1-4"><a href="#cb1-4"></a>n           <span class="ot">=</span> <span class="fu">length</span>(y)                                  <span class="co"># Anzahl der Datenpunkte</span></span>
<span id="cb1-5"><a href="#cb1-5"></a>sigsqr_hat  <span class="ot">=</span> ((n<span class="dv">-1</span>)<span class="sc">/</span>n)<span class="sc">*</span><span class="fu">var</span>(y)                           <span class="co"># Maximum-Likelihood Schätzung des Varianzparameters</span></span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="fu">cat</span>(<span class="st">"mu_hat     :"</span>, mu_hat,<span class="st">"</span><span class="sc">\n</span><span class="st">sigsqr_hat :"</span>, sigsqr_hat) <span class="co"># Ausgabe    </span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>mu_hat     : 3.166667 
sigsqr_hat : 12.63889</code></pre>
</div>
</div>
<p>Basierend auf dem Prinzip der Maximum-Likelihood Schätzung und den vorliegenden <span class="math inline">\(n = 12\)</span> Datenpunkten sind also <span class="math display">\[\begin{equation}
\hat{\mu}^{\mbox{\tiny ML}} = 3.17
\mbox{ und }
\hat{\sigma}^{2^{\mbox{\tiny ML}}} = 12.6
\end{equation}\]</span> Tipps für die wahren, aber unbekannten, Parameter des Modells.</p>
</section>
</section>
<section id="sec-schaetzereigenschaften-bei-endlichen-stichproben" class="level2" data-number="19.2">
<h2 data-number="19.2" class="anchored" data-anchor-id="sec-schaetzereigenschaften-bei-endlichen-stichproben"><span class="header-section-number">19.2</span> Schätzereigenschaften bei endlichen Stichproben</h2>
<p>Allgemein betreffen Frequentistische Schätzereigenschaften die Verteilung von Schätzern in Abhängigkeit der Verteilung der ihn zugrundeliegenden Daten. Weil Daten in der Frequentistischen Inferenz zufällig sind, sind auch Schätzer zufällig. Speziell werden beobachtete Datenwerte als Realisierungen von Zufallsvariablen interpretiert. Schätzer als Funktionen von Zufallsvariablen sind damit auch Zufallsvariablen, auch wenn sie natürlich bei Vorliegen eines konkreten Datensatzes nur einen konkreten Wert annehmen. Wir unterscheiden zwischen <em>Schätzereigenschaften bei endlichen Stichproben</em> und <em>Asymptotischen Schätzereigenschaften</em>. Erstere sind Inhalt dieses Abschnittes und betreffen die Eigenschaften eines Schätzer für einen festen Stichprobenumfang <span class="math inline">\(n\)</span>, letztere sind Inhalt von <a href="#sec-asymptotische-schaetzereigenschaften"><span>Kapitel&nbsp;19.3</span></a> und betreffen die Eigenschaften eines Schätzers im Grenzfall <span class="math inline">\(n \to \infty\)</span> von großen Stichprobenumfängen.</p>
<p>Es sei zunächst <span class="math inline">\((\Sigma,S)\)</span> ein Messraum und <span class="math inline">\(\hat{\tau} : \mathcal{Y} \to \Sigma\)</span> ein Schätzer von <span class="math inline">\(\tau : \Theta \to \Sigma\)</span> (vgl. <a href="301-Grundbegriffe-Frequentistischer-Inferenz.html#def-schätzer">Definition&nbsp;<span>18.5</span></a>). In der Folge betrachten wir neben Parameterschätzern der Form <span class="math display">\[\begin{equation}
\tau: \Theta \to \Sigma, \tau(\theta) := \theta
\end{equation}\]</span> auch wiederholt zunächst solche Schätzer, die bei parametrischen Produktmodellen nur Funktionen der Parameter wie den Erwartungswert, die Varianz und die Standardabweichung der Stichprobenvariablen schätzen. Da nach Annahme die Verteilungen der Stichprobenvariablen <span class="math inline">\(\upsilon_1,...,\upsilon_n\)</span> identisch sind, handelt es sich dabei um Schätzer der Form<br>
<span class="math display">\[\begin{align}
\begin{split}
\tau : \Theta \to \Sigma,\,
\theta \mapsto \tau(\theta)
\mbox{ mit }
\tau(\theta) := \mathbb{E}_\theta(\upsilon_1),
\tau(\theta) := \mathbb{V}_\theta(\upsilon_1), \mbox{ und }
\tau(\theta) := \mathbb{S}_\theta(\upsilon_1).
\end{split}
\end{align}\]</span></p>
<p>Speziell wollen wir in diesem Abschnitt vier Aspekte von Schätzereigenschaften bei endlichen Stichproben beleuchten. In <a href="#sec-erwartungstreue"><span>Kapitel&nbsp;19.2.1</span></a> beschäftigen wir uns zunächst mit der <em>Erwartungstreue</em> eines Schätzers. Dabei heißt ein Schätzer <em>erwartungstreu</em>, wenn sein Erwartungswert mit dem wahren, aber unbekannten, Wert <span class="math inline">\(\tau(\theta)\)</span> für alle <span class="math inline">\(\theta \in \Theta\)</span> identisch ist. In <a href="#sec-varianz-und-standardfehler"><span>Kapitel&nbsp;19.2.2</span></a> führen wir mit den Begriffen der <em>Varianz</em> und des <em>Standardsfehlers</em> eines Schätzers als Bezeichungen für die Varianz der Zufallsvariable <span class="math inline">\(\hat{\tau}(\upsilon)\)</span> und die Standardabweichung der Zufallsvariable <span class="math inline">\(\hat{\tau}(\upsilon)\)</span> zwei Maße für die Frequentistische Variabilität von Schätzern ein. Mit dem <em>mittlere quadratischen Fehler</em> eines Schätzers <span class="math inline">\(\hat{\tau}\)</span> als Erwartungswert der quadrierten Abweichung von <span class="math inline">\(\hat{\tau}(\upsilon)\)</span> von <span class="math inline">\(\tau(\theta)\)</span> führen wir dann in <a href="#sec-mittlerer-quadratischer-fehler"><span>Kapitel&nbsp;19.2.3</span></a> eine Schätzereigenschaft ein, die es erlaubt die Genauigkeit und die Variabilität eines Schätzers im Sinne eines sogenannten <em>Bias-Variance-Tradeoffs</em> miteinander in Beziehung zu setzen. Die in <a href="#sec-cramer-rao-ungleichung"><span>Kapitel&nbsp;19.2.4</span></a> disktutierte <em>Cramér-Rao-Ungleichung</em> schließlich gibt eine untere Schranke für die Varianz erwartungstreuer Schätzer an. Ein erwartungstreuer Schätzer mit Varianz gleich der in der Cramér-Rao-Ungleichung gegebenen unteren Schranke hat die kleinstmögliche Varianz aller erwartungstreuen Schätzer und ist in diesem Sinne ein optimaler Schätzer.</p>
<section id="sec-erwartungstreue" class="level3" data-number="19.2.1">
<h3 data-number="19.2.1" class="anchored" data-anchor-id="sec-erwartungstreue"><span class="header-section-number">19.2.1</span> Erwartungstreue</h3>
<p>Der Begriff der Erwartungstreue eines Schätzers ergibt sich im Kontext des <em>Fehlers</em> und des <em>systematischen Fehlers</em> eines Schätzers wie folgt.</p>
<div id="def-fehler-systematischer-fehler-und-erwartungstreue" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 19.4 (Fehler, Systematischer Fehler und Erwartungstreue) </strong></span><span class="math inline">\(\upsilon\)</span> sei eine Stichprobe eines Frequentischen Inferenzmodells und <span class="math inline">\(\hat{\tau}\)</span> sei ein Schätzer für <span class="math inline">\(\tau\)</span>.</p>
<ul>
<li>Der <em>Fehler</em> von <span class="math inline">\(\hat{\tau}\)</span> ist definiert als <span class="math display">\[\begin{equation}
\hat{\tau}(\upsilon) - \tau(\theta).
\end{equation}\]</span></li>
<li>Der <em>systematische Fehler</em> (engl. <em>Bias</em>) von <span class="math inline">\(\hat{\tau}\)</span> ist definiert als <span class="math display">\[\begin{equation}
\mbox{B}(\hat{\tau} ) := \mathbb{E}_{\theta}(\hat{\tau} (\upsilon)) - \tau(\theta).
\end{equation}\]</span></li>
<li>Der Schätzer <span class="math inline">\(\hat{\tau}\)</span> heißt <em>erwartungstreu</em> (engl. <em>unbiased</em>), wenn <span class="math display">\[\begin{equation}
\mbox{B}(\hat{\tau} ) = 0\Leftrightarrow
\mathbb{E}_{\theta}(\hat{\tau} (\upsilon)) = \tau(\theta) \mbox{ für alle } \theta \in \Theta \mbox{ und alle } n \in \mathbb{N}.
\end{equation}\]</span> Andernfalls heißt <span class="math inline">\(\hat{\tau}\)</span> <em>verzerrt (engl. biased)</em>.</li>
</ul>
</div>
<p>Man beachte, dass in <a href="#def-fehler-systematischer-fehler-und-erwartungstreue">Definition&nbsp;<span>19.4</span></a> der Fehler eines Schätzers von der spezifischen Realisation der Stichprobe <span class="math inline">\(\upsilon\)</span> abhängt. Der systematische Fehler dagegen ist der erwartete Fehler über Stichprobenrealisationen und damit im Sinne eines Erwartungswerts von einer spezifischen Realisation unabhängig. Für den speziellen Fall eines Parameterpunktschätzers gilt nach <a href="#def-fehler-systematischer-fehler-und-erwartungstreue">Definition&nbsp;<span>19.4</span></a>, dass er erwartungstreu ist, wenn gilt, dass <span class="math display">\[\begin{equation}
\mathbb{E}_{\theta}(\hat{\theta}(\upsilon)) = \theta.
\end{equation}\]</span></p>
<p>Als erste Beispiele für erwartungstreue Schätzer betrachten wir in folgendem Theorem das Stichprobenmittel und die Stichprobenvarianz als Schätzer für den Erwartungswert und die Varianz einer Stichprobenvariable.</p>
<div id="thm-erwartungstreue-von-stichprobenmittel-und-stichprobenvarianz" class="theorem">
<p><span class="theorem-title"><strong>Theorem 19.3 (Erwartungstreue von Stichprobenmittel und Stichproenvarianz) </strong></span><span class="math inline">\(\upsilon := (\upsilon_1,...,\upsilon_n)\)</span> sei die Stichprobe eines parametrischen Produktmodells. Dann gelten</p>
<ol type="1">
<li>Das Stichprobenmittel <span class="math display">\[\begin{equation}
\bar{\upsilon} := \frac{1}{n}\sum_{i=1}^n \upsilon_i
\end{equation}\]</span> ist ein erwartungstreuer Schätzer des Erwartungswerts <span class="math inline">\(\mathbb{E}_\theta(\upsilon_1)\)</span>.</li>
<li>Die Stichprobenvarianz <span class="math display">\[\begin{equation}
S^2 := \frac{1}{n-1}\sum_{i=1}^n (\upsilon_i - \bar{\upsilon})^2
\end{equation}\]</span> ist ein erwartungstreuer Schätzer der Varianz <span class="math inline">\(\mathbb{V}_\theta(\upsilon_1)\)</span>.</li>
</ol>
</div>
<div class="proof">
<p><span class="proof-title"><em>Beweis</em>. </span>(1) Die Erwartungstreue des Stichprobenmittels ergibt mit den Eigenschaften des Erwartungswerts (vgl. <a href="205-Erwartungswerte.html#thm-eigenschaften-des-erwartungswerts">Theorem&nbsp;<span>13.3</span></a>) aus <span class="math display">\[\begin{align}
\mathbb{E}_\theta(\bar{\upsilon})
= \mathbb{E}_\theta \left(\frac{1}{n}\sum_{i=1}^n  \upsilon_i \right)
= \frac{1}{n}\sum_{i=1}^n  \mathbb{E}_\theta\left( \upsilon_i \right)
= \frac{1}{n}\sum_{i=1}^n  \mathbb{E}_\theta\left( \upsilon_1 \right)
= \frac{1}{n} n  \mathbb{E}_\theta\left( \upsilon_1 \right)
=  \mathbb{E}_\theta\left( \upsilon_1 \right).
\end{align}\]</span></p>
<p>(2) Um die Erwartungstreue der Stichprobenvarianz zu zeigen, halten wir zunächst fest, dass mit den Eigenschaften der Varianz gilt, dass (vgl. <a href="205-Erwartungswerte.html#thm-eigenschaften-der-varianz">Theorem&nbsp;<span>13.5</span></a>) <span class="math display">\[\begin{equation}
\mathbb{V}_\theta(\bar{\upsilon})
= \mathbb{V}_\theta\left(\frac{1}{n} \sum_{i=1}^n \upsilon_i \right)
= \frac{1}{n^2} \sum_{i=1}^n \mathbb{V}_\theta\left( \upsilon_i \right)
= \frac{1}{n^2} \sum_{i=1}^n  \mathbb{V}_\theta\left( \upsilon_1 \right)
= \frac{1}{n^2} n \mathbb{V}_\theta\left( \upsilon_1 \right)
= \frac{\mathbb{V}_\theta\left( \upsilon_1 \right)}{n}.
\end{equation}\]</span> Weiterhin gilt für den Term der summierten quadratischen Abweichungen in der Stichprobenvarianz, dass <span class="math display">\[\begin{align}
\sum_{i=1}^n \left(\upsilon_i - \bar{\upsilon}\right)^2 = \sum_{i=1}^n (\upsilon_i - \mathbb{E}_\theta(\upsilon_1))^2 - n(\bar{\upsilon} - \mathbb{E}_\theta(\upsilon_1))^2,
\end{align}\]</span> weil <span class="math display">\[\begin{align}
\begin{split}
\sum_{i=1}^n \left(\upsilon_i - \bar{\upsilon}\right)^2
&amp; = \sum_{i=1}^n \left(\upsilon_i - \mathbb{E}_\theta(\upsilon_1) - \bar{\upsilon} + \mathbb{E}_\theta(\upsilon_1) \right)^2 \\
&amp; = \sum_{i=1}^n \left((\upsilon_i - \mathbb{E}_\theta(\upsilon_1)) - (\bar{\upsilon} - \mathbb{E}_\theta(\upsilon_1)) \right)^2 \\
&amp; = \sum_{i=1}^n (\upsilon_i-\mathbb{E}_\theta(\upsilon_1))^2 - 2(\bar{\upsilon}-\mathbb{E}_\theta(\upsilon_1))\left(\sum_{i=1}^n(\upsilon_i-\mathbb{E}_\theta(\upsilon_1))\right) + \sum_{i=1}^n (\bar{\upsilon}-\mathbb{E}_\theta(\upsilon_1))^2 \\
&amp; = \sum_{i=1}^n (\upsilon_i-\mathbb{E}_\theta(\upsilon_1))^2 - 2(\bar{\upsilon}-\mathbb{E}_\theta(\upsilon_1))\left(\sum_{i=1}^n\upsilon_i- n\mathbb{E}_\theta(\upsilon_1)\right) + n(\bar{\upsilon}-\mathbb{E}_\theta(\upsilon_1))^2 \\
&amp; = \sum_{i=1}^n (\upsilon_i-\mathbb{E}_\theta(\upsilon_1))^2 - 2(\bar{\upsilon}-\mathbb{E}_\theta(\upsilon_1))\left(n\left(\frac{1}{n}\sum_{i=1}^n\upsilon_i\right)- n\mathbb{E}_\theta(\upsilon_1)\right) + n(\bar{\upsilon}-\mathbb{E}_\theta(\upsilon_1))^2 \\
&amp; = \sum_{i=1}^n (\upsilon_i-\mathbb{E}_\theta(\upsilon_1))^2 - 2n(\bar{\upsilon}-\mathbb{E}_\theta(\upsilon_1))^2 + n(\bar{\upsilon}-\mathbb{E}_\theta(\upsilon_1))^2 \\
&amp; = \sum_{i=1}^n (\upsilon_i - \mathbb{E}_\theta(\upsilon_1))^2 - n(\bar{\upsilon} - \mathbb{E}_\theta(\upsilon_1))^2.
\end{split}
\end{align}\]</span> Zusammen ergibt sich also <span class="math display">\[\begin{align}
\mathbb{E}_\theta\left((n-1)S^2\right)
&amp; = \mathbb{E}_\theta\left(\sum_{i=1}^n \left(\upsilon_i - \bar{\upsilon}\right)^2 \right) \\
&amp; = \mathbb{E}_\theta\left(\sum_{i=1}^n (\upsilon_i - \mathbb{E}_\theta(\upsilon_1))^2 - n(\bar{\upsilon} - \mathbb{E}_\theta(\upsilon_1))^2 \right) \\
&amp; = \sum_{i=1}^n \mathbb{E}_\theta\left((\upsilon_i - \mathbb{E}_\theta(\upsilon_1))^2\right) - n \mathbb{E}_\theta\left((\bar{\upsilon} - \mathbb{E}_\theta(\upsilon_1))^2 \right) \\
&amp; = n \mathbb{V}_\theta(\upsilon_1) - n \mathbb{V}_\theta(\bar{\upsilon}) \\
&amp; = n \mathbb{V}_\theta(\upsilon_1) - n \frac{\mathbb{V}_\theta(\upsilon_1)}{n} \\
&amp; = n \mathbb{V}_\theta(\upsilon_1) - \mathbb{V}_\theta(\upsilon_1) \\
&amp; = (n - 1) \mathbb{V}_\theta(\upsilon_1).
\end{align}\]</span> Schließlich ergibt sich dann <span class="math display">\[\begin{equation}
\mathbb{E}_\theta\left(S^2\right)
= \mathbb{E}_\theta\left(\frac{1}{n-1}(n-1)S^2 \right)
= \frac{1}{n-1}\mathbb{E}_\theta\left((n-1)S^2 \right)
= \frac{1}{n-1}(n - 1)  \mathbb{V}_\theta(\upsilon_1)
= \mathbb{V}_\theta(\upsilon_1)
\end{equation}\]</span> und damit die Erwartungstreue der Stichprobenvarianz als Schätzer der Varianz.</p>
</div>
<p>Natürlich sind in <a href="#thm-erwartungstreue-von-stichprobenmittel-und-stichprobenvarianz">Theorem&nbsp;<span>19.3</span></a> aufgrund der identischen Verteilung der Stichprobenvariablen eines parametrischen Produktmodells das Stichprobenmittel und die Stichprobenvarianz auch erwartungstreue Schätzer des Erwartungswertes und der Varianz einer beliebigen Stichprobenvariablen <span class="math inline">\(\upsilon_i\)</span> mit <span class="math inline">\(1 \le i \le n\)</span>. Man beachte, dass im Beweis der Erwartungstreue der Stichprobenvarianz der Nenner <span class="math inline">\(n-1\)</span> in der Definition der Stichprobenvarianz eine entscheidende Rolle spielt.</p>
<p>Obwohl die Stichprobenvarianz ein unverzerrter Schätzer der Varianz einer Stichprobenvariable eines parametrischen Produktmodells ist, trifft dies auf die Stichprobenstandardabweichung als Schätzer der Standardabweichung nicht zu. Dies ist Inhalt des folgenden Theorems.</p>
<div id="thm-verzerrtheit-der-stichprobenstandardabweichung" class="theorem">
<p><span class="theorem-title"><strong>Theorem 19.4 (Verzerrtheit der Stichprobenstandardabweichung) </strong></span><span class="math inline">\(\upsilon = (\upsilon_1,...,\upsilon_n)\)</span> sei die Stichprobe eines parametrischen Produktmodells. Dann ist die Stichprobenstandard- abweichung <span class="math display">\[\begin{equation}
S := \sqrt{S^2}
\end{equation}\]</span> ein verzerrter Schätzer der Standardabweichung <span class="math inline">\(\mathbb{S}_\theta(\upsilon_1)\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Beweis</em>. </span>Wir halten zunächst fest, dass <span class="math inline">\(\sqrt{\cdot}\)</span> eine strikt konkave Funktion und <span class="math inline">\(\sigma^2 &gt; 0\)</span> ist. Dann aber gilt mit der Jensenschen Ungleichung <span class="math inline">\(\mathbb{E}(f(\xi)) &lt; f(\mathbb{E}(\xi))\)</span> für strikt konkave Funktionen (vgl. <a href="206-Ungleichungen.html#thm-jensensche-ungleichung">Theorem&nbsp;<span>14.5</span></a>), dass <span class="math display">\[\begin{equation}
\mathbb{E}_\theta(S)
= \mathbb{E}_\theta\left(\sqrt{S^2}\right)
&lt; \sqrt{\mathbb{E}_\theta(S^2)}
= \sqrt{\mathbb{V}_\theta(\upsilon_1)}
= \mathbb{S}_\theta(\upsilon_1).
\end{equation}\]</span></p>
</div>
<p>Allgemein führen nichtlineare Transformationen von erwartungstreuen Schätzern oft auf verzerrte Schätzer, was wir hier aber nicht weiter vertiefen wollen. Folgender <strong>R</strong> Code demonstriert exemplarisch die Begriffe der Unverzerrtheit und Verzerrtheit von Stichprobenmittel, Stichprobenvarianz und Stichprobenstandardabweichung am Beispiel eines parametrischen Produktmodells mit Stichprobenverteilung<br>
<span class="math display">\[\begin{equation}
\upsilon_1,...,\upsilon_{12} \sim N(1.7,2)
\end{equation}\]</span> Dabei werden die Erwartungswerte der Schätzer anhand ihrer Stichprobenmittel über viele Realisierungen von <span class="math inline">\(\upsilon_1,...,\upsilon_{12}\)</span> als Funktion der Anzahl an Realsierungen (Simulationen) geschätzt.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1"></a><span class="co"># Modellformulierung</span></span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="fu">set.seed</span>(<span class="dv">0</span>)                          <span class="co"># Zufallszahlengenerator           </span></span>
<span id="cb3-3"><a href="#cb3-3"></a>mu      <span class="ot">=</span> <span class="fl">1.7</span>                        <span class="co"># wahrer, aber unbekannter, Erwartungswertparameter</span></span>
<span id="cb3-4"><a href="#cb3-4"></a>sigsqr  <span class="ot">=</span> <span class="dv">2</span>                          <span class="co"># wahrer, aber unbekannter, Varianzparameter</span></span>
<span id="cb3-5"><a href="#cb3-5"></a>n       <span class="ot">=</span> <span class="dv">12</span>                         <span class="co"># Stichprobenumfang n</span></span>
<span id="cb3-6"><a href="#cb3-6"></a>nsim    <span class="ot">=</span> <span class="fl">5e4</span>                        <span class="co"># Anzahl der Simulationen</span></span>
<span id="cb3-7"><a href="#cb3-7"></a>y_bar   <span class="ot">=</span> <span class="fu">rep</span>(<span class="cn">NaN</span>,nsim)              <span class="co"># Stichprobenmittelarray</span></span>
<span id="cb3-8"><a href="#cb3-8"></a>s_sqr   <span class="ot">=</span> <span class="fu">rep</span>(<span class="cn">NaN</span>,nsim)              <span class="co"># Stichprobenvarianzarray</span></span>
<span id="cb3-9"><a href="#cb3-9"></a>s       <span class="ot">=</span> <span class="fu">rep</span>(<span class="cn">NaN</span>,nsim)              <span class="co"># Stichprobenstandardabweichungarray</span></span>
<span id="cb3-10"><a href="#cb3-10"></a></span>
<span id="cb3-11"><a href="#cb3-11"></a><span class="co"># Simulationsiterationen</span></span>
<span id="cb3-12"><a href="#cb3-12"></a><span class="cf">for</span>(sim <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nsim){</span>
<span id="cb3-13"><a href="#cb3-13"></a></span>
<span id="cb3-14"><a href="#cb3-14"></a>    <span class="co"># Stichprobenrealisation von \upsilon_1,...,\upsilon_{12}</span></span>
<span id="cb3-15"><a href="#cb3-15"></a>    y          <span class="ot">=</span> <span class="fu">rnorm</span>(n,mu,<span class="fu">sqrt</span>(sigsqr))</span>
<span id="cb3-16"><a href="#cb3-16"></a></span>
<span id="cb3-17"><a href="#cb3-17"></a>    <span class="co"># Erwartungswert-, Varianz-, StandardabweichungSchätzer</span></span>
<span id="cb3-18"><a href="#cb3-18"></a>    y_bar[sim] <span class="ot">=</span> <span class="fu">mean</span>(y)             <span class="co"># Stichprobenmittel</span></span>
<span id="cb3-19"><a href="#cb3-19"></a>    s_sqr[sim] <span class="ot">=</span> <span class="fu">var</span>(y)              <span class="co"># Stichprobenvarianz</span></span>
<span id="cb3-20"><a href="#cb3-20"></a>    s[sim]     <span class="ot">=</span> <span class="fu">sd</span>(y)               <span class="co"># Stichprobenstandardabweichung</span></span>
<span id="cb3-21"><a href="#cb3-21"></a>}</span>
<span id="cb3-22"><a href="#cb3-22"></a></span>
<span id="cb3-23"><a href="#cb3-23"></a><span class="co"># Erwartungswertschaetzung</span></span>
<span id="cb3-24"><a href="#cb3-24"></a>E_hat_y_bar <span class="ot">=</span> <span class="fu">cumsum</span>(y_bar)<span class="sc">/</span>(<span class="dv">1</span><span class="sc">:</span>nsim) <span class="co"># \mathbb{E}(\bar{\upsilon}) Schaetzungen</span></span>
<span id="cb3-25"><a href="#cb3-25"></a>E_hat_s_sqr <span class="ot">=</span> <span class="fu">cumsum</span>(s_sqr)<span class="sc">/</span>(<span class="dv">1</span><span class="sc">:</span>nsim) <span class="co"># \mathbb{E}(S^2) Schaetzungen</span></span>
<span id="cb3-26"><a href="#cb3-26"></a>E_hat_s     <span class="ot">=</span> <span class="fu">cumsum</span>(s)    <span class="sc">/</span>(<span class="dv">1</span><span class="sc">:</span>nsim) <span class="co"># \mathbb{E}(S) Schaetzungen</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><a href="#fig-erwartungstreue">Abbildung&nbsp;<span>19.1</span></a> visualisiert die Ergebnisse obiger Simulation. Gezeigt sind Schätzungen der Erwartungswerte von Stichprobenmittel, Stichprobenvarianz und Stichprobenstandardabweichung als Funktion der Anzahl an Realisierungen der Stichprobenvariablen <span class="math inline">\(\upsilon_1,...,\upsilon_{12}\)</span> sowie die wahren, aber unbekannten, Werte des Erwartungswerts, der Varianz und der Standardabweichung der <span class="math inline">\(\upsilon_i\)</span> mit <span class="math inline">\(1\le i \le 12\)</span>. Es fällt auf, dass diese Schätzungen bei geringer Realisierungsanzahl variabler ausfallen. Ab einer Schätzung basierend auf etwa 10000 Realisierungen von <span class="math inline">\(\upsilon_1,...,\upsilon_{12}\)</span> entsprechen die Stichprobenmittel von <span class="math inline">\(\bar{\upsilon}\)</span> und <span class="math inline">\(S^2\)</span> gemäß ihrer Erwartungstreue ihren wahren, aber unbekannten, Werten. Die Stichprobenstandardabweichung dagegen zeigt gemäß ihrer Verzerrtheit auch bei weiter ansteigenden Anzahlen von der Realsierungen von <span class="math inline">\(\upsilon_1,...,\upsilon_{12}\)</span><br>
konstant eine zu niedrige Schätzung der wahren, aber unbekannten, Standardabweichung.</p>
<div id="fig-erwartungstreue" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="./_figures/302-erwartungstreue.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Abbildung&nbsp;19.1: Simulation der Erwartungstreue von Stichprobenmittel und Stichprobenvarianz als Schätzer des Erwartungswerts und der Varianz bei normalverteilten Stichprobenvariablen und Simulation der Verzerrtheit der Stichprobenstandardabweichung als Schätzer der Standardabweichung bei normalverteilten Stichprobenvariablen</figcaption>
</figure>
</div>
</section>
<section id="sec-varianz-und-standardfehler" class="level3" data-number="19.2.2">
<h3 data-number="19.2.2" class="anchored" data-anchor-id="sec-varianz-und-standardfehler"><span class="header-section-number">19.2.2</span> Varianz und Standardfehler</h3>
<p>Im vorherigen Abschnitt haben wir den Erwartungswert eines Schätzers betrachtet. In diesem Abschnitt betrachten wir seine Varianz und seine Standardabweichung und führen die mit diesen assoziierten Begriffe ein. Wir nutzen folgende Definition.</p>
<div id="def-varianz-und-standardfehler" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 19.5 (Varianz und Standardfehler) </strong></span><span class="math inline">\(\upsilon = (\upsilon_1,...,\upsilon_n)\)</span> sei die Stichprobe eines Frequentistischen Inferenzmodells und <span class="math inline">\(\hat{\tau}\)</span> sei ein Schätzer von <span class="math inline">\(\tau\)</span>.</p>
<ul>
<li>Die <em>Varianz</em> von <span class="math inline">\(\hat{\tau}\)</span> ist definiert als <span class="math display">\[\begin{equation}
\mathbb{V}_\theta(\hat{\tau} ) :=
\mathbb{E}_\theta
\left((\hat{\tau} (\upsilon) - \mathbb{E}_\theta(\hat{\tau} (\upsilon)))^2\right).
\end{equation}\]</span></li>
<li>Der <em>Standardfehler</em> von <span class="math inline">\(\hat{\tau}\)</span> ist definiert als <span class="math display">\[\begin{equation}
\mbox{SE}(\hat{\tau} ) := \sqrt{\mathbb{V}_\theta(\hat{\tau})}.
\end{equation}\]</span></li>
</ul>
</div>
<p>Die Varianz eines Schätzers <span class="math inline">\(\hat{\tau}\)</span> ist also als die Varianz der Zufallsvariable <span class="math inline">\(\hat{\tau}(\upsilon)\)</span> definiert. Der Standardfehler eines Schätzers <span class="math inline">\(\hat{\tau}\)</span> ist als die Standardabweichung von <span class="math inline">\(\hat{\tau}(\upsilon)\)</span> definiert. Als erstes Beispiel für einen Standardfehler betrachten wir den <em>Standardfehler des Stichprobenmittels</em>.</p>
<div id="thm-standardfehler-des-stichprobenmittels" class="theorem">
<p><span class="theorem-title"><strong>Theorem 19.5 (Standardfehler des Stichprobenmittels) </strong></span><span class="math inline">\(\upsilon = (\upsilon_1,...,\upsilon_n)\)</span> sei die Stichprobe eines parametrischen Produktmodells. Dann ist der gegeben durch <span class="math display">\[\begin{equation}
\mbox{SE}(\bar{\upsilon}) = \frac{\mathbb{S}_\theta(\upsilon_1)}{\sqrt{n}}.
\end{equation}\]</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Beweis</em>. </span>Mit der Varianz des Stichprobenmittels ergibt sich <span class="math display">\[\begin{equation}
\mbox{SE}(\bar{\upsilon})
= \sqrt{\mathbb{V}_\theta(\bar{\upsilon})}
= \sqrt{\frac{\mathbb{V}_\theta(\upsilon_1)}{n}}
= \frac{\mathbb{S}_\theta(\upsilon_1)}{\sqrt{n}}.
\end{equation}\]</span></p>
</div>
<p>Der Standardfehler des Mittelwerts beschreibt die Variabilität des Stichprobenmittels. Da die Standardabweichung <span class="math inline">\(\mathbb{S}_\theta(\upsilon_1)\)</span> unbekannt ist, ist auch der Standardfehler <span class="math inline">\(\mbox{SE}(\bar{\upsilon})\)</span> unbekannt, kann also nur geschätzt werden. Mit der Stichprobenstandardabweichung als verzerrter Schätzer der Standardabweichung <span class="math inline">\(\mathbb{S}_\theta(\upsilon_1)\)</span> ergibt sich ein ebenfalls verzerrter Schätzer für den Standardfehler des Stichprobenmittels zu <span class="math display">\[\begin{equation}
\hat{\mbox{SE}}(\bar{\upsilon}) = \frac{S}{\sqrt{n}}.
\end{equation}\]</span></p>
<p>Als zweites Beispiel wollen wir den Standardfehler des Maximum-Likelihood Schätzers für den Parameter eines Bernoulli-Modells betrachten.</p>
<div id="thm-standardfehler-des-bernoulli-ml-parameters-schaetzers" class="theorem">
<p><span class="theorem-title"><strong>Theorem 19.6 (Standardfehler des Maximum-Likelihood Schätzers des Bernoullimodellparameters) </strong></span>Es sei <span class="math inline">\(\upsilon = (\upsilon_1,...,\upsilon_n)\)</span> die Stichproben eines Bernoullimodells und <span class="math inline">\(\hat{\mu}^{\mbox{\tiny ML}}\)</span> sei der Maximum-Likelihood Schätzer für den Bernoullimodellparameter <span class="math inline">\(\mu\)</span>. Dann ist der Standardfehler von <span class="math inline">\(\hat{\mu}^{\mbox{\tiny ML}}\)</span> gegeben durch <span class="math display">\[\begin{equation}
\mbox{SE}\left(\hat{\mu}^{\mbox{\tiny ML}}\right) = \sqrt{\frac{\mu(1-\mu)}{n}}.
\end{equation}\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Beweis</em>. </span>Es gilt <span class="math display">\[\begin{align}
\begin{split}
\mbox{SE}\left(\hat{\mu}^{\mbox{\tiny ML}}\right)
= \sqrt{\mathbb{V}_\mu\left(\hat{\mu}^{\mbox{\tiny ML}}\right)}
= \sqrt{\mathbb{V}_\mu\left(\frac{1}{n}\sum_{i=1}^n \upsilon_i \right)}  
= \sqrt{\frac{1}{n^2}\sum_{i=1}^n \mathbb{V}_\mu(\upsilon_i)}
= \sqrt{\frac{n \mu(1-\mu)}{n^2}}
= \sqrt{\frac{\mu(1-\mu)}{n}},
\end{split}
\end{align}\]</span> wobei die dritte Gleichung mit der Unabhängigkeit der <span class="math inline">\(\upsilon_i\)</span> und die vierte Gleichung mit der Varianz <span class="math inline">\(\mathbb{V}_\mu(\upsilon_1) = \mathbb{V}_\mu(\upsilon_i) = \mu(1-\mu)\)</span> der Stichprobenvariablen folgt.</p>
</div>
<p>Wie im Falle des Standardfehlers des Stichprobenmittels ist auch der Standardfehler des Maximum-Likelihood Schätzers des Bernoullimodellparameters ein wahrer, aber unbekannter, Wert. Ein Schätzer für <span class="math inline">\(\mbox{SE}\left(\hat{\mu}^{\mbox{\tiny ML}}\right)\)</span> ergibt sich mit dem Maximum-Likelihood Schätzer für den Bernoullimodellparameter durch <span class="math display">\[\begin{equation}
\hat{\mbox{SE}}\left(\hat{\mu}^{\mbox{\tiny ML}}\right)
= \sqrt{\frac{\hat{\mu}^{\mbox{\tiny ML}}(1-\hat{\mu}^{\mbox{\tiny ML}})}{n}}.
\end{equation}\]</span></p>
<p>Folgender <strong>R</strong> Code simuliert die Verteilung des Maximum-Likelihood Schätzers für den Parameter eines Bernoullimodells mit wahrem, aber unbekanntem, Parameterwert <span class="math inline">\(\mu := 0.4\)</span> für die Stichprobenumfänge <span class="math inline">\(n = 20, n = 100\)</span> und <span class="math inline">\(n = 200\)</span>. <a href="#fig-sem">Abbildung&nbsp;<span>19.2</span></a> visualisiert die resultierenden Verteilungen mithilfe von Histogrammen. Die Variabilität der Schätzwerte, also die Breite der Histogrammverteilungen, hängt dabei offenbar vom Stichprobenumfang ab und höhere Stichprobenumfänge resultieren in einer geringeren Variabilität des Schätzers. Diesen Gedanken werden wir im Abschnitt <a href="#sec-asymptotische-schaetzereigenschaften"><span>Kapitel&nbsp;19.3</span></a> vertiefen.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1"></a><span class="co"># Modellformulierung</span></span>
<span id="cb4-2"><a href="#cb4-2"></a>mu       <span class="ot">=</span> <span class="fl">0.4</span>                                                      <span class="co"># wahrer, aber unbekannter, Parameterwert</span></span>
<span id="cb4-3"><a href="#cb4-3"></a>n_all    <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">20</span>,<span class="dv">100</span>,<span class="dv">200</span>)                                            <span class="co"># Stichprobenumfänge n</span></span>
<span id="cb4-4"><a href="#cb4-4"></a>ns       <span class="ot">=</span> <span class="fl">1e4</span>                                                      <span class="co"># Anzahl der Simulationen</span></span>
<span id="cb4-5"><a href="#cb4-5"></a>mu_hat   <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(<span class="cn">NaN</span>, <span class="fu">length</span>(n_all)<span class="sc">*</span>ns), <span class="at">nrow =</span> <span class="fu">length</span>(n_all)) <span class="co"># Maximum-Likelihood Schätzearray</span></span>
<span id="cb4-6"><a href="#cb4-6"></a>                    </span>
<span id="cb4-7"><a href="#cb4-7"></a><span class="co"># Stichprobenumfängeiterationen</span></span>
<span id="cb4-8"><a href="#cb4-8"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="fu">seq_along</span>(n_all)){</span>
<span id="cb4-9"><a href="#cb4-9"></a></span>
<span id="cb4-10"><a href="#cb4-10"></a>    <span class="co"># Simulationsiterationen</span></span>
<span id="cb4-11"><a href="#cb4-11"></a>    <span class="cf">for</span>(s <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>ns){</span>
<span id="cb4-12"><a href="#cb4-12"></a>        y               <span class="ot">=</span> <span class="fu">rbinom</span>(n_all[i],<span class="dv">1</span>,mu)                     <span class="co"># Stichprobenrealisation von y_1,...,y_n</span></span>
<span id="cb4-13"><a href="#cb4-13"></a>        mu_hat[i,s]  <span class="ot">=</span> <span class="fu">mean</span>(y)                                      <span class="co"># Stichprobenmittel</span></span>
<span id="cb4-14"><a href="#cb4-14"></a>    }</span>
<span id="cb4-15"><a href="#cb4-15"></a>}</span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="fig-sem" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="./_figures/302-sem.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Abbildung&nbsp;19.2: Simulation der Verteilung des Maximum-Likelihood Schätzers eines Bernoullimodells. Die Variabilität des Schätzers hängt dabei offenbar vom Stichprobenumfang <span class="math inline">\(n\)</span> ab.</figcaption>
</figure>
</div>
</section>
<section id="sec-mittlerer-quadratischer-fehler" class="level3" data-number="19.2.3">
<h3 data-number="19.2.3" class="anchored" data-anchor-id="sec-mittlerer-quadratischer-fehler"><span class="header-section-number">19.2.3</span> Mittlerer quadratischer Fehler</h3>
<p>Mit der Erwartungstreue und der Varianz eines Schätzers haben wir in den beiden vorherigen Abschnitten zwei unabhängige Kriterien für die Güte von Schätzern kennengelernt. Der in diesem Abschnitt eingeführte <em>Mittlere quadratische Fehler</em> eines Schätzers ermöglicht eine integrierte Betrachtung der Genauigkeit (Erwatungstreue) und Variabilität (Varianz) eines Schätzer im Sinne seiner sogenannten <em>Bias-Varianz-Zerlegung</em>. Wir definieren den mittleren quadratischen Fehler eines Schätzers zunächt wie folgt.</p>
<div id="def-mittlerer-quadratischer-fehler" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 19.6 (Mittlerer quadratischer Fehler) </strong></span><span class="math inline">\(\upsilon = (\upsilon_1,...,\upsilon_n)\)</span> sei die Stichprobe eines parametrischen Produktmodells und <span class="math inline">\(\hat{\tau}\)</span> ein Schätzer für <span class="math inline">\(\tau\)</span>. Dann ist der <em>mittlere quadratischer Fehler (engl. mean squared error)</em> von <span class="math inline">\(\hat{\tau}\)</span> definiert als <span class="math display">\[\begin{equation}
\mbox{MQF}(\hat{\tau})
:= \mathbb{E}_\theta\left((\hat{\tau}(\upsilon) - \tau(\theta))^2\right).
\end{equation}\]</span></p>
</div>
<p>Der mittlere quadratische Fehler von <span class="math inline">\(\hat{\tau}\)</span> ist also die erwartete quadrierte Abweichung von <span class="math inline">\(\hat{\tau}(\upsilon)\)</span> von <span class="math inline">\(\tau(\theta)\)</span>. Man beachte, dass in Abgrenzung dazu die Varianz von <span class="math inline">\(\hat{\tau}\)</span> die erwartete quadrierte Abweichung von <span class="math inline">\(\hat{\tau}\)</span> von <span class="math inline">\(\mathbb{E}_\theta(\hat{\tau}(\upsilon))\)</span> ist. Dabei kann, wie in<br>
<a href="#sec-erwartungstreue"><span>Kapitel&nbsp;19.2.1</span></a> gesehen <span class="math inline">\(\mathbb{E}_\theta(\hat{\tau}(\upsilon))\)</span> mit <span class="math inline">\(\tau(\theta)\)</span> übereinstimmen, ein Schätzer also erwartungstreu sein, er muss es aber nicht. Nutzt man den mittleren quadratischen Fehler als Gütekriterium für einen Schätzer, zum Beispiel indem man versucht, einen Schätzer mit möglichst geringem mittleren quadratischen Fehler zu konstruieren, so kann man dabei eventuelle leichte Abweichungen von der Erwartungstreue zugunsten einer geringen Schätzervarianz in Kauf nehmen. Für den mittleren quadratischen Fehler gilt nämlich folgendes Theorem.</p>
<div id="thm-zerlegung-des-mittleren-quadratischen-fehlers" class="theorem">
<p><span class="theorem-title"><strong>Theorem 19.7 (Zerlegung des mittleren quadratischen Fehlers) </strong></span><span class="math inline">\(\upsilon = (\upsilon_1,...,\upsilon_n)\)</span> sei die Stichprobe eines parametrischen Produktmodells, <span class="math inline">\(\hat{\tau}\)</span> sei ein Schätzer für <span class="math inline">\(\tau\)</span>, und <span class="math inline">\(\mbox{MQF}(\hat{\tau})\)</span> sei der mittlere quadratische Fehler von <span class="math inline">\(\hat{\tau}\)</span>. Dann gilt <span class="math display">\[\begin{equation}
\mbox{MQF}(\hat{\tau}) = \mbox{B}(\hat{\tau})^2 + \mathbb{V}_\theta(\hat{\tau}).
\end{equation}\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Beweis</em>. </span>Zur Vereinfachung der Notation seien <span class="math inline">\(\tau := \tau(\theta)\)</span>, <span class="math inline">\(\hat{\tau} := \hat{\tau}(\upsilon)\)</span> und <span class="math inline">\(\bar{\tau}_n := \mathbb{E}_\theta(\hat{\tau}(\upsilon))\)</span>. Dann gilt: <span class="math display">\[\begin{align}
\begin{split}
\mathbb{E}_\theta\left((\hat{\tau} - \tau)^2\right)
&amp; = \mathbb{E}_\theta\left((\hat{\tau} - \bar{\tau}_n + \bar{\tau}_n - \tau)^2\right) \\
&amp; = \mathbb{E}_\theta
\left(
(\hat{\tau} - \bar{\tau}_n)^2 + 2(\hat{\tau} - \bar{\tau}_n)(\bar{\tau}_n - \tau) + (\bar{\tau}_n - \tau)^2
\right)
\\
&amp; = \mathbb{E}_\theta\left((\hat{\tau} - \bar{\tau}_n)^2\right) + 2\mathbb{E}_\theta\left((\hat{\tau} - \bar{\tau}_n)(\bar{\tau}_n - \tau)\right) + \mathbb{E}_\theta\left((\bar{\tau}_n - \tau)^2\right) \\
&amp; = \mathbb{E}_\theta\left((\hat{\tau} - \bar{\tau}_n)^2\right) + 2\mathbb{E}_\theta\left(
\hat{\tau}\bar{\tau}_n - \hat{\tau}\tau - \bar{\tau}_n\bar{\tau}_n + \bar{\tau}_n\tau
\right) + \mathbb{E}_\theta\left((\bar{\tau}_n - \tau)^2\right)
\\
&amp; =
\mathbb{E}_\theta\left((\hat{\tau} - \bar{\tau}_n)^2\right) + 2\left(
\bar{\tau}_n\bar{\tau}_n - \bar{\tau}_n\tau
\right) + \mathbb{E}_\theta\left((\bar{\tau}_n - \tau)^2\right) \\
&amp; =
\mathbb{E}_\theta\left((\hat{\tau} - \bar{\tau}_n)^2\right) + 0 + \mathbb{E}_\theta\left((\bar{\tau}_n - \tau)^2\right) \\
&amp; =
\mathbb{E}_\theta\left((\bar{\tau}_n - \tau)^2\right) + \mathbb{E}_\theta\left((\hat{\tau} - \bar{\tau}_n)^2\right) \\
&amp; =
\mathbb{E}_\theta\left((\mathbb{E}_\theta(\hat{\tau}) - \tau)^2\right) + \mathbb{E}_\theta\left((\hat{\tau} - \mathbb{E}_\theta(\hat{\tau}))^2\right) \\
&amp; =
(\mathbb{E}_\theta(\hat{\tau}) - \tau)^2 + \mathbb{V}_\theta(\hat{\tau}) \\
&amp; =
\mbox{B}(\hat{\tau})^2 + \mathbb{V}_\theta(\hat{\tau}).
\end{split}
\end{align}\]</span></p>
</div>
</section>
<section id="sec-cramer-rao-ungleichung" class="level3" data-number="19.2.4">
<h3 data-number="19.2.4" class="anchored" data-anchor-id="sec-cramer-rao-ungleichung"><span class="header-section-number">19.2.4</span> Cramér-Rao-Ungleichung</h3>
<p>Hat man mehrere erwartungstreue Schätzer vorliegen, so gilt, dass derjenige Schätzer mit der kleinsten Varianz am verlässlichsten seinen Zweck erfüllt. Weil aber die Stichprobenrealisierungen Frequentistischer Inferenzmodelle in aller Regel variabel sind, kann auch die Variabilität erwartungstreuer Schätzer nicht beliebig klein sein. Die <em>Cramér-Rao-Ungleichung</em> gibt eine untere Schranke für die Varianz erwartungstreuer Schätzer an. Ein erwartungstreuer Schätzer mit Varianz gleich dieser unteren Schranke hat damit die kleinstmögliche Varianz aller erwartungstreuer Schätzer und ist - in diesem Sinne - ein optimaler Schätzer.</p>
<p>Die Cramér-Rao-Ungleichung basiert auf dem Begriff der sogenannten <em>Fisher-Information</em>, welche wiederrum auf dem Begriff der <em>Scorefunktion</em> eines Frequentischen Inferenzmodells beruht. Wir führen im Folgenden also zunächst diese beiden Begrifflichkeiten ein, bevor die Cramér-Rao-Ungleichung formuliert und bewiesen werden soll.</p>
<p>Dabei gelten die vorgestellten Resultate allgemein nur unter einer Reihe mathematischer Annahmen, den sogenannten <em>Fisher-Regularitätsbedingungen</em>. Diese bestehen für ein Frequentistisches Inferenzmodell mit WMF oder WDF <span class="math inline">\(p_\theta\)</span> und Parameterraum <span class="math inline">\(\Theta\)</span> darin, dass angenommen wird, dass (1) <span class="math inline">\(\Theta\)</span> eine offene Menge ist, der wahre, aber unbekannte, Parameterwert damit nicht an einer Parameterraumgrenze liegen kann, (2) die Teilmenge von <span class="math inline">\(\Theta\)</span>, auf der <span class="math inline">\(p_\theta\)</span> von Null verschiedene Werte annimmt, nicht von <span class="math inline">\(\theta\)</span> abhängt, (3) das Modell selbst identifizierbar ist, dass also WMFen oder WDFen mit unterschiedliche Parameterwerten unterschiedliche Funktionen sind und damit unterschiedliche Stichprobenverteilungen implizieren, (4) die Likelihood-Funktion des Modells zweimal stetig differenzierbar und (5) dass für die Likelihood-Funktion Integration und Differentiation vertauscht werden dürfen. Wir setzen die Fisher-Regularitätsbedingungen also als erfüllt voraus und wollen nur Modelle mit eindimensionalen Parameterräumen <span class="math inline">\(\Theta \subseteq \mathbb{R}\)</span> betrachten. Wir definieren zunächst die Begriffe der <em>Scorefunktion</em> und der <em>Fisher-Information</em> wie folgt.</p>
<div id="def-scorefunktion-und-fisher-information" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 19.7 (Scorefunktion und Fisher-Information) </strong></span><span class="math inline">\(\upsilon = (\upsilon_1,...,\upsilon_n)\)</span> sei die Stichprobe eines parametrischen Produktmodells mit eindimensionalem Parameter <span class="math inline">\(\theta \in \Theta \subseteq \mathbb{R}\)</span> und <span class="math inline">\(\ell\)</span> sei die zugehörige Log-Likelihood-Funktion. Dann gelten:</p>
<ul>
<li>Die erste Ableitung von <span class="math inline">\(\ell\)</span> wird <em>Scorefunktion der Stichprobe</em> genannt und wird mit <span class="math display">\[\begin{equation}
S(\theta) := \frac{d}{d\theta}\ell(\theta)
\end{equation}\]</span> bezeichnet. Für <span class="math inline">\(n = 1\)</span> schreiben wir <span class="math inline">\(S(\theta) := S_1(\theta)\)</span> und nennen <span class="math inline">\(S(\theta)\)</span> <em>Scorefunktion einer Zufallsvariable</em>.</li>
<li>Die negative zweite Ableitung von <span class="math inline">\(\ell\)</span> wird <em>Fisher-Information der Stichprobe</em> genannt und mit <span class="math display">\[\begin{equation}
I(\theta) := -\frac{d^2}{d\theta^2}\ell(\theta)
\end{equation}\]</span> bezeichnet. Für <span class="math inline">\(n = 1\)</span> schreiben wir <span class="math inline">\(I(\theta) := I_1(\theta)\)</span> und nennen <span class="math inline">\(I(\theta)\)</span> die <em>Fisher-Information einer Zufallsvariable</em>.</li>
</ul>
</div>
<p>Da Likelihood- und Log-Likelihood-Funktionen von der Realisierung einer Stichprobe abhängen, sind sie vor dem Hintegrund eines Frequentistischen Inferenzmodells zufällige Funktionen. Da die Fisher-Information als Funktion der Log-Likelihood-Funktion damit auch eine Zufallsvariable ist, muss man zwischen den <em>beobachteten</em> und den <em>erwarteten</em> Werten der Fisher-Information unterscheiden.</p>
<div id="def-beobachtete-und-erwartete-fisher-information" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 19.8 (Beobachtete und erwartete Fisher-Information) </strong></span><span class="math inline">\(\upsilon = (\upsilon_1,...,\upsilon_n)\)</span> sei die Stichprobe eines parametrischen Produktmodells mit eindimensionalem Parameter <span class="math inline">\(\theta \in \Theta \subseteq \mathbb{R}\)</span>, <span class="math inline">\(\ell\)</span> sei die zugehörige Log-Likelihood-Funktion und <span class="math inline">\(\hat{\theta}^{\mbox{\tiny ML}}\)</span> sei ein Maximum-Likelihood-Schätzer von <span class="math inline">\(\theta\)</span>. Dann gelten:</p>
<ul>
<li>Die <em>beobachtete Fisher-Information der Stichprobe</em> ist definiert als <span class="math display">\[\begin{equation}
I\left(\hat{\theta}^{\mbox{\tiny ML}}\right)
:= -\frac{d^2}{d\theta^2}\ell\left(\hat{\theta}^{\mbox{\tiny ML}}\right),
\end{equation}\]</span> die beobachtete Fisher-Information der Stichprobe ist also die Fisher-Information an der Stelle des Maximum-Likelihood-Schätzers <span class="math inline">\(\hat{\theta}^{\mbox{\tiny ML}}\)</span>.</li>
<li>Die <em>erwartete Fisher-Information der Stichprobe</em> ist definiert als <span class="math display">\[\begin{equation}
J(\theta) := \mathbb{E}_\theta(I(\theta)).
\end{equation}\]</span> Für <span class="math inline">\(n = 1\)</span> schreiben wir <span class="math inline">\(J(\theta) := J_1(\theta)\)</span> und nennen <span class="math inline">\(J(\theta)\)</span> die </li>
</ul>
</div>
<p>Bevor wir diese Begrifflichkeiten anhand des Bernoullimodells (<a href="#thm-scorefunktion-und-fisher-informationen-des-bernoullimodells">Theorem&nbsp;<span>19.10</span></a>) und des Normalverteilungsmodells (<a href="#thm-scorefunktion-und-fisher-informationen-des-normalverteilungsmodells-bei-bekanntem-erwartungswertparameter">Theorem&nbsp;<span>19.12</span></a> und <a href="#thm-scorefunktion-und-fisher-informationen-des-normalverteilungsmodells-bei-bekanntem-varianzparameter">Theorem&nbsp;<span>19.11</span></a>) verdeutlichen wollen, führen wir mit der <em>Additivität der Fisher-Information</em> bei parametrischen Produktmodellen (<a href="#thm-additivität-der-fisher-information">Theorem&nbsp;<span>19.8</span></a>) und dem Erwartungswert und der Varianz der Scorefunktion (<a href="#thm-erwartungswert-und-varianz-der-scorefunktion">Theorem&nbsp;<span>19.9</span></a>) noch wichtige Eigenschaften der genannten Begriffe ein, die die folgende Diskussion vereinfachen.</p>
<div id="thm-additivität-der-fisher-information" class="theorem">
<p><span class="theorem-title"><strong>Theorem 19.8 (Additivität der Fisher-Information) </strong></span><span class="math inline">\(\upsilon = (\upsilon_1,...,\upsilon_n)\)</span> sei die Stichprobe eines parametrischen Produktmodells mit Parameter <span class="math inline">\(\theta \in \Theta \subseteq \mathbb{R}\)</span>, <span class="math inline">\(\ell\)</span> sei die zugehörige Log-Likelihood-Funktion und <span class="math inline">\(I(\theta)\)</span> und <span class="math inline">\(J(\theta)\)</span> seien die Fisher-Information und die erwartete Fisher-Information der Stichprobe, respektive. Dann gilt <span class="math display">\[\begin{equation}
I(\theta) = nI_1(\theta) \mbox{ und } J(\theta) = nJ_1(\theta).
\end{equation}\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Beweis</em>. </span>Wir zeigen das Resultat für die erwartete Fisher-Information, das Resultat für die beobachtete Fisher-Information gilt dann implizit. Mit der Linearität von Ableitungen und Erwartungswerten gilt <span class="math display">\[\begin{align}
\begin{split}
J(\theta)
&amp; = \mathbb{E}_\theta\left(-\frac{d^2}{d\theta^2} \ell(\theta)\right) \\
&amp; = \mathbb{E}_\theta\left(-\frac{d^2}{d\theta^2} \ln \left(\prod_{i=1}^n p_\theta(\upsilon_i)\right)\right) \\
&amp; = \mathbb{E}_\theta\left(-\frac{d^2}{d\theta^2} \sum_{i=1}^n \ln p_\theta(\upsilon_i)\right) \\
&amp; = \mathbb{E}_\theta\left(-\frac{d^2}{d\theta^2} \sum_{i=1}^n \ln p_\theta(y_1)\right) \\
&amp; = \mathbb{E}_\theta\left(-\frac{d^2}{d\theta^2} \ell_1(\theta)n\right) \\
&amp; =  n \mathbb{E}_\theta\left(-\frac{d^2}{d\theta^2}\ell_1(\theta))\right) \\
&amp; =  n J(\theta).
\end{split}
\end{align}\]</span></p>
</div>
<p>Nach <a href="#thm-additivität-der-fisher-information">Theorem&nbsp;<span>19.8</span></a> genügt es zur Berechnung der beobachteten oder erwarteten Fisher-Information einer Stichprobe bei parametrischen Produktmodellen also, die beobachtete oder erwartete Fisher-Information einer der Zufallsvariablen der Stichprobe zu berechnen. Weitere Vereinfachungen in der Bestimmung von Fisher-Informationen und der Begründung der Cramér-Rao-Ungleichung ergeben sich durch die im folgenden Theorem formulierten Identitäten.</p>
<div id="thm-erwartungswert-und-varianz-der-scorefunktion" class="theorem">
<p><span class="theorem-title"><strong>Theorem 19.9 (Erwartungswert und Varianz der Scorefunktion) </strong></span>Der Erwartungswert der Scorefunktion einer Zufallsvariable ist <span class="math display">\[\begin{equation}
\mathbb{E}_\theta(S(\theta)) = 0
\end{equation}\]</span> und die Varianz der Scorefunktion einer Zufallsvariable ist <span class="math display">\[\begin{equation}
\mathbb{V}_\theta(S(\theta)) = J(\theta).
\end{equation}\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Beweis</em>. </span>Wir betrachten nur den Fall, dass <span class="math inline">\(p_\theta\)</span> eine WDF ist und zeigen zunächst, dass <span class="math inline">\(\mathbb{E}_\theta(S(\theta)) = 0\)</span> ist. <span class="math display">\[\begin{align}
\begin{split}
\mathbb{E}_\theta(S(\theta))
&amp; = \int S(\theta)p_\theta(x) \,dx \\
&amp; = \int \frac{d}{d\theta}\ell(\theta)p_\theta(x) \,dx \\
&amp; = \int \frac{d}{d\theta} \ln L(\theta) p_\theta(x) \,dx \\
&amp; = \int \frac{1}{L(\theta)}\frac{d}{d\theta}L(\theta) p_\theta(x) \,dx  \\
&amp; = \int \frac{1}{p_\theta(x)}\frac{d}{d\theta}L(\theta) p_\theta(x) \,dx  \\
&amp; = \int \frac{d}{d\theta}L(\theta) \,dx  \\
&amp; = \frac{d}{d\theta} \int p_\theta(x)\,dx \\
&amp; = \frac{d}{d\theta} 1 \\
&amp; = 0.
\end{split}
\end{align}\]</span> Mit der Definition der Varianz folgt dann sofort, dass <span class="math inline">\(\mathbb{V}_\theta(S(\theta)) = \mathbb{E}_\theta(S(\theta)^2)\)</span> ist. Als nächstes zeigen wir, dass <span class="math inline">\(J(\theta) = \mathbb{E}_\theta(S(\theta)^2)\)</span> und deshalb <span class="math inline">\(\mathbb{V}_\theta(S(\theta)) = J(\theta)\)</span> ist. <span class="math display">\[\begin{align}
\begin{split}
J(\theta)
&amp; = \mathbb{E}_\theta\left(-\frac{d^2}{d\theta^2} \ln L(\theta)\right) \\
&amp; = \mathbb{E}_\theta\left(-\frac{d}{d\theta} \frac{\frac{d}{d\theta}L(\theta)}{L(\theta)}\right) \\
&amp; = \mathbb{E}_\theta\left(-\frac{\frac{d^2}{d\theta^2} L(\theta) L(\theta) - \frac{d}{d\theta}L(\theta)\frac{d}{d\theta}L(\theta)}{L(\theta)L(\theta)}\right) \\
&amp; = - \mathbb{E}_\theta\left(\frac{\frac{d^2}{d\theta^2} L(\theta)}{L(\theta)}\right) +
\mathbb{E}_\theta\left(\frac{\left(\frac{d}{d\theta}L(\theta)\right)^2}{(L(\theta))^2}\right) \\
&amp; = - \int \frac{\frac{d^2}{d\theta^2} L(\theta)}{L(\theta)} p_\theta(x) \,dx +
      \int \frac{\left(\frac{d}{d\theta}L(\theta)\right)^2}{(L(\theta))^2} p_\theta(x) \,dx \\
&amp; = - \frac{d^2}{d\theta^2} \int p_\theta(x) \,dx +
      \int \left(\frac{1}{L(\theta)}\frac{d}{d\theta}L(\theta)\right)^2 p_\theta(x) \,dx \\
&amp; = - \frac{d^2}{d\theta^2} 1 +
\int \left(\frac{d}{d\theta} \ln L(\theta) \right)^2 p_\theta(x) \,dx
= \mathbb{E}_\theta\left(S(\theta)^2\right).
\end{split}
\end{align}\]</span></p>
</div>
<p>Der Erwartungswert der Ableitung der Log-Likelihood-Funktion ist also immer Null und die erwartete Fisher-Information ist immer gleich der Varianz der Scorefunktion. Wir wollen die Scorefunktion und die verschiedenen Formen der Fisher-Information nun für die uns vertrauten Frequentistischen Inferenzmodelle konkret berechnen. Nachfolgendes Theorem fasst zunächst die Ergebnisse für das Bernoullimodell zusammen.</p>
<div id="thm-scorefunktion-und-fisher-informationen-des-bernoullimodells" class="theorem">
<p><span class="theorem-title"><strong>Theorem 19.10 (Scorefunktion und Fisher-Informationen des Bernoullimodells) </strong></span>Es sei <span class="math inline">\(\upsilon = (\upsilon_1,...,\upsilon_n)\)</span> die Stichprobe eines Bernoullimodells mit Parameter <span class="math inline">\(\mu \in ]0,1[\)</span>. Dann gelten:</p>
<ul>
<li>Die Scorefunktion der Stichprobe ist <span class="math display">\[\begin{equation}
S : ]0,1[ \to \mathbb{R}, \mu \mapsto S(\mu) := \frac{1}{\mu}\sum_{i=1}^n y_i  -  \frac{1}{1-\mu} \left(n - \sum_{i=1}^n y_i \right).
\end{equation}\]</span></li>
<li>Die Fisher-Information der Stichprobe ist <span class="math display">\[\begin{equation}
I : ]0,1[ \to \mathbb{R}, \mu \mapsto I(\mu) := I(\mu) =  \frac{ny}{\mu^2} + \frac{n(1 - y)}{1-\mu}^{2}.
\end{equation}\]</span></li>
<li>Die beobachtete Fisher-Information der Stichprobe ist <span class="math display">\[\begin{equation}
I : ]0,1[ \to \mathbb{R}, \hat{\mu}^{\mbox{\tiny ML}} \mapsto
I\left(\hat{\mu}^{\mbox{\tiny ML}}\right)
:= \frac{ny}{\hat{\mu}_{n}^{{\mbox{\tiny ML}}^2}} + \frac{n(1 - y)}{1-\hat{\mu}^{\mbox{\tiny ML}}}.
\end{equation}\]</span></li>
<li>Die erwartete Fisher-Information der Stichprobe ist <span class="math display">\[\begin{equation}
J : ]0,1[ \to \mathbb{R}, \mu \mapsto J(\mu)
:= \frac{n}{\mu(1-\mu)}.
\end{equation}\]</span></li>
</ul>
</div>
<div class="proof">
<p><span class="proof-title"><em>Beweis</em>. </span>Die Scorefunktion wurde bereits im Kontext der Maximum-Likelihood-Schätzung von <span class="math inline">\(\mu\)</span> hergeleitet. Wir betrachten die Fisher-Information einer einzelnen Bernoulli-Zufallsvariable <span class="math inline">\(\upsilon\)</span>. <span class="math display">\[\begin{align}
\begin{split}
I(\mu)
&amp; := -\frac{d^2}{d\mu^2} \ell_1(\mu)                                                                                        \\
&amp;  = -\frac{d^2}{d\mu^2} \ln p_\mu(y)                                                                                       \\
&amp;  = -\frac{d^2}{d\mu^2}\left(y \ln \mu + (1 - y) \ln (1-\mu)\right)                                                        \\
&amp;  = -\frac{\partial}{\partial{\mu}}\left(\frac{\partial}{\partial{\mu}}\left(y \ln \mu + (1 - y) \ln (1-\mu)\right)\right) \\
&amp;  = -\frac{\partial}{\partial{\mu}}\left(\frac{y}{\mu} + \frac{(1 - y)}{1-\mu}\right)                                      \\
&amp;  = -\left(-\frac{y}{\mu^2} - \frac{(1 - y)}{1-\mu}^{2}\right)                                                             \\
&amp;  =  \frac{y}{\mu^2} + \frac{(1 - y)}{1-\mu}^{2}.                                                                          \\
\end{split}
\end{align}\]</span> Damit ergibt sich die erwartete Fisher-Information der Zufallsvariable <span class="math inline">\(\upsilon\)</span> als <span class="math display">\[\begin{align}
\begin{split}
J(\mu)
&amp; = \mathbb{E}_\mu(I(\mu))                                                                  \\
&amp; = \mathbb{E}_\mu \left(\frac{\upsilon}{\mu^2} + \frac{(1 - \upsilon)}{1-\mu}^{2} \right)          \\
&amp; = \frac{\mathbb{E}_\mu(\upsilon)}{\mu^2} + \frac{(1 - \mathbb{E}_\mu(\upsilon))}{1-\mu}^{2}       \\
&amp; = \frac{\mu}{\mu^2} + \frac{(1 - \mu)}{1-\mu}^{2}                                         \\
&amp; = \frac{1}{\mu(1-\mu)}.                                                                   \\
\end{split}
\end{align}\]</span> Mit der Additivitätseigenschaft der Fisher-Information und der Definition der beobachteten Fisher-Information ergibt sich dann sofort <span class="math display">\[\begin{equation}
I(\mu)
=  \frac{ny}{\mu^2} + \frac{n(1 - y)}{1-\mu}^{2}
\mbox{ und }
J(\mu) = \frac{n}{\mu(1-\mu)}.
\end{equation}\]</span></p>
</div>
<p>Die Scorefunktion und die Fisher-Informationen des Normalverteilungsmodells betrachten wir lediglich unter der zusätzlichen Annahme eines bekannten Varianzparameters (<a href="#thm-scorefunktion-und-fisher-informationen-des-normalverteilungsmodells-bei-bekanntem-varianzparameter">Theorem&nbsp;<span>19.11</span></a>) bzw. eines bekannten Erwartungswertparameters (<a href="#thm-scorefunktion-und-fisher-informationen-des-normalverteilungsmodells-bei-bekanntem-erwartungswertparameter">Theorem&nbsp;<span>19.12</span></a>)</p>
<div id="thm-scorefunktion-und-fisher-informationen-des-normalverteilungsmodells-bei-bekanntem-varianzparameter" class="theorem">
<p><span class="theorem-title"><strong>Theorem 19.11 (Scorefunktion und Fisher-Informationen des Normalverteilungsmodells bei bekanntem Varianzparameter) </strong></span><span class="math inline">\(\upsilon = (\upsilon_1,...,\upsilon_n)\)</span> sei die Stichprobe eines Normalverteilungsmodells und der Varianzparameter <span class="math inline">\(\sigma^2\)</span> sei als bekannt vorausgesetzt. Dann gelten:</p>
<ul>
<li>Die Scorefunktion der Stichprobe ist <span class="math display">\[\begin{equation}
S : \mathbb{R} \to \mathbb{R}, \mu \mapsto S(\mu) := \frac{1}{\sigma^2}\sum_{i=1}^n(y_i-\mu).
\end{equation}\]</span></li>
<li>Die Fisher-Information der Stichprobe ist <span class="math display">\[\begin{equation}
I : \mathbb{R} \to \mathbb{R}, \mu \mapsto I(\mu) := \frac{n}{\sigma^2}.
\end{equation}\]</span></li>
<li>Die beobachtete Fisher-Information der Stichprobe ist <span class="math display">\[\begin{equation}
I(\hat{\mu}^{\mbox{\tiny ML}}_n) = \frac{n}{\sigma^2}.
\end{equation}\]</span></li>
<li>Die erwartete Fisher-Information der Stichprobe ist <span class="math display">\[\begin{equation}
J : \mathbb{R} \to \mathbb{R}, \mu \mapsto J(\mu) := \frac{n}{\sigma^2}.
\end{equation}\]</span></li>
</ul>
</div>
<div class="proof">
<p><span class="proof-title"><em>Beweis</em>. </span>Wir erinnern uns, dass die Log-Likelihood-Funktion eines Normalverteilungsmodells bei bekanntem Varianzparameter <span class="math inline">\(\sigma^2\)</span> durch <span class="math display">\[\begin{equation}
\ell : \mathbb{R} \to \mathbb{R},
\mu \mapsto \ell(\mu)
:= -\frac{n}{2} \ln 2\pi - \frac{n}{2} \ln \sigma^2  - \frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-\mu)^2
\end{equation}\]</span> gegeben ist. Damit ergibt sich die Scorefunktion als <span class="math display">\[\begin{align}
\begin{split}
S(\mu)
&amp;  = \frac{\partial}{\partial{\mu}}\ell(\mu)
= \frac{1}{\sigma^2}\sum_{i=1}^n(y_i-\mu)
\end{split}
\end{align}\]</span> Die Fisher-Information der Stichprobe ergibt sich als <span class="math display">\[\begin{align}
\begin{split}
I(\mu)
= -\frac{d^2}{d\mu^2}\ell(\mu)
= -\frac{\partial}{\partial{\mu}}S(\mu)
= -\frac{1}{\sigma^2}\frac{\partial}{\partial{\mu}}\left(\sum_{i=1}^n y_i -  n\mu \right)
= \frac{n}{\sigma^2}.
\end{split}
\end{align}\]</span> Die beobachtete Fisher-Information ist die Fisher-Information an der Stelle des Maximum-Likelihood Schätzes <span class="math inline">\(\hat{\mu}^{\mbox{\tiny ML}}_n\)</span>. Die erwartete Fisher-Information schließlich ergibt sich als <span class="math display">\[\begin{align}
\begin{split}
J(\mu)
= \mathbb{E}_\mu(I(\mu))
= \mathbb{E}_\mu\left(\frac{n}{\sigma^2}\right)
= \frac{n}{\sigma^2}.
\end{split}
\end{align}\]</span></p>
</div>
<div id="thm-scorefunktion-und-fisher-informationen-des-normalverteilungsmodells-bei-bekanntem-erwartungswertparameter" class="theorem">
<p><span class="theorem-title"><strong>Theorem 19.12 (Scorefunktion und Fisher-Informationen des Normalverteilungsmodells bei bekanntem Erwartungswertparameter) </strong></span><span class="math inline">\(\upsilon = (\upsilon_1,...,\upsilon_n)\)</span> sei die Stichprobe eines Normalverteilungsmodells und der Varianzparameter <span class="math inline">\(\sigma^2\)</span> sei als bekannt vorausgesetzt. Dann gelten:</p>
<ul>
<li>Die Scorefunktion der Stichprobe ist gegeben durch <span class="math display">\[\begin{equation}
S : \mathbb{R}_{&gt;0} \to \mathbb{R}, \sigma^2 \mapsto S(\sigma^2) := - \frac{n}{2 \sigma^2} + \frac{1}{2\sigma^4}\sum_{i=1}^n(y_i-\mu)^2
\end{equation}\]</span></li>
<li>Die Fisher-Information der Stichprobe ist gegeben durch <span class="math display">\[\begin{equation}
I : \mathbb{R}_{&gt;0} \to \mathbb{R}, \sigma^2 \mapsto I(\sigma^2) :=
\frac{1}{\sigma^6}\sum_{i=1}^n (y_i - \mu)^2 - \frac{n}{2\sigma^4}
\end{equation}\]</span></li>
<li>Die beobachtete Fisher-Information der Stichprobe ist gegeben durch <span class="math display">\[\begin{equation}
I(\hat{\sigma}^{2\,\mbox{\tiny ML}}_n) =  \frac{n}{2\hat{\sigma}_{\mbox{\tiny ML}}^4}
\end{equation}\]</span></li>
<li>Die erwartete Fisher-Information der Stichprobe ist gegeben durch <span class="math display">\[\begin{equation}
J : \mathbb{R}_{&gt;0} \to \mathbb{R}, \sigma^2 \mapsto J(\sigma^2) := \frac{n}{2\sigma^4}.
\end{equation}\]</span></li>
</ul>
</div>
<div class="proof">
<p><span class="proof-title"><em>Beweis</em>. </span>Wir erinnern uns, dass die Log-Likelihood-Funktion der Stichprobe eines Normalverteilungsmodells bei bekanntem Erwartungswert-Parameter <span class="math inline">\(\mu\)</span> durch <span class="math display">\[\begin{equation}
\ell : \mathbb{R}_{&gt;0} \to \mathbb{R},
\sigma^2 \mapsto \ell(\sigma^2)
:= -\frac{n}{2} \ln 2\pi - \frac{n}{2} \ln \sigma^2  - \frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-\mu)^2.
\end{equation}\]</span> gegeben ist. Die Scorefunktion ergibt sich also als <span class="math display">\[\begin{align}
\begin{split}
S(\sigma^2)
= \frac{\partial}{\partial\sigma^2}\ell(\sigma^2)
= - \frac{n}{2 \sigma^2} + \frac{1}{2\sigma^4}\sum_{i=1}^n(y_i-\mu)^2.
\end{split}
\end{align}\]</span> Die Fisher-Information der Stichprobe ergibt sich als <span class="math display">\[\begin{align}
\begin{split}
I(\sigma^2)
&amp; = -\frac{\partial}{\partial\sigma^2}S(\sigma^2) \\
&amp; = - \frac{\partial}{\partial\sigma^2}\left(\frac{n}{2\sigma^4} - \frac{1}{\sigma^6}\sum_{i=1}^n (y_i - \mu)^2\right) \\
&amp; = \frac{1}{\sigma^6}\sum_{i=1}^n (y_i - \mu)^2 - \frac{n}{2\sigma^4}.
\end{split}
\end{align}\]</span> Die beobachtete Fisher-Information ist die Fisher-Information an der Stelle des Maximum-Likelihood Schätzes <span class="math inline">\(\hat{\sigma}^{2^{\mbox{\tiny ML}}}\)</span>, <span class="math display">\[\begin{align}
\begin{split}
I(\hat{\sigma}^{2\,\mbox{\tiny ML}}_n)
&amp; =   \frac{\sum_{i=1}^n (y_i - \mu)^2}{\left(\hat{\sigma}^{2\,\mbox{\tiny ML}}_n \right)^3}
    - \frac{n}{2\left(\hat{\sigma}^{2\,\mbox{\tiny ML}}_n \right)^2}                                    \\
&amp; =   \frac{\sum_{i=1}^n (y_i - \mu)^2}{\frac{1}{n^3}\left(\sum_{i=1}^n (y_i - \mu)^2 \right)^3}
    - \frac{n}{2\left(\hat{\sigma}^{2\,\mbox{\tiny ML}}_n \right)^2}                                    \\
&amp; =   \frac{1}{\frac{1}{n^3}\left(\sum_{i=1}^n (y_i - \mu)^2 \right)^2}
    - \frac{n}{2\left(\hat{\sigma}^{2\,\mbox{\tiny ML}}_n \right)^2}                                    \\
&amp; =   \frac{n}{\left(\hat{\sigma}^{2\,\mbox{\tiny ML}}_n \right)^2}
    - \frac{n}{2\left(\hat{\sigma}^{2\,\mbox{\tiny ML}}_n \right)^2}                                \\
&amp; =  \frac{n}{2\left(\hat{\sigma}^{2\,\mbox{\tiny ML}}_n \right)^2}                                     \\
&amp; =  \frac{n}{2\hat{\sigma}^{4\,\mbox{\tiny ML}}_n}.
\end{split}
\end{align}\]</span> Die erwartete Fisher-Information ergibt sich schließlich als <span class="math display">\[\begin{align}
\begin{split}
J(\sigma^2)
&amp; = \mathbb{E}_{\sigma^2}(I(\sigma^2))  \\
&amp; = \mathbb{E}_{\sigma^2}\left(\frac{1}{\sigma^6}\sum_{i=1}^n (y_i - \mu)^2  - \frac{n}{2\sigma^4}\right)   \\
&amp; = \frac{1}{\sigma^6}\sum_{i=1}^n \mathbb{E}_{\sigma^2}\left((y_i - \mu)^2 \right)   - \frac{n}{2\sigma^4} \\
&amp; = \frac{1}{\sigma^6}\sum_{i=1}^n \sigma^2 - \frac{n}{2\sigma^4}   \\
&amp; = \frac{n\sigma^2}{\sigma^6} - \frac{n}{2\sigma^4}    \\
&amp; = \frac{n}{\sigma^4} - \frac{n}{2\sigma^4}    \\
&amp; = \frac{n}{2\sigma^4}.    \\
\end{split}
\end{align}\]</span></p>
</div>
<p>Mit den oben diskutierten Eigenschaften der Scorefunktion können wir nun die Cramér-Rao-Ungleichung formulieren und beweisen.</p>
<div id="thm-cramer-rao-ungleichung" class="theorem">
<p><span class="theorem-title"><strong>Theorem 19.13 (Cramér-Rao-Ungleichung) </strong></span>Gegeben sei ein Frequentistisches Inferenzmodell mit eindimensionalen Parameter <span class="math inline">\(\theta \in \Theta \subseteq \mathbb{R}\)</span>, WMF oder WDF <span class="math inline">\(p_\theta\)</span> und <span class="math inline">\(\hat{\tau}\)</span> sei ein erwartungstreuer Schätzer von <span class="math inline">\(\tau(\theta)\)</span>. Dann gilt <span class="math display">\[\begin{equation}
\mathbb{V}_\theta(\hat{\tau}) \ge \frac{\left(\frac{d}{d\theta}\tau(\theta)\right)^2}{J(\theta)}.
\end{equation}\]</span> Speziell gilt für einen Parameterschätzer mit <span class="math inline">\(\tau(\theta) := \theta\)</span> und somit <span class="math display">\[\begin{equation}
\hat{\tau} = \hat{\theta} \mbox{ und } \left(\frac{d}{d\theta}\tau(\theta)\right)^2 = 1,
\end{equation}\]</span> dass <span class="math display">\[\begin{equation}
\mathbb{V}_\theta(\hat{\theta}) \ge \frac{1}{J(\theta)}.
\end{equation}\]</span> Die rechte Seite obiger Ungleichungen wird dabei <em>Cramér-Rao-Schranke</em> genannt.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Beweis</em>. </span>Wir halten zunächst fest, dass für die Zufallsvariablen <span class="math inline">\(S(\theta)\)</span> und <span class="math inline">\(\hat{\tau}\)</span> mit der Korrelationsungleichung (<a href="206-Ungleichungen.html#thm-korrelationsungleichung">Theorem&nbsp;<span>14.4</span></a>) und der Identität von <span class="math inline">\(\mathbb{V}_\theta(S(\theta))\)</span> und <span class="math inline">\(J(\theta)\)</span> (<a href="#thm-erwartungswert-und-varianz-der-scorefunktion">Theorem&nbsp;<span>19.9</span></a>) gilt, dass <span class="math display">\[\begin{equation}
\frac{\mathbb{C}_\theta(S(\theta), \hat{\tau})^2}{\mathbb{V}_\theta(S(\theta))\mathbb{V}_\theta(\hat{\tau})}
\le 1
\Leftrightarrow \mathbb{V}_\theta(\hat{\tau})
\ge \frac{\mathbb{C}_\theta(S(\theta),\hat{\tau})^2}{J(\theta)}.
\end{equation}\]</span> Mit dem Kovarianzverschiebungssatz (<a href="205-Erwartungswerte.html#thm-kovarianzverschiebungssatz">Theorem&nbsp;<span>13.10</span></a>), der Tatsache, dass der Erwartungswert der Scorefunktion immer Null ist (<a href="#thm-erwartungswert-und-varianz-der-scorefunktion">Theorem&nbsp;<span>19.9</span></a>) und der vorausgesetzten Erwartungstreue von <span class="math inline">\(\hat{\tau}\)</span> ergibt sich dann zunächst <span class="math display">\[\begin{align}
\begin{split}
\mathbb{C}_\theta(S(\theta),\hat{\tau})
&amp; = \mathbb{E}_\theta(S(\theta)\hat{\tau})  - \mathbb{E}_\theta(S(\theta))\mathbb{E}_\theta(\hat{\tau}) \\
&amp; = \mathbb{E}_\theta(S(\theta)\hat{\tau})                                                              \\
&amp; = \int S(\theta)\,\hat{\tau}\,p_\theta(x) \,dx                                                        \\
&amp; = \int \frac{d}{d\theta} \ln L(\theta)\,\hat{\tau}\,p_\theta(x) \,dx                                  \\
&amp; = \int \frac{\frac{d}{d\theta} L(\theta)}{L(\theta)}\,\hat{\tau}\,p_\theta(x) \,dx                    \\
&amp; = \int \frac{\frac{d}{d\theta} L(\theta)}{p_\theta(x)}\,\hat{\tau}\,p_\theta(x) \,dx                  \\
&amp; = \int \frac{d}{d\theta} L(\theta)\, \hat{\tau}  \,dx                                                 \\
&amp; = \frac{d}{d\theta} \int L(\theta)\, \hat{\tau}  \,dx                                                 \\
&amp; = \frac{d}{d\theta} \int \hat{\tau}\, p_\theta(x) \,dx                                                \\
&amp; = \frac{d}{d\theta} \mathbb{E}_\theta(\hat{\tau})                                                     \\
&amp; = \frac{d}{d\theta} \tau(\theta).
\end{split}
\end{align}\]</span> Damit folgt dann aber direkt <span class="math display">\[\begin{equation}
\mathbb{V}_\theta(\hat{\tau}) \ge \frac{\left(\frac{d}{d\theta}\tau(\theta) \right)^2}{J(\theta)}.
\end{equation}\]</span></p>
</div>
<p>Für Parameterschätzer gilt also insbesondere, dass die Varianz eines erwartungstreuen Parameterschätzers <span class="math inline">\(\hat{\theta}\)</span> immer größer oder gleich der reziproken erwarteten Fisher-Information <span class="math inline">\(J(\theta)\)</span> ist. Im Fall, dass sogar <span class="math display">\[\begin{equation}
\mathbb{V}_\theta(\hat{\theta}) = \frac{1}{J(\theta)}
\end{equation}\]</span> ist, ist die Varianz des Parameterschätzers minimal und der Schätzer somit als optimaler Schätzer im Sinne der Cramér-Rao-Ungleichung nachgewiesen. Wir kommen auf diesen Gedanken in <a href="#sec-eigenschaften-von-maximum-likelihood-schaetzern"><span>Kapitel&nbsp;19.4</span></a> zurück.</p>
</section>
</section>
<section id="sec-asymptotische-schaetzereigenschaften" class="level2" data-number="19.3">
<h2 data-number="19.3" class="anchored" data-anchor-id="sec-asymptotische-schaetzereigenschaften"><span class="header-section-number">19.3</span> Asymptotische Schätzereigenschaften</h2>
<p>In diesem Abschnitt geben wir eine Kurzeinführung in die <em>Asymptotische Statistik</em> (<span class="citation" data-cites="vaart1998">Vaart (<a href="#ref-vaart1998" role="doc-biblioref">1998</a>)</span>). Die Asymptotische Statistik ist der Bereich der Frequentistischen Inferenz, der sich mit dem Verhalten von Statistiken und Schätzern bei großen Stichprobenumfängen <span class="math inline">\(n\)</span> beschäftigt. Dabei werden Methoden der Asymptotischen Statistik zum einen benutzt um, wie hier, qualitative Schätzereigenschaften zu studieren und andererseits um Schätzereigenschaften bei großen Stichprobenumfänge approximieren zu können. Da Stichprobenumfänge heutzutage durchaus groß sein können (“Big Data”), sind die Methoden der Asymptotischen Statistik also für die Anwendung gut motiviert und dort vielseitig einsetzbar.</p>
<p>In Fortführung von <a href="#sec-schaetzereigenschaften-bei-endlichen-stichproben"><span>Kapitel&nbsp;19.2</span></a> wollen wir in diesem Abschnitt vier asymptotische Schätzereigenschaften beleuchten. Um zu betonen, dass in diesem Abschnitt die Eigenschaften eines Schätzers <span class="math inline">\(\hat{\tau}\)</span> vom Stichprobenumfang <span class="math inline">\(n\)</span> abhängen, schreiben wir in diesem Abschnitt für einen Schätzer <span class="math inline">\(\hat{\tau}_n\)</span>. In <a href="#sec-asymptotische-erwartungstreue"><span>Kapitel&nbsp;19.3.1</span></a> betrachten wir die <em>Asymptotische Erwartungstreue</em> eines Schätzers. Dabei heißt ein Schätzer <span class="math inline">\(\hat{\tau}_n\)</span> für <span class="math inline">\(\tau\)</span> <em>asymptotisch erwartungstreu</em>, wenn der Erwartungswert von <span class="math inline">\(\hat{\tau}_n\)</span> für große Stichprobenumfänge <span class="math inline">\(n \to \infty\)</span> mit dem wahren, aber unbekannten, Wert <span class="math inline">\(\tau(\theta)\)</span> identisch ist. In <a href="#sec-konsistenz"><span>Kapitel&nbsp;19.3.2</span></a> führen wir den Begriff der <em>Konsistenz</em> eines Schätzers ein. Intuitiv heißt ein Schätzer <span class="math inline">\(\hat{\tau}_n\)</span> für <span class="math inline">\(\tau\)</span> <em>konsistent</em>, wenn für große Stichprobenumfänge <span class="math inline">\(n \to \infty\)</span> die Wahrscheinlichkeit dafür, dass <span class="math inline">\(\hat{\tau}_n(\upsilon)\)</span> vom wahren, aber unbekannten, Wert <span class="math inline">\(\tau(\theta)\)</span> abweicht, beliebig klein wird. Für große Stichprobenumfänge resultieren die Verteilungen von Schätzern oft in Normalverteilungen. In <a href="#sec-asymptotische-normalitaet"><span>Kapitel&nbsp;19.3.3</span></a> führen wir mit dem Begriff der <em>Asymptotischen Normalverteilung</em> eine entsprechende Formalisierung ein. Ein Schätzer <span class="math inline">\(\hat{\tau}_n\)</span> für <span class="math inline">\(\tau\)</span> heißt dann <em>asymptotisch normalverteilt</em>, wenn für große Stichprobenumfänge <span class="math inline">\(n \to \infty\)</span>, die Verteilung von <span class="math inline">\(\hat{\tau}_n\)</span> durch eine Normalverteilung gegeben ist. In <a href="#sec-asymptotische-effizienz"><span>Kapitel&nbsp;19.3.4</span></a> schließlich betrachten wir mit der <em>Asymptotischen Effizienz</em> ein Optimalitätskriterium für Schätzer bei gegen unendlich strebenden Stichprobenumfängen mit folgender Bedeutung: ein Schätzer <span class="math inline">\(\hat{\tau}_n\)</span> für <span class="math inline">\(\tau\)</span> heißt <em>asymptotisch effizient</em>, wenn für große Stichprobenumfänge <span class="math inline">\(n \to \infty\)</span> die Verteilung von <span class="math inline">\(\hat{\tau}_n\)</span> durch eine Normalverteilung mit Erwartungswertparameter <span class="math inline">\(\tau(\theta)\)</span> und Varianzparameter gleich der Cramér-Rao-Schranke gegeben ist.</p>
<section id="sec-asymptotische-erwartungstreue" class="level3" data-number="19.3.1">
<h3 data-number="19.3.1" class="anchored" data-anchor-id="sec-asymptotische-erwartungstreue"><span class="header-section-number">19.3.1</span> Asymptotische Erwartungstreue</h3>
<p>Die Asymptotische Erwartungstreue eines Schätzers verfeinert den Begriff des erwartungstreuen Schätzers wie folgt.</p>
<div id="def-asymptotische-erwartungstreue" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 19.9 (Asymptotische Erwartungstreue) </strong></span><span class="math inline">\(\upsilon = (\upsilon_1,...,\upsilon_n)\)</span> sei die Stichprobe eines parametrischen Produktmodells und <span class="math inline">\(\hat{\tau}_n\)</span> sei ein Schätzer für <span class="math inline">\(\tau\)</span>. <span class="math inline">\(\hat{\tau}_n\)</span> heißt <em>asymptotisch erwartungstreu</em>, wenn gilt, dass <span class="math display">\[\begin{equation}
\lim_{n\to\infty} \mathbb{E}_\theta(\hat{\tau}_n(\upsilon)) = \tau(\theta) \mbox{ für alle } \theta \in \Theta.
\end{equation}\]</span></p>
</div>
<p>Asymptotisch erwartungstreue Schätzer sind also nur erwartungstreu, wenn der Stichprobenumfang gegen Unendlich geht. Erwartungstreue Schätzer sind immer auch asymptotisch erwartungstreu, da ihre Erwartungstreue vom Stichprobenumfang unabhängig sind. Als Beispiel für einen nur asymptotisch erwartungstreuen Schätzer betrachten wir den Maximum-Likelihood Schätzer des Varianzparameters des Normalverteilungsmodells.</p>
<div id="thm-asymptotische-erwartungstreue-varianzparameterschaetzers" class="theorem">
<p><span class="theorem-title"><strong>Theorem 19.14 (Asymptotische Erwartungstreue des Varianzparameterschätzers) </strong></span><span class="math inline">\(\upsilon = (\upsilon_1,...,\upsilon_n)\)</span> sei die Stichprobe eines Normalverteilungsmodells mit Varianzparameter <span class="math inline">\(\sigma^2\)</span>. Dann ist der Maximum-Likelihood Schätzer von <span class="math inline">\(\sigma^2\)</span> <span class="math display">\[\begin{equation}
\hat{\sigma}_n^{2^{\mbox{\tiny ML}}}
:= \frac{1}{n}\sum_{i=1}^n \left(\upsilon_i - \bar{\upsilon}_n \right)^2
\end{equation}\]</span> nicht erwartungstreu, aber asymptotisch erwartungstreu.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Beweis</em>. </span>Mit der Erwartungstreue der Stichprobenvarianz ergibt sich zunächst (vgl. <a href="#thm-erwartungstreue-von-stichprobenmittel-und-stichprobenvarianz">Theorem&nbsp;<span>19.3</span></a>) <span class="math display">\[\begin{equation}
\mathbb{E}_{\mu,\sigma^2}\left(\hat{\sigma}_n^{2^{\mbox{\tiny ML}}} \right)
= \mathbb{E}_{\mu,\sigma^2}\left(\frac{1}{n}\sum_{i=1}^n \left(\upsilon_i - \bar{\upsilon}_n \right)^2 \right)
= \frac{1}{n}\mathbb{E}_{\mu,\sigma^2}\left(\sum_{i=1}^n \left(\upsilon_i - \bar{\upsilon}_n \right)^2 \right)
=  \frac{n-1}{n}\sigma^2.
\end{equation}\]</span> Also gilt <span class="math display">\[\begin{equation}
\mathbb{E}_{\mu,\sigma^2}\left(\hat{\sigma}_n^{2^{\mbox{\tiny ML}}} \right) \neq \sigma^2
\end{equation}\]</span> und <span class="math inline">\(\hat{\sigma}_n^{2^{\mbox{\tiny ML}}}\)</span> kein erwartungstreuer Schätzer von <span class="math inline">\(\sigma^2\)</span>. Allerdings gilt auch <span class="math display">\[\begin{equation}
\frac{n-1}{n} \to 1 \mbox{ für } n \to \infty,
\end{equation}\]</span> so dass <span class="math display">\[\begin{equation}
\lim_{n \to \infty} \mathbb{E}_{\mu,\sigma^2}\left(\hat{\sigma}_n^{2^{\mbox{\tiny ML}}}\right) =
\lim_{n \to \infty}  \frac{n-1}{n}\sigma^2 =
\sigma^2 \lim_{n \to \infty}  \frac{n-1}{n} =
\sigma^2
\end{equation}\]</span> gilt und der Maximum-Likelihood Schätzer von <span class="math inline">\(\sigma^2\)</span> damit asymptotisch erwartungstreu ist.</p>
</div>
<p>Folgender <strong>R</strong> Code simuliert das Verhalten des Maximum-Likelihood Schätzers für den Varianzparameter eines Normalverteilungsmodells mit wahren, aber unbekannten, Parametern <span class="math inline">\(\mu = 1\)</span> und <span class="math inline">\(\sigma^2 = 2\)</span> in Abhängigkeit des Stichprobenumfangs.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1"></a><span class="co"># Modellformulierung</span></span>
<span id="cb5-2"><a href="#cb5-2"></a>mu        <span class="ot">=</span> <span class="dv">1</span>                                               <span class="co"># wahrer, aber unbekannter, Erwartungswertparameter</span></span>
<span id="cb5-3"><a href="#cb5-3"></a>sigsqr    <span class="ot">=</span> <span class="dv">2</span>                                               <span class="co"># wahrer, aber unbekannter, Varianzparameter</span></span>
<span id="cb5-4"><a href="#cb5-4"></a>n         <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">1</span>,<span class="dv">100</span>, <span class="at">by =</span> <span class="dv">2</span>)                              <span class="co"># Stichprobenumfänge</span></span>
<span id="cb5-5"><a href="#cb5-5"></a>ns        <span class="ot">=</span> <span class="fl">1e3</span>                                             <span class="co"># Anzahl Simulation pro Stichprobenumfang</span></span>
<span id="cb5-6"><a href="#cb5-6"></a>sigsqr_ml <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(<span class="cn">NaN</span>, <span class="fu">length</span>(n)<span class="sc">*</span>ns),<span class="at">ncol =</span> <span class="fu">length</span>(n)) <span class="co"># \hat{\sigma^2}^{ML} Array</span></span>
<span id="cb5-7"><a href="#cb5-7"></a>            </span>
<span id="cb5-8"><a href="#cb5-8"></a><span class="co"># Simulation</span></span>
<span id="cb5-9"><a href="#cb5-9"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="fu">seq_along</span>(n)){                                     <span class="co"># Stichprobenumfangsiterationen </span></span>
<span id="cb5-10"><a href="#cb5-10"></a>    <span class="cf">for</span>(s <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>ns){                                         <span class="co"># Stichprobenrealisierungsiterationen   </span></span>
<span id="cb5-11"><a href="#cb5-11"></a>        y               <span class="ot">=</span> <span class="fu">rnorm</span>(n[i], mu, <span class="fu">sqrt</span>(sigsqr))     <span class="co"># Stichprobenrealisation</span></span>
<span id="cb5-12"><a href="#cb5-12"></a>        sigsqr_ml[s,i]  <span class="ot">=</span> ((n[i]<span class="sc">-</span><span class="dv">1</span>)<span class="sc">/</span>n[i])<span class="sc">*</span><span class="fu">var</span>(y)            <span class="co"># \hat{\sigma^2}^{ML}</span></span>
<span id="cb5-13"><a href="#cb5-13"></a>    }</span>
<span id="cb5-14"><a href="#cb5-14"></a>}</span>
<span id="cb5-15"><a href="#cb5-15"></a>E_sigsqr_ml <span class="ot">=</span> <span class="fu">colMeans</span>(sigsqr_ml)                           <span class="co"># Schätzererwartungswertschaetzung</span></span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Wir visualisieren den basierend auf obigen Simulationen geschätzten Erwartungswert des Schätzers in Abhängigkeit des Stichprobenumfangs in <a href="#fig-sigsqr-ml">Abbildung&nbsp;<span>19.3</span></a>. Für kleine Stichprobenumfänge unterschätzt der Schätzer den wahren, aber unbekannten, Varianzparameter deutlich, für größere Stichprobenumfänge dagegen nicht.</p>
<div id="fig-sigsqr-ml" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="./_figures/302-sigsqr-ml.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Abbildung&nbsp;19.3: Simulation des Erwartungswerts des Maximum-Likelihood Schätzers für den Varianzparameter eines Normalverteilungsmodells. Bei kleinen Stichprobenumfängen unterschätzt der Maximum-Likelihood Schätzer den wahren, aber unbekannten, Varianzparameter systematisch, bei größeren Stichprobenumfängen ist der Schätzer in etwa erwartungstreu.</figcaption>
</figure>
</div>
</section>
<section id="sec-konsistenz" class="level3" data-number="19.3.2">
<h3 data-number="19.3.2" class="anchored" data-anchor-id="sec-konsistenz"><span class="header-section-number">19.3.2</span> Konsistenz</h3>
<p>Der Begriff der Konsistenz eines Schätzers verallgemeinert das Schwache Gesetz der Großen Zahlen von Stichprobenmitteln und Erwartungswerten (vgl. <a href="207-Grenzwerte.html#thm-schwaches-gesetz-der-großen-zahlen">Theorem&nbsp;<span>15.1</span></a>) auf beliebige Schätzer und wahre, aber unbekannte, Parameterwerte.</p>
<div id="def-konsistenz" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 19.10 (Konsistenz) </strong></span><span class="math inline">\(\upsilon := (\upsilon_1,...,\upsilon_n)\)</span> sei die Stichprobe eines parametrischen Produktmodells und <span class="math inline">\(\hat{\tau}_n\)</span> sei ein Schätzer von <span class="math inline">\(\tau\)</span>. Dann heißt eine Folge von Schätzern <span class="math inline">\(\hat{\tau}_1, \hat{\tau}_2, ...\)</span> eine <em>konsistente Folge von Schätzern</em>, wenn für jedes noch so kleine <span class="math inline">\(\epsilon &gt; 0\)</span> und jedes <span class="math inline">\(\theta \in \Theta\)</span> gilt, dass <span class="math display">\[\begin{equation}
\lim_{n\to \infty}
\mathbb{P}_\theta\left(|\hat{\tau}_n(\upsilon) - \tau(\theta)| \ge \epsilon \right) = 0.
\end{equation}\]</span> Wenn <span class="math inline">\(\hat{\tau}_1,\hat{\tau}_2,...\)</span> eine konsistente Folge von Schätzern ist, dann heißt <span class="math inline">\(\hat{\tau}_n\)</span> ein <em>konsistenter Schätzer</em>.</p>
</div>
<p>Die Intuition zu <a href="#def-konsistenz">Definition&nbsp;<span>19.10</span></a> entspricht der dem Gesetz der Großen Zahlen. Für Stichprobenumfänge mit <span class="math inline">\(n \to \infty\)</span> wird die Wahrscheinlichkeit, dass <span class="math inline">\(\hat{\tau}_n(\upsilon)\)</span> beliebig nah bei <span class="math inline">\(\tau(\theta)\)</span> liegt bzw. die Wahrscheinlichkeit, dass <span class="math inline">\(\hat{\tau}_n(\upsilon)\)</span> weit von <span class="math inline">\(\tau(\theta)\)</span> abweicht, klein. Allerdings müssen diese Eigenschaften für alle möglichen wahren, aber unbekannten, Parameterwerte gelten. Wie unten gezeigt werden soll ist das Stichprobenmittel ein konsistenter Schätzer für den Erwartungswertparameter eines Normalverteilungsmodells. Analog zu <a href="207-Grenzwerte.html#sec-gesetze-der-grossen-zahlen"><span>Kapitel&nbsp;15.1</span></a> demonstrieren wir die Bedeutung der Konsistenz zunächst mithilfe der Simulation für <span class="math inline">\(\mu = 1\)</span> und <span class="math inline">\(\sigma^2 = 2\)</span> anhand folgenden <strong>R</strong> Codes.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1"></a><span class="co"># Modellformulierung</span></span>
<span id="cb6-2"><a href="#cb6-2"></a>mu      <span class="ot">=</span> <span class="dv">1</span>                                          <span class="co"># wahrer, aber unbekannter, Wert von \mu  </span></span>
<span id="cb6-3"><a href="#cb6-3"></a>sigsqr  <span class="ot">=</span> <span class="dv">2</span>                                          <span class="co"># wahrer, aber unbekannter, Wert von \sigma^2  </span></span>
<span id="cb6-4"><a href="#cb6-4"></a>n       <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">1</span>,<span class="fl">1e3</span>,<span class="at">by =</span> <span class="dv">10</span>)                         <span class="co"># Stichprobenumfang n</span></span>
<span id="cb6-5"><a href="#cb6-5"></a>eps     <span class="ot">=</span> <span class="fu">c</span>(<span class="fl">0.15</span>, <span class="fl">0.10</span>, <span class="fl">0.05</span>)                        <span class="co"># \epsilon Werte</span></span>
<span id="cb6-6"><a href="#cb6-6"></a>ne      <span class="ot">=</span> <span class="fu">length</span>(eps)                                <span class="co"># Anzahl \epsilon Werte</span></span>
<span id="cb6-7"><a href="#cb6-7"></a>nn      <span class="ot">=</span> <span class="fu">length</span>(n)                                  <span class="co"># Anzahl Stichprobenumfänge</span></span>
<span id="cb6-8"><a href="#cb6-8"></a>ns      <span class="ot">=</span> <span class="dv">1000</span>                                       <span class="co"># Anzahl Simulationen</span></span>
<span id="cb6-9"><a href="#cb6-9"></a>E       <span class="ot">=</span> <span class="fu">array</span>(<span class="fu">rep</span>(<span class="cn">NaN</span>,nn<span class="sc">*</span>ne<span class="sc">*</span>ns),<span class="at">dim =</span> <span class="fu">c</span>(nn,ne,ns)) <span class="co"># Ereignisindikatorarray             </span></span>
<span id="cb6-10"><a href="#cb6-10"></a></span>
<span id="cb6-11"><a href="#cb6-11"></a><span class="co"># Simulation</span></span>
<span id="cb6-12"><a href="#cb6-12"></a><span class="cf">for</span>(e <span class="cf">in</span> <span class="fu">seq_along</span>(eps)){                            <span class="co"># \epsilon Iterationen</span></span>
<span id="cb6-13"><a href="#cb6-13"></a>    <span class="cf">for</span>(i <span class="cf">in</span> <span class="fu">seq_along</span>(n)){                          <span class="co"># n Iterationen</span></span>
<span id="cb6-14"><a href="#cb6-14"></a>        <span class="cf">for</span>(s <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>ns){                              <span class="co"># Simulationsiterationen</span></span>
<span id="cb6-15"><a href="#cb6-15"></a></span>
<span id="cb6-16"><a href="#cb6-16"></a>            <span class="co"># Stichprobenrealisationen</span></span>
<span id="cb6-17"><a href="#cb6-17"></a>            y   <span class="ot">=</span> <span class="fu">rnorm</span>(n[i], mu, <span class="fu">sqrt</span>(sigsqr))</span>
<span id="cb6-18"><a href="#cb6-18"></a>            <span class="cf">if</span>(<span class="fu">abs</span>(<span class="fu">mean</span>(y) <span class="sc">-</span> mu) <span class="sc">&gt;=</span> eps[e]){         <span class="co"># |y_bar - \mu)| \ge \epsilon</span></span>
<span id="cb6-19"><a href="#cb6-19"></a>                E[i,e,s] <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb6-20"><a href="#cb6-20"></a>            } <span class="cf">else</span> {                                 <span class="co"># |y_bar - \mu)| &lt; \epsilon</span></span>
<span id="cb6-21"><a href="#cb6-21"></a>                E[i,e,s] <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb6-22"><a href="#cb6-22"></a>            }</span>
<span id="cb6-23"><a href="#cb6-23"></a>        }</span>
<span id="cb6-24"><a href="#cb6-24"></a>    }</span>
<span id="cb6-25"><a href="#cb6-25"></a>}</span>
<span id="cb6-26"><a href="#cb6-26"></a></span>
<span id="cb6-27"><a href="#cb6-27"></a><span class="co"># Schaetzung von \mathbb{P}(|\hat{\tau}_n(\upsilon)-\tau(\theta)| \ge \epsilon)</span></span>
<span id="cb6-28"><a href="#cb6-28"></a>P_hat       <span class="ot">=</span> <span class="fu">apply</span>(E, <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>), mean)</span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Wir visualisieren die Schätzung der Wahrscheinlichkeit <span class="math inline">\(\mathbb{P}(|\hat{\tau}_n(\upsilon)-\tau(\theta)| \ge \epsilon)\)</span> für dieses Beispiel in <a href="#fig-konsistenz">Abbildung&nbsp;<span>19.4</span></a> in Abhängigkeit des Stichprobenumfangs und des Kriteriumwerts <span class="math inline">\(\epsilon\)</span>. Bei größeren Werten von <span class="math inline">\(\epsilon\)</span> genügen geringe Stichprobenumfänge für eine geringe Wahrscheinlichkeit der Abweichung des Schätzers vom wahren, aber unbekannten, Parameterwert, bei kleineren Werten von <span class="math inline">\(\epsilon\)</span> sind dazu größere Stichprobenumfänge nötig.</p>
<div id="fig-konsistenz" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="./_figures/302-konsistenz.png" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption class="figure-caption">Abbildung&nbsp;19.4: Simulation der Konsistenz des Stichprobenmittels als Schätzer für den Erwartungswertparameter des Normalverteilungsmodells.</figcaption>
</figure>
</div>
<p>Die in <a href="#thm-mittlerer-quadratischer-fehler-kriterium-für-konsistenz">Theorem&nbsp;<span>19.15</span></a> und <a href="#thm-bias-varianzkriterium-für-konsistenz">Theorem&nbsp;<span>19.16</span></a> angegebenen Kriterien für die Konsistenz von Schätzern vereinfachen den Nachweis der Konsistenz eines Schätzers mithilfe des Mittleren Quadratischen Fehlers (<a href="#def-mittlerer-quadratischer-fehler">Definition&nbsp;<span>19.6</span></a>).</p>
<div id="thm-mittlerer-quadratischer-fehler-kriterium-für-konsistenz" class="theorem">
<p><span class="theorem-title"><strong>Theorem 19.15 (Mittlerer Quadratischer Fehler Kriterium für Konsistenz) </strong></span><span class="math inline">\(\upsilon := (\upsilon_1,...,\upsilon_n)\)</span> sei die Stichprobe eines parametrischen Produktmodells und <span class="math inline">\(\hat{\tau}_n\)</span> sei ein Schätzer von <span class="math inline">\(\tau\)</span>. Wenn gilt, dass <span class="math display">\[\begin{equation}
\lim_{n\to \infty} \mbox{MQF}(\hat{\tau}_n) = 0,
\end{equation}\]</span> dann ist <span class="math inline">\(\hat{\tau}_n\)</span> ein konsistenter Schätzer.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Beweis</em>. </span>Mit der Chebychev-Ungleichung gilt, dass <span class="math display">\[\begin{equation}
\mathbb{P}_\theta\left(|\hat{\tau}_n(\upsilon) - \tau(\theta)| \ge \epsilon \right) \le
\frac{\mathbb{E}_\theta\left((\hat{\tau}_n(\upsilon) -  \tau(\theta))^2\right)}{\epsilon^2}
\end{equation}\]</span> Grenzwertbildung ergibt dann <span class="math display">\[\begin{equation}
\lim_{n\to \infty}\mathbb{P}_\theta\left(|\hat{\tau}_n(\upsilon) - \tau(\theta)| \ge \epsilon \right) \le
\frac{1}{\epsilon^2}\lim_{n\to\infty}\mathbb{E}_\theta\left((\hat{\tau}_n(\upsilon) - \tau(\theta))^2\right).
\end{equation}\]</span> Wenn also <span class="math inline">\(\lim_{n\to\infty}\mathbb{E}_\theta\left((\hat{\tau}_n(\upsilon) - \tau(\theta))^2\right) = 0\)</span> gilt, dann gilt mit <span class="math inline">\(\mathbb{P}_\theta(|\hat{\tau}_n(\upsilon) - \tau(\theta)| \ge \epsilon)\ge 0\)</span>, dass <span class="math display">\[\begin{equation}
\lim_{n\to \infty}\mathbb{P}_\theta\left(|\hat{\tau}_n(\upsilon) - \tau(\theta)| \ge \epsilon \right) = 0.
\end{equation}\]</span> Also ist <span class="math inline">\(\hat{\tau}_n\)</span> ein konsistenter Schätzer.</p>
</div>
<div id="thm-bias-varianzkriterium-für-konsistenz" class="theorem">
<p><span class="theorem-title"><strong>Theorem 19.16 (Bias-Varianz-Kriterium für Konsistenz) </strong></span><span class="math inline">\(\upsilon := (\upsilon_1,...,\upsilon_n)\)</span> sei die Stichprobe eines parametrischen Produktmodells und <span class="math inline">\(\hat{\tau}_n\)</span> sei ein Schätzer von <span class="math inline">\(\tau\)</span>. Wenn <span class="math display">\[\begin{equation}
\lim_{n\to \infty} \mbox{B}(\hat{\tau}_n) = 0
\mbox{ und }
\lim_{n\to \infty} \mathbb{V}_\theta(\hat{\tau}_n) = 0
\end{equation}\]</span> gelten, dann ist <span class="math inline">\(\hat{\tau}_n\)</span> ein konsistenter Schätzer</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Beweis</em>. </span>Wenn <span class="math inline">\(n \to \infty\)</span>, dann gilt <span class="math inline">\(\mbox{B}(\hat{\tau}_n) \to 0\)</span>, also auch <span class="math inline">\(\mbox{B}(\hat{\tau}_n)^2 \to 0\)</span>. Wenn für <span class="math inline">\(n \to \infty\)</span> sowohl <span class="math inline">\(\mbox{B}(\hat{\tau}_n)^2 \to 0\)</span> als auch <span class="math inline">\(\mathbb{V}_\theta(\hat{\tau}_n) \to 0\)</span>, dann gilt auch <span class="math inline">\(\lim_{n\to \infty} \mbox{MQF}(\hat{\theta}_n) = 0\)</span>. Also gilt mit <a href="#thm-mittlerer-quadratischer-fehler-kriterium-für-konsistenz">Theorem&nbsp;<span>19.15</span></a>, dass <span class="math inline">\(\hat{\tau}_n\)</span> konsistent ist.</p>
</div>
<p>Als Anwendung von <a href="#thm-bias-varianzkriterium-für-konsistenz">Theorem&nbsp;<span>19.16</span></a> wollen wir mit folgendem Theorem die Konsistenz des Stichprobenmittels als Schätzer für den Erwartungswert bei Normalverteilung nachweisen.</p>
<div id="thm-konsistenz-des-erwartungswertschätzers-bei-normalverteilung" class="theorem">
<p><span class="theorem-title"><strong>Theorem 19.17 (Konsistenz des Erwartungswertschätzers bei Normalverteilung) </strong></span>Es sei <span class="math inline">\(\upsilon\)</span> die Stichprobe eines Normalverteilungsmodells. Dann ist <span class="math inline">\(\bar{\upsilon}_n\)</span> ein konsistenter Schätzer von <span class="math inline">\(\mathbb{E}(\upsilon_1)\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Beweis</em>. </span>Mit der Erwartungstreue des Stichprobenmittels als Schätzer für den Erwartungswert gilt zunächst <span class="math display">\[\begin{equation}
\lim_{n \to \infty} \mbox{B}(\bar{\upsilon}_n) =  0.
\end{equation}\]</span> Weiterhin gilt mit der Varianz des Stichprobenmittels <span class="math display">\[\begin{equation}
\lim_{n \to \infty} \mathbb{V}_\theta(\bar{\upsilon}_n) = \lim_{n\to \infty} \frac{1}{n}\mathbb{V}(\upsilon_1) = 0.
\end{equation}\]</span> Mit dem Bias-Varianz-Kriterium folgt dann die Konsistenz von <span class="math inline">\(\bar{\upsilon}_n\)</span> als Schätzer von <span class="math inline">\(\mathbb{E}(\upsilon_1)\)</span></p>
</div>
<p>Man beachte, dass <a href="#thm-konsistenz-des-erwartungswertschätzers-bei-normalverteilung">Theorem&nbsp;<span>19.17</span></a> natürlich auch schon im Schwachen Gesetz der Großen Zahlen impliziert ist (vgl. <a href="207-Grenzwerte.html#thm-schwaches-gesetz-der-großen-zahlen">Theorem&nbsp;<span>15.1</span></a>).</p>
</section>
<section id="sec-asymptotische-normalitaet" class="level3" data-number="19.3.3">
<h3 data-number="19.3.3" class="anchored" data-anchor-id="sec-asymptotische-normalitaet"><span class="header-section-number">19.3.3</span> Asymptotische Normalität</h3>
<p>In manchen Fällen nähert sich die Frequentistische Verteilung eines Schätzers bei großen Stichprobenumfängen einer Normalverteilung. Man nennt einen solchen Schätzer dann <em>asymptotisch normalverteilt</em></p>
<div id="def-asymptotisch-normalverteilter-schaetzer" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 19.11 (Asymptotisch normalverteilter Schätzer) </strong></span><span class="math inline">\(\upsilon = (\upsilon_1,...,\upsilon_n)\)</span> sei die Stichprobe eines parametrischen Produktmodells und <span class="math inline">\(\hat{\theta}_n\)</span> sei ein Parameterschätzer für <span class="math inline">\(\theta\)</span>. Weiterhin sei <span class="math display">\[\begin{equation}
\tilde{\theta} \sim N(\mu,\sigma^2)
\end{equation}\]</span> eine normalverteilte Zufallsvariable mit Erwartungswertparameter <span class="math inline">\(\mu\)</span> und Varianzparameter <span class="math inline">\(\sigma^2\)</span>. Wenn <span class="math inline">\(\hat{\theta}_n\)</span> in Verteilung gegen <span class="math inline">\(\tilde{\theta}\)</span> konvergiert, wenn also für die KVFen <span class="math inline">\(P_n\)</span> und <span class="math inline">\(P\)</span> von <span class="math inline">\(\hat{\theta}_n\)</span> und <span class="math inline">\(\tilde{\theta}\)</span>, respektive, gilt, dass <span class="math display">\[\begin{equation}
\lim_{n\to \infty} P_n(\hat{\theta}_n) = P(\tilde{\theta}),
\end{equation}\]</span> dann heißt <span class="math inline">\(\hat{\theta}_n\)</span> <em>asymptotisch normalverteilt</em> und wir schreiben <span class="math display">\[\begin{equation}
\hat{\theta}_n  \stackrel{a}{\sim}  N(\mu,\sigma^2).
\end{equation}\]</span></p>
</div>
<p>Als Beispiel für einen asymptotisch normalverteilten Schätzer betrachten wir in <a href="#sec-asymptotische-effizienz"><span>Kapitel&nbsp;19.3.4</span></a> den Maximum-Likelihood-Schätzer des Bernoullimodellparameters. Es ist bemerkenswert, dass dieser Schätzer asymptotisch normalverteilt ist, da die Stichprobenvariablen des Bernoullimodells lediglich die Werte Null und Eins annehmen.</p>
</section>
<section id="sec-asymptotische-effizienz" class="level3" data-number="19.3.4">
<h3 data-number="19.3.4" class="anchored" data-anchor-id="sec-asymptotische-effizienz"><span class="header-section-number">19.3.4</span> Asymptotische Effizienz</h3>
<p>In manchen Fällen lassen sich neben der asymptotischen Normalität eines Schätzers auch in der Form des Frequentistischen Modells begründete Aussagen zum Erwartungswertparameter und Varianzparameter dieser asymptotischen Normalverteilung machen. Der Begriff des <em>effizienten Schätzers</em> formuliert einen solchen Spezialfall.</p>
<div id="def-asymptotische-effizienz" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 19.12 </strong></span><span class="math inline">\(\upsilon = (\upsilon_1,...,\upsilon_n)\)</span> sei die Stichprobe eines parametrischen Produktmodells und <span class="math inline">\(\hat{\theta}_n\)</span> sei ein Parameterschätzer für <span class="math inline">\(\theta\)</span>. Weiterhin sei <span class="math inline">\(J(\theta)\)</span> die erwartete Fisher-Information der Stichprobe <span class="math inline">\(\upsilon\)</span>. Wenn gilt, dass <span class="math display">\[\begin{equation}
\hat{\theta}_n \stackrel{a}{\sim} N\left(\theta, J(\theta)^{-1}\right),
\end{equation}\]</span> dann heißt <span class="math inline">\(\hat{\theta}_n\)</span> .</p>
</div>
<p>Ein asymptotisch effizienter Schätzer ist offenbar auch immer asymptotisch normalverteilt und erwartungstreu. Die Varianz der asymptotischen Verteilung eines Schätzers nennt man auch die <em>asymptotische Varianz</em>. Für einen asymptotisch effizienten Schätzers ist die asymptotische Varianz mit der Cramér-Rao-Schranke identisch und damit in der Menge der erwartungstreuen asymptotisch normalverteilten Schätzer minimal. Als Beispiel für einen asymptotisch effizienten Schätzer betrachten wir den Maximum-Likelihood-Schätzer des Bernoullimodellparameters. Folgender <strong>R</strong> simuliert die Frequentistische Verteilung dieses Schätzers und bestimmt die WDF seiner asymptotischen Verteilung</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1"></a><span class="co"># Modellformulierung</span></span>
<span id="cb7-2"><a href="#cb7-2"></a>mu          <span class="ot">=</span> <span class="fl">0.4</span>                                                               <span class="co"># wahrer, aber unbekannter, Parameterwert</span></span>
<span id="cb7-3"><a href="#cb7-3"></a>n_all       <span class="ot">=</span> <span class="fu">c</span>(<span class="fl">1e1</span>,<span class="fl">5e1</span>,<span class="fl">1e2</span>)                                                    <span class="co"># Stichprobenumfang  </span></span>
<span id="cb7-4"><a href="#cb7-4"></a>ns          <span class="ot">=</span> <span class="fl">1e4</span>                                                               <span class="co"># Anzahl Stichprobenrealisierungen</span></span>
<span id="cb7-5"><a href="#cb7-5"></a>mu_hat   <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(<span class="cn">NaN</span>, <span class="fu">length</span>(n_all)<span class="sc">*</span>ns), <span class="at">nrow =</span> <span class="fu">length</span>(n_all))             <span class="co"># Maximum-Likelihood-Schätzerarray                  </span></span>
<span id="cb7-6"><a href="#cb7-6"></a>mu_hat_r <span class="ot">=</span> <span class="fl">1e3</span>                                                                  <span class="co"># Maximum-Likelihood Schätzerraumaufloesung</span></span>
<span id="cb7-7"><a href="#cb7-7"></a>mu_hat_y <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="at">len =</span> mu_hat_r)                                              <span class="co"># Maximum-LikelihoodSchätzerraum</span></span>
<span id="cb7-8"><a href="#cb7-8"></a>mu_hat_p <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(<span class="cn">NaN</span>, <span class="fu">length</span>(n_all)<span class="sc">*</span>mu_hat_r), <span class="at">nrow =</span> <span class="fu">length</span>(n_all))       <span class="co"># Maximum-Likelihood WDF Array</span></span>
<span id="cb7-9"><a href="#cb7-9"></a></span>
<span id="cb7-10"><a href="#cb7-10"></a><span class="co"># Stichprobenumfängeiterationen</span></span>
<span id="cb7-11"><a href="#cb7-11"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="fu">seq_along</span>(n_all)){</span>
<span id="cb7-12"><a href="#cb7-12"></a></span>
<span id="cb7-13"><a href="#cb7-13"></a>    <span class="co"># Simulationsiterationen</span></span>
<span id="cb7-14"><a href="#cb7-14"></a>    <span class="cf">for</span>(s <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>ns){</span>
<span id="cb7-15"><a href="#cb7-15"></a>        y           <span class="ot">=</span> <span class="fu">rbinom</span>(n_all[i],<span class="dv">1</span>,mu)                                     <span class="co"># Stichprobenrealisation</span></span>
<span id="cb7-16"><a href="#cb7-16"></a>        mu_hat[i,s] <span class="ot">=</span> <span class="fu">mean</span>(y)                                                   <span class="co"># Maximum-Likelihood-Schätzerrealisation</span></span>
<span id="cb7-17"><a href="#cb7-17"></a>    }</span>
<span id="cb7-18"><a href="#cb7-18"></a>    mu_hat_p[i,] <span class="ot">=</span> <span class="fu">dnorm</span>(mu_hat_y, mu, <span class="fu">sqrt</span>(mu<span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span>mu)<span class="sc">/</span>n_all[i]))                <span class="co"># WDF der asymptotischen Verteilung</span></span>
<span id="cb7-19"><a href="#cb7-19"></a>}</span></code><button title="In die Zwischenablage kopieren" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="fig-effizienz" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="./_figures/302-effizienz.png" class="img-fluid figure-img" style="width:90.0%"></p>
<figcaption class="figure-caption">Abbildung&nbsp;19.5: Simulation der asymptotischen Effizienz des Maximum-Likelihood-Parameterschätzers für den Parameter eines Bernoullimodells. Mit steigendem Stichprobenumfang gleicht sich die durch ein Histogramm dargestellte Verteilung der simulierten Schätzerwerte der in <a href="#def-asymptotische-effizienz">Definition&nbsp;<span>19.12</span></a> formulierten Normalverteilung</figcaption>
</figure>
</div>
</section>
</section>
<section id="sec-eigenschaften-von-maximum-likelihood-schaetzern" class="level2" data-number="19.4">
<h2 data-number="19.4" class="anchored" data-anchor-id="sec-eigenschaften-von-maximum-likelihood-schaetzern"><span class="header-section-number">19.4</span> Eigenschaften von Maximum-Likelihood Schätzern</h2>
<p>Das Maximum-Likelihood-Prinzip zur Gewinnung von Schätzern für Parameter Frequentistischer Inferenzmodelle ist durch folgendes Theorem zu den Eigenschaften von Maximum-Likelihood-Schätzern begründet.</p>
<div id="thm-eigenschaften-von-maximum-likelihood-schätzern" class="theorem">
<p><span class="theorem-title"><strong>Theorem 19.18 (Eigenschaften von Maximum-Likelihood Schätzern) </strong></span><span class="math inline">\(\upsilon\)</span> sei die Stichprobe eines parametrischen Produktmodells und <span class="math inline">\(\hat{\theta}_n^{\mbox{\tiny{ML}}}\)</span> sei ein Maximum-Likelihood Schätzer für <span class="math inline">\(\theta\)</span>. Dann gilt, dass <span class="math inline">\(\hat{\theta}_n^{\mbox{\tiny{ML}}}\)</span></p>
<ol type="1">
<li>nicht notwendigerweise erwartungstreu, aber</li>
<li>asymptotisch erwartungstreu,</li>
<li>konsistent,</li>
<li>asymptotisch normalverteilt und</li>
<li>asymptotisch effizient</li>
</ol>
<p>ist.</p>
</div>
<p>Für einen nicht unaufwändigen Beweis von <a href="#thm-eigenschaften-von-maximum-likelihood-schätzern">Theorem&nbsp;<span>19.18</span></a> verweisen wir auf <span class="citation" data-cites="held2014">Held &amp; Sabanés Bové (<a href="#ref-held2014" role="doc-biblioref">2014</a>)</span>, Abschnitt 3.4. Nutzt man also das Maximum-Likelihood Prinzip zur Gewinnung eines Schätzers, so erfüllt der gewonnene Schätzer also garantiert die in <a href="#thm-eigenschaften-von-maximum-likelihood-schätzern">Theorem&nbsp;<span>19.18</span></a> Schätzergütekriterien.</p>
</section>
<section id="literaturhinweise" class="level2" data-number="19.5">
<h2 data-number="19.5" class="anchored" data-anchor-id="literaturhinweise"><span class="header-section-number">19.5</span> Literaturhinweise</h2>
<p>Die in diesem Kapitel vorgestellten Resultate gehen in ganz wesentlicher Weise auf <span class="citation" data-cites="fisher1922">Fisher (<a href="#ref-fisher1922" role="doc-biblioref">1922</a>)</span> zurück. <span class="citation" data-cites="aldrich1997">Aldrich (<a href="#ref-aldrich1997" role="doc-biblioref">1997</a>)</span> gibt dazu eine historische Einordnung.</p>
</section>
<section id="selbstkontrollfragen" class="level2" data-number="19.6">
<h2 data-number="19.6" class="anchored" data-anchor-id="selbstkontrollfragen"><span class="header-section-number">19.6</span> Selbstkontrollfragen</h2>
<ol type="1">
<li>Geben Sie die Definition des Begriffs eines Parameterpunktschätzers wieder.</li>
<li>Erläutern Sie den Begriff des Parameterpunktschätzers.</li>
<li>Geben Sie Definition der Begriffe der Likelihood-Funktion und der Log-Likelihood-Funktion wieder.</li>
<li>Geben Sie Definition des Begriffs des Maximum-Likelihood Schätzes wieder.</li>
<li>Erläutern Sie das Vorgehen zur Gewinnung von Maximum-Likelihood-Schätzern.</li>
<li>Geben Sie das Theorem zum Maximum-Likelihood-Schätzer des Bernoullimodellparameters wieder.</li>
<li>Geben Sie das Theorem zu den Maximum-Likelihood-Schätzern der Normalverteilungsmodellparameter wieder.</li>
<li>Geben Sie die Definition des Begriffs der Erwartungstreue eines Schätzers wieder.</li>
<li>Erläutern Sie den Begriff der Erwartungstreue eines Schätzers.</li>
<li>Geben Sie Definition der Begriffe der Varianz und des Standardfehlers eines Schätzers wieder.</li>
<li>Erläutern Sie den Begriff der asymptotischen Erwartungstreue eines Schätzers.</li>
<li>Erläutern Sie den Begriff der Konsistenz eines Schätzers.</li>
<li>Erläutern Sie den Begriff der asymptotischen Normalität eines Schätzers.</li>
<li>Geben Sie das Theorem zu den Eigenschaften von Maximum-Likelihood-Schätzern wieder. </li>
</ol>


<div id="refs" class="references csl-bib-body hanging-indent" data-line-spacing="2" role="list">
<div id="ref-aldrich1997" class="csl-entry" role="listitem">
Aldrich, J. (1997). R.<span>A</span>. <span>Fisher</span> and the Making of Maximum Likelihood 1912-1922. <em>Statistical Science</em>, <em>12</em>(3), 162–176. <a href="https://doi.org/10.1214/ss/1030037906">https://doi.org/10.1214/ss/1030037906</a>
</div>
<div id="ref-fisher1922" class="csl-entry" role="listitem">
Fisher, R. A. (1922). On the Mathematical Foundations of Theoretical Statistics. <em>Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character</em>, <em>222</em>(594-604), 309–368. <a href="https://doi.org/10.1098/rsta.1922.0009">https://doi.org/10.1098/rsta.1922.0009</a>
</div>
<div id="ref-held2014" class="csl-entry" role="listitem">
Held, L., &amp; Sabanés Bové, D. (2014). <em>Applied <span>Statistical Inference</span></em>. <span>Springer Berlin Heidelberg</span>. <a href="https://doi.org/10.1007/978-3-642-37887-4">https://doi.org/10.1007/978-3-642-37887-4</a>
</div>
<div id="ref-vaart1998" class="csl-entry" role="listitem">
Vaart, A. W. van der. (1998). <em>Asymptotic Statistics</em>. <span>Cambridge University Press</span>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Kopiert");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Kopiert");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./301-Grundbegriffe-Frequentistischer-Inferenz.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Grundbegriffe Frequentistischer Inferenz</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./303-Konfidenzintervalle.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Konfidenzintervalle</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>