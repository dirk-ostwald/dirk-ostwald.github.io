# Datendeskription {#sec-datendeskription}
\normalsize

## Datenanalyseszenarien {#sec-datenanalyseszenarien}

Wir wollen hier zunächst die im folgenden zu betrachtenden Datenanalyseverfahren 
anhand der Dimensionalität ihrer unabhängigen Variablen und abhängigen Variablen 
klassifizieren. Dazu bezeichnen wir wie üblich eine unabhängige Variable mit $x$ 
und eine abhängige Variable mit $y$. Weiterhin sei mit den Subskripten $i$ und $j$ 
bei $x_{ij}$ und $y_{ij}$ der Wert der $j$ten univariaten Komponente der jeweiligen 
Variable, beispielsweise ein Testwert, bei der $i$ten experimentellen Einheit, 
beispielsweise einer Proband:in, bezeichnet. Die Gesamtzahl experimenteller 
Einheiten sei mit $n$ bezeichnet. 

@tbl-uu zeigt das Szenario einer univariaten unabhängigen Variable und einer 
univariaten abhängigen Variable. Typische in diesem Szenario genutzte 
Inferenzverfahren sind die Bestimmung der *Korrelation* von $x_1$ und $y_1$, 
die Durchführung einer *einfachen linearen Regression* von $y_1$ auf $x_1$ und, 
wenn $x_1$ eine kategoriale Faktorvariable ist, *T-Tests* und *Varianzanalysen*.


| $x_{1}$  |   $y_1$  |
|:--------:|:--------:|
| $x_{11}$ | $y_{11}$ |
| $\vdots$ | $\vdots$ |
| $x_{n1}$ | $y_{n1}$ |

: Univariate unabhängige Variable $x$ und univariate abhängige Variable $y$ {#tbl-uu}

@tbl-mu zeigt das Szenario einer multivariaten unabhängigen Variablen und einer 
univariaten abhängigen Variablen. Typische in diesem Szenario eingesetzte 
Inferenzverfahren sind die Bestimmung von *multiplen und partiellen Korrelationen* 
zwischen $x_1,...,x_m$ und $y_1$, die Durchführung von *multiplen Regressionsanalysen* 
und *Kovarianzanalysen* und generell alle datenanalytischen Spezialfälle des 
*Allgemeinen Linearen Modells*.

| $x_{1}$  | $\cdots$ | $x_{m}$  | $y_{1}$  |
|:--------:|:--------:|:--------:|:--------:|
| $x_{11}$ | $\cdots$ | $x_{1m}$ | $y_{11}$ |
| $\vdots$ | $\ddots$ | $\vdots$ | $\vdots$ |
| $x_{n1}$ | $\cdots$ | $x_{nm}$ | $y_{nm}$ |

: Multivariate unabhängige Variable $x$ und univariate abhängige Variable $y$ {#tbl-mu}

@tbl-um zeigt das im Kontext von *Einstichproben-T$^2$-Tests*, der *einfaktoriellen 
multivariaten Varianzanalyse* und vielen Szenarien der *prädiktiven Modellierung* 
relevante Szenario. In diesem Fall ist die unabhängige Variable univariat und kodiert 
kategorial ein Faktorlevel bzw. eine Gruppenzugehörigkeit, während die abhängige 
Variable multivariat ist. Insbesondere in prädiktiven Modellierung wird die 
unabhängige Variable in diesem Kontext auch als *Targetvariable* oder *Label* 
und die Komponenten der abhängigen Variable als *Features* bezeichnet.  

| $x_{1}$  | $y_{1}$  | $\cdots$ | $y_{m}$  |
|:--------:|:--------:|:--------:|:--------:|
| $x_{11}$ | $y_{11}$ | $\cdots$ | $y_{1m}$ |
| $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ |
| $x_{n1}$ | $y_{n1}$ | $\cdots$ | $y_{nm}$ |

: Univariate unabhängige Variable $x$ und multivariate abhängige Variable $y$ {#tbl-um}

@tbl-mm schließlich zeigt das Szenario einer multivariaten unabhängigen Variablen
und einer multivariaten abhängigen Variablen. Dies ist das datenanalytische 
Szenario, das wir im Rahmen der *Kanonischen Korrelationsanalyse* genauer betrachten 
wollen und das generell durch das *Multivariate Allgemeine Lineare Modell* 
abgebildet und datenanalytisch behandelt werden kann.

|  $x_1$   | $\cdots$  | $x_{m_x}$  |  $y_1$  | $\cdots$  | $y_{m_y}$  |
|:--------:|:---------:|:----------:|:--------:|:--------:|:----------:|
| $x_{11}$ | $\cdots$  | $x_{1m_x}$ | $y_{11}$ | $\cdots$ | $y_{1m_y}$ |
| $\vdots$ | $\ddots$  | $\vdots$   | $\vdots$ | $\ddots$ | $\vdots$   |
| $x_{n1}$ | $\cdots$  | $x_{nm_x}$ | $y_{n1}$ | $\cdots$ | $y_{nm_y}$ |

: Multivariate unabhängige Variable $x$ und multivariate abhängige Variable $y$ {#tbl-mm}

## Deskriptivstatistiken {#sec-deskriptivstatistiken}

Wir wollen nun einige Standarddeskriptivstatistiken zur Beschreibung multivariater 
Datensätze diskutieren. Dazu verallgemeinern wir zunächst die aus der univariaten 
Deskriptivstatistik bekannten Begriffe des Stichprobenmittels und der 
Stichprobenvarianz und betrachten dann mit den Mahalanobis-Distanzen multivariate
Maße für das Verhältnis von Signal zu Rauschen. Wie immer entwickeln sich diese 
Begriffe vor dem Hintergrund der Annahme, dass es sich bei beobachteten Daten 
um Realisierungen entsprechender Zufallsvektoren handelt. Im Gegensatz zu der in
@sec-datenanalyseszenarien betrachteten und aus dem empirischen Kontext bekannten 
Organisation von Daten experimenteller Einheiten in Zeilen und ihrer jeweiligen 
abhängigen Variablenkomponenten in Spalten ist dabei eine Organisation der zu 
einer experimentellen Einheit gehörenden Variablenkomponenten in Spaltenform
zielführender und mit den Schreibweisen des univariaten Falles konsistenter.

### Stichprobenmittel, -kovarianzmatrix,  und -korrelationsmatrix {-}

:::{#def-stichprobenmittel-stichprobenkovarianmatrix-stichprobenkorrelationsmatrix}
## Stichprobenmittel, -kovarianmatrix und -korrelationsmatrix
$\upsilon_1,...,\upsilon_n$ sei eine Menge von $m$-dimensionalen Zufallsvektoren, genannt 
*Stichprobe*.

\begin{itemize}
\item Das \textit{Stichprobenmittel} der $\upsilon_1,...,\upsilon_n$ ist definiert 
als der $m$-dimensionale Vektor
\begin{equation}
\bar{\upsilon} := \frac{1}{n} \sum_{i=1}^n \upsilon_i.
\end{equation}
\item Die \textit{Stichprobenkovarianzmatrix} der $\upsilon_1,...,\upsilon_n$ ist definiert als die $m \times m$ Matrix
\begin{equation}
C := \frac{1}{n-1}\sum_{i=1}^n (\upsilon_i - \bar{\upsilon})(\upsilon_i - \bar{\upsilon})^T .
\end{equation}
\item Die \textit{Stichprobenkorrelationsmatrix} der $\upsilon_1,...,\upsilon_n$ ist 
definiert als die $m \times m$ Matrix
\begin{equation}
D := \left(\frac{(C )_{ij}}{\sqrt{ (C )_{ii}}\sqrt{ (C )_{jj}}}\right)_{1 \le i,j \le m}.
\end{equation}
\end{itemize}
:::

Ohne Beweis halten wir fest, dass analog zum univariaten Fall das Stichprobenmittel
bei unabhängig und identisch verteilten Zufallsvektoren $\upsilon_1,...,\upsilon_n$ 
ein unverzerrter Schätzer des Stichprobenvariablenerwartungswerts 
$\mathbb{E}(\upsilon_i) \in \mathbb{R}^m, i = 1,...,n$ ist. Ebenso ist in diesem Fall 
die Stichprobenkovarianzmatrix ein unverzerrter Schätzer der Stichprobenvariablenkovarianzmatrix 
$\mathbb{C}(\upsilon_i) \in \mathbb{R}^m, i = 1,...,n$. Zur konkreten Berechnung 
von Stichprobenmittel, Stichprobenkovarianzmatrix und Stichprobenkorrrelationsmatrix 
basierend auf einem Datensatz bieten sich die Aussagen des folgenden Theorems an.

:::{#thm-datenmatrix-und-stichprobenstatistiken}
## Datenmatrix und Stichprobenstatistiken
Es sei
\begin{equation}
\Upsilon :=
\begin{pmatrix}
\upsilon_1 & \cdots & \upsilon_n
\end{pmatrix}
\end{equation}
eine $m \times n$ \textit{Datenmatrix}, die durch die spaltenweise Konkatenation
von $n$ $m$-dimensionalen Zufallvektoren $\upsilon_1, ...,\upsilon_n$ gegeben sei. 
Dann ergeben sich
\begin{itemize}
\item für das Stichprobenmittel 
\begin{equation}
\bar{\upsilon} =  \frac{1}{n}\Upsilon 1_{n},
\end{equation}
\item für die Stichprobenkovarianzmatrix 
\begin{equation}
C = \frac{1}{n-1}\left(\Upsilon\left(I_n - \frac{1}{n}1_{nn}\right)\Upsilon^T\right),
\end{equation}
\item und für Stichprobenkorrelationsmatrix mit
\begin{equation}
D := \mbox{diag}\left(\sqrt{C_{y_{ii}}}^{-1}, i = 1,...,m\right),
\end{equation}
dass
\begin{equation}
R = DCD.
\end{equation}
\end{itemize}
:::

:::{.proof}
Die Darstellung des Stichprobenmittels ergibt sich aus
\begin{align}
\begin{split}
\bar{\upsilon} 
& := \frac{1}{n} \sum_{i=1}^n\upsilon_i \\
&  = \frac{1}{n}\begin{pmatrix} \sum_{i=1}^n\upsilon_{i1} \\ \vdots \\ \sum_{i=1}^n\upsilon_{im} \end{pmatrix} \\
&  = \frac{1}{n}\left(\begin{pmatrix}\upsilon_{11}    & \cdots  &\upsilon_{n1} \\
                                      \vdots    & \ddots  & \vdots     \\
                                     \upsilon_{1m}    & \cdots  &\upsilon_{nm} \\
                   \end{pmatrix}
                   \begin{pmatrix} 1 \\ \vdots \\ 1 \end{pmatrix}
              \right) \\
& = \frac{1}{n}\Upsilon 1_{n}.
\end{split}
\end{align}
Hinsichtlich der Darstellung der Stichprobenkovarianzmatrix halten wir zunächst fest, dass 
nach Definition gilt, dass 
\begin{align}
\begin{split}
C  
& := \frac{1}{n-1}\sum_{i=1}^n (\upsilon_i - \bar{\upsilon})(\upsilon_i - \bar{\upsilon})^T \\
&  = \frac{1}{n-1}\sum_{i=1}^n \left(\upsilon_i\upsilon_i^T-\upsilon_i\bar{\upsilon}^T - \bar{\upsilon}\upsilon_i^T+ \bar{\upsilon}\bar{\upsilon}^T\right) \\
&  = \frac{1}{n-1}\left(\sum_{i=1}^n\upsilon_i\upsilon_i^T- \sum_{i=1}^n\upsilon_i\bar{\upsilon}^T - \sum_{i=1}^n \bar{\upsilon}\upsilon_i^T+ \sum_{i=1}^n \bar{\upsilon}\bar{\upsilon}^T\right) \\
&  = \frac{1}{n-1}\left(\sum_{i=1}^n\upsilon_i\upsilon_i^T- n\bar{\upsilon}\bar{\upsilon}^T - n\bar{\upsilon}\bar{\upsilon}^T + n\bar{\upsilon}\bar{\upsilon}^T\right) \\
&  = \frac{1}{n-1}\left(\sum_{i=1}^n\upsilon_i\upsilon_i^T- n\bar{\upsilon}\bar{\upsilon}^T\right).
\end{split}
\end{align}
Mit $1_{n}1_{n}^T = 1_{nn}$ ergibt sich dann weiterhin
\begin{align}
\begin{split}
\Upsilon\left(I_n - \frac{1}{n}1_{nn}\right)\Upsilon^T
& = \left(\Upsilon I_n - \frac{1}{n}\Upsilon 1_{nn}\right)\Upsilon^T                                         \\
& = \Upsilon\Upsilon^T - \frac{1}{n}\Upsilon 1_{nn}\Upsilon^T                                                      \\
& = \begin{pmatrix} \upsilon_1 & \cdots & \upsilon_n\end{pmatrix} \begin{pmatrix} \upsilon_1^T \\ \vdots \\ \upsilon_n^T\end{pmatrix} - \frac{1}{n}\Upsilon 1_n 1_n^T\Upsilon^T    \\
& = \sum_{i=1}^n\upsilon_i\upsilon_i^T- n\left(\frac{1}{n}\Upsilon 1_n\right)\left(\frac{1}{n}1_n^T\Upsilon^T\right)              \\
& = \sum_{i=1}^n\upsilon_i\upsilon_i^T- n\bar{\upsilon}\bar{\upsilon}^T                                                          \\
& = \frac{1}{n-1}\sum_{i=1}^n (\upsilon_i - \bar{\upsilon})(\upsilon_i - \bar{\upsilon})^T \\
& = C.
\end{split}
\end{align}
Hinsichtlich der Korrelationsmatrix ergibt sich nach Definition und für ein
 beliebiges Indexpaar $i,j$ mit $1 \le i,j \le m$ schließlich, dass
\begin{align}
\begin{split}
R_{{y}_{ij}} 
& = \frac{(C)_{ij}}{\sqrt{ (C)_{ii}}\sqrt{ (C)_{jj}}}             \\
& = \frac{1}{\sqrt{(C)_{ii}}}(C)_{ij}\frac{1}{\sqrt{(C)_{jj}}}    \\
& = (DCD)_{ij}.
\end{split}
\end{align}
:::


```{r, echo = F, message = F}
library(matrixcalc)                                                             # Matrix Paket (is.positive.definite())
library(MASS)                                                                   # Multivariate Normalverteilung (mvrnorm())
set.seed(1)                                                                     # Reproduzierbare Randomisierung
m     = 4                                                                       # Datenpunktdimension
n     = 30                                                                      # Anzahl Realisierungen
mu    = rep(0,m)                                                                # Erwartungswertparameter
Sigma = matrix(runif(m^2), nrow = m)                                            # zufällige Matrix
Sigma = 0.5*(Sigma+t(Sigma))                                                    # symmetrische Matrix
Sigma = Sigma + m*diag(m)                                                       # positiv definite Matrix
Y     = t(mvrnorm(n,mu,Sigma))                                                  # Datensatzgeneration    
write.csv(Y, "./_data/501-daten-deskription.csv" , row.names = FALSE)           # Speichern    
```
Folgender **R** Code wendet die in @thm-datenmatrix-und-stichprobenstatistiken
diskutierten Resultate auf einen Beispieldatensatz mit Datendimensionalität $m = 4$ 
und Anzahl experimenteller Einheiten $n := 12$ an.

\tiny
```{r}
# Simulation eines Datensatzes
library(MASS)                                                                   # multivariate Normalverteilung
m       = 4                                                                     # Datenvektordimension
n       = 12                                                                    # Anzahl Datenvektoren
mu      = matrix(c(1,2,3,4))                                                    # Erwartungswertparameter
Sigma   = diag(4)                                                               # Kovarianzmatrixparameter
Y       = t(mvrnorm(n, mu,Sigma))                                               # Datensatzrealisierung    
```

```{r, echo = F}
print(Y)
```

```{r}
# Auswertung von Deskriptivstatistiken
n       = ncol(Y)                                                               # Anzahl Datenvektorealisierungen
I_n     = diag(n)                                                               # Einheitsmatrix I_n
J_n     = matrix(rep(1,n^2), nrow = n)                                          # 1_{nn}
y_bar   = (1/n)* Y %*% J_n[,1]                                                  # Stichprobenmittel
C       = (1/(n-1))*(Y %*% (I_n-(1/n)*J_n) %*% t(Y))                            # Stichprobenkovarianzmatrix
D       = diag(1/sqrt(diag(C)))                                                 # Kov-Korr-Transformationsmatrix
R       = D %*% C %*% D                                                         # Stichprobenkorrelationsmatrix
```

```{r echo = F}
# Ausgabe
print(y_bar)
print(C)
print(R)
```
\normalsize



### Mahalanobis-Distanzen {-}

Abschließend wollen wir mit den sogenannten *Mahalanobis-Distanzen* 
multivariate Generalisierungen von aus der univariaten Anwendung bekannten 
Signal-zu-Rauschen-Maßen betrachten. Wir definieren den Begriff der Mahalanobis-Distanz 
wie folgt.

:::{#def-mahalanobis-distanzen}
## Mahalanobis-Distanzen
$\xi_1$ sei ein Zufallsvektor, eine Realisation eines Zufallsvektors,
ein multivariater Erwartungswert oder ein multivariates Stichprobenmittel,
$\xi_2$ sei ein Zufallsvektor, eine Realisation eines Zufallsvektors,
ein multivariater Erwartungswert oder ein multivariates Stichprobenmittel
und $\Xi$ sei eine Kovarianzmatrix oder eine Stichprobenkovarianzmatrix. Dann heißt
\begin{equation}
D = \left(\xi_1 - \xi_2 \right)^T\Xi^{-1}\left(\xi_1 - \xi_2\right)
\end{equation}
\textit{Mahalanobis-Distanz} von $\xi_1$ und $\xi_2$ hinsichtlich $\Xi$.
:::

Eine Mahalanobis-Distanz ist damit eine durch eine Kovarianzmatrix normalisierte 
quadrierte Euklidische Distanz (vgl. @sec-euklidischer-vektorraum). Ähnliche Maße 
für das Verhältnis eines Abstandes und einer Variabilität sind bekanntlich die 
*$z$-Transformation* $z = (y - \mu)/\sigma$ für $y \in \mathbb{R}$ und die Parameter 
$\mu,\sigma^2>0$ einer univariaten Normalverteilung sowie *Cohen's* 
$d = (\bar{\upsilon}_1-\bar{\upsilon}_2)/s_{12}$ für zwei Stichprobenmittel $\bar{\upsilon}_1$ und $\bar{\upsilon}_2$ 
und ihre korrespondierende gepoolte Stichprobenstandardabweichung $s_{12}$. Im Unterschied
zur Mahalanobis-Distanz sind diese Maße allerdings nicht quadriert und damit Vorzeichen
behaftet. In Analogie zur $z$-Transformation oder zu Cohen's $d$ wird allerdings 
auch bei Mahalanobis-Distanzen ein Abstand in Einheiten von Variabilität gemessen.
Bei Cohen's $d$ bedeutet ja ein Wert von $d = 1$ gerade, dass der 
Abstand von $\bar{\upsilon}_1$ und $\bar{\upsilon}_2$ *eine* gepoolte Standardabweichung
beträgt. Ebenso verhält es sich mit den Mahalanobis-Distanzen.

Anhand von @fig-mahalanobis-distanzen-1 und @fig-mahalanobis-distanzen-2 wollen wir
den Einfluss der Varianz und der Kovarianz von Komponenten der $\xi_1$ und $\xi_2$ 
auf ihre Mahalanobis-Distanz noch etwas genauer betrachten. Die Titel der Unterabbildungen 
von @fig-mahalanobis-distanzen-1 zeigen die Mahalanobis-Distanzen der 
Vektoren $\xi_1 := (-1,-1)^T$ und $\xi_2 := (1,1)^T$ bei Kovarianzmatrizen von 
\begin{equation}
\Sigma_1 := \begin{pmatrix} 1.0 & 0.0 \\ 0.0 & 1.0\end{pmatrix}, 
\Sigma_2 := \begin{pmatrix} 0.5 & 0.0 \\ 0.0 & 0.5\end{pmatrix} \mbox{ und } 
\Sigma_3 := \begin{pmatrix} 1.5 & 0.0 \\ 0.0 & 1.5\end{pmatrix},
\end{equation}
die mithilfe von Normalverteilungsisokonturen dargestellt sind. Für $\Sigma_1$ 
entspricht die Mahalanobis-Distanz dabei der quadrierten Euklidischen Distanz von 
$\xi_1$ und $\xi_2$. An der Darstellung zu $\Sigma_2$ erkennt man, dass im Fall 
sphärischer Kovarianzmatrizen eine geringere Komponentenvarianz von $\xi_1$ 
und $\xi_2$ zu einer größeren Mahalanobis-Distanz führt. Umgekehrt erkennt man
an der Darstellung zu $\Sigma_3$, dass im Fall sphärischer Kovarianzmatrizen eine 
höhere Komponentenvarianz von $\xi_1$ und $\xi_2$ in einer kleineren Mahalanobis-Distanz 
resultiert. Intuitiv nähert die Komponentenvarianz die Komponenten also an.

```{r, echo = F, eval = F}
library(ellipse)
library(matlib)
library(latex2exp)
pdf(
file        = "./_figures/501-mahalanobis-distanzen-1.pdf",
width       = 9,
height      = 3)
par(
family      = "sans",
mfcol       = c(1,3),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2,1,0),
xaxs        = "i",
yaxs        = "i",
font.main   = 1,
cex         = 1,
cex.main    = 1)

# Mahalanobis-Distanzszenarien
xi          = matrix(c(-1,-1,1,1), nrow = 2)
Xi          = array(dim = c(2,2,3))
Xi[,,1]     = matrix(c(1.0, 0.0, 0.0,1.0), nrow = 2)
Xi[,,2]     = matrix(c(0.5, 0.0, 0.0,0.5), nrow = 2)
Xi[,,3]     = matrix(c(1.5, 0.0, 0.0,1.5), nrow = 2)
for(i in 1:3){
    plot(
    ellipse(as.matrix(Xi[,,i]), level = 0.40, centre = c(0,0)),
    type = "l",
    col = "Black",
    xlim = c(-2,2),
    ylim = c(-2,2),
    xlab = TeX("$\\xi_{i_1}$"),
    ylab = TeX("$\\xi_{i_2}$"),
    main = sprintf("D = %0.2f ", t(xi[,1] - xi[,2]) %*% inv(Xi[,,i]) %*% (xi[,1] - xi[,2])))
    points(
    xi[1,],
    xi[2,],
    col  = "Black",
    bg   = "Black",
    pch  = 21,
    cex  = 1.2)
}
dev.off()
```

![Mahalanobis-Distanzen als Funktion von Komponentenvarianzen](./_figures/501-mahalanobis-distanzen-1){#fig-mahalanobis-distanzen-1 fig-align="center" width=100%}

Die Titel der Unterabbildungen von @fig-mahalanobis-distanzen-2 zeigen die 
Mahalanobis-Distanzen derselben Vektoren bei Kovarianzmatrizen von 
\begin{equation}
\Sigma_1 := \begin{pmatrix} 1.0 & 0.0       \\ 0.0 & 1.0\end{pmatrix},
\Sigma_2 := \begin{pmatrix} 1.0 & 0.9       \\ 0.9 & 1.0\end{pmatrix}\mbox{ und }
\Sigma_3 := \begin{pmatrix*}[r] 1.0 & -0.9  \\ -0.9 & 1.0\end{pmatrix*},
\end{equation}
Für $\Sigma_1$ entspricht dabei wiederrum die die Mahalanobis-Distanz der 
quadrierten Euklidischen Distanz von $\xi_1$ und $\xi_2$. An der Darstellung zu 
$\Sigma_2$ erkennt man, dass eine stark positive Kovarianz der Komponenten von
$\xi_1$ und $\xi_2$ in einer kleineren Mahalanobis-Distanz resultiert. Umgekehrt 
erkennt man an der Darstellung zu $\Sigma_3$, dass eine stark negative Kovarianz
der Komponenten von $\xi_1$ und $\xi_2$ zu einer größeren Mahalanobis-Distanz 
führt. Intuitiv nähert also auch die Kovarianz von Komponenten $\xi_1$ und 
$\xi_2$ an. Alternativ kann man die Höhe einer Mahalanobis-Distanz dabei auch 
als ein Maß für die Unwahrscheinlichkeit der Realisierung zweier Werte eines 
Zufallsvektors bei einer gegebenen Kovarianzmatrix verstehen.

```{r, echo = F, eval = F}
library(ellipse)
library(matlib)
library(latex2exp)
pdf(
file   = "./_figures/501-mahalanobis-distanzen-2.pdf",
width  = 9,
height = 3)
par(
family      = "sans",
mfcol       = c(1,3),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2,1,0),
xaxs        = "i",
yaxs        = "i",
font.main   = 1,
cex         = 1,
cex.main    = 1)

# Mahalanobis-Distanzszenarien
xi          = matrix(c(-1,-1,1,1), nrow = 2)
Xi          = array(dim = c(2,2,3))
Xi[,,1]     = matrix(c(1.0, 0.0, 0.0,1.0), nrow = 2)
Xi[,,2]     = matrix(c(1.0, 0.9, 0.9,1.0), nrow = 2)
Xi[,,3]     = matrix(c(1.0,-0.9,-0.9,1.0), nrow = 2)
for(i in 1:3){
    plot(
    ellipse(as.matrix(Xi[,,i]), level = 0.40, centre = c(0,0)),
    type = "l",
    col = "Black",
    xlim = c(-2,2),
    ylim = c(-2,2),
    xlab = TeX("$\\xi_{i_1}$"),
    ylab = TeX("$\\xi_{i_2}$"),
    main = sprintf("D = %0.2f ", t(xi[,1] - xi[,2]) %*% inv(Xi[,,i]) %*% (xi[,1] - xi[,2])))
    points(
    xi[1,],
    xi[2,],
    col  = "Black",
    bg   = "Black",
    pch  = 21,
    cex  = 1.2)
}
dev.off()
```

![Mahalanobis-Distanzen als Funktion von Komponentenkovarianzen](./_figures/501-mahalanobis-distanzen-2){#fig-mahalanobis-distanzen-2 fig-align="center" width=100%}

## Literaturhinweise

Die Resultate zur Matrixdarstellung von Stichprobenmittel, Stichprobenkovarianzmatrix
und Stichprobenkorrelationsmatrix folgen @rencher2012. Der Begriff der Mahalanobis-Distanz
geht zurück auf @mahalanobis1936.

## Selbstkontrollfragen 

\footnotesize
1. Erläutern Sie vier prinzipielle Datenanalyseszenarien anhand der Dimensionalität ihrer unabhängigen und abhängigen Variablen. Nennen Sie Beispiele für die in dem jeweiligen Szenario
häufig eingesetzten Datenanalyseverfahren.
1. Geben Sie die Definition des Stichprobenmittels wieder.
1. Geben Sie die Definition der Stichprobenkovarianzmatrix wieder.
1. Geben Sie die Definition des Stichprobenkorrelationsmatrix wieder.
1. Geben Sie das Theorem zu Datenmatrix und Stichprobenstatistiken wieder.
1. Geben Sie die Definition einer Mahalanobis-Distanz wieder.
1. Erläutern Sie die intuitive Bedeutung einer Mahalanobis-Distanz.
